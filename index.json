[{"categories":["iam"],"content":"服务部署 ","date":"2022-07-07 15:20:34","objectID":"/iam_service_deployment/:0:0","tags":["iam"],"title":"iam_service_deployment","uri":"/iam_service_deployment/"},{"categories":["iam"],"content":"40 | 软件部署实战（上）：部署方案及负载均衡、高可用组件介绍 接下来，我们就进入到这门课的最后一个模块，服务部署部分的学习。在这一模块中，我会带着你一步一步地部署一个生产级可用的 IAM 应用。在 03 讲 中，我们快速在单机上部署了 IAM 系统，但这样的系统缺少高可用、弹性扩容等能力，是很脆弱的，遇到流量波峰、发布变更很容易出问题。在系统真正上线前，我们需要重新调整部署架构，来保证我们的系统具有负载均衡、高可用、弹性伸缩等核心运维能力。考虑到你手中的系统资源有限，这一模块会尽量简单地展示如何部署一个相对高可用的 IAM 系统。按照我讲的部署方法，基本上可以上线一个中小型的系统。 在这一模块中，我会介绍两种部署方式。第一种是传统的部署方式，基于物理机 / 虚拟机来部署，容灾、弹性伸缩能力要部署人员自己实现。第二种是容器化部署方式，基于 Docker、Kubernetes 来部署，容灾、弹性伸缩等能力，可以借助 Kubernetes 自带的能力来实现。接下来的三讲，我们先来看下传统的部署方式，也就是如何基于虚拟机来部署 IAM 应用。今天我主要讲跟 IAM 部署相关的两个组件，Nginx + Keepalived 的相关功能。 接下来的三讲，我们先来看下传统的部署方式，也就是如何基于虚拟机来部署 IAM 应用。今天我主要讲跟 IAM 部署相关的两个组件，Nginx + Keepalived 的相关功能。 ","date":"2022-07-07 15:20:34","objectID":"/iam_service_deployment/:1:0","tags":["iam"],"title":"iam_service_deployment","uri":"/iam_service_deployment/"},{"categories":["iam"],"content":"部署方案 先来整体看下我们的部署方案。这里，我采用 Nginx + Keepalived 来部署一个高可用的架构，同时将组件都部署在内网，来保证服务的安全和性能。部署需要两台物理机 / 虚拟机，组件之间通过内网访问。所需的服务器如下表所示： 两台服务器均为腾讯云 CVM，VIP（Virtual IP，虚拟 IP）为10.0.4.99。部署架构如下图所示： 这里我来具体介绍下图中的部署架构。部署采用的这两台 CVM 服务器，一主一备，它们共享同一个 VIP。同一时刻，VIP 只在一台主设备上生效，当主服务器出现故障时，备用服务器会自动接管 VIP，继续提供服务。 主服务器上部署了iam-apiserver、iam-authz-server、iam-pump和数据库mongodb、redis、mysql。备服务器部署了iam-apiserver、iam-authz-server和iam-pump。备服务器中的组件通过内网10.0.4.20访问主服务器中的数据库组件。 主备服务器同时安装了 Keepalived 和 Nginx，通过 Nginx 的反向代理功能和负载均衡功能，实现后端服务iam-apiserver和iam-authz-server的高可用，通过 Keepalived 实现 Nginx 的高可用。 我们通过给虚拟 IP 绑定腾讯云弹性公网 IP，从而使客户端可以通过外网 IP 访问内网的 Nginx 服务器（443端口），如果想通过域名访问内网，还可以申请域名指向该弹性公网 IP。 通过以上部署方案，我们可以实现一个具有较高可用性的 IAM 系统，它主要具备下面这几个能力。 高性能：可以通过 Nginx 的负载均衡功能，水平扩容 IAM 服务，从而实现高性能。 具备容灾能力：通过 Nginx 实现 IAM 服务的高可用，通过 Keepalived 实现 Nginx 的高可用，从而实现核心组件的高可用。 具备水平扩容能力：通过 Nginx 的负载均衡功能，实现 IAM 服务的水平扩容。 高安全性：将所有组件部署在内网，客户端只能通过VIP:443端口访问 Nginx 服务，并且通过开启 TLS 认证和 JWT 认证，保障服务有一个比较高的安全性。因为是腾讯云 CVM，所以也可以借助腾讯云的能力再次提高服务器的安全性，比如安全组、DDoS 防护、主机安全防护、云监控、云防火墙等。 这里说明下，为了简化 IAM 应用的安装配置过程，方便你上手实操，有些能力，例如数据库高可用、进程监控和告警、自动伸缩等能力的构建方式，这里没有涉及到。这些能力的构建方式，你可以在日后的工作中慢慢学习和掌握。 接下来，我们看下这个部署方案中用到的两个核心组件，Nginx 和 Keepalived。我会介绍下它们的安装和配置方法，为你下一讲的学习做准备。 ","date":"2022-07-07 15:20:34","objectID":"/iam_service_deployment/:1:1","tags":["iam"],"title":"iam_service_deployment","uri":"/iam_service_deployment/"},{"categories":["iam"],"content":"Nginx 安装和配置 Nginx 功能简介 这里先简单介绍下 Nginx。Nginx 是一个轻量级、高性能、开源的 HTTP 服务器和反向代理服务器。IAM 系统使用了 Nginx 反向代理和负载均衡的功能，下面我就来分别介绍下。 为什么需要反向代理呢？在实际的生产环境中，服务部署的网络（内网）跟外部网络（外网）通常是不通的，这就需要一台既能够访问内网又能够访问外网的服务器来做中转，这种服务器就是反向代理服务器。Nginx 作为反向代理服务器，简单的配置如下： server { listen 80; server_name iam.marmotedu.com; client_max_body_size 1024M; location / { proxy_set_header Host $http_host; proxy_set_header X-Forwarded-Host $http_host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_pass http://127.0.0.1:8080/; client_max_body_size 100m; } } Nginx 的反向代理功能，能够根据不同的配置规则转发到不同的后端服务器上。假如我们在 IP 为x.x.x.x的服务器上，用上面说的 Nginx 配置启动 Nginx，当我们访问http://x.x.x.x:80/时，会将请求转发到http://127.0.0.1:8080/。listen 80指定了 Nginx 服务器的监听端口，proxy_pass http://127.0.0.1:8080/则指定了转发路径。 Nginx 另一个常用的功能是七层负载均衡。所谓的负载均衡，就是指当 Nginx 收到一个 HTTP 请求后，会根据负载策略将请求转发到不同的后端服务器上。比如 iam-apiserver 部署在两台服务器 A 和 B 上，当请求到达 Nginx 后，Nginx 会根据 A 和 B 服务器上的负载情况，将请求转发到负载较小的那台服务器上。 这里要求 iam-apiserver 是无状态的服务。Nginx 有多种负载均衡策略，可以满足不同场景下的负载均衡需求。 Nginx 安装步骤 接下来，我就来介绍下如何安装和配置 Nginx。我们分别在10.0.4.20和10.0.4.21服务器上执行如下步骤，安装 Nginx。 在 CentOS 8.x 系统上，我们可以使用 yum 命令来安装，具体安装过程可以分为下面 4 个步骤。第一步，安装 Nginx： $ sudo yum -y install nginx 第二步，确认 Nginx 安装成功： $ nginx -v nginx version: nginx/1.14.1 第三步，启动 Nginx，并设置开机启动： $ sudo systemctl start nginx $ sudo systemctl enable nginx Nginx 默认监听80端口，启动 Nginx 前要确保80端口没有被占用。当然，你也可以通过修改 Nginx 配置文件/etc/nginx/nginx.conf修改 Nginx 监听端口。 第四步，查看 Nginx 启动状态： $ systemctl status nginx 输出中有active (running)字符串，说明成功启动。如果 Nginx 启动失败，你可以查看/var/log/nginx/error.log日志文件，定位错误原因。 ","date":"2022-07-07 15:20:34","objectID":"/iam_service_deployment/:1:2","tags":["iam"],"title":"iam_service_deployment","uri":"/iam_service_deployment/"},{"categories":["iam"],"content":"Keepalived 安装和配置 Nginx 自带负载均衡功能，并且当 Nginx 后端某个服务器故障后，Nginx 会自动剔除该服务器，将请求转发到可用的服务器，通过这种方式实现后端 API 服务的高可用。但是 Nginx 是单点的，如果 Nginx 挂了，后端的所有服务器就都不能访问，所以在实际生产环境中，也需要对 Nginx 做高可用。 业界最普遍采用的方法是通过 Keepalived 对前端 Nginx 实现高可用。Keepalived + Nginx 的高可用方案具有服务功能强大、维护简单等特点。接下来，我们来看下如何安装和配置 Keepalived。 Keepalived 安装步骤 我们分别在10.0.4.20和10.0.4.21服务器上执行下面 5 个步骤，安装 Keepalived。第一步，下载 Keepalived 的最新版本（这门课安装了当前的最新版本 2.1.5）： $ wget https://www.keepalived.org/software/keepalived-2.1.5.tar.gz 第二步，安装 Keepalived： $ sudo yum -y install openssl-devel # keepalived依赖OpenSSL，先安装依赖 $ tar -xvzf keepalived-2.1.5.tar.gz $ cd keepalived-2.1.5 $ ./configure --prefix=/usr/local/keepalived $ make $ sudo make install 第三步，配置 Keepalived： $ sudo mkdir /etc/keepalived # 安装后，默认没有创建/etc/keepalived目录 $ sudo cp /usr/local/keepalived/etc/keepalived/keepalived.conf /etc/keepalived/keepalived.conf $ sudo cp /usr/local/keepalived/etc/sysconfig/keepalived /etc/sysconfig/keepalived Keepalived 的 systemd uint 配置，默认使用了/usr/local/keepalived/etc/sysconfig/keepalived作为其EnvironmentFile，我们还需要把它修改为/etc/sysconfig/keepalived文件。编辑/lib/systemd/system/keepalived.service文件，设置EnvironmentFile，值如下： EnvironmentFile=-/etc/sysconfig/keepalived 第四步，启动 Keepalived，并设置开机启动： $ sudo systemctl start keepalived $ sudo systemctl enable keepalived 这里要注意，Keepalived 启动时不会校验配置文件是否正确，所以我们要小心修改配置，防止出现意想不到的问题。第五步，查看 Keepalived 的启动状态： $ systemctl status keepalived 输出中有active (running)字符串，说明成功启动。Keepalived 的日志保存在/var/log/messages中，你有需要的话可以查看。 Keepalived 配置文件解析 Keepalived 的默认配置文件为/etc/keepalived/keepalived.conf，下面是一个 Keepalived 配置： # 全局定义，定义全局的配置选项 global_defs { # 指定keepalived在发生切换操作时发送email，发送给哪些email # 建议在keepalived_notify.sh中发送邮件 notification_email { acassen@firewall.loc } notification_email_from Alexandre.Cassen@firewall.loc # 发送email时邮件源地址 smtp_server 192.168.200.1 # 发送email时smtp服务器地址 smtp_connect_timeout 30 # 连接smtp的超时时间 router_id VM-4-21-centos # 机器标识，通常可以设置为hostname vrrp_skip_check_adv_addr # 如果接收到的报文和上一个报文来自同一个路由器，则不执行检查。默认是跳过检查 vrrp_garp_interval 0 # 单位秒，在一个网卡上每组gratuitous arp消息之间的延迟时间，默认为0 vrrp_gna_interval 0 # 单位秒，在一个网卡上每组na消息之间的延迟时间，默认为0 } # 检测脚本配置 vrrp_script checkhaproxy { script \"/etc/keepalived/check_nginx.sh\" # 检测脚本路径 interval 5 # 检测时间间隔（秒） weight 0 # 根据该权重改变priority，当值为0时，不改变实例的优先级 } # VRRP实例配置 vrrp_instance VI_1 { state BACKUP # 设置初始状态为'备份' interface eth0 # 设置绑定VIP的网卡，例如eth0 virtual_router_id 51 # 配置集群VRID，互为主备的VRID需要是相同的值 nopreempt # 设置非抢占模式，只能设置在state为backup的节点上 priority 50 # 设置优先级，值范围0～254，值越大优先级越高，最高的为master advert_int 1 # 组播信息发送时间间隔，两个节点必须设置一样，默认为1秒 # 验证信息，两个节点必须一致 authentication { auth_type PASS # 认证方式，可以是PASS或AH两种认证方式 auth_pass 1111 # 认证密码 } unicast_src_ip 10.0.4.21 # 设置本机内网IP地址 unicast_peer { 10.0.4.20 # 对端设备的IP地址 } # VIP，当state为master时添加，当state为backup时删除 virtual_ipaddress { 10.0.4.99 # 设置高可用虚拟VIP，如果是腾讯云的CVM，需要填写控制台申请到的HAVIP地址。 } notify_master \"/etc/keepalived/keepalived_notify.sh MASTER\" # 当切换到master状态时执行脚本 notify_backup \"/etc/keepalived/keepalived_notify.sh BACKUP\" # 当切换到backup状态时执行脚本 notify_fault \"/etc/keepalived/keepalived_notify.sh FAULT\" # 当切换到fault状态时执行脚本 notify_stop \"/etc/keepalived/keepalived_notify.sh STOP\" # 当切换到stop状态时执行脚本 garp_master_delay 1 # 设置当切为主状态后多久更新ARP缓存 garp_master_refresh 5 # 设置主节点发送ARP报文的时间间隔 # 跟踪接口，里面任意一块网卡出现问题，都会进入故障(FAULT)状态 track_interface { eth0 } # 要执行的检查脚本 track_script { checkhaproxy } } 这里解析下配置文件，大致分为下面 4 个部分。 global_defs：全局定义，定义全局的配置选项。 vrrp_script checkhaproxy：检测脚本配置。 vrrp_instance VI_1：VRRP 实例配置。 virtual_server：LVS 配置。如果没有配置 LVS+Keepalived，就不用设置这个选项。这门课中，我们使用 Nginx 代替 LVS，所以无需配置virtual_server（配置示例中不再展示）。 只有在网络故障或者自身出问题时，Keepalived 才会进行 VIP 切换。但实际生产环境中，我们往往使用 Keepalived 来监控其他进程，当业务进程出故障时切换 VIP，从而保障业务进程的高可用。 为了让 Keepalived 感知到 Nginx 的运行状况，我们需要指定vrrp_script脚本，vrrp_script脚本可以根据退出码，判断 Nginx 进程是否正常，0正常，非0不正常。当不正常时，Keepalived 会进行 VIP 切换。为了实现业务进程的监控，我们需要设置vrrp_script和track_script： vrrp_script checkhaproxy { script \"/etc/keepalived/check_nginx.sh\" interval 3 weight -20 } vrrp_instance test { ... track_script { checkhaproxy } ... } 这里，我介绍下上面配置中的一些配置项。 script：指定脚本路径。 interva","date":"2022-07-07 15:20:34","objectID":"/iam_service_deployment/:1:3","tags":["iam"],"title":"iam_service_deployment","uri":"/iam_service_deployment/"},{"categories":["iam"],"content":"总结 今天我主要讲了跟 IAM 部署相关的两个组件，Nginx + Keepalived 的相关功能。我们可以基于物理机 / 虚拟机来部署 IAM 应用，在部署 IAM 应用时，需要确保整个应用具备高可用和弹性扩缩容能力。你可以通过 Nginx 的反向代理功能和负载均衡功能实现后端服务 iam-apiserver 和 iam-authz-server 的高可用，通过 Keepalived 实现 Nginx 的高可用，通过 Nginx + Keepalived 组合，来实现 IAM 应用的高可用和弹性伸缩能力。 ","date":"2022-07-07 15:20:34","objectID":"/iam_service_deployment/:1:4","tags":["iam"],"title":"iam_service_deployment","uri":"/iam_service_deployment/"},{"categories":["iam"],"content":"41 | 软件部署实战（中）：IAM 系统生产环境部署实战 上一讲，我介绍了 IAM 部署用到的两个核心组件，Nginx 和 Keepalived。那么这一讲，我们就来看下，如何使用 Nginx 和 Keepalived 来部署一个高可用的 IAM 应用。下一讲，我再介绍下 IAM 应用安全和弹性伸缩能力的构建方式。这一讲，我们会通过下面四个步骤来部署 IAM 应用： 在服务器上部署 IAM 应用中的服务。 配置 Nginx，实现反向代理功能。通过反向代理，我们可以通过 Nginx 来访问部署在内网的 IAM 服务。 配置 Nginx，实现负载均衡功能。通过负载均衡，我们可以实现服务的水平扩缩容，使 IAM 应用具备高可用能力。 配置 Keepalived，实现 Nginx 的高可用。通过 Nginx + Keepalived 的组合，可以实现整个应用架构的高可用。 ","date":"2022-07-07 15:20:34","objectID":"/iam_service_deployment/:2:0","tags":["iam"],"title":"iam_service_deployment","uri":"/iam_service_deployment/"},{"categories":["iam"],"content":"部署 IAM 应用 部署一个高可用的 IAM 应用，需要至少两个节点。所以，我们按照先后顺序，分别在10.0.4.20和10.0.4.21服务器上部署 IAM 应用。 在10.0.4.20服务器上部署 IAM 应用 首先，我来介绍下如何在10.0.4.20服务器上部署 IAM 应用。我们要在这个服务器上部署如下组件： iam-apiserver iam-authz-server iam-pump MariaDB Redis MongoDB 这些组件的部署方式，03 讲 有介绍，这里就不再说明。此外，我们还需要设置 MariaDB，给来自于10.0.4.21服务器的数据库连接授权，授权命令如下： $ mysql -hlocalhost -P3306 -uroot -proot # 先以root用户登陆数据库 MariaDB [(none)]\u003e grant all on iam.* TO iam@10.0.4.21 identified by 'iam1234'; Query OK, 0 rows affected (0.000 sec) MariaDB [(none)]\u003e flush privileges; Query OK, 0 rows affected (0.000 sec) 在10.0.4.21服务器上部署 IAM 应用 然后，在10.0.4.21服务器上安装好 iam-apiserver、iam-authz-server 和 iam-pump。这些组件通过10.0.4.20 IP 地址，连接10.0.4.20服务器上的 MariaDB、Redis 和 MongoDB。 ","date":"2022-07-07 15:20:34","objectID":"/iam_service_deployment/:2:1","tags":["iam"],"title":"iam_service_deployment","uri":"/iam_service_deployment/"},{"categories":["iam"],"content":"配置 Nginx 作为反向代理 假定要访问的 API Server 和 IAM Authorization Server 的域名分别为iam.api.marmotedu.com和iam.authz.marmotedu.com，我们需要分别为 iam-apiserver 和 iam-authz-server 配置 Nginx 反向代理。整个配置过程可以分为 5 步（在10.0.4.20服务器上操作）。 第一步，配置 iam-apiserver。 新建 Nginx 配置文件/etc/nginx/conf.d/iam-apiserver.conf，内容如下： server { listen 80; server_name iam.api.marmotedu.com; root /usr/share/nginx/html; location / { proxy_set_header X-Forwarded-Host $http_host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_pass http://127.0.0.1:8080/; client_max_body_size 5m; } error_page 404 /404.html; location = /40x.html { } error_page 500 502 503 504 /50x.html; location = /50x.html { } } 有几点你在配置时需要注意，这里说明下。 server_name需要为iam.api.marmotedu.com，我们通过iam.api.marmotedu.com访问 iam-apiserver。 iam-apiserver 默认启动的端口为8080。 由于 Nginx 默认允许客户端请求的最大单文件字节数为1MB，实际生产环境中可能太小，所以这里将此限制改为 5MB（client_max_body_size 5m）。如果需要上传图片之类的，可能需要设置成更大的值，比如50m。 server_name 用来说明访问 Nginx 服务器的域名，例如curl -H ‘Host: iam.api.marmotedu.com’ http://x.x.x.x:80/healthz，x.x.x.x为 Nginx 服务器的 IP 地址。 proxy_pass 表示反向代理的路径。因为这里是本机的 iam-apiserver 服务，所以 IP 为127.0.0.1。端口要和 API 服务端口一致，为8080。 最后还要提醒下，因为 Nginx 配置选项比较多，跟实际需求和环境有关，所以这里的配置是基础的、未经优化的配置，在实际生产环境中需要你再做调节。 第二步，配置 iam-authz-server。 新建 Nginx 配置文件/etc/nginx/conf.d/iam-authz-server.conf，内容如下： server { listen 80; server_name iam.authz.marmotedu.com; root /usr/share/nginx/html; location / { proxy_set_header X-Forwarded-Host $http_host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_pass http://127.0.0.1:9090/; client_max_body_size 5m; } error_page 404 /404.html; location = /40x.html { } error_page 500 502 503 504 /50x.html; location = /50x.html { } } 下面是一些配置说明。 server_name 需要为iam.authz.marmotedu.com，我们通过iam.authz.marmotedu.com访问 iam-authz-server。 iam-authz-server 默认启动的端口为9090。 其他配置跟/etc/nginx/conf.d/iam-apiserver.conf一致。 第三步，配置完 Nginx 后，重启 Nginx： $ sudo systemctl restart nginx 第四步，在 /etc/hosts 中追加下面两行： 127.0.0.1 iam.api.marmotedu.com 127.0.0.1 iam.authz.marmotedu.com 第五步，发送 HTTP 请求： $ curl http://iam.api.marmotedu.com/healthz {\"status\":\"ok\"} $ curl http://iam.authz.marmotedu.com/healthz {\"status\":\"ok\"} 我们分别请求 iam-apiserver 和 iam-authz-server 的健康检查接口，输出了{“status”:“ok”}，说明我们可以成功通过代理访问后端的 API 服务。 在用 curl 请求http://iam.api.marmotedu.com/healthz后，后端的请求流程实际上是这样的： 因为在/etc/hosts中配置了127.0.0.1 iam.api.marmotedu.com，所以请求http://iam.api.marmotedu.com/healthz实际上是请求本机的 Nginx 端口（127.0.0.1:80）。 Nginx 在收到请求后，会解析请求，得到请求域名为iam.api.marmotedu.com。根据请求域名去匹配 Nginx 的 server 配置，匹配到server_name iam.api.marmotedu.com;配置。 匹配到 server 后，把请求转发到该 server 的proxy_pass路径。 等待 API 服务器返回结果，并返回客户端。 ","date":"2022-07-07 15:20:34","objectID":"/iam_service_deployment/:2:2","tags":["iam"],"title":"iam_service_deployment","uri":"/iam_service_deployment/"},{"categories":["iam"],"content":"配置 Nginx 作为负载均衡 这门课采用 Nginx 轮询的负载均衡策略转发请求。负载均衡需要至少两台服务器，所以会分别在10.0.4.20和10.0.4.21服务器上执行相同的操作。下面我分别来介绍下如何配置这两台服务器，并验证配置是否成功。 10.0.4.20服务器配置 登陆10.0.4.20服务器，在/etc/nginx/nginx.conf中添加 upstream 配置，配置过程可以分为 3 步。 第一步，在/etc/nginx/nginx.conf中添加 upstream： http { log_format main '$remote_addr - $remote_user [$time_local] \"$request\" ' '$status $body_bytes_sent \"$http_referer\" ' '\"$http_user_agent\" \"$http_x_forwarded_for\"'; access_log /var/log/nginx/access.log main; sendfile on; tcp_nopush on; tcp_nodelay on; keepalive_timeout 65; types_hash_max_size 2048; include /etc/nginx/mime.types; default_type application/octet-stream; # Load modular configuration files from the /etc/nginx/conf.d directory. # See http://nginx.org/en/docs/ngx_core_module.html#include # for more information. include /etc/nginx/conf.d/*.conf; upstream iam.api.marmotedu.com { server 127.0.0.1:8080 server 10.0.4.21:8080 } upstream iam.authz.marmotedu.com { server 127.0.0.1:9090 server 10.0.4.21:9090 } } 配置说明： upstream 是配置在/etc/nginx/nginx.conf文件中的http{ … }部分的。 因为我们要分别为 iam-apiserver 和 iam-authz-server 配置负载均衡，所以我们创建了两个 upstream，分别是iam.api.marmotedu.com和iam.authz.marmotedu.com。为了便于识别，upstream 名称和域名最好保持一致。 在 upstream 中，我们需要分别添加所有的 iam-apiserver 和 iam-authz-server 的后端（ip:port），本机的后端为了访问更快，可以使用127.0.0.1:，其他机器的后端，需要使用\u003c内网\u003e:port，例如10.0.4.21:8080、10.0.4.21:9090。 第二步，修改 proxy_pass。修改/etc/nginx/conf.d/iam-apiserver.conf文件，将proxy_pass修改为： proxy_pass http://iam.api.marmotedu.com/; 修改/etc/nginx/conf.d/iam-authz-server.conf文件，将proxy_pass修改为： proxy_pass http://iam.authz.marmotedu.com/; 当 Nginx 转发到http://iam.api.marmotedu.com/域名时，会从iam.api.marmotedu.com upstream 配置的后端列表中，根据负载均衡策略选取一个后端，并将请求转发过去。转发http://iam.authz.marmotedu.com/域名的逻辑也一样。 第三步，配置完 Nginx 后，重启 Nginx： $ sudo systemctl restart nginx 最终配置好的配置文件，你可以参考下面这些（保存在configs/ha/10.0.4.20目录下）： nginx.conf：configs/ha/10.0.4.20/nginx.conf。 iam-apiserver.conf：configs/ha/10.0.4.20/iam-apiserver.conf。 iam-authz-server.conf：configs/ha/10.0.4.20/iam-authz-server.conf。 10.0.4.21服务器配置 登陆10.0.4.21服务器，在/etc/nginx/nginx.conf中添加 upstream 配置。配置过程可以分为下面 4 步。 第一步，在/etc/nginx/nginx.conf中添加 upstream： http { log_format main '$remote_addr - $remote_user [$time_local] \"$request\" ' '$status $body_bytes_sent \"$http_referer\" ' '\"$http_user_agent\" \"$http_x_forwarded_for\"'; access_log /var/log/nginx/access.log main; sendfile on; tcp_nopush on; tcp_nodelay on; keepalive_timeout 65; types_hash_max_size 2048; include /etc/nginx/mime.types; default_type application/octet-stream; # Load modular configuration files from the /etc/nginx/conf.d directory. # See http://nginx.org/en/docs/ngx_core_module.html#include # for more information. include /etc/nginx/conf.d/*.conf; upstream iam.api.marmotedu.com { server 127.0.0.1:8080 server 10.0.4.20:8080 } upstream iam.authz.marmotedu.com { server 127.0.0.1:9090 server 10.0.4.20:9090 } } upstream 中，需要配置10.0.4.20服务器上的 iam-apiserver 和 iam-authz-server 的后端，例如10.0.4.20:8080、10.0.4.20:9090。 第二步，创建/etc/nginx/conf.d/iam-apiserver.conf文件（iam-apiserver 的反向代理 + 负载均衡配置），内容如下： server { listen 80; server_name iam.api.marmotedu.com; root /usr/share/nginx/html; location / { proxy_set_header X-Forwarded-Host $http_host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_pass http://iam.api.marmotedu.com/; client_max_body_size 5m; } error_page 404 /404.html; location = /40x.html { } error_page 500 502 503 504 /50x.html; location = /50x.html { } } 第三步，创建/etc/nginx/conf.d/iam-authz-server文件（iam-authz-server 的反向代理 + 负载均衡配置），内容如下： server { listen 80; server_name iam.authz.marmotedu.com; root /usr/share/nginx/html; location / { proxy_set_header X-Forwarded-Host $http_host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_pass http://iam.authz.marmotedu.com/; client_max_body_size 5m; } error_page 404 /404.html; location = /40x.html { } error_page 500 502 503 504 /50x.html; location = /50x.html { } } 第四步，配置完 Nginx 后，重启 Nginx： $ sudo systemctl ","date":"2022-07-07 15:20:34","objectID":"/iam_service_deployment/:2:3","tags":["iam"],"title":"iam_service_deployment","uri":"/iam_service_deployment/"},{"categories":["iam"],"content":"配置 Keepalived 在 40 讲，我们分别在10.0.4.20和10.0.4.21服务器上安装了 Keepalived。这里，我来介绍下如何配置 Keepalived，实现 Nginx 的高可用。为了避免故障恢复时，VIP 切换造成的服务延时，这一讲采用 Keepalived 的非抢占模式。 配置 Keepalived 的流程比较复杂，分为创建腾讯云 HAVIP、主服务器配置、备服务器配置、测试 Keepalived、VIP 绑定公网 IP 和测试公网访问六大步，每一步中都有很多小步骤，下面我们来一步步地看下。 第一步：创建腾讯云 HAVIP 公有云厂商的普通内网 IP，出于安全考虑（如避免 ARP 欺骗等），不支持主机通过 ARP 宣告 IP 。如果用户直接在keepalived.conf文件中指定一个普通内网 IP 为 virtual IP，当 Keepalived 将 virtual IP 从 MASTER 机器切换到 BACKUP 机器时，将无法更新 IP 和 MAC 地址的映射，而需要调 API 来进行 IP 切换。所以，这里的 VIP 需要申请腾讯云的 HAVIP。 申请的流程可以分为下面 4 步： 登录私有网络控制台。 在左侧导航栏中，选择【IP 与网卡】\u003e【高可用虚拟 IP】。 在 HAVIP 管理页面，选择所在地域，单击【申请】。 在弹出的【申请高可用虚拟 IP】对话框中输入名称，选择 HAVIP 所在的私有网络和子网等信息，单击【确定】即可。 这里选择的私有网络和子网，需要和10.0.4.20、10.0.4.21相同。HAVIP 的 IP 地址可以自动分配，也可以手动填写，这里我们手动填写为 10.0.4.99。申请页面如下图所示： 第二步：主服务器配置 进行主服务器配置，可以分为两步。 首先，修改 Keepalived 配置文件。登陆服务器10.0.4.20，编辑/etc/keepalived/keepalived.conf，修改配置，修改后配置内容如下（参考：configs/ha/10.0.4.20/keepalived.conf）： # 全局定义，定义全局的配置选项 global_defs { # 指定keepalived在发生切换操作时发送email，发送给哪些email # 建议在keepalived_notify.sh中发送邮件 notification_email { acassen@firewall.loc } notification_email_from Alexandre.Cassen@firewall.loc # 发送email时邮件源地址 smtp_server 192.168.200.1 # 发送email时smtp服务器地址 smtp_connect_timeout 30 # 连接smtp的超时时间 router_id VM-4-20-centos # 机器标识，通常可以设置为hostname vrrp_skip_check_adv_addr # 如果接收到的报文和上一个报文来自同一个路由器，则不执行检查。默认是跳过检查 vrrp_garp_interval 0 # 单位秒，在一个网卡上每组gratuitous arp消息之间的延迟时间，默认为0 vrrp_gna_interval 0 # 单位秒，在一个网卡上每组na消息之间的延迟时间，默认为0 } # 检测脚本配置 vrrp_script checkhaproxy { script \"/etc/keepalived/check_nginx.sh\" # 检测脚本路径 interval 5 # 检测时间间隔（秒） weight 0 # 根据该权重改变priority，当值为0时，不改变实例的优先级 } # VRRP实例配置 vrrp_instance VI_1 { state BACKUP # 设置初始状态为'备份' interface eth0 # 设置绑定VIP的网卡，例如eth0 virtual_router_id 51 # 配置集群VRID，互为主备的VRID需要是相同的值 nopreempt # 设置非抢占模式，只能设置在state为backup的节点上 priority 100 # 设置优先级，值范围0～254，值越大优先级越高，最高的为master advert_int 1 # 组播信息发送时间间隔，两个节点必须设置一样，默认为1秒 # 验证信息，两个节点必须一致 authentication { auth_type PASS # 认证方式，可以是PASS或AH两种认证方式 auth_pass 1111 # 认证密码 } unicast_src_ip 10.0.4.20 # 设置本机内网IP地址 unicast_peer { 10.0.4.21 # 对端设备的IP地址 } # VIP，当state为master时添加，当state为backup时删除 virtual_ipaddress { 10.0.4.99 # 设置高可用虚拟VIP，如果是腾讯云的CVM，需要填写控制台申请到的HAVIP地址。 } notify_master \"/etc/keepalived/keepalived_notify.sh MASTER\" # 当切换到master状态时执行脚本 notify_backup \"/etc/keepalived/keepalived_notify.sh BACKUP\" # 当切换到backup状态时执行脚本 notify_fault \"/etc/keepalived/keepalived_notify.sh FAULT\" # 当切换到fault状态时执行脚本 notify_stop \"/etc/keepalived/keepalived_notify.sh STOP\" # 当切换到stop状态时执行脚本 garp_master_delay 1 # 设置当切为主状态后多久更新ARP缓存 garp_master_refresh 5 # 设置主节点发送ARP报文的时间间隔 # 跟踪接口，里面任意一块网卡出现问题，都会进入故障(FAULT)状态 track_interface { eth0 } # 要执行的检查脚本 track_script { checkhaproxy } } 这里有几个注意事项： 确保已经配置了 garp 相关参数。因为 Keepalived 依赖 ARP 报文更新 IP 信息，如果缺少这些参数，会导致某些场景下主设备不发送 ARP，进而导致通信异常。garp 相关参数配置如下： garp_master_delay 1 garp_master_refresh 5 确定没有采用 strict 模式，即需要删除 vrrp_strict 配置。 配置中的/etc/keepalived/check_nginx.sh和/etc/keepalived/keepalived_notify.sh脚本文件，可分别拷贝自scripts/check_nginx.sh和scripts/keepalived_notify.sh。 然后，重启 Keepalived： $ sudo systemctl restart keepalived 第三步：备服务器配置 进行备服务器配置也分为两步。 首先，修改 Keepalived 配置文件。登陆服务器10.0.4.21，编辑/etc/keepalived/keepalived.conf，修改配置，修改后配置内容如下（参考：configs/ha/10.0.4.21/keepalived.conf）： # 全局定义，定义全局的配置选项 global_defs { # 指定keepalived在发生切换操作时发送email，发送给哪些email # 建议在keepalived_notify.sh中发送邮件 notification_email { acassen@firewall.loc } notification_email_from Alexandre.Cassen@firewall.loc # 发送email时邮件源地址 smtp_server 192.168.200.1 # 发送email时smtp服务器地址 smtp_connect_timeout 30 # 连接smtp的超时时间 router_id VM-4-21-centos # 机器标识，通常可以设置为hostname vrrp_skip_check_adv_addr # 如果接收到的报文和上一个报文来自同一个路由器，则不执行检查。默认是跳过检查 vrrp_garp_interval 0 # 单位秒，在一个网卡上每组gratuitous arp消息之间的延迟时间，默认为0 vrrp_gna_interval 0 # 单位秒，在一个网卡上每组na消息之间的延迟时间，默认为0 } # 检测脚本配置 vrrp_script checkhaproxy { script \"/etc/keepalived/check_nginx.sh\" # 检测脚本路径 interval 5 # 检测时间间隔（秒） weight 0 # 根据该权重改变priority，当值为0时，不改变实例的优先级 } # VRRP实例配置 vrrp_instance VI_1 { state BACKUP # 设置初始状态为'备份' interface eth0 # 设置绑定VIP的网卡，例如eth0 virtual_router_id 51 # 配置集群VRID，互为主备的","date":"2022-07-07 15:20:34","objectID":"/iam_service_deployment/:2:4","tags":["iam"],"title":"iam_service_deployment","uri":"/iam_service_deployment/"},{"categories":["iam"],"content":"总结 今天，我主要讲了如何使用 Nginx 和 Keepalived，来部署一个高可用的 IAM 应用。 为了部署一个高可用的 IAM 应用，我们至少需要两台服务器，并且部署相同的服务 iam-apiserver、iam-authz-server、iam-pump。而且，选择其中一台服务器部署数据库服务：MariaDB、Redis、MongoDB。为了安全和性能，iam-apiserver、iam-authz-server、iam-pump 服务都是通过内网来访问数据库服务的。这一讲，我还介绍了如何配置 Nginx 来实现负载均衡，如何配置 Keepalived 来实现 Nginx 的高可用。 ","date":"2022-07-07 15:20:34","objectID":"/iam_service_deployment/:2:5","tags":["iam"],"title":"iam_service_deployment","uri":"/iam_service_deployment/"},{"categories":["iam"],"content":"42 | 软件部署实战（下）：IAM系统安全加固、水平扩缩容实战 这一讲和前面两讲，都是介绍如何基于物理机 / 虚拟机来部署 IAM 的。在前面两讲，我们了解了如何部署一个高可用的 IAM 应用，今天就再来看看 IAM 应用安全和弹性伸缩能力的构建方式。在这一讲中，我会带你加固 IAM 应用的安全性，并介绍如何具体执行扩缩容步骤。接下来，我们先来看下如何加固 IAM 应用的安全性。 ","date":"2022-07-07 15:20:34","objectID":"/iam_service_deployment/:3:0","tags":["iam"],"title":"iam_service_deployment","uri":"/iam_service_deployment/"},{"categories":["iam"],"content":"IAM 应用安全性加固 iam-apiserver、iam-authz-server、MariaDB、Redis 和 MongoDB 这些服务，都提供了绑定监听网卡的功能。我们可以将这些服务绑定到内网网卡上，从而只接收来自于内网的请求，通过这种方式，可以加固我们的系统。 我们也可以通过 iptables 来实现类似的功能，通过将安全问题统一收敛到 iptables 规则，可以使我们更容易地维护安全类设置。这门课通过 iptables 来加固系统，使系统变得更加安全。下面，我先来对 iptables 工具进行一些简单的介绍。 ","date":"2022-07-07 15:20:34","objectID":"/iam_service_deployment/:3:1","tags":["iam"],"title":"iam_service_deployment","uri":"/iam_service_deployment/"},{"categories":["iam"],"content":"iptables 简介 iptables 是 Linux 下最优秀的防火墙工具，也是 Linux 内核中 netfilter 网络子系统用户态的工具。 netfilter 提供了一系列的接口，在一个到达本机的数据包，或者经本机转发的数据包流程中添加了一些可供用户操作的点，这些点被称为 HOOK 点。通过在 HOOK 点注册数据包处理函数，可以实现数据包转发、数据包过滤、地址转换等功能。 用户通过 iptables 工具定义各种规则，这些规则通过 iptables 传给内核中的 netfilter。最终，netfilter 会根据规则对网络包进行过滤。Linux 系统一般会默认安装 iptables 软件。防火墙根据 iptables 里的规则，对收到的网络数据包进行处理。 iptables 里的数据组织结构分为表、链、规则。 表（tables）: 表可以提供特定的功能，每个表里包含多个链。iptables 里面一共有 5 个表，分别是 filter、nat、mangle、raw、security。这些表，分别用来实现包过滤、网络地址转换、包重构、数据追踪处理和 SELinux 标记设置。 链（chains）: 链是数据包传播的路径，每一条链中可以有一个或多个规则。当一个数据包到达一个链时，iptables 会从链中第一条规则开始，检查该数据包是否满足规则所定义的条件。如果满足，就会根据该条规则所定义的方法，处理该数据包。否则，就继续检查下一条规则。如果该数据包不符合链中任一条规则，iptables 就会根据该链预先定义的默认策略来处理数据包。 规则（rules）：规则存储在内核空间的信息包过滤表中，用来描述“如果数据包满足所描述的条件，就按照要求处理这个数据包，如果不满足，就判断下一条规则”。 其中，iptables 中表和链的种类及其功能，如下表所示： 上面的表格中，五张表的处理是有顺序的。当数据包到达某一条链时，会按照 RAW、MANGLE、NAT、FILTER、SECURITY 的顺序进行处理。到这里，我介绍了关于 iptables 的一些基础知识，但这还远远不够。要想使用 iptables 来加固你的系统，你还需要掌握 iptables 工具的使用方法。接下来，我先来介绍下 iptables 是如何处理网络数据包的。 ","date":"2022-07-07 15:20:34","objectID":"/iam_service_deployment/:3:2","tags":["iam"],"title":"iam_service_deployment","uri":"/iam_service_deployment/"},{"categories":["iam"],"content":"网络数据包处理流程 网络数据包的处理流程如下图所示： 具体可以分为两个步骤。 第一步，当数据包进入网卡后，它首先进入 PREROUTING 链，根据目的 IP 判断是否转发出去。第二步分为两种情况：如果数据包目的地是本机，它会到达 INPUT 链。到达后，任何进程都会收到它。本机上的程序可以发送数据包，这些数据包会经过 OUTPUT 链，然后经 POSTROUTING 链输出；如果数据包是要转发出去，并且内核允许转发，那么数据包会经过 FORWARD 链，最后从 POSTROUTING 链输出。 ","date":"2022-07-07 15:20:34","objectID":"/iam_service_deployment/:3:3","tags":["iam"],"title":"iam_service_deployment","uri":"/iam_service_deployment/"},{"categories":["iam"],"content":"iptables 工具使用方式介绍 iptables 的功能强大，所以使用方法也非常多样。这里，我来介绍下 iptables 工具的使用方式，并给出一些使用示例。 命令格式 iptables 的语法格式为： iptables [-t 表名] 命令选项 [链名] [条件匹配] [-j 目标动作或跳转] 下面是一个 iptables 的使用示例： iptables -t nat -I PREROUTING -p tcp --dport 8080 -j DNAT --to 10.0.4.88 这里对上面涉及到的一些参数进行说明。 表名 / 链名：指定 iptables 命令所操作的表 / 链。 命令选项：指定处理 iptables 规则的方式，例如插入、增加、删除、查看等。 条件匹配：指定对符合条件的数据包进行处理。 目标动作或跳转：防火墙处理数据包的方式。 iptables 的命令选项又分为管理控制选项和通用选项。管理控制选项如下： 通用选项如下： 处理数据包的方式（目标动作或跳转）有多种，具体如下表所示： 上面，我介绍了 iptables 工具的使用方式。因为内容有点多，你可能仍然不知道如何使用 iptables 工具。没关系，接下来你可以结合我举的一些例子来看下。 命令示例 下面的命令示例，默认使用了 FILTER 表，也即规则存放在 FILTER 表中，相当于每一条 iptables 命令都添加了-t filter 参数。 拒绝进入防火墙的所有 ICMP 协议数据包： $ iptables -I INPUT -p icmp -j REJECT 允许防火墙转发除 ICMP 协议以外的所有数据包： $ iptables -A FORWARD -p ! icmp -j ACCEPT 拒绝转发来自 192.168.1.10 主机的数据，允许转发来自 192.168.0.0/24 网段的数据： $ iptables -A FORWARD -s 192.168.1.11 -j REJECT $ iptables -A FORWARD -s 192.168.0.0/24 -j ACCEPT 丢弃从外网接口（eth1）进入防火墙本机的源地址为私网地址的数据包： $ iptables -A INPUT -i eth1 -s 192.168.0.0/16 -j DROP $ iptables -A INPUT -i eth1 -s 172.16.0.0/12 -j DROP $ iptables -A INPUT -i eth1 -s 10.0.0.0/8 -j DROP 只允许管理员从 202.13.0.0/16 网段使用 SSH 远程登录防火墙主机： $ iptables -A INPUT -p tcp --dport 22 -s 202.13.0.0/16 -j ACCEPT $ iptables -A INPUT -p tcp --dport 22 -j DROP 允许本机开放从 TCP 端口 20-1024 提供的应用服务： $ iptables -A INPUT -p tcp --dport 20:1024 -j ACCEPT $ iptables -A OUTPUT -p tcp --sport 20:1024 -j ACCEPT 允许转发来自 192.168.0.0/24 局域网段的 DNS 解析请求数据包： $ iptables -A FORWARD -s 192.168.0.0/24 -p udp --dport 53 -j ACCEPT $ iptables -A FORWARD -d 192.168.0.0/24 -p udp --sport 53 -j ACCEPT 禁止其他主机 ping 防火墙主机，但是允许从防火墙上 ping 其他主机： $ iptables -I INPUT -p icmp --icmp-type Echo-Request -j DROP $ iptables -I INPUT -p icmp --icmp-type Echo-Reply -j ACCEPT $ iptables -I INPUT -p icmp --icmp-type destination-Unreachable -j ACCEPT 禁止转发来自 MAC 地址为 00：0C：29：27：55：3F 的数据包和主机的数据包： $ iptables -A FORWARD -m mac --mac-source 00:0c:29:27:55:3F -j DROP 对外开放 TCP 端口 20、21、25、110，以及被动模式 FTP 端口 1250-1280： $ iptables -A INPUT -p tcp -m multiport --dport 20,21,25,110,1250:1280 -j ACCEPT 禁止转发源 IP 地址为 192.168.1.20-192.168.1.99 的 TCP 数据包： $ iptables -A FORWARD -p tcp -m iprange --src-range 192.168.1.20-192.168.1.99 -j DROP 禁止转发与正常 TCP 连接无关的非 syn 请求数据包： $ iptables -A FORWARD -m state --state NEW -p tcp ! --syn -j DROP 拒绝访问防火墙的新数据包，但允许响应连接或与已有连接相关的数据包： $ iptables -A INPUT -p tcp -m state --state NEW -j DROP $ iptables -A INPUT -p tcp -m state --state ESTABLISHED,RELATED -j ACCEPT 只开放本机的 web 服务（80）、FTP(20、21、20450-20480)，放行外部主机发往服务器其他端口的应答数据包，将其他入站数据包都进行丢弃处理： $ iptables -I INPUT -p tcp -m multiport --dport 20,21,80 -j ACCEPT $ iptables -I INPUT -p tcp --dport 20450:20480 -j ACCEPT $ iptables -I INPUT -p tcp -m state --state ESTABLISHED -j ACCEPT $ iptables -P INPUT DROP 到这里，我们已经了解了 iptables 的功能，下面来看看如何使用 iptables 来加固 IAM 应用。我把它分成内网不安全和内网安全两种情况。 ","date":"2022-07-07 15:20:34","objectID":"/iam_service_deployment/:3:4","tags":["iam"],"title":"iam_service_deployment","uri":"/iam_service_deployment/"},{"categories":["iam"],"content":"IAM 安全加固（内网不安全） 在设置 iptables 规则之前，我们需要先梳理系统的访问关系，然后根据这些访问关系设置 iptables 规则。访问关系如下图所示： 你可以看到，IAM 系统服务互访关系分为下面这 4 种： 允许公网客户端访问 Nginx 的 80 和 443 端口。 Keepalived 服务之间能够互发 VRRP 协议包。 Nginx 访问各节点上 iam-apiserver、iam-authz-server 和 iam-pump 组件开启的 HTTP/HTTPS/GRPC 服务。 iam 服务可以从各节点访问 Redis、MariaDB、MongoDB 数据库。 这里，我们假定 IAM 系统部署在一个非常大的内网中，该内网部署了很多其他团队的服务，有很多其他团队的研发、测试等人员在内网中执行各种操作。也就是说，我们处在一个不安全的内网中。这时候，如果要加固我们的系统，最安全的方式是屏蔽掉未知的来源 IP。 内网不安全的情况下，加固系统可以分为 3 大步骤，每个步骤中又有一些小步骤。另外，需要新增节点或者删除节点时，也需要进行一些变更操作。下面我们来具体看下。 第一步，设置防火墙规则。 基于上面说到的几种互访关系，我们可以在各个节点上设置 iptables 规则来加固系统。我将这些规则设置编写成了 go 工具，用来自动生成设置这些规则的 shell 脚本。 具体设置的过程可以分为 5 步。 进入 iam 项目源码根目录。 配置 accesss.yaml（工具根据此配置，自动生成 iptables 设置脚本），内容如下（位于configs/access.yaml文件）： # 允许登录SSH节点的来源IP，可以是固定IP(例如10.0.4.2)，也可以是个网段，0.0.0.0/0代表不限制来源IPssh-source:10.0.4.0/24# IAM应用节点列表（来源IP）hosts:- 10.0.4.20- 10.0.4.21# 来源IP可以访问的应用端口列表（iam-apiserver, iam-authz-server, iam-pump对外暴露的的端口）ports:- 8080- 8443- 9090- 9443- 7070# 来源IP可以访问的数据库端口列表（Redis, MariaDB, MongoDB）dbports:- 3306- 6379- 27017 上面的配置中，我们指定了允许登陆机器的子网、Nginx 需要访问的端口列表和各节点需要访问的数据库端口列表。 生成 iptables 初始化脚本： $ go run tools/geniptables/main.go -c access.yaml -t app -a -o firewall.sh $ ls firewall.sh firewall.sh 你可以打开 firewall.sh 文件，查看该脚本设置的规则。 将 firewall.sh 脚本拷贝到 10.0.4.20 和 10.0.4.21 节点执行： $ scp firewall.sh root@10.0.4.20:/tmp/ $ scp firewall.sh root@10.0.4.21:/tmp/ 登陆 10.0.4.20 和 10.0.4.21 机器，执行/tmp/firewall.sh。 在 10.0.4.20（数据库节点）节点上，设置 iptables 规则，以允许各节点访问：因为数据库节点也位于 10.0.4.20 节点，所以只需要添加新的 rule，并将iptables -A INPUT -j DROP规则放到最后执行即可。 $ go run tools/geniptables/main.go -c access.yaml -t db -o addrules.sh 然后，将 addrules.sh 脚本拷贝到 10.0.4.20 节点执行。注意，因为 iptables 是按顺序进行规则过滤的，所以需要将iptables -A INPUT -j DROP规则放在新设置规则的后面，否则执行不到新设置的规则。你可以在设置完 iptables 规则之后，执行下面的命令来将 DROP 放到最后： iptables -A INPUT -j LOG --log-level 7 --log-prefix \"Default Deny\" iptables -A INPUT -j DROP 生成的 addrules.sh 脚本加入以上设置。 第二步，设置重启自动加载 iptables 规则。 前面我们在各个节点设置了 iptables 规则，但是这些规则在系统重启后会丢失。为了使系统重启后自动重新设置这些规则，我们需要将当前的 iptables 规则保存起来，让系统重启时自动加载。需要进行下面两个步骤。 保存现有的规则： $ sudo iptables-save \u003e /etc/sysconfig/iptables 添加下面的命令行到 /etc/rc.d/rc.local 文件中： $ iptables-restore \u003c /etc/sysconfig/iptables 第三步，自动化。 在上面的步骤中，我们自动生成了 iptables 规则，并手动登陆到节点进行设置。你肯定也发现了，整个流程手动操作过多，容易出错，效率还低。你可以参考设置过程，将这些设置工作自动化，比如编写脚本，一键刷新所有节点的 iptables 规则。 另外，我们再来看下在新增节点和删除节点两种场景下，如何设置 iptables 规则。 场景 1：新增节点 如果我们要扩容一个节点，也需要在新节点设置防火墙规则，并在数据库节点设置防火墙规则允许来自新节点的访问。假如我们新增一个 10.0.4.22 节点，这里要设置防火墙规则，需要下面的 4 个步骤。 编辑 access.yaml，在 hosts 列表下新增 10.0.4.22 节点 IP。编辑后内容如下： # 允许登录SSH节点的来源IP，可以是固定IP(例如10.0.4.2)，也可以是个网段，0.0.0.0/0代表不限制来源IP ssh-source: 10.0.4.0/24 # IAM应用节点列表（来源IP） hosts: - 10.0.4.20 - 10.0.4.21 - 10.0.4.22 # 来源IP可以访问的应用端口列表（iam-apiserver, iam-authz-server, iam-pump对外暴露的的端口） ports: - 8080 - 8443 - 9090 - 9443 - 7070 # 来源IP可以访问的数据库端口列表（Redis, MariaDB, MongoDB） dbports: - 3306 - 6379 - 27017 在 10.0.4.22 节点设置 iptables 规则： $ go run tools/geniptables/main.go -c access.yaml -t app -a -o firewall.sh 将 firewall.sh 脚本拷贝到 10.0.4.22 节点，并执行。 在已有节点新增规则，允许来自 10.0.4.22 的 Nginx 服务的访问： $ go run tools/geniptables/main.go -c access.yaml -t app 10.0.4.22 -o addrules.sh 将 addrules.sh 脚本拷贝到存量节点，并执行。 在数据库节点新增 iptables 规则，以允许来自新节点的访问： $ go run tools/geniptables/main.go -c access.yaml -t db 10.0.4.22 -o addrules.sh 将 addrules.sh 脚本拷贝到 10.0.4.20 节点执行即可。 场景 2：删除节点。 如果我们要删除一个节点，需要在保留的节点和数据库节点中，将该节点的访问权限删除。假如我们要删除 10.0.4.22 节点，设置防火墙规则需要下面 3 个步骤。 在保留节点删除 10.0.4.22 节点访问权限： $ go run tools/geniptables/main.go -c access.yaml -t app --delete 10.0.4.22 -o delete.sh 将 delete.sh 脚本拷贝到 10.0.4.20 节点执行即可。 将下线的节点从 access.yaml 文件中的 hosts 部分删除。 ","date":"2022-07-07 15:20:34","objectID":"/iam_service_deployment/:3:5","tags":["iam"],"title":"iam_service_deployment","uri":"/iam_service_deployment/"},{"categories":["iam"],"content":"IAM 安全加固（内网安全） 这里，我们来看第二种情况：假定我们系统部署在一个安全的内网环境中，这时候加固系统就会变得异常简单，只需要允许来源 IP 为内网 IP 的客户端访问我们提供的各类端口即可。在我们设置完 iptables 规则之后，后续再新增或者删除节点，就不需要再做变更了。具体可以分为 5 个步骤。 具体可以分为 5 个步骤。 第一步，进入 iam 项目源码根目录。 第二步，配置 accesss.yaml（工具根据此配置，自动生成 iptables 设置脚本），内容如下（configs/access.yaml文件）： # 允许登录SSH节点的来源IP，可以是固定IP(例如10.0.4.2)，也可以是个网段，0.0.0.0/0代表不限制来源IPssh-source:10.0.4.0/24# 来源IP可以访问的应用端口列表（iam-apiserver, iam-authz-server, iam-pump对外暴露的的端口）ports:- 8080- 8443- 9090- 9443- 7070# 来源IP可以访问的数据库端口列表（Redis, MariaDB, MongoDB）dbports:- 3306- 6379- 27017 上面配置中，我们仅仅指定了 IAM 服务端口和数据库端口。 第三步，生成 iptables 初始化脚本： $ go run tools/geniptables/main.go -c access.yaml -t app --cidr=10.0.4.0/24 -a -o firewall.sh $ ls firewall.sh firewall.sh 第四步，将 firewall.sh 脚本拷贝到 10.0.4.20 和 10.0.4.21 节点执行： $ scp firewall.sh root@10.0.4.20:/tmp/ $ scp firewall.sh root@10.0.4.21:/tmp/ 登陆 10.0.4.20 和 10.0.4.21 机器执行 /tmp/firewall.sh 。 第五步，在 10.0.4.20（数据库节点）节点上，设置 iptables 规则，以允许各节点访问。 因为数据库节点也位于 10.0.4.20 节点，所以只需要添加新的 rule，并将 iptables -A INPUT -j DROP 规则放到最后执行即可。 $ go run tools/geniptables/main.go -c access.yaml -t db --cidr=10.0.4.0/24 -o addrules.sh 然后，将 addrules.sh 脚本拷贝到 10.0.4.20 节点执行。如果要增加节点，你只需要重新执行第三步，生成 firewall.sh 脚本，并将 firewall.sh 脚本拷贝到新节点上执行即可。删除节点，则不需要做任何操作。 接下来，我们再来看下如何对 IAM 应用进行弹性伸缩操作。 ","date":"2022-07-07 15:20:34","objectID":"/iam_service_deployment/:3:6","tags":["iam"],"title":"iam_service_deployment","uri":"/iam_service_deployment/"},{"categories":["iam"],"content":"弹性伸缩 弹性伸缩包括扩容和缩容。扩容是指当业务量越来越大时，能够很容易地增加计算节点，来分散工作负载，从而实现计算等能力的扩展。缩容是指当业务量变小时，能够很容易地减少计算节点，从而减小成本。 在系统上线初期，通常业务量不会很大，但是随着产品的迭代，用户量的增多，系统承载的请求量会越来越多，系统承载的压力也会越来越大。这时，就需要我们的系统架构有能力进行水平扩容，以满足业务需求，同时避免因为系统负载过高造成系统雪崩。一些电商系统，在双 11 这类促销活动之前会提前扩容计算节点，以应对即将到来的流量高峰。但是活动过后，流量会逐渐下降，这时就需要我们的系统有能力进行缩容，以减少计算节点，从而节省成本。 一个可伸缩的系统架构，是我们在进行系统设计时必须要保证的。如果系统不具有伸缩性，那么当我们后期需要扩缩容时，就需要对代码进行大改，不仅会增加额外的工作量，还会拖累产品的迭代速度。而且你想想，改完之后还要测试，发布之后，还可能因为代码变更引入 Bug。总之，不具伸缩性的系统架构可以说是后患无穷。 IAM 系统在设计之初就考虑到了系统的伸缩能力，我们可以很容易地对系统进行扩缩容。下面，我来分别介绍下如何对系统进行扩容和缩容。 系统扩容 系统扩容的步骤很简单，你只需要进行下面这 5 步： 根据需要申请计算节点，如无特殊需求，计算节点的配置、操作系统等要跟已有的节点保持一致。在新的节点上部署 iam-apiserver、iam-authz-server、iam-pump，部署方式跟部署其他节点一样。在新节点部署 Nginx，并将新节点的 IP 加入到已有所有节点的 Nginx upstream 配置中，重启 Nginx。在新节点部署 Keepalived，并将新节点的 IP 加入到已有所有节点的 unicast_peer 配置中，重启 Keepalived。修改 iptables 规则，并刷新所有机器的 iptables。 系统缩容 系统缩容是系统扩容的逆向操作，也是 5 个步骤： 根据需要，确定要删除的节点。关闭待删除节点的 iam-apiserver、iam-authz-server、iam-pump 服务。从所有保留节点的 Nginx upstream 配置中，删除待删除节点的 IP 地址, 重启 Nginx。从所有保留节点的 Keepalived unicast_peer 配置中，删除待删除节点的 IP 地址, 重启 Keepalived。修改 iptables 规则，并刷新所有保留机器的 iptables。 ","date":"2022-07-07 15:20:34","objectID":"/iam_service_deployment/:3:7","tags":["iam"],"title":"iam_service_deployment","uri":"/iam_service_deployment/"},{"categories":["iam"],"content":"总结 安全对于应用软件来说至关重要，在部署应用时，也一定要评估应用的安全性，并采取一定的措施来保证安全性。 在进行软件部署时，保证应用安全性最简单有效的方式是使用 iptables 规则来加固系统。实现思路也很简单，就是使用 iptables 规则，只允许特定来源的 IP 访问特定的端口。在业务正式上线之后，可能会遇到业务高峰期或低峰期。业务高峰期，可能需要添加机器，提高系统的吞吐量，可以在新机器上安装需要扩容的服务组件，并安装和配置好 Nginx 和 Keepalived，之后将该服务器添加到 Nginx 的 upstream 中。在业务低峰期时，可以将服务器从 Nginx 的 upstream 列表中移除，并关停 IAM 应用的服务。 ","date":"2022-07-07 15:20:34","objectID":"/iam_service_deployment/:3:8","tags":["iam"],"title":"iam_service_deployment","uri":"/iam_service_deployment/"},{"categories":["iam"],"content":"43｜技术演进（上）：虚拟化技术演进之路 在前面的三讲中，我介绍了传统应用的部署方式。但是，随着软件架构进入云原生时代，我们越来越多地使用云原生架构来构建和部署我们的应用。为了给你演示如何使用云原生化的方式来部署 IAM 应用，接下来我会介绍如何基于 Kubernetes 来部署 IAM 应用。 在 Kubernetes 集群中部署 IAM 应用，会涉及到一些重要的云原生技术，例如 Docker、Kubernetes、微服务等。另外，云原生架构中还包含了很多其他的技术。为了让你提前了解后面部署需要的相关技术，同时比较通透地了解当前最火热的云原生架构，这一讲我就采用技术演进的思路，来详细讲解下云原生技术栈的演进中的虚拟化技术演进部分。 因为这一讲涉及的技术栈很多，所以我会把重点放在演进过程上，不会详细介绍每种技术的具体实现原理和使用方法。如果你感兴趣，可以自行学习，也可以参考我为你整理的这个资料：awesome-books。 在讲这个演进过程之前，我们先来看下这个问题：我们为什么使用云？ ","date":"2022-07-07 15:20:34","objectID":"/iam_service_deployment/:4:0","tags":["iam"],"title":"iam_service_deployment","uri":"/iam_service_deployment/"},{"categories":["iam"],"content":"我们为什么使用云？ 使用云的原因其实很简单，我们只是想在云上部署一个能够对外稳定输出业务能力的服务，这个服务以应用的形态部署在云上。为了启动一个应用，我们还需要申请系统资源。此外，我们还需要确保应用能够快速迭代和发布，出故障后能够快速恢复等，这就需要我们对应用进行生命周期管理。 应用、系统资源、应用生命周期管理这 3 个维度就构成了我们对云的所有诉求，如下图所示： 接下来的两讲，我就围绕着这 3 个维度，来给你详细介绍下每个维度的技术演进。这一讲，我会先介绍下系统资源维度的技术演进。在 44 讲，我会再介绍下应用维度和应用生命周期管理维度的技术演进。当前有 3 种系统资源形态，分别是物理机、虚拟机和容器，这 3 种系统资源形态都是围绕着虚拟化来演进的。所以，介绍系统资源技术的演进，其实就是介绍虚拟化技术的演进。接下来，我们就来看下虚拟化技术是如何演进的。 ","date":"2022-07-07 15:20:34","objectID":"/iam_service_deployment/:4:1","tags":["iam"],"title":"iam_service_deployment","uri":"/iam_service_deployment/"},{"categories":["iam"],"content":"虚拟化技术的演进 虚拟化这个概念，其实在 20 世纪 60 年代就已经出现了。但因为技术、场景等限制，虚拟化技术曾沉寂过一段时间，直到 21 世纪虚拟机出现，虚拟化技术又迎来了一波爆发期，并逐渐走向成熟。 那么，什么是虚拟化技术呢？简单来讲，就是把计算机上的硬件、系统资源划分为逻辑组的技术，由此生成的仅仅是一个逻辑角度的视图。通过虚拟化技术，我们可以在一台计算机上运行多个虚拟机进程，进而发挥计算机硬件的最大利用率。 虚拟化分为很多种，例如操作系统虚拟化、存储虚拟化、网络虚拟化、桌面虚拟化等。其中，最重要的是操作系统虚拟化，支撑操作系统虚拟化的是底层 CPU、内存、存储、网络等的虚拟化，这些资源我们统称为计算资源。 因为计算资源的虚拟化在虚拟化领域占主导地位，所以很多时候我们说虚拟化技术演进，其实就是在说计算资源技术的演进。在我看来，虚拟化技术的演进过程如下：物理机阶段 -\u003e 虚拟机阶段 -\u003e 容器阶段（Docker + Kubernetes） -\u003e Serverless 阶段。 物理机阶段 上面我提到虚拟化技术包含很多方面，但是整个虚拟化技术是围绕着 CPU 虚拟化技术来演进的。这是因为，内存虚拟化、I/O 虚拟化的正确实现，都依赖于对内存、I/O 中一些敏感指令的正确处理，这就涉及到 CPU 虚拟化，所以 CPU 虚拟化是虚拟化技术的核心。因此，这一讲我会围绕着 CPU 虚拟化的演进，来讲解虚拟化技术的演进。这里，我先来介绍一下物理机阶段 CPU 的相关知识。 CPU 是由一系列指令集构成的，这些指令集主要分为两种，分别是特权指令集和非特权指令集。特权指令集是指那些可以改变系统状态的指令集，非特权指令集是指那些不会影响系统状态的指令集。我举个例子你就明白了：写内存是特权指令集，因为它可以改变系统的状态；读内存是非特权指令集，因为它不会影响系统的状态。 因为非特权指令集可能会影响整个系统，所以芯片厂商在 x86 架构上又设计了一种新模式，保护模式，这个模式可以避免非特权指令集非法访问系统资源。 保护模式是通过 Ring 来实现的。在 x86 架构上，一共有 4 个 Ring，不同的 Ring 有不同的权限级别：Ring 0 有最高的权限，可以操作所有的系统资源，Ring 3 的权限级别最低。Kernel 运行在 Ring 0 上，Application 运行在 Ring 3 上。Ring 3 的 Application 如果想请求系统资源，需要通过 system call 调用 Ring 0 的内核功能，来申请系统资源。 这种方式有个好处：可以避免 Applicaiton 直接请求系统资源，影响系统稳定性。通过具有更高权限级的 Kernel 统一调度、统一分配资源，可以使整个系统更高效，更安全。 x86 架构的 Ring 和调用关系如下图所示： 在物理机阶段，对外提供物理资源，这种资源提供方式面临很多问题，例如成本高，维护麻烦、需要建机房、安装制冷设备、服务器不方便创建、销毁等等。所以在云时代，和物理机相比，我们用得更多的是虚拟机。下面我们就来看虚拟机阶段。 虚拟机阶段 这里，在讲虚拟化技术之前，我想先介绍下 x86 的虚拟化漏洞，CPU 虚拟化技术的演进也主要是围绕着解决这个漏洞来演进的。 虚拟化漏洞 一个虚拟化环境分为三个部分，分别是硬件、虚拟机监控器（又叫 VMM，Virtual Machine Manager），还有虚拟机。 你可以把虚拟机看作物理机的一种高效隔离的复制，它具有三个特性：同质、高效、资源受控。这三个特点决定了不是所有体系都可以虚拟化，比如目前我们用得最多的 x86 架构，就不是一个可虚拟化的架构，我们称之为虚拟化漏洞。 在虚拟化技术产生后，诞生了一个新的概念：敏感指令。敏感指令是指可以操作特权资源的指令，比如修改虚拟机运行模式、物理机状态，读写敏感寄存器 / 内存等。显然，所有的特权指令都是敏感指令，但不是所有的敏感指令都是特权指令。特权指令和敏感指令的关系，可以简单地用这张图来表示： 在一个可虚拟化的架构中，所有的敏感指令应该都是特权指令。x86 架构中有些敏感指令不是特权指令，最简单的例子是企图访问或修改虚拟机模式的指令。所以，x86 架构是有虚拟化漏洞的。 Hypervisor 技术的演进 为了解决 x86 架构的虚拟化漏洞，衍生出了一系列的虚拟化技术，这些虚拟化技术中最核心的是 Hypervisor 技术。所以接下来，我就介绍下 Hypervisor 技术的演进。 Hypervisor，也称为虚拟机监控器 VMM，可用于创建和运行虚拟机 （VM）。它是一种中间软件层，运行在基础物理服务器和操作系统之间，可允许多个操作系统和应用共享硬件。通过让 Hypervisor 以虚拟化的方式共享系统资源（如内存、CPU 资源），一台主机计算机可以支持多台客户机虚拟机。 Hypervisor、物理机和虚拟机的关系如下图： 按时间顺序，Hypervisor 技术的发展依次经历了下面 3 个阶段： 软件辅助的完全虚拟化（Software-assisted full virtualization）：该虚拟化技术在 1999 年出现，里面又包含了解释执行（如 Bochs）、扫描与修补（如 VirtualBox）、二进制代码翻译（如 Vmware、Qemu）三种技术。 半虚拟化（Para-virtualization）：该虚拟化技术在 2003 年出现，也叫类虚拟化技术，典型的 Hypervisor 代表是 Xen。 硬件辅助的完全虚拟化（Hardware-assistant full virtualization ）：该虚拟化技术在 2006 年出现，典型的 Hypervisor 代表是 KVM。当前普遍使用的主流虚拟化技术，就是以 KVM 为代表的硬件辅助的完全虚拟化。 下面，我就来简单介绍下这三个阶段。 先来看第一个阶段，软件辅助的完全虚拟化，它又分为解释执行、扫描与修补、二进制代码翻译三个演进阶段。 解释执行 简单地说，解释执行的过程就是取一条指令，模拟出这条指令的执行效果，再取下一条指令。这种技术因为思路比较简单，所以容易实现，复杂度低。执行时，编译好的二进制代码是不会被载入到物理 CPU 直接运行的，而是由解释器逐条解码，再调入对应的函数来模拟指令的功能。解释过程如下图所示： 因为每一条指令都要模拟，所以就解决了虚拟化漏洞，同时也可以模拟出一个异构的 CPU 结构，比如在 x86 架构上模拟出一个 ARM 架构的虚拟机。也正是因为每一条指令都需要模拟，不区别对待，导致这种技术的性能很低。 扫描与修补 由于解释执行性能损失很大，再加上虚拟机中模拟的虚拟 CPU 和物理 CPU 的体系结构相同（同质），这样大多数指令可以直接在物理 CPU 上运行。因此，CPU 虚拟化过程中，可以采用更优化的模拟技术来弥补虚拟化漏洞。 扫描与修补技术就是通过这种方式，让大多数指令直接在物理 CPU 上运行，而把操作系统中的敏感指令替换为跳转指令，或者会陷入到 VMM 中去的指令。这样，VMM 一旦运行到敏感指令，控制流就会进入 VMM 中，由 VMM 代为模拟执行。过程如下图所示： 使用这种方式，因为大部分指令不需要模拟，可以直接在 CPU 上运行，所以性能损失相对较小，实现起来比较简单。 二进制代码翻译 这个算是软件辅助的完全虚拟化的主流方式了，早期的 VMware 用的就是这个技术。二进制代码翻译会在 VMM 中开辟一段缓存，将翻译好的代码放在缓存中。在执行到某条指令的时候，直接从内存中找到这条指令对应的翻译后的指令，然后在 CPU 上执行。 在性能上，二进制代码翻译跟扫描与修补技术各有长短，但是实现方式最为复杂。它的过程如下图所示： 看到这里，你可能会对模拟和翻译这两个概念有疑惑，我在这里解释下模拟和翻译的区别：模拟是将 A 动作模拟成 B 动作，而翻译是将 A 指令翻译成 B 指令，二者是有本质不同的。 然后，我们来看 Hypervisor 技术发展的第二个阶段，Para-virtualization。 软件辅助的完全虚拟化对 x86 的指令做了翻译或者模拟，在性能上，多多少少都会有些损失，而这些性能损失在一些生产级的场景是不可接受的。所以，在 2003 年出现了 Para-virtualization 技术，也叫半虚拟化 / 类虚拟化。和之前的虚拟化技术相比，Para-virtualization 在性能上有了大幅度的提升，甚至接近于原生的物理机。 Para-virtualization 的大概原理是这样的：Hypervisor 运行在 Ring 0 中，修改客户机操作系统内核，将其中的敏感指令换成 hypercall。hypercall 是一个可以直接跟 VMM 通信的函数，这样就绕过了虚拟化的漏洞（相当于所有敏感指令都被 VMM 捕获了），同时不存在模拟和翻译的过程，所以性能是最高的。这个过程如下图所示： 因为要修改操作系统，所以不能模拟一些闭源的操作系统，比如 Windows 系列。另外，修改客户机操作系统内核还是有些开发和维护工作量的。所以，随着硬件辅助完全虚拟化技术的成熟，Para-virtualization 也逐渐被替换掉了。 然后，我们来看 Hypervisor 技术发展的第三个阶段，硬件辅助的完全虚拟化。 在 2006 年，Intel 和 AMD 分别在硬件层面支持了虚拟化，比如 Intel 的 VT-X 技术和 AMD 的 SVM。它们的核心思想都是引入新运行模式，可以理解为增加了一个新的 CPU Ring -1，权限比","date":"2022-07-07 15:20:34","objectID":"/iam_service_deployment/:4:2","tags":["iam"],"title":"iam_service_deployment","uri":"/iam_service_deployment/"},{"categories":["iam"],"content":"总结 这一讲，我主要通过虚拟化技术的演进，介绍了系统资源维度的技术演进。 虚拟化技术的演进流程为：物理机阶段 -\u003e 虚拟机阶段 -\u003e 容器阶段 -\u003e Serverless 阶段。其中，物理机到虚拟机阶段的演进技术，主要是为了解决 x86 架构的虚拟化漏洞。要虚拟 CPU、内存和 I/O，就需要捕获其中的敏感指令，防止这些敏感指令修改系统状态，影响系统的稳定性。x86 架构有些敏感指令不是特权指令，导致这些指令可以从客户机中直接在物理 CPU 上执行，从而可能会影响系统状态。所以，我们说 x86 架构是有虚拟化漏洞的。 在虚拟机阶段，又诞生了 3 种不同的虚拟化技术，分别是软件辅助的完全虚拟化、半虚拟化和硬件辅助的完全虚拟化。因为硬件辅助的完全虚拟化技术不需要修改客户机内核，并且有着接近物理机的性能，所以成为当前的虚拟化主流技术，并以 KVM 为事实技术标准。 因为容器技术比虚拟机更加轻量，再加上 Docker、Kubernetes 项目的诞生，使得大规模使用容器技术变得可行，所以这几年系统资源的提供形态已经由虚拟机转变成了容器。 系统资源的最终形态，我认为会是 Serverless。Serverless 技术中，又分为 3 种技术形态：云函数、Serverless 容器和 BaaS。在业务架构 Serverless 化的过程中，整个部署架构会以 Serverless 容器为主，云函数为辅。 ","date":"2022-07-07 15:20:34","objectID":"/iam_service_deployment/:4:3","tags":["iam"],"title":"iam_service_deployment","uri":"/iam_service_deployment/"},{"categories":["iam"],"content":"44｜技术演进（下）：软件架构和应用生命周期技术演进之路 应用、系统资源、应用生命周期管理这 3 个维度，构成了我们对云的所有诉求。上一讲，我从系统资源维度，介绍了虚拟化技术的演进之路。这一讲，我会介绍下应用维度和应用生命周期管理维度的技术演进。应用软件架构是用来构建应用的，不同的软件架构，构建应用的方式、效率，以及所构建应用的可维护度、性能都是不同的。随着技术的不断更新迭代，应用软件架构也在不断往前演进。这一讲我们就来看看，应用软件架构都有哪些，这些软件架构都有什么特点，以及它们之间是如何演进的。 至于应用生命周期管理维度，我在 09 讲 中已经介绍了应用生命周期管理技术的演进，这一讲也会再补充一些核心的技术，比如日志、监控告警、调用链等。接下来，我们就先来看下软件架构的演进之路。 ","date":"2022-07-07 15:20:34","objectID":"/iam_service_deployment/:5:0","tags":["iam"],"title":"iam_service_deployment","uri":"/iam_service_deployment/"},{"categories":["iam"],"content":"软件架构的演进 软件架构技术演进如下图所示： 最开始，我们使用单体架构来构建应用，后面逐渐演进为 SOA 架构。不管是单体架构，还是 SOA 架构，都很难满足互联网时代应用快速迭代的诉求。所以，在互联网时代，应用软件架构又演进成了微服务架构。当前我们正处在微服务架构阶段，也有很多团队的业务正在尝试使用 Service Mesh 替代微服务架构中的一些功能。 随着 Serverless 云函数的诞生，也诞生了一种新的软件架构，FaaS 架构。这里我先简单介绍下它，后面再详细讲。FaaS 架构因为限制多、使用场景局限，目前还仅仅适用于云函数这种系统资源形态，我个人认为它不会成为未来主流的软件架构。还要说明下，业界目前并没有 FaaS 软件架构这个说法，大家说到 FaaS，一般指的都是云函数这种技术形态。这里为了方便描述，我们先这样表达。 接下来，我仍然以技术演进的思路，来介绍下这些软件架构。首先来看下最早的单体架构。 单体架构 在最早的时候，我们用的软件架构是单体架构。在单体架构中，我们会将应用程序的所有功能都存放在一个代码仓库中，并且发布时，也是发布整个代码仓库的代码和功能。在单体架构中，应用软件一般会包含四层，分别是表示层、业务逻辑层、数据访问层、数据库，如下图所示： 这里简单介绍下每层的功能。 表示层：用于直接和用户交互，通常是网页、UI 界面。 业务逻辑层：用来进行业务逻辑处理。使用表示层传来的参数，进行业务逻辑处理，并将结果返回给表示层。 数据访问层：用来操作数据库，通常包括数据的 CURD 操作。例如，从数据库中查询用户信息，或者往数据库增加一条用户记录。 数据库：存储数据的物理介质。我们通过数据访问层来访问数据库中的数据。 单体架构的优点是应用开发简单，技术单一，测试、部署相对简单明了。因此它比较适合用户访问量较小的应用服务端。但它的缺陷也是非常明显的。随着业务的发展，项目越来越大，单体架构会带来开发效率低、发布周期长、维护困难、稳定性差、扩展性差等问题。另外，单体架构的技术栈也不易扩展，只能在原有的基础上，不断地进行局部优化。 SOA 架构 为了解决单体架构在业务代码变大时带来的各种问题，SOA 架构出现了。 SOA 架构是面向服务的软件架构，它的核心理念是：基于 SOA 的架构思想，将重复共用的功能抽取为组件，以服务的方式给各系统提供服务，服务之间通过 ESB 企业服务总线进行通信。如下图所示： SOA 架构中，主要有两个角色，分别是服务提供者和服务消费者。服务消费者可以通过发送消息来调用购买商品、申请售后的服务，这些消息由 ESB 总线转换后，发送给对应的服务，实现 SOA 服务之间的交互通信。SOA 架构主要适用于大型软件服务企业对外提供服务的场景，至于一般业务场景就并不适用了。这是因为，SOA 服务的定义、注册和调用都需要繁琐的编码或者配置来实现，并且 ESB 总线也容易导致系统的单点风险，并拖累整体性能。 微服务架构 在互联网时代，越来越多的企业推出了面向普通大众的网站和应用。这些企业没有能力，也没有必要构建和维护 ESB 企业服务总线。于是，基于 SOA 架构，又演进出了微服务架构。 微服务架构由 Matrin Fowler 在 2014 年提出，它的理念是将业务系统彻底地组件化和服务化，形成多个可以独立开发、部署和维护的服务或应用的集合。微服务之间采用 RESTful 等轻量的传输协议，来应对更快的需求变更和更短的开发迭代周期。如下图所示： 微服务架构提出得比较早，但在这几年才逐渐流行起来。这是什么原因呢？一方面，微服务架构基于自身的特点，确实能够解决其他软件架构中存在的一些问题；另一方面，Docker + Kubernetes 等云原生技术这几年也发展了起来，能够很好地支撑微服务的部署和生命周期管理。 总体来说，微服务架构有下面这几个特点： 微服务遵循单一原则，每个微服务负责一个独立的上下文边界； 微服务架构提供的服务之间采用 RESTful 等轻量协议传输，比 ESB 更轻量； 每个服务都有自己独立的业务开发活动和周期； 微服务一般使用容器技术独立部署，运行在自己的独立进程中，合理分配其所需的系统资源。这样，开发者就可以更加方便地制定每个服务的优化方案，提高系统可维护性。 微服务架构有很多优点，但也存在着问题。因为一个应用被拆分成一个个的微服务，随着微服务的增多，就会引入一些问题，比如微服务过多导致服务部署复杂。微服务的分布式特点也带来了一些复杂度，比如需要提供服务发现能力、调用链难以追踪、测试困难，等等。服务之间相互依赖，有可能形成复杂的依赖链路，往往单个服务异常，其他服务都会受到影响，出现服务雪崩效应。 目前业界针对这些问题也有一些标准的解决方案，比如，可以通过 Kubernetes、Helm 和 CI/CD 技术，解决微服务部署复杂的问题。至于微服务的分布式特点所带来的复杂性，可以通过一些微服务开发框架来解决。一些业界比较知名的微服务开发框架，比如 Spring Cloud 和 Dubbo，已经很好地解决了上面的问题。另外，云原生相关的技术也可以解决微服务调用链跟踪复杂、故障排障困难等问题。 另外，在我的日常开发中，经常会有开发者把 SOA 架构和微服务架构给搞混，所以我在这里再来介绍下二者的相同点和不同点。 微服务架构是 SOA 架构设计思想的另一种实现方式，这是二者相同的地方。至于区别，主要有三个。理解了下面这三点，以后你在开发中就很容易区分它们了。 SOA 中的服务，其实只能属于某个应用的服务之一，微服务中的服务则是一个独立的服务，可以被多个应用共用。 SOA 强调尽可能多地共享，而微服务强调尽可能少地共享。 SOA 架构中，服务之间通过 ESB 来通信，而微服务中，服务之间通过轻量化机制，比如 RESTful 来实现通信。 Service Mesh 在讲微服务的时候，我提到微服务架构的一些问题可以通过一些微服务开发框架来解决，比如 Spring Cloud 和 Dubbo。但这里也有个问题：这些框架通常是侵入式的，比如语言只能限制在 Java，并且开发的时候要按框架的指定方式来开发。这个理念跟微服务的独立技术栈也是相反的。 2017 年底 Service Mesh（服务网格）的出现解决了这个问题，它是一种非侵入式技术，可以提供服务之间的网络调用、限流、熔断和服务监控等功能。Service Mesh 类似于 TCP/IP 协议，无需应用层感知，开发者只需要开发应用程序即可。所以，Service Mesh 是致力于解决服务间通讯的基础设施层，它具有下面这几个特点： 应用程序间通讯的中间层。 轻量级网络代理。 非侵入式，应用程序无感知。 可以将服务治理功能，例如重试、超时、监控、链路追踪、服务发现等功能，以及服务本身解耦。 Service Mesh 目前的发展比较火热，社区有很多优秀的 Service Mesh 开源项目，例如 Istio 、Linkerd 等。当前最受欢迎的开源项目是 Istio。 Istio 是一个完全开源的服务网格，作为透明的一层接入到现有的分布式应用程序里，提供服务治理等功能。它也是一个平台，拥有可以集成任何日志、遥测和策略系统的 API 接口。 Istio 的大概实现原理是：每个服务都会被注入一个 Sidecar（边车）组件，服务之间通信是先通过 Sidecar，然后 Sidecar 再将流量转发给另一个服务。因为所有流量都经过一个 Sidecar，所以可以通过 Sidecar 实现很多功能，比如认证、限流、调用链等。同时还有一个控制面，控制面通过配置 Sidecar 来实现各种服务治理功能。 目前 Istio 的最新版本是 1.8，1.8 版本的 Istio 架构图如下： 从图中你可以看到，Istio 主要包含两大平面。一个是数据平面（Data plane），由 Envoy Proxy 充当的 Sidecar 组成。另一个是控制平面（Control plane），主要由三大核心组件 Pilot、Citadel、Galley 组成。下面，我来分别介绍下这三大核心组件的功能。 Pilot：主要用来管理部署在 Istio 服务网格中的 Envoy 代理实例，为它们提供服务发现、流量管理以及弹性功能，比如 A/B 测试、金丝雀发布、超时、重试、熔断等。 Citadel：Istio 的核心安全组件，负责服务的密钥和数字证书管理，用于提供自动生成、分发、轮换及撤销密钥和数据证书的功能。 Galley：负责向 Istio 的其他组件提供支撑功能，可以理解为 Istio 的配置中心，它用于校验进入网络配置信息的格式内容正确性，并将这些配置信息提供给 Pilot。 FaaS 架构 这几年，以云函数为代表的 Serverless 技术异常火爆。伴随着 Serverless 技术的发展，一个新的软件开发模式也诞生了，这就是 FaaS 架构。FaaS 架构提供了一种比微服务更加服务碎片化的软件架构模式。简单来说，FaaS 架构就是把之前一个完整的业务拆分成一个个 Function 来部署，通过事件来触发底层 Function 的执行。 Function 里可能会调用第三方组件，比如数据库、消息队列服务等，这些第三方组件在 Serverless 架构中，统称为 BaaS（Backend as a Serivce）。BaaS 把这些后端的服务能力抽象成 API 让用户调用，用户不需要关注这些后端组件的高可用、扩缩容等运维层面的点，只需要去使用就可以了。 下面是 FaaS 架构的示意图： 从这张图里你可以看到，用","date":"2022-07-07 15:20:34","objectID":"/iam_service_deployment/:5:1","tags":["iam"],"title":"iam_service_deployment","uri":"/iam_service_deployment/"},{"categories":["iam"],"content":"应用生命周期管理技术：监控告警、日志、调用链 在这门课的 09 讲 中，我已经详细介绍了应用生命周期管理技术的演进。这里我们可以再回顾一下：应用生命周期，最开始主要是通过研发模式来管理的，按时间线先后出现了瀑布模式、迭代模式、敏捷模式。接着，为了解决研发模式中的一些痛点，出现了另一种管理技术，也就是 CI/CD 技术。随着 CI/CD 技术的成熟，又催生了另一种更高级的管理技术 DevOps。 其他的细节内容，如果有遗忘，你可以返回 09 讲 再复习一下，这里就不再重复介绍了。接下来，对于应用生命周期管理技术，我会补充一些之前没有讲到的重要技术，包括下面这三个： 监控告警组件，Prometheus； 统一日志管理框架，EFK； 调用链跟踪组件，Jaeger。 需要说明的是，这些技术之间不存在演进关系，而是平级的，共同作为应用生命周期管理技术的补充。 监控告警组件：Prometheus 对于应用来说，监控告警功能是必不可少的一项功能，能够让开发者或运维人员及时感知到程序异常，并及时修复。另外，监控也能够收集一些有用的数据，供后面的运营分析使用。云原生技术栈中，也有很多开源的优秀监控告警项目，例如 Zabbix、Prometheus 等，其中最受欢迎的是Prometheus。 Prometheus 是一款开源的、自带时序数据库的监控告警系统。目前，Prometheus 已经成为 Kubernetes 集群中监控告警系统的标配。它具有下面这几个特点： 强大的多维度数据模型； 在多维度上灵活地查询语言； 不依赖分布式存储，单主节点工作； 通过基于 HTTP 的 pull 方式，采集时序数据； 可以通过 Push Gateway 进行时序列数据推送； 可以通过服务发现或者静态配置，去获取要采集的目标服务器； 多种可视化图表及仪表盘支持 (Grafana)。 Prometheus 的架构如下图所示： 从上图可以看出，Prometheus 的主要模块包括 Prometheus Server、Exporters、Pushgateway、Alertmanager 以及 Grafana 图形界面。这些模块，有些是可选的，有些是必选的，大部分组件使用 Golang 编写。下面我来分别介绍下。 Prometheus Server（必选）：Prometheus 的核心服务，会定期从 Jobs/exporters 或者 Pushgateway 中拉取监控数据，并将时间序列（time-series）数据保存 TSDB 中，TSDB 是一个时间序列数据库。 Client Library（必选）: Prometheus 的客户端，应用程序使用 Client Library，可以很方便地生成 metrics，并暴露一个 API 接口，供 Prometheus server 从中拉取（pull）metrics 数据。 Pushgateway（可选）: 接收短期的 Jobs（Short-lived）推送（push）过来的 metrics 数据并缓存，供 Prometheus server 定期来 pull 这些监控数据。 Exporters（可选）: 以 agent 的形式运行在需要采集监控数据的应用服务器上，收集应用程序监控数据，并提供 API 接口，供 Prometheus server 来 pull metrics 数据。 Alertmanager（可选）: Prometheus 的告警组件，接收来自于 Prometheus server 的 alerts，将这些 alerts 去重、分组，并往配置的接收目的地发送告警。 Grafana（可选）：Grafana 是一款跨平台、开源的可视化数据展示工具，可以用来统计和展示 Prometheus 监控数据，并带有告警功能，采用 Go 语言开发。 Prometheus 大致的工作流程是： Prometheus Server 定期从配置好的 jobs 或者 Exporters 中拉 metrics，或者接收来自 Pushgateway 的 metrics，再或者从其他的 Prometheus Server 中拉 metrics。 Prometheus Server 在本地存储收集到的 metrics，并运行已经定义好的 alert.rules，记录新的时间序列，或者向 Alertmanager 推送警报。 Alertmanager 根据配置文件，对接收到的警报进行处理，发出告警。 Grafana 在图形界面中，可视化地展示采集数据。 Prometheus 会将所有采集到的样本数据以时间序列的方式保存在内存数据库中，并且定时保存到硬盘上。time-series 是按照时间戳和值的序列顺序存放的。每条 time-series 通过指标名称 (metrics name) 和一组标签集 (labelset) 命名，如下所示： \u003c--------------- metric ---------------------\u003e\u003c-timestamp -\u003e\u003c-value-\u003e http_request_total{status=\"200\", method=\"GET\"}@1434417560938 =\u003e 94355 http_request_total{status=\"200\", method=\"GET\"}@1434417561287 =\u003e 94334 http_request_total{status=\"404\", method=\"GET\"}@1434417560938 =\u003e 38473 http_request_total{status=\"404\", method=\"GET\"}@1434417561287 =\u003e 38544 http_request_total{status=\"200\", method=\"POST\"}@1434417560938 =\u003e 4748 http_request_total{status=\"200\", method=\"POST\"}@1434417561287 =\u003e 4785 在 time-series 中的每一个点，我们称为一个样本（sample）。样本由下面三个部分组成。 指标 (metric)：metric name 和描述当前样本特征的 labelsets。 时间戳 (timestamp)：一个精确到毫秒的时间戳。 样本值 (value)： 一个 folat64 的浮点型数据，表示当前样本的值。 统一日志管理框架：EFK 我们通过监控告警服务感知到程序异常，这时候需要开发者或者运维人员介入排障。排障最有效的手段，是查看日志。所以，对于一个应用来说，一个优秀的日志系统也是必不可少的功能。 在一个大型的分布式系统中，有很多组件，这些组件分别部署在不同的服务器上。如果系统出故障，需要查看日志排障。这时候，你可能需要登陆不同的服务器，查看不同组件的日志，这个过程是非常繁琐、低效的，也会导致排障时间变长。故障时间越久，意味着给客户带来的损失越大。 所以，在一个大型系统中，传统的日志查看手段已经满足不了我们的需求了。这时候，我们需要有一个针对分布式系统的日志解决方案。当前，业界有不少成熟的分布式日志解决方案，其中使用最多的是 EFK 日志解决方案。甚至可以说，EFK 已经成为分布式日志解决方案的事实标准。 EFK 中包含三个开源的软件，分别是 Elasticsearch、FlieBeat、Kibana。下面，我来介绍下这三个开源软件： Elasticsearch：简称 ES，是一个实时的、分布式的搜索引擎，通常用来索引和搜索大规模的日志数据，并支持全文、结构化的搜索。 FlieBeat：轻量的数据采集组件，以 agent 的方式运行在需要采集日志的服务器上。FlieBeat 采集指定的文件，并上报给 ES。如果采集日志量大，也可以上报给 Kafka，再由其他组件消费 Kafka 中的日志并转储到 ES 中。 Kibana：用于展示 ES 中存储的日志数据，支持通过图表进行高级数据分析及展示。 EFK 的架构图如下： 通过 Filebeat 采集所在服务器上各服务组件的日志，并上传到 Kafka 中。Logstash 消费 Kafka 中的日志，过滤后上报给 Elasticsearch 进行存储。最后，通过 Kibana 可视化平台来检索这些日志。Kibana 是通过调用 Elasticsearch 提供的 API 接口，来检索日志数据的。 当 Filebeat 的日志生产速度和 Logstash 的日志消费速度不匹配时，中间的 Kafka 服务，会起到削峰填谷的作用。 调用链跟踪组件：Jaeger 在云原生架构中，应用普遍采用微服务。一个应用包含多个微服务，微服务之间会相互调用，这会给排障带来很大的挑战。比如，当我们通过前端访问应用报错时，我们根本不知道具体哪个服务、哪个步骤出问题了。所以这时候，应用就需要有分布式链路追踪能力。目前，业界也有多种分布式链路追踪系统，但用得最多的是Jaeger。 Jaeger 是 Uber 推出的一款开源分布式追踪系统，兼容 OpenTracing API。这里我们先来介绍两个概念： OpenTracing：它是一套开源的调用链追踪标准，通过提供厂商无关、平台无关的 API，来支持开发人员方便地添加 / 更换追踪系统的实现。 分布式追踪系统：用于记录请求范围内的信息，是我们排查系统问题和系统性能的利器。分布式追踪系统种类繁多，但核心步骤都有三个，分别是代码埋点、数据存储和查询展示。 Jaeger 架构图如下： ","date":"2022-07-07 15:20:34","objectID":"/iam_service_deployment/:5:2","tags":["iam"],"title":"iam_service_deployment","uri":"/iam_service_deployment/"},{"categories":["iam"],"content":"总结 最后，我们通过下面这张图，来对整个云技术的演进之路做个整体性的回顾： 通过这张图你可以看到，每种技术并不是孤立存在的，而是相互促进的。在物理机阶段，我们用的是瀑布开发模式和单体架构；在虚拟机阶段，用得比较多的是敏捷开发模式和 SOA 架构；在容器这个阶段，则使用 CI/CD 的开发模式和微服务架构。 在 Serverless 阶段，软件架构仍然采用微服务，不过在一些触发器场景，也可能会编写一些 FaaS 架构的函数，部署在类似腾讯云云函数这样的 FaaS 平台上；底层系统资源主要使用 Serverless 容器，并配合 Kubernetes 资源编排技术。在一些触发器场景中，也可能会使用云函数。应用程序中的第三方服务（BaaS），也都是越来越 Serverless 化的服务。应用生命周期管理技术也会演进为 CI/CD/CO 这种模式，其中 CI/CD 更加智能化，自动化程度更高。 这张图里，阴影部分是我们当前所处的阶段：容器技术得到了大规模普及，业界也在积极探索 Serverless 技术，并取得了卓有成效的结果。 ","date":"2022-07-07 15:20:34","objectID":"/iam_service_deployment/:5:3","tags":["iam"],"title":"iam_service_deployment","uri":"/iam_service_deployment/"},{"categories":["iam"],"content":"45｜基于Kubernetes的云原生架构设计 前面两讲，我们一起看了云技术的演进之路。软件架构已经进入了云原生时代，云原生架构是当下最流行的软件部署架构。那么这一讲，我就和你聊聊什么是云原生，以及如何设计一种基于 Kubernetes 的云原生部署架构。 ","date":"2022-07-07 15:20:34","objectID":"/iam_service_deployment/:6:0","tags":["iam"],"title":"iam_service_deployment","uri":"/iam_service_deployment/"},{"categories":["iam"],"content":"云原生简介 云原生包含的概念很多，对于一个应用开发者来说，主要关注点是如何开发应用，以及如何部署应用。所以，这里我在介绍云原生架构的时候，会主要介绍应用层的云原生架构设计和系统资源层的云原生架构设计。 在设计云原生架构时，应用生命周期管理层的云原生技术，我们主要侧重在使用层面，所以这里我就不详细介绍应用生命周期管理层的云原生架构了。后面的云原生架构鸟瞰图中会提到它，你可以看看。另外，在介绍云原生时，也总是绕不开云原生计算基金会。接下来，我们就先来简单了解下 CNCF 基金会。 ","date":"2022-07-07 15:20:34","objectID":"/iam_service_deployment/:6:1","tags":["iam"],"title":"iam_service_deployment","uri":"/iam_service_deployment/"},{"categories":["iam"],"content":"CNCF（云原生计算基金会）简介 CNCF（Cloud Native Computing Foundation，云原生计算基金会），2015 年由谷歌牵头成立，目前已有一百多个企业与机构作为成员，包括亚马逊、微软、思科、红帽等巨头。CNCF 致力于培育和维护一个厂商中立的开源社区生态，用以推广云原生技术。 CNCF 目前托管了非常多的开源项目，其中有很多我们耳熟能详的项目，例如 Kubernetes、Prometheus、Envoy、Istio、etcd 等。更多的项目，你可以参考 CNCF 公布的Cloud Native Landscape，它给出了云原生生态的参考体系，如下图所示： ","date":"2022-07-07 15:20:34","objectID":"/iam_service_deployment/:6:2","tags":["iam"],"title":"iam_service_deployment","uri":"/iam_service_deployment/"},{"categories":["iam"],"content":"什么是云原生？ CNCF 官方在 2018 年发布了云原生 v1.0，并给出了定义： “云原生技术有利于各组织在公有云、私有云和混合云等新型动态环境中，构建和运行可弹性扩展的应用。云原生的代表技术包括容器、服务网格、微服务、不可变基础设施和声明式 API。 这些技术能够构建容错性好、易于管理和便于观察的松耦合系统。结合可靠的自动化手段，云原生技术使工程师能够轻松地对系统作出频繁和可预测的重大变更。” 简单点说，云原生（Cloud Native）是一种构建和运行应用程序的方法，是一套技术体系和方法论。云原生中包含了 3 个概念，分别是技术体系、方法论和云原生应用。整个云原生技术栈是围绕着 Kubernetes 来构建的，具体包括了以下核心技术栈： 这里来介绍下这些核心技术栈的基本内容。 容器：Kubernetes 的底层计算引擎，提供容器化的计算资源。 微服务：一种软件架构思想，用来构建云原生应用。 服务网格：建立在 Kubernetes 之上，作为服务间通信的底座，提供强大的服务治理功能。 声明式 API ：一种新的软件开发模式，通过描述期望的应用状态，来使系统更加健壮。 不可变基础设施：一种新的软件部署模式，应用实例一旦被创建，便只能重建不能更新，是现代运维的基础。 在 43 讲 和 44 讲 中，我介绍了容器、服务网格和微服务，这里再补充介绍下不可变基础设施和声明式 API。 不可变基础设施（Immutable Infrastructure）的构想，是由 Chad Fowler 于 2013 年提出的。具体来说就是：一个应用程序的实例，一旦被创建，就会进入只读的状态，后面如果想变更这个应用程序的实例，只能重新创建一个新的实例。通过这种模式，可以确保应用程序实例的一致性，这使得落地 DevOps 更加容易，并可以有效减少运维人员管理配置的负担。 声明式 API 是指我们通过工具描述期望的应用状态，并由工具保障应用一直处在我们期望的状态。 Kubernetes 的 API 设计，就是一种典型的声明式 API。例如，我们在创建 Deployment 时，在 Kubernetes YAML 文件中声明应用的副本数为2，即设置replicas: 2，Deployment Controller 就会确保应用的副本数一直为2。也就是说，如果当前副本数大于2，Deployment Controller 会删除多余的副本；如果当前副本数小于2，会创建新的副本。 声明式设计是一种设计理念，同时也是一种工作模式，它使得你的系统更加健壮。分布式系统环境可能会出现各种不确定的故障，面对这些组件故障，如果使用声明式 API ，你只需要查看对应组件的 API 服务器状态，再确定需要执行的操作即可。 ","date":"2022-07-07 15:20:34","objectID":"/iam_service_deployment/:6:3","tags":["iam"],"title":"iam_service_deployment","uri":"/iam_service_deployment/"},{"categories":["iam"],"content":"什么是云原生应用？ 上面，我介绍了什么是云原生，接下来再介绍下什么是云原生应用。 整体来看，云原生应用是指生而为云的应用，应用程序从设计之初就考虑到了云的环境，可以在云上以最佳姿势运行，充分利用和发挥云平台提供的各种能力。具体来看，云原生应用具有以下三大特点： 从应用生命周期管理维度来看，使用 DevOps 和 CI/CD 的方式，进行开发和交付。 从应用维度来看，以微服务原则进行划分设计。 从系统资源维度来看，采用 Docker + Kubernetes 的方式来部署。 看完上面的介绍，你应该已经对云原生和云原生应用有了一定的理解，接下来我就介绍一种云原生架构实现。因为云原生内容很多，所以这里的介绍只是起到抛砖引玉的作用，让你对云原生架构有初步的理解。至于在具体业务中如何设计云原生架构，你还需要根据业务、团队和技术栈等因素综合考虑。 ","date":"2022-07-07 15:20:34","objectID":"/iam_service_deployment/:6:4","tags":["iam"],"title":"iam_service_deployment","uri":"/iam_service_deployment/"},{"categories":["iam"],"content":"云原生架构包含很多内容，如何学习？ 云原生架构中包含了很多概念、技术，那么我们到底如何学习呢？在前面的两讲中，我分别从系统资源层、应用层、应用生命周期管理层介绍了云技术。这 3 个层次基本上构成了整个云计算的技术栈。 今天，我仍然会从这三个层次入手，来对整个云原生架构设计进行相对完整的介绍。每个层次涉及到的技术很多，这一讲我只介绍每一层的核心技术，通过这些核心技术来看每一层的构建方法。 另外，因为应用生命周期管理层涉及到的技术栈非常多，所以今天不会详细讲解每种生命周期管理技术的实现原理，但会介绍它们提供的能力。 除了功能层面的架构设计之外，我们还要考虑部署层面的架构设计。对于云原生架构的部署，通常我们需要关注以下两点： 容灾能力：容灾能力是指应用程序遇到故障时的恢复能力。在互联网时代，对应用的容灾能力有比较高的要求。理想情况是系统在出现故障时，能够无缝切换到另外一个可用的实例上，继续提供服务，并做到用户无感知。但在实际开发中，无缝切换在技术上比较难以实现，所以也可以退而求其次，允许系统在一定时间内不可用。通常这个时间需要控制在秒级，例如 5s。容灾能力可以通过负载均衡、健康检查来实现。 扩缩容能力：扩缩容能力指的是系统能够根据需要扩缩容，可以手动扩缩容，也可以自动扩缩容。互联网时代对扩缩容能力的要求也比较高，需要实现自动扩缩容。我们可以基于一些自定义指标，例如 CPU 使用率、内存使用率等来自动扩缩容。扩容也意味着能够承载更多的请求，提高系统的吞吐量；缩容，意味着能够节省成本。扩缩容能力的实现，需要借助于负载均衡和监控告警能力。 容灾能力和扩缩容能力都属于高可用能力。也就是说，在部署层面，需要我们的架构具备高可用能力。 接下来，我就重点介绍下系统资源层和应用层的云原生架构设计，并简单介绍下应用生命周期管理层的核心功能构建。在介绍完架构设计之后，我还会介绍下这些层面的高可用架构设计。 ","date":"2022-07-07 15:20:34","objectID":"/iam_service_deployment/:6:5","tags":["iam"],"title":"iam_service_deployment","uri":"/iam_service_deployment/"},{"categories":["iam"],"content":"系统资源层的云原生架构设计 先来看系统资源层面的云原生架构设计。对于一个系统来说，系统资源的架构是需要优先考虑的。在云原生架构中，当前的业界标准是通过 Docker 提供系统资源（例如 CPU、内存等），通过 Kubernetes 来编排 Docker 容器。Docker 和 Kubernetes 的架构，我在43 讲中介绍过，这里我主要介绍下系统资源层面的高可用架构设计。 基于 Docker+Kubernetes 的方案，高可用架构是通过 Kubernetes 高可用架构来实现的。要实现整个 Kubernetes 集群的高可用，我们需要分别实现以下两类高可用： Kubernetes 集群的高可用。 Kubernetes 集群中所部署应用的高可用。 我们来分别看下这两个高可用方案。 Kubernetes 集群高可用方案设计 通过43 讲的学习，我们知道 Kubernetes 由 kube-apiserver、kube-controller-manager、kube-scheduler、cloud-controller-manager、etcd、kubelet、kube-proxy、container runtime 8 大核心组件组成。 其中，kube-apiserver、kube-controller-manager、kube-scheduler、cloud-controller-manager、etcd 通常部署在 master 节点，kubelet、kube-proxy、container runtime 部署在 Node 节点上。实现 Kubernetes 集群的高可用，需要分别实现这 8 大核心组件的高可用。 Kubernetes 集群的高可用架构图如下： 上面图片展示的方案中，所有管理节点都部署了 kube-apiserver、kube-controller-manager、kube-scheduler、etcd 等组件。kube-apiserver 均与本地的 etcd 进行通信，etcd 在三个节点间同步数据；而 kube-controller-manager、kube-scheduler 和 cloud-controller-manager，也只与本地的 kube-apiserver 进行通信，或者通过负载均衡访问。 一个 Kubernetes 集群中有多个 Node 节点，当一个 Node 节点故障时，Kubernetes 的调度组件 kube-controller-manager 会将 Pod 调度到其他节点上，并将故障节点的 Pod 在其他可用节点上重建。也就是说，只要集群中有两个以上的节点，当其中一个 Node 故障时，整个集群仍然能够正常提供服务。换句话说，集群的 kubelet、kube-proxy、container runtime 组件可以是单点的，不用实现这些组件的高可用。 接下来，我们来看下 Master 节点各组件是如何实现高可用的。先来说下 kube-apiserver 组件的高可用方案设计。因为 kube-apiserver 是一个无状态的服务，所以可以通过部署多个 kube-apiserver 实例，其上挂载负载均衡的方式来实现。其他所有需要访问 kube-apiserver 的组件，都通过负载均衡来访问，以此实现 kube-apiserver 的高可用。 kube-controller-manager、cloud-controller-manager 和 kube-scheduler 因为是有状态的服务，所以它们的高可用能力不能通过负载均衡来实现。kube-controller-manager/kube-scheduler/cloud-controller-manager 通过–leader-elect=true 参数开启分布式锁机制，来进行 leader election。 你可以创建多个 kube-controller-manager/kube-scheduler/cloud-controller-manager 实例，同一时刻只有一个实例能够获取到锁，成为 leader，提供服务。如果当前 leader 故障，其他实例感知到 leader 故障之后会自动抢锁，成为 leader 继续提供服务。通过这种方式，我们实现了 kube-controller-manager/kube-scheduler/cloud-controller-manager 组件的高可用。 当 kube-apiserver、kube-controller-manager、kube-scheduler、cloud-controller-manager 故障时，我们期望这些组件能够自动恢复，这时候可以将这些组件以 Static Pod 的方式进行部署，这样当 Pod 故障时，上述实例就能够自动被拉起。 etcd 的高可用方案有下面这 3 种思路： 使用独立的 etcd 集群，独立的 etcd 集群自带高可用能力。 在每个 Master 节点上，使用 Static Pod 来部署 etcd，多个节点上的 etcd 实例数据相互同步。每个 kube-apiserver 只与本 Master 节点的 etcd 通信。 使用 CoreOS 提出的 self-hosted 方案，将 etcd 集群部署在 kubernetes 集群中，通过 kubernetes 集群自身的容灾能力来实现 etcd 的高可用。 这三种思路，需要你根据实际需要进行选择，在实际生产环境中，第二种思路用得最多。到这里，我们就实现了整个 Kubernetes 集群的高可用。接下来，我们来看下 Kubernetes 集群中，应用的高可用是如何实现的。 Kubernetes 应用的高可用 Kubernetes 自带了应用高可用能力。在 Kubernetes 中，应用以 Pod 的形式运行。你可以通过 Deployment/StatefulSet 来创建应用，并在 Deployment/StatefulSet 中指定多副本和 Pod 的健康检查方式。当 Pod 健康检查失败时，Deployment/StatefulSet 的控制器（ReplicaSet）会自动销毁故障 Pod，并创建一个新的 Pod，替换故障的 Pod。 你可能会问：当 Pod 故障时，怎么才能避免请求被调度到已故障的 Pod 上，造成请求失败？这里我也详细介绍下。 在 Kubernetes 中，我们可以通过 Kubernetes Service 或者负载均衡来访问这些 Pod。当通过负载均衡来访问 Pod 时，负载均衡后端的 RS（Real Server）实例其实就是 Pod。我们创建了多个 Pod，负载均衡可以自动根据 Pod 的健康状况来进行负载。 接下来，我们主要看下这个问题：当通过 Kubernetes Service 访问 Pod 时，如何实现高可用？ 高可用原理如下图所示： 在 Kubernetes 中，我们可以给每个 Pod 打上标签（Label），标签是一个 key-value 对，例如label: app=Nginx。当我们访问 Service 时，Service 会根据它配置的 Label Selector，匹配具有相同 Label 的 Pod，并将这些 Pod 的 endpoint 地址作为其后端 RS。 举个例子，你可以看看上面的图片：Service 的 Label Selector 是 Labelsapp=Nginx，这样就会选择我们创建的具有label: app=Nginx的 3 个 Pod 实例。这时候，Service 会根据其负载均衡策略，选取一个 Pod 将请求流量转发过去。当其中一个 Pod 故障时，Kubernetes 会自动将故障 Pod 的 endpoint 从 Service 后端对应的 RS 列表中剔除。 由 Deployment 创建的 ReplicaSet，这时候也会发现有一个 Pod 故障，健康的 Pod 实例数变为2，这时候跟其期望的值3不匹配，就会自动创建一个新的健康 Pod，替换掉故障的 Pod。因为新 Pod 满足 Service 的 Label Selector，所以新 Pod 的 endpoint 会被 Kubernetes 自动添加到 Service 对应的 endpoint 列表中。 通过上面这些操作，Service 后端的 RS 中，故障的 Pod IP 被新的、健康的 Pod IP 所替换，通过 Service 访问的后端 Pod 就都是健康的。这样，就通过 Service 实现了应用的高可用。 从上面的原理分析中，我们也可以发现，Service 本质上是一个负载均衡器。 Kubernetes 还提供了滚动更新（RollingUpdate）机制，来确保在发布时服务正常可用。这个机制的大致原理是：在更新时，先创建一个 Pod，再销毁一个 Pod，依次循环，直到所有的 Pod 都更新完成。在更新时，我们还可以控制每次更新的 Pod 数，以及最小可用的 Pod 数。 接下来，我们再来看下应用层的云原生架构设计和高可用设计。 ","date":"2022-07-07 15:20:34","objectID":"/iam_service_deployment/:6:6","tags":["iam"],"title":"iam_service_deployment","uri":"/iam_service_deployment/"},{"categories":["iam"],"content":"应用层的云原生架构设计 在云原生架构中，我们采用微服务架构来构建应用。所以，这里我主要围绕着微服务架构的构建方式来介绍。先和你谈谈我对微服务的理解。 从本质上来说，微服务是一个轻量级的 Web 服务，只不过在微服务场景下，我们通常考虑的不是单个微服务，而是更多地考虑由多个微服务组成的应用。也就是说，一个应用由多个微服务组成，多个微服务就带来了一些单个 Web 服务不会面临的问题，例如部署复杂、排障困难、服务依赖复杂、通信链路长，等等。在微服务场景下，除了编写单个微服务（轻量级的 Web 服务）之外，我们更多是要专注于解决应用微服务化所带来的挑战。所以，在我看来，微服务架构设计包括两个重要内容： 单个微服务的构建方式；解决应用微服务化带来的挑战。 微服务实现 我们可以通过两种方式来构建微服务： 采用 Gin、Echo 等轻量级 Web 框架。 采用微服务框架，例如 go-chassis、go-micro、go-kit等。 如果要解决应用微服务化带来的挑战，我们需要采用多种技术和手段，每种技术和手段会解决一个或一部分挑战。 综上，在我看来，微服务本质上是一个轻量级的 Web 服务，但又包含一系列的技术和手段，用来解决应用微服务化带来的挑战。微服务的技术栈如下图所示： 不同的技术栈可以由不同的方式来实现，并解决不同的问题： 监控告警、日志、CI/CD、分布式调度，可以由 Kubernetes 平台提供的能力来实现。 服务网关、权限验证、负载均衡、限流 / 熔断 / 降级，可以由网关来实现，例如 Tyk 网关。 进程间通信、REST/RPC 序列化，可以借助 Web 框架来实现，例如 Gin、Go Chassis、gRPC、Sprint Cloud。 分布式追踪可以由 Jaeger 来实现。 统一配置管理可以由 Apollo 配置中心来实现。 消息队列可以由 NSQ、Kafka、RabbitMQ 来实现。 上面的服务注册 / 服务发现，有 3 种实现方式： 通过 Kubernetes Service 来进行服务注册 / 服务发现，Kubernetes 自带服务注册 / 服务发现功能。使用此方式，我们不需要额外的开发。 通过服务中心来实现服务注册 / 服务发现功能。采用这种方式，需要我们开发并部署服务中心，服务中心通常可以使用 etcd/consul/mgmet 来实现，使用 etcd 的较多。 通过网关，来进行服务注册 / 服务发现。这种情况下，可以将服务信息直接上报给网关服务，也可以将服务信息上报到一个服务中心，例如 etcd 中，再由网关从服务中心中获取。 这里要注意，原生的 Kubernetes 集群是不支持监控告警、日志、CI/CD 等功能的。我们在使用 Kubernetes 集群时，通常会使用一个基于 Kubernetes 开发而来的 Kubernetes 平台，例如腾讯云容器服务 TKE。 在 Kubernetes 平台中，通常会基于一些优秀的开源项目，进行二次开发，来实现平台的监控告警、日志、CI/CD 等功能。 监控告警：基于 Prometheus 来实现。 日志：基于 EFK 日志解决方案来实现。 CI/CD：可以自己研发，也可以基于优秀的开源项目来实现，例如 drone。 微服务架构设计 上面我介绍了如何实现微服务，这里我再来具体讲讲，上面提到的各个组件 / 功能是如何有机组合在一起，共同构建一个微服务应用的。下面是微服务的架构图： 在上图中，我们将微服务应用层部署在 Kubernetes 集群中，在 Kubernetes 集群之上，可以构建微服务需要的其他功能，例如监控告警、CI/CD、日志、调用链等。这些功能共同完成应用的生命周期管理。 我们在微服务的最上面挂载负载均衡。客户端，例如移动端应用、Web 应用、API 调用等，都通过负载均衡来访问微服务。 微服务在启动时会将自己的 endpoint 信息（通常是ip:port格式）上报到服务中心。微服务也会定时上报自己的心跳到服务中心。在服务中心中，我们可以监控微服务的状态，剔除不健康的微服务，获取微服务之间的访问数据，等等。如果要通过网关调用微服务，或者需要使用网关做负载均衡，那我们还需要网关从服务中心中获取微服务的 endpoint 信息。 微服务高可用架构设计 我们再来看下如何设计微服务应用的高可用能力。 我们可以把所有微服务组件以 Deployment/StatefulSet 的形式部署在 Kubernetes 集群中，副本数至少设置为两个，更新方式为滚动更新，设置服务的监控检查，并通过 Kubernetes Service 或者负载均衡的方式访问服务。这样，我们就可以不用做任何改造，直接使用 Kubernetes 自有的容灾能力，实现微服务架构的高可用。 ","date":"2022-07-07 15:20:34","objectID":"/iam_service_deployment/:6:7","tags":["iam"],"title":"iam_service_deployment","uri":"/iam_service_deployment/"},{"categories":["iam"],"content":"云原生架构鸟瞰图 上面，我介绍了系统资源层和应用层的云原生架构设计，但还不能构成整个云原生架构设计。这里，我通过一张云原生架构鸟瞰图，来整体介绍下云原生架构的设计方案。 上图的云原生架构分为 4 层，除了前面提到的系统资源层、应用层、应用生命周期管理层之外，又加了统一接入层。接下来，我来介绍下这些层在云原生架构中的作用。 在最下面的系统资源层，我们除了提供传统的计算资源（物理机、虚拟机）之外，还可以提供容器化的计算资源和高可用的存储服务。其中，容器化的计算资源是基于传统的物理机 / 虚拟机来构建的。 在云原生架构中，我们更应该使用容器化的计算资源，通过 Docker 容器技术来隔离并对外提供计算资源，通过 Kubernetes 来编排容器。Docker + Kubernetes 的组合使用，可以构建出一个非常优秀的系统资源层。这个系统资源层，自带了资源管理、容器调度、自动伸缩、网络通信、服务发现、健康检查等企业应用需要的核心能力。 在云原生时代，这些系统资源除了具有容器化、轻量化的特点之外，还越来越倾向于朝着 Serverless 化的方向去构建：系统资源免申请、免运维，按需计费，具备极致的弹性伸缩能力，并能够缩容到 0。Serverless 化的系统资源，可以使开发者只聚焦在应用层的应用功能开发上，而不用再把时间浪费在系统层的运维工作上。 在系统资源层之上，就可以构建我们的应用层了。云原生架构中，应用的构建方式，基本上都是采用的微服务架构。开发一个微服务应用，我们可以使用微服务框架，也可以不使用。二者的区别是，微服务框架替我们完成了服务治理相关功能，让我们不需要再开发这些功能。 在我看来，这一点有利有弊。好处当然是节省了开发工作量。至于坏处，主要有两方面：一方面，在实现方式和实现思路上，微服务框架所集成的服务治理功能并不一定是最适合我们的方案。另一方面，使用微服务框架还意味着我们的应用会跟微服务框架耦合，不能自由选择服务治理技术和方式。所以，在实际开发中，你应该根据需要，自行选择微服务的构建方式。 一般来说，一个微服务框架中，至少集成了这些服务治理功能：配置中心、调用链追踪、日志系统、监控、服务注册 / 服务发现。 再往上，我们就实现了统一接入层。统一接入层中包含了负载均衡和网关两个组件，其中负载均衡作为服务的唯一入口，供 API、Web 浏览器、手机终端等客户端访问。通过负载均衡，可以使我们的应用在故障时，能够自动切换实例，在负载过高时能够水平扩容。负载均衡下面还对接了网关，网关提供了一些通用能力，例如安全策略、路由策略、流量策略、统计分析、API 管理等能力。 最后，我们还可以构建一系列的应用生命周期管理技术，例如服务编排、配置管理、日志、存储、审计、监控告警、消息队列、分布式链路追踪。这些技术中，一些可以基于 Kubernetes，集成在我们的 Kubernetes 平台中，另一些则可以单独构建，供所有产品接入。 ","date":"2022-07-07 15:20:34","objectID":"/iam_service_deployment/:6:8","tags":["iam"],"title":"iam_service_deployment","uri":"/iam_service_deployment/"},{"categories":["iam"],"content":"公有云版云原生架构 上面我们提到，云原生架构涉及到很多的技术栈。如果公司有能力，可以选择自己开发；如果觉得人力不够、成本太高，也可以使用公有云厂商已经开发好的云原生基础设施。使用云厂商的云原生基础设施，好处很明显：这些基础设施专业、稳定、免开发、免运维。 为了补全云原生架构设计版图，这里我也介绍一个公用云版的云原生架构设计。那么，公有云厂商会提供哪些云原生基础设施呢？这里我介绍下腾讯云提供的云原生解决方案。解决方案全景如下图所示： 可以看到，腾讯云提供了全栈的云原生能力。腾讯云基于底层的云原生能力，提供了一系列的云原生解决方案。这些解决方案，是已经设计好的云原生架构构建方案，可以帮助企业快速落地云原生架构，例如混合云解决方案、AI 解决方案、IoT 解决方案等。 那么，腾讯云底层提供了哪些云原生能力呢？我们一起来看下。在应用层，通过 TSF 微服务平台，我们可以实现微服务的构建，以及微服务的服务治理能力。另外，还提供了更多的应用构建架构，例如： Serverless Framework，可以构建 Serverless 应用。 CloudBase，云原生一体化应用开发平台，可以快速构建小程序、Web、移动应用。 … 在系统资源层，腾讯云提供了多种计算资源提供形态。例如：通过 TKE，可以创建原生的 Kubernetes 集群；通过 EKS，可以创建 Serverless 化的 Kubernetes 集群；通过TKE-Edge，可以创建能够纳管边缘节点的 Kubernetes 集群。此外，还提供了开源容器服务平台TKEStack，TKEStack 是一个非常优秀的容器云平台，在代码质量、稳定性、平台功能等方面，都在开源的容器云平台中处于龙头地位，也欢迎你 Star。 在应用生命周期管理这一层，提供了云原生的 etcd、Prometheus 服务。此外，还提供了 CLS 日志系统，供你保存并查询应用日志；提供了云监控，供你监控自己的应用程序；提供了容器镜像服务（TCR），用来保存 Docker 镜像；提供了 CODING DevOps 平台，用来支持应用的 CI/CD；提供了调用链跟踪服务（TDW），用来展示微服务的调用链。 在统一接入层，腾讯云提供了功能强大的 API 网关。此外，还提供了多种 Serverless 化的中间件服务，例如消息队列 TDMQ、云原生数据库 TDSQL 等。所有这些云原生基础设施，都有共同的特点，就是免部署、免运维。换句话说，在腾讯云，你可以只专注于使用编程语言编写你的业务逻辑，其他的一切都交给腾讯云来搞定。 ","date":"2022-07-07 15:20:34","objectID":"/iam_service_deployment/:6:9","tags":["iam"],"title":"iam_service_deployment","uri":"/iam_service_deployment/"},{"categories":["iam"],"content":"总结 云原生架构设计，包含了系统资源层、应用层、统一接入层和应用生命周期管理层 4 层。 在系统资源层，可以采用 Docker + Kubernetes 的方式来提供计算资源。我们所有的应用和应用生命周期管理相关的服务，都可以部署在 Kubernetes 集群中，利用 Kubernetes 集群的能力实现服务发现 / 服务注册、弹性伸缩、资源调度等核心能力。 在应用层，可以采用微服务架构，来构建我们的应用。具体构建时，我们可以根据需要，采用类似 Gin 这种轻量级的 Web 框架来构建应用，然后再实现旁路的服务治理功能；也可以采用集成了很多服务治理功能的微服务框架，例如 go-chassis、go-micro 等。 因为我们采用了微服务架构，为了能够将微服务的一些功能，例如：认证授权、限流等功能最大化的复用，我们又提供了统一接入层。可以通过 API 网关、负载均衡、服务网格等技术来构建统一接入层。 在应用生命周期管理这一层，我们可以实现一些云原生的管理平台，例如 DevOps、监控告警、日志、配置中心等，并使我们的应用以云原生化的方式接入这些平台，使用这些平台提供的能力。 最后，我还介绍了腾讯云的云原生基础设施。通过腾讯云提供的云原生能力，你可以专注于使用编程语言编写你的业务逻辑，其他的各种云原生能力，都可以交给云厂商来帮你实现。 ","date":"2022-07-07 15:20:34","objectID":"/iam_service_deployment/:6:10","tags":["iam"],"title":"iam_service_deployment","uri":"/iam_service_deployment/"},{"categories":["iam"],"content":"46 | 如何制作Docker镜像？ 要落地云原生架构，其中的一个核心点是通过容器来部署我们的应用。如果要使用容器来部署应用，那么制作应用的 Docker 镜像就是我们绕不开的关键一步。今天，我就来详细介绍下如何制作 Docker 镜像。 在这一讲中，我会先讲解下 Docker 镜像的构建原理和方式，然后介绍 Dockerfile 的指令，以及如何编写 Dockerfile 文件。最后，介绍下编写 Dockerfile 文件时要遵循的一些最佳实践。 ","date":"2022-07-07 15:20:34","objectID":"/iam_service_deployment/:7:0","tags":["iam"],"title":"iam_service_deployment","uri":"/iam_service_deployment/"},{"categories":["iam"],"content":"Docker 镜像的构建原理和方式 首先，我们来看下 Docker 镜像构建的原理和方式。我们可以用多种方式来构建一个 Docker 镜像，最常用的有两种： 通过docker commit命令，基于一个已存在的容器构建出镜像。 编写 Dockerfile 文件，并使用docker build命令来构建镜像。 上面这两种方法中，镜像构建的底层原理是相同的，都是通过下面 3 个步骤来构建镜像： 基于原镜像，启动一个 Docker 容器。 在容器中进行一些操作，例如执行命令、安装文件等。由这些操作产生的文件变更都会被记录在容器的存储层中。 将容器存储层的变更 commit 到新的镜像层中，并添加到原镜像上。 下面，我们来具体讲解这两种构建 Docker 镜像的方式。 通过docker commit命令构建镜像 我们可以通过docker commit来构建一个镜像，命令的格式为docker commit [选项] [\u003c仓库名\u003e[:\u003c标签\u003e]]。 下图中，我们通过 4 个步骤构建了 Docker 镜像ccr.ccs.tencentyun.com/marmotedu/iam-apiserver-amd64:test： 具体步骤如下： 执行docker ps获取需要构建镜像的容器 ID 48d1dbb89a7f。 执行docker pause 48d1dbb89a7f暂停48d1dbb89a7f容器的运行。 执行docker commit 48d1dbb89a7f ccr.ccs.tencentyun.com/marmotedu/iam-apiserver-amd64:test，基于容器 ID 48d1dbb89a7f构建 Docker 镜像。 执行docker images ccr.ccs.tencentyun.com/marmotedu/iam-apiserver-amd64:test，查看镜像是否成功构建。 这种镜像构建方式通常用在下面两个场景中： 构建临时的测试镜像； 容器被入侵后，使用docker commit，基于被入侵的容器构建镜像，从而保留现场，方便以后追溯。 除了这两种场景，我不建议你使用docker commit来构建生产现网环境的镜像。我这么说的主要原因有两个： 使用docker commit构建的镜像包含了编译构建、安装软件，以及程序运行产生的大量无用文件，这会导致镜像体积很大，非常臃肿。 使用docker commit构建的镜像会丢失掉所有对该镜像的操作历史，无法还原镜像的构建过程，不利于镜像的维护。 下面，我们再来看看如何使用Dockerfile来构建镜像。 通过Dockerfile来构建镜像 在实际开发中，使用Dockerfile来构建是最常用，也最标准的镜像构建方法。Dockerfile是 Docker 用来构建镜像的文本文件，里面包含了一系列用来构建镜像的指令。 docker build命令会读取Dockerfile的内容，并将Dockerfile的内容发送给 Docker 引擎，最终 Docker 引擎会解析Dockerfile中的每一条指令，构建出需要的镜像。 docker build的命令格式为docker build [OPTIONS] PATH | URL | -。PATH、URL、-指出了构建镜像的上下文（context），context 中包含了构建镜像需要的Dockerfile文件和其他文件。默认情况下，Docker 构建引擎会查找 context 中名为Dockerfile的文件，但你可以通过-f, –file选项，手动指定Dockerfile文件。例如： $ docker build -f Dockerfile -t ccr.ccs.tencentyun.com/marmotedu/iam-apiserver-amd64:test . 使用 Dockerfile 构建镜像，本质上也是通过镜像创建容器，并在容器中执行相应的指令，然后停止容器，提交存储层的文件变更。和用docker commit构建镜像的方式相比，它有三个好处： Dockerfile 包含了镜像制作的完整操作流程，其他开发者可以通过 Dockerfile 了解并复现制作过程。 Dockerfile 中的每一条指令都会创建新的镜像层，这些镜像可以被 Docker Daemnon 缓存。再次制作镜像时，Docker 会尽量复用缓存的镜像层（using cache），而不是重新逐层构建，这样可以节省时间和磁盘空间。 Dockerfile 的操作流程可以通过docker image history [镜像名称]查询，方便开发者查看变更记录。 这里，我们通过一个示例，来详细介绍下通过Dockerfile构建镜像的流程。 首先，我们需要编写一个Dockerfile文件。下面是 iam-apiserver 的Dockerfile文件内容： FROMcentos:centos8LABEL maintainer=\"\u003ccolin404@foxmail.com\u003e\"RUN ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtimeRUN echo \"Asia/Shanghai\" \u003e /etc/timezoneWORKDIR/opt/iamCOPY iam-apiserver /opt/iam/bin/ENTRYPOINT [\"/opt/iam/bin/iam-apiserver\"] 这里选择centos:centos8作为基础镜像，是因为centos:centos8镜像中包含了基本的排障工具，例如vi、cat、curl、mkdir、cp等工具。 接着，执行docker build命令来构建镜像： $ docker build -f Dockerfile -t ccr.ccs.tencentyun.com/marmotedu/iam-apiserver-amd64:test . 执行docker build后的构建流程为： 第一步，docker build会将 context 中的文件打包传给 Docker daemon。如果 context 中有.dockerignore文件，则会从上传列表中删除满足.dockerignore规则的文件。这里有个例外，如果.dockerignore文件中有.dockerignore或者Dockerfile，docker build命令在排除文件时会忽略掉这两个文件。如果指定了镜像的 tag，还会对 repository 和 tag 进行验证。 第二步，docker build命令向 Docker server 发送 HTTP 请求，请求 Docker server 构建镜像，请求中包含了需要的 context 信息。 第三步，Docker server 接收到构建请求之后，会执行以下流程来构建镜像： 创建一个临时目录，并将 context 中的文件解压到该目录下。 读取并解析 Dockerfile，遍历其中的指令，根据命令类型分发到不同的模块去执行。 Docker 构建引擎为每一条指令创建一个临时容器，在临时容器中执行指令，然后 commit 容器，生成一个新的镜像层。 最后，将所有指令构建出的镜像层合并，形成 build 的最后结果。最后一次 commit 生成的镜像 ID 就是最终的镜像 ID。 为了提高构建效率，docker build默认会缓存已有的镜像层。如果构建镜像时发现某个镜像层已经被缓存，就会直接使用该缓存镜像，而不用重新构建。如果不希望使用缓存的镜像，可以在执行docker build命令时，指定–no-cache=true参数。 Docker 匹配缓存镜像的规则为：遍历缓存中的基础镜像及其子镜像，检查这些镜像的构建指令是否和当前指令完全一致，如果不一样，则说明缓存不匹配。对于ADD、COPY指令，还会根据文件的校验和（checksum）来判断添加到镜像中的文件是否相同，如果不相同，则说明缓存不匹配。 这里要注意，缓存匹配检查不会检查容器中的文件。比如，当使用RUN apt-get -y update命令更新了容器中的文件时，缓存策略并不会检查这些文件，来判断缓存是否匹配。 最后，我们可以通过docker history命令来查看镜像的构建历史，如下图所示： 其他制作镜像方式 上面介绍的是两种最常用的镜像构建方式，还有一些其他的镜像创建方式，这里我简单介绍两种。 通过docker save和docker load命令构建 docker save用来将镜像保存为一个 tar 文件，docker load用来将 tar 格式的镜像文件加载到当前机器上，例如： # 在 A 机器上执行，并将 nginx-v1.0.0.tar.gz 复制到 B 机器 $ docker save nginx | gzip \u003e nginx-v1.0.0.tar.gz # 在 B 机器上执行 $ docker load -i nginx-v1.0.0.tar.gz 通过上面的命令，我们就在机器 B 上创建了nginx镜像。 通过docker export和docker import命令构建 我们先通过docker export 保存镜像，再通过docker import 加载镜像，具体命令如下： # 在 A 机器上执行，并将 nginx-v1.0.0.tar.gz 复制到 B 机器 $ docker export nginx \u003e nginx-v1.0.0.tar.gz # 在 B 机器上执行 $ d","date":"2022-07-07 15:20:34","objectID":"/iam_service_deployment/:7:1","tags":["iam"],"title":"iam_service_deployment","uri":"/iam_service_deployment/"},{"categories":["iam"],"content":"Dockerfile 指令介绍 上面，我介绍了一些与 Docker 镜像构建有关的基础知识。在实际生产环境中，我们标准的做法是通过 Dockerfile 来构建镜像，这就要求你会编写 Dockerfile 文件。接下来，我就详细介绍下如何编写 Dockerfile 文件。 Dockerfile 指令的基本格式如下： # Comment INSTRUCTION arguments INSTRUCTION是指令，不区分大小写，但我的建议是指令都大写，这样可以与参数进行区分。Dockerfile 中，以 # 开头的行是注释，而在其他位置出现的 # 会被当成参数，例如： # Comment RUN echo 'hello world # dockerfile' 一个 Dockerfile 文件中包含了多条指令，这些指令可以分为 5 类。 定义基础镜像的指令：FROM； 定义镜像维护者的指令：MAINTAINER（可选）； 定义镜像构建过程的指令：COPY、ADD、RUN、USER、WORKDIR、ARG、ENV、VOLUME、ONBUILD； 定义容器启动时执行命令的指令：CMD、ENTRYPOINT； 其他指令：EXPOSE、HEALTHCHECK、STOPSIGNAL。 其中，加粗的指令是编写 Dockerfile 时经常用到的指令，需要你重点了解下。我把这些常用 Dockerfile 指令的介绍放在了 GitHub 上，你可以看看这个Dockerfile 指令详解。 下面是一个 Dockerfile 示例： # 第一行必须指定构建该镜像所基于的容器镜像 FROM centos:centos8 # 维护者信息 MAINTAINER Lingfei Kong \u003ccolin404@foxmail.com\u003e # 镜像的操作指令 RUN ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime RUN echo \"Asia/Shanghai\" \u003e /etc/timezone WORKDIR /opt/iam COPY iam-apiserver /opt/iam/bin/ # 容器启动时执行指令 ENTRYPOINT [\"/opt/iam/bin/iam-apiserver\"] Docker 会顺序解释并执行 Dockerfile 中的指令，并且第一条指令必须是FROM，FROM 用来指定构建镜像的基础镜像。接下来，一般会指定镜像维护者的信息。后面是镜像操作的指令，最后会通过CMD或者ENTRYPOINT来指定容器启动的命令和参数。 ","date":"2022-07-07 15:20:34","objectID":"/iam_service_deployment/:7:2","tags":["iam"],"title":"iam_service_deployment","uri":"/iam_service_deployment/"},{"categories":["iam"],"content":"Dockerfile指令详解 Dockerfile中包含了大量的指令，这些指令完成的功能，使用的格式都不同。这里，我详细介绍下这些指令。 FROM 格式：FROM \u003cimage\u003e或FROM \u003cimage\u003e:\u003ctag\u003e FROM指令的功能是为后面的指令提供基础镜像，因此，Dockerfile必须以FROM指令作为第一条非注释指令。我们可以根据需要，选择任何有效的镜像作为基础镜像。 可以在一个Dockerfile文件中，使用多个 FROM 指令，来构建多个镜像。每个镜像构建完成之后，Docker会打印出该镜像的ID。如果 FROM 指令中没有指定镜像的tag，则默认tag是 latest 。如果镜像不存在，则报错退出。 MAINTAINER 格式：MAINTAINER \u003cname\u003e \u003cEmail\u003e 用来指定维护该Dockerfile的作者信息。 ENV ENV指令有两种格式： ENV \u003ckey\u003e \u003cvalue\u003e ENV \u003ckey1\u003e=\u003cvalue1\u003e \u003ckey2\u003e=\u003cvalue2\u003e ENV指令用来在镜像创建出来的容器中声明环境变量，被声明的环境变量可被后面的特定指令，例如：ENV、ADD、COPY、WORKDIR、EXPOSE、VOLUME、USER使用。其他指令引用格式为：$variableName或${variableName}。可以使用斜杠 \\ 来转义环境变量，例如：\\$test或者\\${test}，这样二者将会被分别转换为$test和${test}字符串，而不是环境变量所保存的值。这里要注意，ONBUILD指令不支持环境替换。 WORKDIR 格式：WORKDIR \u003c工作目录路径\u003e 该指令用于指定当前的工作目录，使用该命令后，接下来每一层的工作目录都会切换到指定的目录（除非重新使用 WORKDIR 切换）。 WORKDIR 的路径始终使用绝对路径。这可以保证指令的准确和可靠。 同时，使用 WORKDIR 来替代RUN cd ... \u0026\u0026 do-something 这样难以维护的指令。 COPY 格式：COPY \u003csrc\u003e \u003cdest\u003e COPY 指令复制本机上的 \u003csrc\u003e 文件或目录到镜像的 \u003cdest\u003e 路径下。若\u003cdest\u003e或\u003csrc\u003e以反斜杠/结尾则说明其指向的是目录，否则指向文件。 \u003csrc\u003e可以是多个文件或目录，但必须是上下文根目录中的相对路径。不能使用形如 COPY ../filename /filename这样的指令。此外，\u003csrc\u003e支持使用通配符指向所有匹配通配符的文件或目录，例如，COPY iam* /opt/表示添加所有以 iam 开头的文件到目录/opt/中。 \u003cdest\u003e可以是文件或目录，但必须是镜像中的绝对路径或者相对于WORKDIR的相对路径。 若\u003cdest\u003e是一个文件，则\u003csrc\u003e的内容会被复制到\u003cdest\u003e中；否则\u003csrc\u003e指向的文件或目录中的内容会被复制到\u003cdest\u003e目录中。当\u003csrc\u003e指定多个源时，\u003cdest\u003e必须是目录。如果\u003cdest\u003e在镜像中不存在，则会被自动创建。 ADD 格式：ADD \u003csrc\u003e \u003cdest\u003e ADD与COPY指令具有相似的功能，都支持复制本地文件到镜像的功能。但ADD指令还支持其他功能。在 ADD 指令中，\u003csrc\u003e可以是一个网络文件的下载地址， 比如 ADD http://example.com/iamctl.yaml /会在镜像中创建文件/iamctl.yaml。 \u003csrc\u003e还可以是一个压缩归档文件，该文件在复制到容器时会被解压提取，例如ADD iam.tar.xz /。但是若URL中的文件为归档文件，则不会被解压提取。 在编写Dockerfile时，推荐使用COPY，因为COPY只支持本地文件，它比 ADD 更加透明。 RUN RUN指令有两种格式： RUN \u003ccommand\u003e (shell格式) RUN [\"executable\", \"param1\", \"param2\"] (exec格式，推荐使用) RUN指令会在前一条命令创建出的镜像的基础上创建一个容器，并在容器中运行命令，在命令结束运行后提交容器为新镜像，新镜像被Dockerfile中的下一条指令使用。 当使用shell格式时，命令通过/bin/sh -c运行。当使用exec格式时，命令是直接运行的，即不通过shell来运行命令。这里要注意，exec格式中的参数会以 JSON 数组的格式被Docker解析，所以参数必须使用双引号而不是单引号。 因为exec格式不会在shell中执行，所以环境变量不会被替换。比如，执行RUN [\"echo\", \"$USER\"]指令时，$USER不会做变量替换。如果希望运行shell程序，指令可以写成 RUN [\"/bin/bash\", \"-c\", \"echo\", \"$USER\"]。 CMD CMD指令有3种格式: CMD \u003ccommand\u003e (shell格式) CMD [\"executable\", \"param1\", \"param2\"] (exec格式，推荐使用) CMD [\"param1\", \"param2\"] (为ENTRYPOINT指令提供参数) CMD指令提供容器运行时的默认命令或参数，一个Dockerfile中可以有多条CMD指令，但只有最后一条CMD指令有效。CMD [\"param1\", \"param2\"]格式用来跟ENTRYPOINT指令配合使用，CMD指令中的参数会添加到ENTRYPOINT指令中。当使用shell和exec格式时，命令在容器中的运行方式与RUN指令相同。如果在执行docker run 时指定了命令行参数，则会覆盖CMD指令中的命令。 ENTRYPOINT ENTRYPOINT指令有两种格式： ENTRYPOINT \u003ccommand\u003e (shell格式) ENTRYPOINT [\"executable\", \"param1\", \"param2\"] (exec格式，推荐格式) ENTRYPOINT指令和CMD指令类似，都可以让容器在每次启动时执行指定的命令，但二者又有不同。ENTRYPOINT只能是命令 ，而CMD可以是参数，也可以是指令。另外，docker run命令行参数可以覆盖CMD，但不能覆盖ENTRYPOINT。 当使用Shell格式时，ENTRYPOINT指令会忽略任何CMD指令和docker run命令的参数，并且会运行在bin/sh -c中。 推荐使用exec格式。使用exec格式时，docker run传入的命令行参数会覆盖CMD指令的内容并且追加到ENTRYPOINT指令的参数中。 一个Dockerfile中可以有多条ENTRYPOINT指令，但只有最后一条ENTRYPOINT指令有效。 ONBUILD 格式：ONBUILD [INSTRUCTION] ONBUILD 指令后面跟的是其它指令，例如 RUN , COPY 等。这些指令，在当前镜像构建时不会被执行，当以当前镜像为基础镜像，构建下一级镜像时才会被执行。 这里要注意，使用包含ONBUILD指令的Dockerfile，构建的镜像应该打上特殊的标签，比如：python:3.0-onbuild。另外，在ONBUILD指令中使用ADD或COPY指令时要特别注意。假如新的构建过程的上下文中缺失了被添加的资源，则新的构建过程会失败。我们可以通过给ONBUILD镜像添加特殊标签，来提示编写Dockerfile的开发人员要特别注意。 VOLUME VOLUME指令有两种格式： VOLUMN [\"\u003c路径1\u003e\", \"路径2\"...] VOLUMN \u003c路径\u003e 为了防止容器内的重要数据因为容器重启而丢失，且避免容器不断变大，应该将容器内的某些目录挂载到宿主机上。在Dockerfile中，我们可以通过 VOLUME 指令事先将某些目录挂载为匿名卷，这样在执行docker run时，如果没有指定 -v 选项，则默认会将VOLUMN指定的目录挂载为匿名卷 。 EXPOSE 格式：EXPOSE \u003c端口1\u003e [\u003c端口2\u003e ...] EXPOSE指定了该镜像生成容器时提供服务的端口（默认对外不暴露端口），可以配合docker run -P使用，也可以配合docker run --net=host使用。 LABEL 格式：LABEL \u003ckey\u003e=\u003cvalue\u003e \u003ckey\u003e=\u003cvalue\u003e \u003ckey\u003e=\u003cvalue\u003e ... LABEL 指令用来给镜像添加一些元数据（metadata）。 USER 格式：USER \u003c用户名\u003e[:\u003c用户组\u003e] USER指令设定一个用户或者用户ID，在执行RUN、CMD、ENTRYPOINT等指令时指定以那个用户得身份去执行。如果容器中的服务不需要以特权身份来运行，则可以使用 USER 指令，切换到非root用户，以此来增加安全性。 ARG 格式：ARG \u003c参数名\u003e[=\u003c默认值\u003e] 与ENV作用一致，但作用域不相同。通过ARG指定的环境变量仅在docker build的过程中有效，构建好的镜像内不存在此变量。虽然构建好的镜像内不存在此变量，但是使用docker history可以查看到。所以，敏感数据不建议使用ARG。 可以在docker build时用--build-arg \u003c参数名\u003e=\u003c值\u003e来覆盖ARG指定的参数值 。 HEALTHCHECK HEALTHCHECK指令有两种格式： HEALTHCHECK [选项] CMD \u003c命令\u003e：设置检查容器健康状况的命令 HEALTHCHECK NONE：用来屏蔽掉已有的健康检查指令。 定时检测容器进程是否健康，可以设置检查的时间间隔、超时时间、重","date":"2022-07-07 15:20:34","objectID":"/iam_service_deployment/:7:3","tags":["iam"],"title":"iam_service_deployment","uri":"/iam_service_deployment/"},{"categories":["iam"],"content":"Dockerfile 最佳实践 上面我介绍了 Dockerfile 的指令，但在编写 Dockerfile 时，只知道这些指令是不够的，还不能编写一个合格的 Dockerfile。我们还需要遵循一些编写 Dockerfile 的最佳实践。这里，我总结了一份编写 Dockerfile 的最佳实践清单，你可以参考。 建议所有的 Dockerfile 指令大写，这样做可以很好地跟在镜像内执行的指令区分开来。 在选择基础镜像时，尽量选择官方的镜像，并在满足要求的情况下，尽量选择体积小的镜像。**目前，Linux 镜像大小有以下关系：busybox \u003c debian \u003c centos \u003c ubuntu。**最好确保同一个项目中使用一个统一的基础镜像。如无特殊需求，可以选择使用debian:jessie或者alpine。 在构建镜像时，删除不需要的文件，只安装需要的文件，保持镜像干净、轻量。 使用更少的层，把相关的内容放到一个层，并使用换行符进行分割。这样可以进一步减小镜像的体积，也方便查看镜像历史。 不要在 Dockerfile 中修改文件的权限。因为如果修改文件的权限，Docker 在构建时会重新复制一份，这会导致镜像体积越来越大。 给镜像打上标签，标签可以帮助你理解镜像的功能，例如：docker build -t=\"nginx:3.0-onbuild”。 FROM指令应该包含 tag，例如使用FROM debian:jessie，而不是FROM debian。 充分利用缓存。Docker 构建引擎会顺序执行 Dockerfile 中的指令，而且一旦缓存失效，后续命令将不能使用缓存。为了有效地利用缓存，需要尽量将所有的 Dockerfile 文件中相同的部分都放在前面，而将不同的部分放在后面。 优先使用COPY而非ADD指令。和ADD相比，COPY 功能简单，而且也够用。ADD可变的行为会导致该指令的行为不清晰，不利于后期维护和理解。 推荐将CMD和ENTRYPOINT指令结合使用，使用 execl 格式的ENTRYPOINT指令设置固定的默认命令和参数，然后使用CMD指令设置可变的参数。 尽量使用 Dockerfile 共享镜像。通过共享 Dockerfile，可以使开发者明确知道 Docker 镜像的构建过程，并且可以将 Dockerfile 文件加入版本控制，跟踪起来。 使用.dockerignore忽略构建镜像时非必需的文件。忽略无用的文件，可以提高构建速度。 使用多阶段构建。多阶段构建可以大幅减小最终镜像的体积。例如，COPY指令中可能包含一些安装包，安装完成之后这些内容就废弃掉。下面是一个简单的多阶段构建示例： FROMgolang:1.11-alpine AS build# 安装依赖包RUN go get github.com/golang/mock/mockgen# 复制源码并执行build，此处当文件有变化会产生新的一层镜像层COPY . /go/src/iam/RUN go build -o /bin/iam# 缩小到一层镜像FROMbusyboxCOPY --from=build /bin/iam /bin/iamENTRYPOINT [\"/bin/iam\"]CMD [\"--help\"] ","date":"2022-07-07 15:20:34","objectID":"/iam_service_deployment/:7:4","tags":["iam"],"title":"iam_service_deployment","uri":"/iam_service_deployment/"},{"categories":["iam"],"content":"总结 如果你想使用 Docker 容器来部署应用，那么就需要制作 Docker 镜像。今天，我介绍了如何制作 Docker 镜像。 你可以使用这两种方式来构建 Docker 镜像： 通过 docker commit 命令，基于一个已存在的容器构建出镜像。 通过编写 Dockerfile 文件，并使用 docker build 命令来构建镜像。 这两种方法中，镜像构建的底层原理是相同的： 基于原镜像启动一个 Docker 容器。 在容器中进行一些操作，例如执行命令、安装文件等，由这些操作产生的文件变更都会被记录在容器的存储层中。 将容器存储层的变更 commit 到新的镜像层中，并添加到原镜像上。 此外，我们还可以使用 docker save / docker load 和 docker export / docker import 来复制 Docker 镜像。在实际生产环境中，我们标准的做法是通过 Dockerfile 来构建镜像。使用 Dockerfile 构建镜像，就需要你编写 Dockerfile 文件。Dockerfile 支持多个指令，这些指令可以分为 5 类，对指令的具体介绍你可以再返回复习一遍。 另外，我们在构建 Docker 镜像时，也要遵循一些最佳实践，具体你可以参考我给你总结的最佳实践清单。 ","date":"2022-07-07 15:20:34","objectID":"/iam_service_deployment/:7:5","tags":["iam"],"title":"iam_service_deployment","uri":"/iam_service_deployment/"},{"categories":["iam"],"content":"47 | 如何编写Kubernetes资源定义文件？ 在接下来的 48 讲，我会介绍如何基于腾讯云 EKS 来部署 IAM 应用。EKS 其实是一个标准的 Kubernetes 集群，在 Kubernetes 集群中部署应用，需要编写 Kubernetes 资源的 YAML（Yet Another Markup Language）定义文件，例如 Service、Deployment、ConfigMap、Secret、StatefulSet 等。 这些 YAML 定义文件里面有很多配置项需要我们去配置，其中一些也比较难理解。为了你在学习下一讲时更轻松，这一讲我们先学习下如何编写 Kubernetes YAML 文件。 ","date":"2022-07-07 15:20:34","objectID":"/iam_service_deployment/:8:0","tags":["iam"],"title":"iam_service_deployment","uri":"/iam_service_deployment/"},{"categories":["iam"],"content":"为什么选择 YAML 格式来定义 Kubernetes 资源？ 首先解释一下，我们为什么使用 YAML 格式来定义 Kubernetes 的各类资源呢？这是因为 YAML 格式和其他格式（例如 XML、JSON 等）相比，不仅能够支持丰富的数据，而且结构清晰、层次分明、表达性极强、易于维护，非常适合拿来供开发者配置和管理 Kubernetes 资源。 其实 Kubernetes 支持 YAML 和 JSON 两种格式，JSON 格式通常用来作为接口之间消息传递的数据格式，YAML 格式则用于资源的配置和管理。YAML 和 JSON 这两种格式是可以相互转换的，你可以通过在线工具json2yaml，来自动转换 YAML 和 JSON 数据格式。 例如，下面是一个 YAML 文件中的内容： apiVersion:v1kind:Servicemetadata:name:iam-apiserverspec:clusterIP:192.168.0.231externalTrafficPolicy:Clusterports:- name:httpsnodePort:30443port:8443protocol:TCPtargetPort:8443selector:app:iam-apiserversessionAffinity:Nonetype:NodePort 它对应的 JSON 格式的文件内容为： { \"apiVersion\": \"v1\", \"kind\": \"Service\", \"metadata\": { \"name\": \"iam-apiserver\" }, \"spec\": { \"clusterIP\": \"192.168.0.231\", \"externalTrafficPolicy\": \"Cluster\", \"ports\": [ { \"name\": \"https\", \"nodePort\": 30443, \"port\": 8443, \"protocol\": \"TCP\", \"targetPort\": 8443 } ], \"selector\": { \"app\": \"iam-apiserver\" }, \"sessionAffinity\": \"None\", \"type\": \"NodePort\" } } 我就是通过json2yaml在线工具，来转换 YAML 和 JSON 的，如下图所示： 在编写 Kubernetes 资源定义文件的过程中，如果因为 YAML 格式文件中的配置项缩进太深，导致不容易判断配置项的层级，那么，你就可以将其转换成 JSON 格式，通过 JSON 格式来判断配置型的层级。 如果想学习更多关于 YAML 的知识，你可以参考YAML 1.2 (3rd Edition)。这里，可以先看看我整理的 YAML 基本语法： 属性和值都是大小写敏感的。 使用缩进表示层级关系。 禁止使用 Tab 键缩进，只允许使用空格，建议两个空格作为一个层级的缩进。元素左对齐，就说明对齐的两个元素属于同一个级别。 使用 # 进行注释，直到行尾。 key: value格式的定义中，冒号后要有一个空格。 短横线表示列表项，使用一个短横线加一个空格；多个项使用同样的缩进级别作为同一列表。 使用 — 表示一个新的 YAML 文件开始。 现在你知道了，Kubernetes 支持 YAML 和 JSON 两种格式，它们是可以相互转换的。但鉴于 YAML 格式的各项优点，我建议你使用 YAML 格式来定义 Kubernetes 的各类资源。 ","date":"2022-07-07 15:20:34","objectID":"/iam_service_deployment/:8:1","tags":["iam"],"title":"iam_service_deployment","uri":"/iam_service_deployment/"},{"categories":["iam"],"content":"Kubernetes 资源定义概述 Kubernetes 中有很多内置的资源，常用的资源有 Deployment、StatefulSet、ConfigMap、Service、Secret、Nodes、Pods、Events、Jobs、DaemonSets 等。除此之外，Kubernetes 还有其他一些资源。如果你觉得 Kubernetes 内置的资源满足不了需求，还可以自定义资源。 Kubernetes 的资源清单可以通过执行以下命令来查看： $ kubectl api-resources NAME SHORTNAMES APIVERSION NAMESPACED KIND bindings v1 true Binding componentstatuses cs v1 false ComponentStatus configmaps cm v1 true ConfigMap endpoints ep v1 true Endpoints events ev v1 true Event 上述输出中，各列的含义如下。 NAME：资源名称。 SHORTNAMES：资源名称简写。 APIVERSION：资源的 API 版本，也称为 group。 NAMESPACED：资源是否具有 Namespace 属性。 KIND：资源类别。 这些资源有一些共同的配置，也有一些特有的配置。这里，我们先来看下这些资源共同的配置。下面这些配置是 Kubernetes 各类资源都具备的： ---apiVersion:\u003cstring\u003e# string类型，指定group的名称，默认为core。可以使用 `kubectl api-versions` 命令，来获取当前kubernetes版本支持的所有group。kind:\u003cstring\u003e# string类型，资源类别。metadata:\u003cObject\u003e# 资源的元数据。name:\u003cstring\u003e# string类型，资源名称。namespace:\u003cstring\u003e# string类型，资源所属的命名空间。lables:\u003cmap[string]string\u003e# map类型，资源的标签。annotations:\u003cmap[string]string\u003e# map类型，资源的标注。selfLink:\u003cstring\u003e# 资源的 REST API路径，格式为：/api/\u003cgroup\u003e/namespaces/\u003cnamespace\u003e/\u003ctype\u003e/\u003cname\u003e。例如：/api/v1/namespaces/default/services/iam-apiserverspec:\u003cObject\u003e# 定义用户期望的资源状态（disired state）。status:\u003cObject\u003e# 资源当前的状态，以只读的方式显示资源的最近状态。这个字段由kubernetes维护，用户无法定义。 你可以通过kubectl explain 命令来查看 Object 资源对象介绍，并通过kubectl explain .来查看的子对象的资源介绍，例如： $ kubectl explain service $ kubectl explain service.spec $ kubectl explain service.spec.ports Kubernetes 资源定义 YAML 文件，支持以下数据类型： string，表示字符串类型。 object，表示一个对象，需要嵌套多层字段。 map[string]string，表示由 key:value 组成的映射。 string，表示字串列表。 []object，表示对象列表。 boolean，表示布尔类型。 integer，表示整型。 ","date":"2022-07-07 15:20:34","objectID":"/iam_service_deployment/:8:2","tags":["iam"],"title":"iam_service_deployment","uri":"/iam_service_deployment/"},{"categories":["iam"],"content":"常用的 Kubernetes 资源定义 上面说了，Kubernetes 中有很多资源，其中 Pod、Deployment、Service、ConfigMap 这 4 类是比较常用的资源，我来一个个介绍下。 Pod 资源定义 下面是一个 Pod 的 YAML 定义： apiVersion:v1# 必须 版本号， 常用v1 apps/v1kind:Pod# 必须metadata:# 必须，元数据name:string# 必须，名称namespace:string# 必须，命名空间，默认上default,生产环境为了安全性建议新建命名空间分类存放labels:# 非必须，标签，列表值- name:stringannotations:# 非必须，注解，列表值- name:stringspec:# 必须，容器的详细定义containers:#必须，容器列表，- name:string#必须，容器1的名称image:string#必须，容器1所用的镜像imagePullPolicy:[Always|Never|IfNotPresent]#非必须，镜像拉取策略，默认是Alwayscommand:[string]# 非必须 列表值，如果不指定，则是一镜像打包时使用的启动命令args:[string]# 非必须，启动参数workingDir:string# 非必须，容器内的工作目录volumeMounts:# 非必须，挂载到容器内的存储卷配置- name:string# 非必须，存储卷名字，需与【@1】处定义的名字一致readOnly:boolean#非必须，定义读写模式，默认是读写ports:# 非必须，需要暴露的端口- name:string# 非必须 端口名称containerPort:int# 非必须 端口号hostPort:int# 非必须 宿主机需要监听的端口号，设置此值时，同一台宿主机不能存在同一端口号的pod， 建议不要设置此值proctocol:[tcp|udp]# 非必须 端口使用的协议，默认是tcpenv:# 非必须 环境变量- name:string# 非必须 ，环境变量名称value:string# 非必须，环境变量键值对resources:# 非必须，资源限制limits:# 非必须，限制的容器使用资源的最大值，超过此值容器会推出cpu:string# 非必须，cpu资源，单位是core，从0.1开始memory:string内存限制，单位为MiB,GiBrequests:# 非必须，启动时分配的资源cpu:stringmemory:stringlivenessProbe:# 非必须，容器健康检查的探针探测方式exec:# 探测命令command:[string]# 探测命令或者脚本httpGet:# httpGet方式path:string# 探测路径，例如 http://ip:port/pathport:numberhost:stringscheme:stringhttpHeaders:- name:stringvalue:stringtcpSocket:# tcpSocket方式，检查端口是否存在port:numberinitialDelaySeconds:0#容器启动完成多少秒后的再进行首次探测，单位为stimeoutSeconds:0#探测响应超时的时间,默认是1s,如果失败，则认为容器不健康，会重启该容器periodSeconds:0# 探测间隔时间，默认是10ssuccessThreshold:0# failureThreshold:0securityContext:privileged:falserestartPolicy:[Always|Never|OnFailure]# 容器重启的策略，nodeSelector:object# 指定运行的宿主机imagePullSecrets:# 容器下载时使用的Secrets名称，需要与valumes.secret中定义的一致- name:stringhostNetwork:falsevolumes:## 挂载的共享存储卷类型- name:string# 非必须，【@1】emptyDir:{}hostPath:path:stringsecret:# 类型为secret的存储卷，使用内部的secret内的items值作为环境变量secrectName:stringitems:- key:stringpath:stringconfigMap:## 类型为configMap的存储卷name:stringitems:- key:stringpath:string Pod 是 Kubernetes 中最重要的资源，我们可以通过 Pod YAML 定义来创建一个 Pod，也可以通过 DaemonSet、Deployment、ReplicaSet、StatefulSet、Job、CronJob 来创建 Pod。 Deployment 资源定义 Deployment 资源定义 YAML 文件如下： apiVersion:apps/v1kind:Deploymentmetadata:labels:# 设定资源的标签app:iam-apiservername:iam-apiservernamespace:defaultspec:progressDeadlineSeconds:10# 指定多少时间内不能完成滚动升级就视为失败，滚动升级自动取消replicas:1# 声明副本数，建议 \u003e= 2revisionHistoryLimit:5# 设置保留的历史版本个数，默认是10selector:# 选择器matchLabels:# 匹配标签app: iam-apiserver # 标签格式为key:value对strategy:# 指定部署策略rollingUpdate:maxSurge:1# 最大额外可以存在的副本数，可以为百分比，也可以为整数maxUnavailable:1# 表示在更新过程中能够进入不可用状态的 Pod 的最大值，可以为百分比，也可以为整数type:RollingUpdate# 更新策略，包括：重建(Recreate)、RollingUpdate(滚动更新)template:# 指定Pod创建模板。注意：以下定义为Pod的资源定义metadata:# 指定Pod的元数据labels:# 指定Pod的标签app:iam-apiserverspec:affinity:podAntiAffinity:# Pod反亲和性，尽量避免同一个应用调度到相同NodepreferredDuringSchedulingIgnoredDuringExecution:# 软需求- podAffinityTerm:labelSelector:matchExpressions:# 有多个选项，只有同时满足这些条件的节点才能运行 Pod- key:appoperator:In# 设定标签键与一组值的关系，In、NotIn、Exists、DoesNotExistvalues:- iam-apiservertopologyKey:kubernetes.io/hostnameweight:100# weight 字段值的范围是1-100。containers:- command:# 指定运行命令- /opt/iam/bin/iam-apiserver# 运行参数- --config=/etc/iam/iam-apiserver.yamlimage:ccr.ccs.tencentyun.com/lkccc/iam-apiserver-amd64:v1.0.6# 镜像名，遵守镜像命名规范imagePullPolicy:Always# 镜像拉取策略。IfNotPresent：优先使用本地镜像；Never：使用本地镜像，本地镜像不存在，则报错；Always：默认值，每次都重新拉取镜像# lifecycle: # kubernetes支持postStart和preStop事件。当一个容器启动后，Kubernetes将立即发送postStart事件；在容器被终结之前，Kubernetes将发送一个preStop事件name:iam-apiserver# 容器名称，与应用名称保持一致ports:# 端口设置- containerPort:8443# 容器暴露的端口name:secure# 端口名称protocol:TCP# 协议，TCP和UDPlivenessProbe:# 存活检查，检查容器是否正常，不正常则重启实例httpGet:# HTTP请求检查方法path:/healthz# 请求路径port:8080# 检查端口scheme:HTTP# 检查协议initialDelaySeconds:5# 启动延时，容器延时启动健康检查的时间periodSeconds:10# 间隔时间，进行健康检查的时间间隔successThreshold:1# 健康阈值，表示后端容器从失败到成功的连续健康检查成功次数failureThreshold:1# 不健康阈值，表示后端容器从成功到失败的连续健康检查成功次数timeoutSeconds:3# 响应超时，每次健康检查响应的最大超时时间readinessProbe:# 就绪检查，检查容器是否就绪，不就绪则停止转发流量到当前实例httpGet:# HTTP请求检查方法path:/healthz# 请求路径port:8080# 检查端口scheme:HTTP# 检查协议initialDelaySeconds:","date":"2022-07-07 15:20:34","objectID":"/iam_service_deployment/:8:3","tags":["iam"],"title":"iam_service_deployment","uri":"/iam_service_deployment/"},{"categories":["iam"],"content":"YAML 文件编写技巧 这里我主要介绍三个技巧。 1）使用在线的工具来自动生成模板 YAML 文件。 YAML 文件很复杂，完全从 0 开始编写一个 YAML 定义文件，工作量大、容易出错，也没必要。我比较推荐的方式是，使用一些工具来自动生成所需的 YAML。 这里我推荐使用k8syaml工具。k8syaml是一个在线的 YAML 生成工具，当前能够生成 Deployment、StatefulSet、DaemonSet 类型的 YAML 文件。k8syaml具有默认值，并且有对各字段详细的说明，可以供我们填参时参考。 2）使用kubectl run命令获取 YAML 模板： $ kubectl run --dry-run=client --image=nginx nginx -o yaml \u003e my-nginx.yaml $ cat my-nginx.yaml apiVersion: v1 kind: Pod metadata: creationTimestamp: null labels: run: nginx name: nginx spec: containers: - image: nginx name: nginx resources: {} dnsPolicy: ClusterFirst restartPolicy: Always status: {} 然后，我们可以基于这个模板，来修改配置，形成最终的 YAML 文件。 3）导出集群中已有的资源描述。 有时候，如果我们想创建一个 Kubernetes 资源，并且发现该资源跟集群中已经创建的资源描述相近或者一致的时候，可以选择导出集群中已经创建资源的 YAML 描述，并基于导出的 YAML 文件进行修改，获得所需的 YAML。例如： $ kubectl get deployment iam-apiserver -o yaml \u003e iam-authz-server.yaml 接着，修改iam-authz-server.yaml。通常，我们需要删除 Kubernetes 自动添加的字段，例如kubectl.kubernetes.io/last-applied-configuration、deployment.kubernetes.io/revision、creationTimestamp、generation、resourceVersion、selfLink、uid、status。 这些技巧可以帮助我们更好地编写和使用 Kubernetes YAML。 ","date":"2022-07-07 15:20:34","objectID":"/iam_service_deployment/:8:4","tags":["iam"],"title":"iam_service_deployment","uri":"/iam_service_deployment/"},{"categories":["iam"],"content":"使用 Kubernetes YAML 时的一些推荐工具 接下来，我再介绍一些比较流行的工具，你可以根据自己的需要进行选择。 kubeval kubeval可以用来验证 Kubernetes YAML 是否符合 Kubernetes API 模式。 安装方法如下： $ wget https://github.com/instrumenta/kubeval/releases/latest/download/kubeval-linux-amd64.tar.gz $ tar xf kubeval-linux-amd64.tar.gz $ mv kubeval $HOME/bin 安装完成后，我们对 Kubernetes YAML 文件进行验证： $ kubeval deployments/iam.invalid.yaml ERR - iam/templates/iam-configmap.yaml: Duplicate 'ConfigMap' resource 'iam' in namespace '' 根据提示，查看iam.yaml，发现在iam.yaml文件中，我们定义了两个同名的iam ConfigMap： apiVersion: v1 kind: ConfigMap metadata: name: iam data: {} --- # Source: iam/templates/iam-configmap.yaml apiVersion: v1 kind: ConfigMap metadata: name: iam data: iam-: \"\" iam-apiserver.yaml: | ... 可以看到，使用kubeval之类的工具，能让我们在部署的早期，不用访问集群就能发现 YAML 文件的错误。 kube-score kube-score能够对 Kubernetes YAML 进行分析，并根据内置的检查对其评分，这些检查是根据安全建议和最佳实践而选择的，例如： 以非 Root 用户启动容器。 为 Pods 设置健康检查。 定义资源请求和限制。 你可以按照这个方法安装： $ go get github.com/zegl/kube-score/cmd/kube-score 然后，我们对 Kubernetes YAML 进行评分： $ kube-score score -o ci deployments/iam.invalid.yaml [OK] iam-apiserver apps/v1/Deployment [OK] iam-apiserver apps/v1/Deployment [OK] iam-apiserver apps/v1/Deployment [OK] iam-apiserver apps/v1/Deployment [CRITICAL] iam-apiserver apps/v1/Deployment: The pod does not have a matching NetworkPolicy [CRITICAL] iam-apiserver apps/v1/Deployment: Container has the same readiness and liveness probe [CRITICAL] iam-apiserver apps/v1/Deployment: (iam-apiserver) The pod has a container with a writable root filesystem [CRITICAL] iam-apiserver apps/v1/Deployment: (iam-apiserver) The container is running with a low user ID [CRITICAL] iam-apiserver apps/v1/Deployment: (iam-apiserver) The container running with a low group ID [OK] iam-apiserver apps/v1/Deployment ... 检查的结果有OK、SKIPPED、WARNING和CRITICAL。CRITICAL是需要你修复的；WARNING是需要你关注的；SKIPPED是因为某些原因略过的检查；OK是验证通过的。如果你想查看详细的错误原因和解决方案，可以使用-o human选项，例如： $ kube-score score -o human deployments/iam.invalid.yaml 上述命令会检查 YAML 资源定义文件，如果有不合规的地方会报告级别、类别以及错误详情，如下图所示： 当然，除了 kubeval、kube-score 这两个工具，业界还有其他一些 Kubernetes 检查工具，例如config-lint、copper、conftest、polaris等。 这些工具，我推荐你这么来选择：首先，使用 kubeval 工具做最基本的 YAML 文件验证。验证通过之后，我们就可以进行更多的测试。如果你没有特别复杂的 YAML 验证要求，只需要用到一些最常见的检查策略，这时候可以使用 kube-score。如果你有复杂的验证要求，并且希望能够自定义验证策略，则可以考虑使用 copper。当然，polaris、config-lint、copper也值得你去尝试下。 ","date":"2022-07-07 15:20:34","objectID":"/iam_service_deployment/:8:5","tags":["iam"],"title":"iam_service_deployment","uri":"/iam_service_deployment/"},{"categories":["iam"],"content":"总结 今天，我主要讲了如何编写 Kubernetes YAML 文件。 YAML 格式具有丰富的数据表达能力、清晰的结构和层次，因此被用于 Kubernetes 资源的定义文件中。如果你要把应用部署在 Kubernetes 集群中，就要创建多个关联的 K8s 资源，如果要创建 K8s 资源，目前比较多的方式还是编写 YAML 格式的定义文件。 这一讲我介绍了 K8s 中最常用的四种资源（Pod、Deployment、Service、ConfigMap）的 YAML 定义的写法，你可以常来温习。 另外，在编写 YAML 文件时，也有一些技巧。比如，可以通过在线工具k8syaml来自动生成初版的 YAML 文件，再基于此 YAML 文件进行二次修改，从而形成终版。最后，我还给你分享了编写和使用 Kubernetes YAML 时，社区提供的多种工具。比如，kubeval 可以校验 YAML，kube-score 可以给 YAML 文件打分。了解了如何编写 Kubernetes YAML 文件，下一讲的学习相信你会进行得更顺利。 ","date":"2022-07-07 15:20:34","objectID":"/iam_service_deployment/:8:6","tags":["iam"],"title":"iam_service_deployment","uri":"/iam_service_deployment/"},{"categories":["iam"],"content":"48 | 基于腾讯云 EKS 的容器化部署实战 在 45 讲中，我介绍了一种基于 Kubernetes 的云原生架构设计方案。在云原生架构中，我们是通过 Docker + Kubernetes 来部署云原生应用的。那么这一讲，我就手把手教你如何在 Kubernetes 集群中部署好 IAM 应用。因为步骤比较多，所以希望你能跟着我完成每一个操作步骤。相信在实操的过程中，你也会学到更多的知识。 ","date":"2022-07-07 15:20:34","objectID":"/iam_service_deployment/:9:0","tags":["iam"],"title":"iam_service_deployment","uri":"/iam_service_deployment/"},{"categories":["iam"],"content":"准备工作 在部署 IAM 应用之前，我们需要做以下准备工作：开通腾讯云容器服务镜像仓库。安装并配置 Docker。准备一个 Kubernetes 集群。 ","date":"2022-07-07 15:20:34","objectID":"/iam_service_deployment/:9:1","tags":["iam"],"title":"iam_service_deployment","uri":"/iam_service_deployment/"},{"categories":["iam"],"content":"开通腾讯云容器服务镜像仓库 在 Kubernetes 集群中部署 IAM 应用，需要从镜像仓库下载指定的 IAM 镜像，所以首先需要有一个镜像仓库来托管 IAM 的镜像。我们可以选择将 IAM 镜像托管到DockerHub上，这也是 docker 运行时默认获取镜像的地址。 但因为 DockerHub 服务部署在国外，国内访问速度很慢。所以，我建议将 IAM 镜像托管在国内的镜像仓库中。这里我们可以选择腾讯云提供的镜像仓库服务，访问地址为容器镜像服务个人版。 如果你已经有腾讯云的镜像仓库，可以忽略腾讯云镜像仓库开通步骤。 在开通腾讯云镜像仓库之前，你需要注册腾讯云账号，并完成实名认证。开通腾讯云镜像仓库的具体步骤如下： 第一步，开通个人版镜像仓库。 登录容器服务控制台，选择左侧导航栏中的【镜像仓库】\u003e【个人版】。根据以下提示，填写相关信息，并单击【开通】进行初始化。如下图所示： 用户名：默认是当前用户的账号 ID，是你登录到腾讯云 Docker 镜像仓库的身份，可在 账号信息 页面获取。密码：是你登录到腾讯云 Docker 镜像仓库的凭证。 这里需要你记录用户名及密码，用于推送及拉取镜像。假如我们开通的镜像仓库，用户名为10000099xxxx，密码为iam59!z$。 这里要注意，10000099xxxx要替换成你镜像仓库的用户名。 第二步，登录到腾讯云 Registry（镜像仓库）。 在我们开通完 Registry，就可以登录 Registry 了。可以通过以下命令来登录腾讯云 Registry： $ docker login --username=[username] ccr.ccs.tencentyun.com 这里的 username 是腾讯云账号 ID，开通时已注册，可在 账号信息 页面获取。docker 命令会在后面安装。 第三步，新建镜像仓库命名空间。 如果想使用镜像仓库，那么你首先需要创建一个用来创建镜像的命名空间。上一步，我们开通了镜像仓库，就可以在“命名空间”页签新建命名空间了，如下图所示： 上图中，我们创建了一个叫marmotedu的命名空间。这里，镜像仓库服务、命名空间、镜像仓库、标签这几个概念你可能弄不清楚。接下来，我详细介绍下四者的关系，关系如下图所示： 先来看下我们使用镜像仓库的格式：\u003c镜像仓库服务地址\u003e/\u003c命名空间\u003e/\u003c镜像仓库\u003e:\u003c标签\u003e，例如ccr.ccs.tencentyun.com/marmotedu/iam-apiserver-amd64:v1.1.0。 如果想使用一个 Docker 镜像，我们首先需要开通一个镜像仓库服务（Registry），镜像仓库服务都会对外提供一个固定的地址供你访问。在 Registry 中，我们（User）可以创建一个或多个命名空间（Namespace），命名空间也可以简单理解为镜像仓库逻辑上的一个分组。接下来，就可以在 Namespace 中创建一个或多个镜像仓库，例如iam-apiserver-amd64、iam-authz-server-amd64、iam-pump-amd64等。针对每一个镜像仓库，又可以创建多个标签（Tag），例如v1.0.1、v1.0.2等。 \u003c镜像仓库\u003e:\u003c标签\u003e又称为镜像。镜像又分为私有镜像和公有镜像，公有镜像可供所有能访问 Registry 的用户下载使用，私有镜像只提供给通过授权的用户使用。 ","date":"2022-07-07 15:20:34","objectID":"/iam_service_deployment/:9:2","tags":["iam"],"title":"iam_service_deployment","uri":"/iam_service_deployment/"},{"categories":["iam"],"content":"安装 Docker 开通完镜像仓库之后，我们还需要安装 Docker，用来构建和测试 Docker 镜像。下面我来讲解下具体的安装步骤。 第一步，安装 Docker 前置条件检查。 需要确保 CentOS 系统启用了centos-extras yum 源，默认情况下已经启用，检查方式如下： $ cat /etc/yum.repos.d/CentOS-Extras.repo # Qcloud-Extras.repo [extras] name=Qcloud-$releasever - Extras baseurl=http://mirrors.tencentyun.com/centos/$releasever/extras/$basearch/os/ gpgcheck=1 enabled=1 gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-Qcloud-8 如果/etc/yum.repos.d/CentOS-Extras.repo文件存在，且文件中extras部分的enabled配置项值为1，说明已经启用了centos-extras yum 源。如果/etc/yum.repos.d/CentOS-Extras.repo 文件不存在，或者enabled 不为 1，则需要创建/etc/yum.repos.d/CentOS-Extras.repo 文件，并将上述内容复制进去。 第二步，安装 docker。 Docker 官方文档 Install Docker Engine on CentOS提供了 3 种安装方法: 通过 Yum 源安装。通过 RPM 包安装。通过脚本安装。 这里，我们选择最简单的安装方式：通过 Yum 源安装。它具体又分为下面 3 个步骤。 # 安装 docker。 sudo yum install -y yum-utils # 1. 安装 `yum-utils` 包，该包提供了 `yum-config-manager` 工具 sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo # 2. 安装 `docker-ce.repo` yum 源 sudo yum-config-manager --enable docker-ce-nightly docker-ce-test # 3. 启用 `nightly` 和 `test` yum 源 sudo yum install -y docker-ce docker-ce-cli containerd.io # 4. 安装最新版本的 docker 引擎和 containerd # 启动 docker。 $ sudo systemctl start docker # docker 的配置文件是 /etc/docker/daemon.json ，这个配置文件默认是没有的，需要我们手动创建： $ sudo tee /etc/docker/daemon.json \u003c\u003c EOF { \"bip\": \"172.16.0.1/24\", \"registry-mirrors\": [], \"graph\": \"/data/lib/docker\" } EOF 这里，我来解释下常用的配置参数。 registry-mirrors：仓库地址，可以根据需要修改为指定的地址。 graph：镜像、容器的存储路径，默认是 /var/lib/docker。如果你的 / 目录存储空间满足不了需求，需要设置 graph 为更大的目录。 bip：指定容器的 IP 网段。 配置完成后，需要重启 Docker： $ sudo systemctl restart docker 测试 Docker 是否安装成功。 $ sudo docker run hello-world Unable to find image 'hello-world:latest' locally latest: Pulling from library/hello-world b8dfde127a29: Pull complete Digest: sha256:0fe98d7debd9049c50b597ef1f85b7c1e8cc81f59c8d623fcb2250e8bec85b38 Status: Downloaded newer image for hello-world:latest ... Hello from Docker! This message shows that your installation appears to be working correctly. .... docker run hello-world命令会下载hello-world镜像，并启动容器，打印安装成功提示信息后退出。这里注意，如果你通过 Yum 源安装失败，可以尝试 Docker 官方文档 Install Docker Engine on CentOS提供的其他方式安装。 第三步，安装后配置。 安装成功后，我们还需要做一些其他配置。主要有两个，一个是配置 docker，使其可通过non-root用户使用，另一个是配置 docker 开机启动。 使用non-root用户操作 Docker 我们在 Linux 系统上操作，为了安全，需要以普通用户的身份登录系统并执行操作。所以，我们需要配置 docker，使它可以被non-root用户使用。具体配置方法如下： $ sudo groupadd docker # 1. 创建`docker`用户组 $ sudo usermod -aG docker $USER # 2. 将当前用户添加到`docker`用户组下 $ newgrp docker # 3. 重新加载组成员身份 $ docker run hello-world # 4. 确认能够以普通用户使用docker 如果在执行 sudo groupadd docker 时报 groupadd: group ‘docker’ already exists 错误，说明 docker 组已经存在了，可以忽略这个报错。如果你在将用户添加到 docker 组之前，使用 sudo 运行过 docker 命令，你可能会看到以下错误： WARNING: Error loading config file: /home/user/.docker/config.json - stat /home/user/.docker/config.json: permission denied 这个错误，我们可以通过删除~/.docker/目录来解决，或者通过以下命令更改~/.docker/目录的所有者和权限： $ sudo chown \"$USER\":\"$USER\" /home/\"$USER\"/.docker -R $ sudo chmod g+rwx \"$HOME/.docker\" -R 配置 docker 开机启动 大部分 Linux 发行版（RHEL、CentOS、Fedora、Debian、Ubuntu 16.04 及更高版本）使用 systemd 来管理服务，包括指定开启时启动的服务。在 Debian 和 Ubuntu 上，Docker 默认配置为开机启动。 在其他系统，我们需要手动配置 Docker 开机启动，配置方式如下（分别需要配置 docker 和 containerd 服务）：要在引导时为其他发行版自动启动 Docker 和 Containerd，你可以使用以下命令： $ sudo systemctl enable docker.service # 设置 docker 开机启动 $ sudo systemctl enable containerd.service # 设置 containerd 开机启动 如果要禁止docker、containerd开启启动，可以用这个命令： $ sudo systemctl disable docker.service # 禁止 docker 开机启动 $ sudo systemctl disable containerd.service # 禁止 containerd 开机启动 ","date":"2022-07-07 15:20:34","objectID":"/iam_service_deployment/:9:3","tags":["iam"],"title":"iam_service_deployment","uri":"/iam_service_deployment/"},{"categories":["iam"],"content":"安装docker-compose sudo yum install epel-release sudo yum install python3-pip sudo pip3 install --upgrade pip sudo pip3 install docker-compose #这里会报错：ModuleNotFoundError: No module named 'setuptools_rust' #解决方法：pip3 install -U pip setuptools docker-compose --version ","date":"2022-07-07 15:20:34","objectID":"/iam_service_deployment/:9:4","tags":["iam"],"title":"iam_service_deployment","uri":"/iam_service_deployment/"},{"categories":["iam"],"content":"准备一个 Kubernetes 集群 安装完 Docker 之后，我们还需要一个 Kubernetes 集群，来调度 docker 容器。安装 Kubernetes 集群极其复杂，这里选择一种最简单的方式来准备一个 Kubernetes 集群：购买一个腾讯云弹性集群（EKS）。 如果你想自己搭建 Kubernetes 集群，这里建议你购买 3 台腾讯云 CVM 机器，并参照follow-me-install-kubernetes-cluster教程来一步步搭建 Kubernetes 集群，CVM 机器建议的最小配置如下： EKS 简介 我先简单介绍一下 EKS 是什么。EKS（Elastic Kubernetes Service）即腾讯云弹性容器服务，是腾讯云容器服务推出的无须用户购买节点即可部署工作负载的服务模式。它完全兼容原生的 Kubernetes，支持使用原生方式创建及管理资源，按照容器真实的资源使用量计费。弹性容器服务 EKS 还扩展支持腾讯云的存储及网络等产品，同时确保用户容器的安全隔离，开箱即用。 EKS 费用 那它是如何收费呢？EKS 是全托管的 Serverless Kubernetes 服务，不会收取托管的 Master、etcd 等资源的费用。弹性集群内运行的工作负载采用后付费的按量计费模式，费用根据实际配置的资源量按使用时间计算。也就是说：Kubernetes 集群本身是免费的，只有运行工作负载，消耗节点资源时收费。 EKS 有 3 种计费模式：预留券、按量计费、竞价模式，这里我建议选择按量计费。按量计费，支持按秒计费，按小时结算，随时购买随时释放，从专栏学习的角度来说，费用是最低的。EKS 会根据工作负载申请的 CPU、内存数值以及工作负载的运行时间来核算费用，具体定价，你可以参考：定价|弹性容器服务。 这里我通过例子来说明一下费用问题，IAM 应用会部署 4 个 Deployment，每个 Deployment 一个副本： iam-apiserver：IAM REST API 服务，提供用户、密钥、策略资源的 CURD 功能的 API 接口。 iam-authz-server：IAM 资源授权服务，对外提供资源授权接口。 iam-pump：IAM 数据清洗服务，从 Redis 中获取授权日志，处理后保存在 MongoDB 中。 iamctl：IAM 应用的测试服务，登陆 iamctl Pod 可以执行 iamctl 命令和 smoke 测试脚本，完成对 IAM 应用的运维和测试。 上述 4 个 Deployment 中的 Pod 配置均为 0.25 核、512Mi 内存。 这里，我们根据 EKS 的费用计算公式 费用 = 相关计费项配置 × 资源单位时间价格 × 运行时间 计算 IAM 部署一天的费用： 总费用 = (4 x 1) x (0.25 x 0.12 + 0.5 x 0.05) x 24 = 4.8 元 也就是按最低配置部署 IAM 应用，运行一天的费用是 4.8 元（一瓶水的钱，就能学到如何将 IAM 应用部署在 Kubernetes 平台上，很值！）。你可能想这个计算公式里每个数值都代表什么呢？我来解释一下，其中： (4 x 1)：Kubernetes Pod 总个数（一共是 4 个 Deployment，每个 Pod 1 个副本）。0.25 x 0.12：连续运行 1 小时的 CPU 配置费用。0.5 x 0.05：连续运行 1 小时的内存配置费用。24：24 小时，也即一天。 这里需要注意，为了帮助你节省费用，上述配置都是最低配置。在实际生产环境中，建议的配置如下： 因为 iam-pump 组件是有状态的，并且目前没有实现抢占机制，所以副本数需要设置为 1。另外，Intel 按量计费的配置费用见下图： 在这里有个很重要的事情提醒你：学完本节课，销毁这些 Deployment，避免被继续扣费。建议腾讯云账户余额不要超过 50 元。 申请 EKS 集群 了解了 EKS 以及费用相关的问题，接下来我们看看如何申请 EKS 集群。你可以通过以下 5 步来申请 EKS 集群。在正式申请前，请先确保腾讯云账户有大于 10 元的账户余额，否则在创建和使用 EKS 集群的过程中可能会因为费用不足而报错。 创建腾讯云弹性集群 具体步骤如下：首先，登录容器服务控制台，选择左侧导航栏中的【弹性集群】。然后，在页面上方选择需创建弹性集群的地域，并单击【新建】。在“创建弹性集群”页面，根据以下提示设置集群信息。如下图所示： 页面中各选择项的意思，我来给你解释一下： 集群名称：创建的弹性集群名称，不超过 60 个字符。 Kubernetes 版本：弹性集群支持 1.12 以上的多个 Kubernetes 版本选择，建议选择最新的版本。 所在地域：建议你根据所在地理位置选择靠近的地域，可降低访问延迟，提高下载速度。 集群网络：已创建的弹性集群 VPC 网络，你可以选择私有网络中的子网用于弹性集群的容器网络，详情请见 私有网络（VPC） 。 容器网络：为集群内容器分配在容器网络地址范围内的 IP 地址。弹性集群的 Pod 会直接占用 VPC 子网 IP，请尽量选择 IP 数量充足且与其他产品使用无冲突的子网。 Service CIDR：集群的 ClusterIP Service 默认分配在所选 VPC 子网中，请尽量选择 IP 数量充足且与其他产品使用无冲突的子网。 集群描述：创建集群的相关信息，该信息将显示在“集群信息”页面。 设置完成后，单击【完成】即可开始创建，可在“弹性集群”列表页面查看集群的创建进度。等待弹性集群创建完成，创建完成后的弹性集群页面如下图所示： 我们创建的弹性集群 ID 为 cls-dc6sdos4。 开启外网访问 如果想访问 EKS 集群，需要先开启 EKS 的外网访问能力，开启方法如下：登录容器服务控制台 -\u003e 选择左侧导航栏中的【弹性集群】 -\u003e 进入 cls-dc6sdos4 集群的详情页中 -\u003e 选择【基本信息】 -\u003e 点击【外网访问】按钮。如下图所示： 这里要注意，开启外网访问时，为了安全，需要设置允许访问 kube-apiserver 的 IP 段。为了避免不必要的错误，外网访问地址我们设置为0.0.0.0/0 。如下图所示： 注意，只有测试时才可这么设置为 0.0.0.0/0 ，如果是生产环境，建议严格限制可以访问 kube-apiserver 的来源 IP。 安装 kubectl 命令行工具 如果要访问 EKS（标准的 Kubernetes 集群），比较高效的方式是通过 Kubernetes 提供的命令行工具 kubectl 来访问。所以，还需要安装 kubectl 工具。 安装方式如下： $ curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\" $ mkdir -p $HOME/bin $ mv kubectl $HOME/bin $ chmod +x $HOME/bin/kubectl 具体可参考安装和设置 kubectl。你可以通过以下命令来配置 kubectl 的 bash 自动补全： $ kubectl completion bash \u003e $HOME/.kube-completion.bash $ echo 'source $HOME/.kube-completion.bash' \u003e\u003e ~/.bashrc $ bash 下载并安装 kubeconfig 安装完 kubectl 工具之后，需要配置 kubectl 所读取的配置文件。这里注意，在上一步，我们开启了外网访问，开启后 EKS 会生成一个 kubeconfig 配置（ kubeconfig 即为 kubectl 的配置文件）。我们可以从页面下载并安装。在弹性集群的基本信息页面，点击【复制】按钮，复制 kubeconfig 文件内容，如下图所示： 复制后，将粘贴板的内容保存在$HOME/.kube/config文件中。需要先执行mkdir -p $HOME/.kube创建.kube目录，再将粘贴版中的内容写到 config 文件中。你可以通过以下命令，来测试 kubectl 工具是否成功安装和配置： $ kubectl get nodes NAME STATUS ROLES AGE VERSION eklet-subnet-lowt256k Ready \u003cnone\u003e 2d1h v2.5.21 如果输出了 Kubernetes 的 eklet 节点，并且节点状态为 Ready，说明 Kubernetes 集群运行正常，并且 kubectl 安装和配置正确。 EKS 集群开通集群内服务访问外网能力 因为 IAM 应用中的数据库：MariaDB、Redis、MongoDB 可能需要通过外网访问，所以还需要开通 EKS 中 Pod 访问外网的能力。EKS 支持通过配置 NAT 网关 和 路由表 来实现集群内服务访问外网。具体开启步骤，需要你查看腾讯云官方文档：通过 NAT 网关访问外网。 在开通过程中有以下两点需要你注意：在创建指向 NAT 网关的路由表步骤中，目的端要选择：0.0.0.0/0。在关联子网至路由表步骤中，只关联创建 EKS 集群时选择的子网。 如果你的数据库需要通过外网访问，这里一定要确保 EKS 集群成功开通集群内服务访问外网能力，否则部署 IAM 应","date":"2022-07-07 15:20:34","objectID":"/iam_service_deployment/:9:5","tags":["iam"],"title":"iam_service_deployment","uri":"/iam_service_deployment/"},{"categories":["iam"],"content":"安装 IAM 应用 上面，我们开通了镜像仓库、安装了 Docker 引擎、安装和配置了 Kubernetes 集群，那么接下来，我们就来看下如何将 IAM 应用部署到 Kubernetes 集群中。 假设 IAM 项目仓库根目录路径为 $IAM_ROOT，具体安装步骤如下： 配置scripts/install/environment.sh scripts/install/environment.sh文件中包含了各类自定义配置。你可能需要配置跟数据库相关的配置（当然，也可以都使用默认值）： MariaDB 配置：environment.sh 文件中以MARIADB_开头的变量。 Redis 配置：environment.sh 文件中以REDIS_开头的变量。 MongoDB 配置：environment.sh 文件中以MONGO_开头的变量。 其他配置，使用默认值。 创建 IAM 应用的配置文件 $ cd ${IAM_ROOT} $ make gen.defaultconfigs # 生成iam-apiserver、iam-authz-server、iam-pump、iamctl组件的默认配置文件 $ make gen.ca # 生成 CA 证书 上述命令会将 IAM 的配置文件存放在这个${IAM_ROOT}/_output/configs/目录下。 创建 IAM 命名空间 我们将 IAM 应用涉及到的各类资源，都创建在iam命名空间中。将 IAM 资源创建在独立的命名空间中，不仅方便维护，还可以有效避免影响其他 Kubernetes 资源。 $ kubectl create namespace iam 将 IAM 各服务的配置文件，以 ConfigMap 资源的形式保存在 Kubernetes 集群中 $ kubectl -n iam create configmap iam --from-file=${IAM_ROOT}/_output/configs/ $ kubectl -n iam get configmap iam NAME DATA AGE iam 4 13s 执行kubectl -n iam get configmap iam命令，可以成功获取创建的iam configmap。如果你觉得每次执行kubectl命令都要指定-n iam选项很繁琐，你可以使用以下命令，将kubectl上下文环境中的命名空间指定为iam。设置后，执行kubectl命令，默认在iam命名空间下执行： $ kubectl config set-context `kubectl config current-context` --namespace=iam 将 IAM 各服务使用的证书文件，以 ConfigMap 资源的形式创建在 Kubernetes 集群中 $ kubectl -n iam create configmap iam-cert --from-file=${IAM_ROOT}/_output/cert $ kubectl -n iam get configmap iam-cert NAME DATA AGE iam-cert 14 12s 执行kubectl -n iam get configmap iam-cert命令，可以成功获取创建的iam-cert configmap。 创建镜像仓库访问密钥 在准备阶段，我们开通了腾讯云镜像仓库服务（访问地址为 ccr.ccs.tencentyun.com），并创建了用户10000099xxxx，其密码为iam59!z$。接下来，我们就可以创建 docker-registry secret。Kubernetes 在下载 Docker 镜像时，需要 docker-registry secret 来进行认证。创建命令如下： $ kubectl -n iam create secret docker-registry ccr-registry --docker-server=ccr.ccs.tencentyun.com --docker-username=10000099xxxx --docker-password='iam59!z$' 创建 Docker 镜像，并 Push 到镜像仓库 将镜像 Push 到 CCR 镜像仓库，需要确保你已经登录到腾讯云 CCR 镜像仓库，如果没登录，可以执行以下命令来登录： $ docker login --username=[username] ccr.ccs.tencentyun.com 执行 make push 命令构建镜像，并将镜像 Push 到 CCR 镜像仓库： $ make push REGISTRY_PREFIX=ccr.ccs.tencentyun.com/marmotedu VERSION=v1.1.0 上述命令，会构建 iam-apiserver-amd64、iam-authz-server-amd64、iam-pump-amd64、iamctl-amd64 四个镜像，并将这些镜像 Push 到腾讯云镜像仓库的marmotedu命名空间下。构建的镜像如下： $ docker images|grep marmotedu ccr.ccs.tencentyun.com/marmotedu/iam-pump-amd64 v1.1.0 e078d340e3fb 10 seconds ago 244MB ccr.ccs.tencentyun.com/marmotedu/iam-apiserver-amd64 v1.1.0 5e90b67cc949 2 minutes ago 239MB ccr.ccs.tencentyun.com/marmotedu/iam-authz-server-amd64 v1.1.0 6796b02be68c 2 minutes ago 238MB ccr.ccs.tencentyun.com/marmotedu/iamctl-amd64 v1.1.0 320a77d525e3 2 minutes ago 235MB 修改 ${IAM_ROOT}/deployments/iam.yaml 配置 这里请你注意，如果在上一个步骤中，你构建的镜像 tag 不是 v1.1.0 ，那么你需要修改 ${IAM_ROOT}/deployments/iam.yaml 文件，并将 iam-apiserver-amd64、 iam-authz-server-amd64、 iam-pump-amd64、iamctl-amd64 镜像的 tag 修改成你构建镜像时指定的 tag。 部署 IAM 应用 $ kubectl -n iam apply -f ${IAM_ROOT}/deployments/iam.yaml 执行上述命令，会在iam命令空间下，创建一系列 Kubernetes 资源，可以使用以下命令，来获取这些资源的状态： $ kubectl -n iam get all NAME READY STATUS RESTARTS AGE pod/iam-apiserver-d8dc48596-wkhpl 1/1 Running 0 94m pod/iam-authz-server-6bc899c747-fbpbk 1/1 Running 0 94m pod/iam-pump-7dcbfd4f59-2w9vk 1/1 Running 0 94m pod/iamctl-6fc46b8ccb-gs62l 1/1 Running 1 98m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/iam-apiserver ClusterIP 192.168.0.174 \u003cnone\u003e 8443/TCP,8080/TCP,8081/TCP 101m service/iam-authz-server ClusterIP 192.168.0.76 \u003cnone\u003e 9443/TCP,9090/TCP 101m service/iam-pump ClusterIP 192.168.0.155 \u003cnone\u003e 7070/TCP 101m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/iam-apiserver 1/1 1 1 101m deployment.apps/iam-authz-server 1/1 1 1 101m deployment.apps/iam-pump 1/1 1 1 101m deployment.apps/iamctl 1/1 1 1 101m NAME DESIRED CURRENT READY AGE replicaset.apps/iam-apiserver-d8dc48596 1 1 1 101m replicaset.apps/iam-authz-server-6bc899c747 1 1 1 101m replicaset.apps/iam-pump-7dcbfd4f59 1 1 1 101m replicaset.apps/iamctl-6fc46b8ccb 1 1 1 101m 我们看到pod/iam-apiserver-d8dc48596-wkhpl、pod/iam-authz-server-6bc899c747-fbpbk、pod/iam-pump-7dcbfd4f59-2w9vk、pod/iamctl-6fc46b8ccb-gs6","date":"2022-07-07 15:20:34","objectID":"/iam_service_deployment/:9:6","tags":["iam"],"title":"iam_service_deployment","uri":"/iam_service_deployment/"},{"categories":["iam"],"content":"测试 IAM 应用 我们在iam命令空间下创建了一个测试 Deployment iamctl。你可以登陆iamctl Deployment 所创建出来的 Pod，执行一些运维操作和冒烟测试。登陆命令如下： $ kubectl -n iam exec -it `kubectl -n iam get pods -l app=iamctl | awk '/iamctl/{print $1}'` -- bash 登陆到iamctl-xxxxxxxxxx-xxxxx Pod 中后，就可以执行运维操作和冒烟测试了。 运维操作在 iamctl 容器中，你可以使用 iamctl 工具提供的各类功能，iamctl 以子命令的方式对外提供功能。命令执行效果见下图： 冒烟测试 # cd /opt/iam/scripts/install # ./test.sh iam::test::smoke 如果./test.sh iam::test::smoke命令，打印的输出中，最后一行为congratulations, smoke test passed!字符串，说明 IAM 应用安装成功。如下图所示： ","date":"2022-07-07 15:20:34","objectID":"/iam_service_deployment/:9:7","tags":["iam"],"title":"iam_service_deployment","uri":"/iam_service_deployment/"},{"categories":["iam"],"content":"销毁 EKS 集群及其资源 好了，到这里，你已经成功在 EKS 集群中部署了 IAM 应用，EKS 的使命也就完成了。接下来，为避免账户被持续扣费，需要删除 EKS 内的资源和集群。 删除 EKS 内创建的 IAM 资源 $ kubectl delete namespace iam 因为删除 Namespace 会删除 Namespace 下的所有资源，所以上述命令执行时间会久点。 删除 EKS 集群 首先，登录容器服务控制台，选择左侧导航栏中的【弹性集群】。然后，选择创建的 EKS 集群：cls-dc6sdos4，点击最右侧的【删除】按钮，删除 EKS 集群。如下图所示： ","date":"2022-07-07 15:20:34","objectID":"/iam_service_deployment/:9:8","tags":["iam"],"title":"iam_service_deployment","uri":"/iam_service_deployment/"},{"categories":["iam"],"content":"总结 云原生架构设计中，需要将 IAM 应用部署到 Kubernetes 集群中。所以，首先需要你准备一个 Kubernetes 集群。你可以自己购买腾讯云 CVM 机器搭建 Kubernetes 集群，但这种方式费用高、操作复杂。所以，我建议你直接申请一个 EKS 集群来部署 IAM 应用。EKS 集群是一个标准的 Kubernetes 集群，可以快速申请，并免运维。EKS 集群只收取实际的资源使用费用。在专栏学习过程中，部署 IAM 应用期间产生的资源使用费用其实是很低的，所以推荐使用这种方式来部署 IAM 应用。 有了 Kubernetes 集群，就可以直接通过以下命令来部署整个 IAM 应用： $ kubectl -n iam apply -f ${IAM_ROOT}/deployments/iam.yaml 应用部署起来之后，我们可以登陆到iamctl-xxxxxxxxxx-xxxxxPod，并执行以下命令来测试整个 IAM 应用是否被成功部署： # cd /opt/iam/scripts/install # ./test.sh iam::test::smoke ","date":"2022-07-07 15:20:34","objectID":"/iam_service_deployment/:9:9","tags":["iam"],"title":"iam_service_deployment","uri":"/iam_service_deployment/"},{"categories":["iam"],"content":"49 | 服务编排（上）：Helm服务编排基础知识 我们将应用部署在 Kubernetes 时，可能需要创建多个服务。我就见过一个包含了 40 多个微服务的超大型应用，每个服务又包含了多个 Kubernetes 资源，比如 Service、Deployment、StatefulSet、ConfigMap 等。相同的应用又要部署在不同的环境中，例如测试环境、预发环境、现网环境等，也就是说应用的配置也不同。 对于一个大型的应用，如果基于 YAML 文件一个一个地部署 Kubernetes 资源，是非常繁琐、低效的，而且这些 YAML 文件维护起来极其复杂，还容易出错。那么，有没有一种更加高效的方式？比如，像 Docker 镜像一样，将应用需要的 Kubernetes 资源文件全部打包在一起，通过这个包来整体部署和管理应用，从而降低应用部署和维护的复杂度。 答案是有。我们可以通过 Helm Chart 包来管理这些 Kubernetes 文件，并通过helm命令，基于 Chart 包来创建和管理应用。接下来，我就来介绍下 Helm 的基础知识，并给你演示下如何基于 Helm 部署 IAM 应用。 ","date":"2022-07-07 15:20:34","objectID":"/iam_service_deployment/:10:0","tags":["iam"],"title":"iam_service_deployment","uri":"/iam_service_deployment/"},{"categories":["iam"],"content":"Helm 基础知识介绍 Helm 目前是 Kubernetes 服务编排事实上的标准。Helm 提供了多种功能来支持 Kubernetes 的服务编排，例如 helm 命令行工具、Chart 包、Chart 仓库等。下面，我就来详细介绍下。 Helm 是什么？ Helm 是 Kubernetes 的包管理器，类似于 Python 的 pip ，centos 的 yum 。Helm 主要用来管理 Chart 包。Helm Chart 包中包含一系列 YAML 格式的 Kubernetes 资源定义文件，以及这些资源的配置，可以通过 Helm Chart 包来整体维护这些资源。 Helm 也提供了一个helm命令行工具，该工具可以基于 Chart 包一键创建应用，在创建应用时，可以自定义 Chart 配置。应用发布者可以通过 Helm 打包应用、管理应用依赖关系、管理应用版本，并发布应用到软件仓库；对于使用者来说，使用 Helm 后不需要编写复杂的应用部署文件，可以非常方便地在 Kubernetes 上查找、安装、升级、回滚、卸载应用程序。 Helm 最新的版本是 v3，Helm3 以 Helm2 的核心功能为基础，对 Chart repo、发行版管理、安全性和 library Charts 进行了改进。和 Helm2 比起来，Helm3 最明显的变化是删除了 Tiller（Helm2 是一种 Client-Server 结构，客户端称为 Helm，服务器称为 Tiller）。Helm3 还新增了一些功能，并废弃或重构了 Helm2 的部分功能，与 Helm2 不再兼容。此外，Helm3 还引入了一些新的实验功能，包括 OCI 支持。 Helm3 架构图如下： 上面的架构图中，核心是 Helm Client（helm命令）和 Helm Chart 包。helm命令可以从Chart Repository中下载 Helm Chart 包，读取kubeconfig文件，并构建 kube-apiserver REST API 接口的 HTTP 请求。通过调用 Kubernetes 提供的 REST API 接口，将 Chart 包中包含的所有以 YAML 格式定义的 Kubernetes 资源，在 Kubernetes 集群中创建。 这些资源以 Release 的形式存在于 Kubernetes 集群中，每个 Release 又包含多个 Kubernetes 资源，例如 Deployment、Pod、Service 等。 Helm 中的三大基本概念 要学习和使用 Helm，一定要了解 Helm 中的三大基本概念，Helm 的所有操作基本都是围绕着这些概念来进行的。下面我来介绍下 Helm 的三大基本概念。 Chart： 代表一个 Helm 包。它包含了在 Kubernetes 集群中运行应用程序、工具或服务所需的所有 YAML 格式的资源定义文件。 Repository（仓库）： 它是用来存放和共享 Helm Chart 的地方，类似于存放源码的 GitHub 的 Repository，以及存放镜像的 Docker 的 Repository。 Release：它是运行在 Kubernetes 集群中的 Chart 的实例。一个 Chart 通常可以在同一个集群中安装多次。每一次安装都会创建一个新的 Release。 我们为什么要使用 Helm？ 现在你对 Helm 已经有了一定了解，这里我再来详细介绍下为什么要使用 Helm。先来看下传统的应用部署模式： 我们有测试环境、预发环境、现网环境三个环境，每个环境中部署一个应用 A，应用 A 中包含了多个服务，每个服务又包含了自己的配置，不同服务之间的配置有些是共享的，例如配置A。每个服务由一个复杂的 Kubernetes YAML 格式的文件来定义并创建，可以看到如果靠传统的方式，去维护这些 YAML 格式文件，并在不同环境下使用不同的配置去创建应用，是一件非常复杂的工作，并且后期 YAML 文件和 Kubernetes 集群中部署应用的维护都很复杂。随着微服务规模越来越大，会面临以下挑战： 微服务化服务数量急剧增多，给服务管理带来了极大的挑战。服务数量急剧增多，增加了管理难度，对运维部署是一种挑战。服务数量的增多，对服务配置管理也提出了更高的要求。随着服务数量增加，服务依赖关系也变得更加复杂，服务依赖关系的管理难度增大。在环境信息管理方面，在新环境快速部署一个复杂应用变得更加困难。 所以，我们需要一种更好的方式，来维护和管理这些 YAML 文件和 Kubernetes 中部署的应用。Helm 可以帮我们解决上面这些问题。接下来，我们来看下 Helm 是如何解决这些问题的。 在 Helm 中，可以理解为主要包含两类文件：模板文件和配置文件。模板文件通常有多个，配置文件通常有一个。Helm 的模板文件基于text/template模板文件，提供了更加强大的模板渲染能力。Helm 可以将配置文件中的值渲染进模板文件中，最终生成一个可以部署的 Kubernetes YAML 格式的资源定义文件，如下图所示： 上图中，我们将以下配置渲染进了模板中，生成了 Kubernetes YAML 文件： replicas: 2 tag: latest common: username: colin password: iam1234 所以在 Helm 中，部署一个应用可以简化为Chart模板（多个服务） + Chart配置 -\u003e 应用，如下图所示： Chart 模板一个应用只用编写一次，可以重复使用。在部署时，可以指定不同的配置，从而将应用部署在不同的环境中，或者在同一环境中部署不同配置的应用。 ","date":"2022-07-07 15:20:34","objectID":"/iam_service_deployment/:10:1","tags":["iam"],"title":"iam_service_deployment","uri":"/iam_service_deployment/"},{"categories":["iam"],"content":"Helm 基本操作实战 上面，我介绍了 Helm 的一些基础知识，这里我们再来学习下如何使用 Helm 对应用进行生命周期管理。 前置条件 在开始之前，你需要确保你有一个可以使用的 Kubernetes 集群。目前最方便快捷、最经济的方式是申请一个腾讯云 EKS 集群。至于如何申请和访问，你可以参考 48 讲 “准备一个 Kubernetes 集群”部分的教程。这里再提醒下，用完集群后，记得删除集群资源，免得被持续扣费。 安装 Helm Helm 提供了多种安装方式，在能连通外网的情况下，可以通过脚本来安装，安装命令如下： $ mkdir -p $HOME/bin $ wget https://get.helm.sh/helm-v3.6.3-linux-amd64.tar.gz $ tar -xvzf helm-v3.6.3-linux-amd64.tar.gz $ mv linux-amd64/helm $HOME/bin $ chmod +x $HOME/bin/helm $ helm version version.BuildInfo{Version:\"v3.6.3\", GitCommit:\"d506314abfb5d21419df8c7e7e68012379db2354\", GitTreeState:\"clean\", GoVersion:\"go1.16.5\"} 如果执行helm version可以成功打印出 helm 命令的版本号，说明 Helm 安装成功。Helm 各版本安装包地址见 Helm Releases。 安装完helm命令后，可以安装helm命令的自动补全脚本。假如你用的 shell 是bash，安装方法如下： $ helm completion bash \u003e $HOME/.helm-completion.bash $ echo 'source $HOME/.helm-completion.bash' \u003e\u003e ~/.bashrc $ bash 执行 helm comp\u003c TAB \u003e，就会自动补全为helm completion。 ","date":"2022-07-07 15:20:34","objectID":"/iam_service_deployment/:10:2","tags":["iam"],"title":"iam_service_deployment","uri":"/iam_service_deployment/"},{"categories":["iam"],"content":"Helm 快速入门 你可以通过以下六个步骤，来快速创建一个 Chart 应用。 第一步，初始化一个 Helm Chart 仓库。 安装完 Helm 之后，就可以使用 helm 命令添加一个 Chart 仓库。类似于用来托管 Docker 镜像的 DockerHub、用来托管代码的 GitHub，Chart 包也有一个托管平台，当前比较流行的 Chart 包托管平台是Artifact Hub。 Artifact Hub 上有很多 Chart 仓库，我们可以添加需要的 Chart 仓库，这里我们添加 BitNami 提供的 Chart 仓库： $ helm repo add bitnami https://charts.bitnami.com/bitnami # 添加 Chart Repository $ helm repo list # 查看添加的 Repository 列表 添加完成后，我们可以通过helm search命令，来查询需要的 Chart 包。helm search支持两种不同的查询方式，这里我来介绍下。 helm search repo：从你使用 helm repo add 添加到本地 Helm 客户端中的仓库里查找。该命令基于本地数据进行搜索，无需连接外网。 helm search hub：从 Artifact Hub 中查找并列出 Helm Charts。 Artifact Hub 中存放了大量的仓库。 Helm 搜索使用模糊字符串匹配算法，所以你可以只输入名字的一部分。下面是一个helm search的示例： $ helm search repo bitnami NAME CHART VERSION APP VERSION DESCRIPTION bitnami/bitnami-common 0.0.9 0.0.9 DEPRECATED Chart with custom templates used in ... bitnami/airflow 10.2.8 2.1.2 Apache Airflow is a platform to programmaticall... bitnami/apache 8.6.1 2.4.48 Chart for Apache HTTP Server bitnami/argo-cd 1.0.2 2.0.5 Declarative, GitOps continuous delivery tool fo... bitnami/aspnet-core 1.3.14 3.1.18 ASP.NET Core is an open-source framework create... bitnami/cassandra 8.0.2 4.0.0 Apache Cassandra is a free and open-source dist... bitnami/cert-manager 0.1.15 1.5.1 Cert Manager is a Kubernetes add-on to automate... # ... and many more 第二步，安装一个示例 Chart。 查询到自己需要的 Helm Chart 后，就可以通过helm install命令来安装一个 Chart。helm install支持从多种源进行安装： Chart 的 Repository。 本地的 Chart Archive，例如helm install foo foo-1.0.0.tgz。 一个未打包的 Chart 路径，例如helm install foo path/to/foo。 一个完整的 URL，例如helm install foo https://example.com/charts/foo-1.0.0.tgz。 这里，我们选择通过bitnami/mysql Chart 包来安装一个 MySQL 应用。你可以执行 helm show chart bitnami/mysql 命令，来简单了解这个 Chart 的基本信息。 或者，你也可以执行 helm show all bitnami/mysql，获取关于该 Chart 的所有信息。 接下来，就可以使用helm install命令来安装这个 Chart 包了。安装命令如下： $ helm repo update # Make sure we get the latest list of charts $ helm install bitnami/mysql --generate-name NAME: mysql-1629528555 LAST DEPLOYED: Sat Aug 21 14:49:19 2021 NAMESPACE: default STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: ... 在上面的例子中，我们通过安装bitnami/mysql这个 Chart，创建了一个mysql-1629528555 Release。–generate-name参数告诉 Helm 自动为这个 Release 命名。在安装过程中，Helm 客户端会打印一些有用的信息，包括哪些资源已经被创建，Release 当前的状态，以及你是否还需要执行额外的配置步骤。例如，从上述例子的输出中，你可以获取到数据库的 Root 密码、登陆方式、更新方式等信息。 安装完之后，你可以使用 helm status 来追踪 Release 的状态。每当你执行 helm install 的时候，都会创建一个新的发布版本。所以一个 Chart 在同一个集群里面可以被安装多次，每一个都可以被独立地管理和升级。 helm install命令会将 templates 渲染成最终的 Kubernetes 能够识别的 YAML 格式，然后安装到 Kubernetes 集群中。helm install 功能非常强大，想了解更多功能，你可以参考这个指南：使用 Helm。 第三步，安装前自定义 Chart。 上一步中的安装方式只会使用 Chart 的默认配置选项，很多时候我们需要自定义 Chart 来指定我们想要的配置。使用 helm show values 可以查看 Chart 中的可配置选项： $ helm show values bitnami/mysql # 为了方便展示，我删除了 `helm show values`输出中的`#`注释 # ... and many more architecture: standalone auth: rootPassword: \"\" database: my_database username: \"\" password: \"\" replicationUser: replicator replicationPassword: \"\" existingSecret: \"\" forcePassword: false usePasswordFiles: false customPasswordFiles: {} initdbScripts: {} # ... and many more 然后，你可以使用 YAML 格式的文件，覆盖上述任意配置项，并在安装过程中使用该文件。 $ echo '{auth.database: iam, auth.username: iam, auth.password: iam59!z$}' \u003e values.yaml $ helm install bitnami/mysql -f values.yaml --generate-name 上述命令将为 MySQL 创建一个名称为 iam 的默认用户，密码为iam59!z$，并且授予该用户访问新建的 iam 数据库的权限。Chart 中的其他默认配置保持不变。安装过程中，有两种传递配置数据的方式。 -f, –values：使用 YAML 文件覆盖配置。可以指定多次，优先使用最右边的文件。–set：通过命令行的方式对指定配置项进行覆盖。 如果同时使用两种方式，则 –set 中的值会被合并到 –values 中，但是 –set 中的值优先级更高。在–set中覆盖的内容会被保存在 ConfigMap 中。你可以通过 helm get values 来查看指定 Release 中 –set 设置的值，也可以通过运行 helm upgrade 并指定 –reset-values 字段，来清除 –set中设置的值。 这里我讲解下–set的格式和限制。–set 选项使用0或多个key-value 对。最简单的用法类似于–set name=value，等价于下面这个 YAML 格式： name:value 多个值之间使用逗号分割，因此–set a=b,c=d 的 YAML 表示是： a:bc:d –set还支持更复杂的表达式。例如，–set outer.inner=value 被转换成了： outer:inner:value 列表使用花括号{}来表示。例如，–set name={a, b, c} 被转换成了： name:- a- b- c 从 2.5.0 版本开始，我们可以使用数组下标的语法来访问列表中的元素了。例如 –set servers[0].port=80 就变成了： servers:- port:80 多个值也可以通过这种方式来设置。–set servers[0] [0].host=``marmotedu 变成了： servers:- port:80","date":"2022-07-07 15:20:34","objectID":"/iam_service_deployment/:10:3","tags":["iam"],"title":"iam_service_deployment","uri":"/iam_service_deployment/"},{"categories":["iam"],"content":"Helm 命令 上面我介绍了 Helm 的一些命令的用法，如果你想查看 Helm 提供的所有命令，可以执行helm help。或者，你也可以执行helm -h来查看某个子命令的用法，例如： $ helm get -h This command consists of multiple subcommands which can be used to get extended information about the release, including: - The values used to generate the release - The generated manifest file - The notes provided by the chart of the release - The hooks associated with the release Usage: helm get [command] # ... and many more 我整理了一份命令列表，供你参考： 上面这些命令中，有些提供了子命令和命令行参数，具体你可以执行helm -h来查看。 ","date":"2022-07-07 15:20:34","objectID":"/iam_service_deployment/:10:4","tags":["iam"],"title":"iam_service_deployment","uri":"/iam_service_deployment/"},{"categories":["iam"],"content":"总结 今天，我介绍了 Helm 的基础知识，并给你演示了如何基于 Helm 部署 IAM 应用。当一个应用包含了很多微服务时，手动在 Kubernetes 集群中部署、升级、回滚这些微服务是一件非常复杂的工作。这时候，我们就需要一个服务编排方案来编排这些服务，从而提高服务部署和维护的效率。 目前业界提供了多种服务编排方案，其中最流行的是 Helm，Helm 已经成为一个事实上的 Kubernetes 服务编排标准。在 Helm 中，有 Chart、Repository 和 Release 三大基本概念。Chart 代表一个 Helm 包，里面包含了运行 Kubernetes 应用需要的所有资源定义 YAML 文件；Repository 是 Chart 仓库，用来存放和共享 Helm Chart；Release 是运行在 Kubernetes 集群中的 Chart 的实例。 我们可以通过 helm install [NAME] [CHART] [flags] 来安装一个 Chart 包；通过 helm upgrade [RELEASE] [CHART] [flags] 来更新一个 Helm Release；通过 helm uninstall RELEASE_NAME […] [flags] 来卸载一个 Helm Release。另外，helm 命令行工具还提供了其他的功能，你可以再回顾一遍。 ","date":"2022-07-07 15:20:34","objectID":"/iam_service_deployment/:10:5","tags":["iam"],"title":"iam_service_deployment","uri":"/iam_service_deployment/"},{"categories":["iam"],"content":"50 | 服务编排（下）：基于Helm的服务编排部署实战 上一讲，我介绍了 Helm 的基础知识，并带着你部署了一个简单的应用。掌握 Helm 的基础知识之后，今天我们就来实战下，一起通过 Helm 部署一个 IAM 应用。通过 Helm 部署 IAM 应用，首先需要制作 IAM Chart 包，然后通过 Chart 包来一键部署 IAM 应用。在实际开发中，我们需要将应用部署在不同的环境中，所以我也会给你演示下如何在多环境中部署 IAM 应用。 ","date":"2022-07-07 15:20:34","objectID":"/iam_service_deployment/:11:0","tags":["iam"],"title":"iam_service_deployment","uri":"/iam_service_deployment/"},{"categories":["iam"],"content":"制作 IAM Chart 包 在部署 IAM 应用之前，我们首先需要制作一个 IAM Chart 包。我们假设 IAM 项目源码根目录为${IAM_ROOT}，进入 ${IAM_ROOT}/deployments目录，在该目录下创建 Chart 包。具体创建流程分为四个步骤，下面我来详细介绍下。 第一步，创建一个模板 Chart。Chart 是一个组织在文件目录中的集合，目录名称就是 Chart 名称（没有版本信息）。你可以看看这个 Chart 开发指南 ，它介绍了如何开发你自己的 Chart。不过，这里你也可以使用 helm create 命令来快速创建一个模板 Chart，并基于该 Chart 进行修改，得到你自己的 Chart。创建命令如下： $ helm create iam helm create iam会在当前目录下生成一个iam目录，里面存放的就是 Chart 文件。Chart 目录结构及文件如下： $ tree -FC iam/ ├── charts/ # [可选]: 该目录中放置当前Chart依赖的其他Chart ├── Chart.yaml # YAML文件，用于描述Chart的基本信息，包括名称版本等 ├── templates/ # [可选]: 部署文件模版目录，模版使用的值来自values.yaml和由Tiller提供的值 │ ├── deployment.yaml # Kubernetes Deployment object │ ├── _helpers.tpl # 用于修改Kubernetes objcet配置的模板 │ ├── hpa.yaml # Kubernetes HPA object │ ├── ingress.yaml # Kubernetes Ingress object │ ├── NOTES.txt # [可选]: 放置Chart的使用指南 │ ├── serviceaccount.yaml │ ├── service.yaml │ └── tests/ # 定义了一些测试资源 │ └── test-connection.yaml └── values.yaml # Chart的默认配置文件 上面的目录中，有两个比较重要的文件：Chart.yaml 文件templates 目录 下面我来详细介绍下这两个文件。我们先来看 Chart.yaml 文件。Chart.yaml 用来描述 Chart 的基本信息，包括名称、版本等，内容如下： apiVersion:ChartAPI版本（必需）name:Chart名称（必需）version:语义化版本（必需）kubeVersion:兼容Kubernetes版本的语义化版本（可选）description:对这个项目的一句话描述（可选）type:Chart类型（可选）keywords:- 关于项目的一组关键字（可选）home:项目home页面的URL（可选）sources:- 项目源码的URL列表（可选）dependencies:# chart 必要条件列表 （可选）- name:Chart名称(nginx)version:Chart版本(\"1.2.3\")repository:（可选）仓库URL(\"https://example.com/charts\")或别名(\"@repo-name\")condition:（可选）解析为布尔值的YAML路径，用于启用/禁用Chart(e.g.subchart1.enabled)tags:# （可选）- 用于一次启用/禁用一组Chart的tagimport-values:# （可选）- ImportValue保存源值到导入父键的映射。每项可以是字符串或者一对子/父列表项alias:（可选）Chart中使用的别名。当你要多次添加相同的Chart时会很有用maintainers:# （可选）- name:维护者名字（每个维护者都需要）email:维护者邮箱（每个维护者可选）url:维护者URL（每个维护者可选）icon:用作icon的SVG或PNG图片URL（可选）appVersion:包含的应用版本（可选）。不需要是语义化，建议使用引号deprecated:不被推荐的Chart（可选，布尔值）annotations:example:按名称输入的批注列表（可选）. 我们再来看下templates 目录这个文件。templates 目录中包含了应用中各个 Kubernetes 资源的 YAML 格式资源定义模板，例如： apiVersion:v1kind:Servicemetadata:labels:app:{{.Values.pump.name}}name:{{.Values.pump.name}}spec:ports:- name:httpprotocol:TCP{{- toYaml.Values.pump.service.http|nindent4}}selector:app:{{.Values.pump.name}}sessionAffinity:Nonetype:{{.Values.serviceType}} {{ .Values.pump.name }}会被deployments/iam/values.yaml文件中pump.name的值替换。上面的模版语法扩展了 Go text/template包的语法： # 这种方式定义的模版，会去除test模版尾部所有的空行{{- define\"test\"}}模版内容{{- end}}# 去除test模版头部的第一个空行{{- template\"test\"}} 下面是用于 YAML 文件前置空格的语法： # 这种方式定义的模版，会去除test模版头部和尾部所有的空行{{- define\"test\"-}}模版内容{{- end-}}# 可以在test模版每一行的头部增加4个空格，用于YAML文件的对齐{{include\"test\"|indent4}} 最后，这里有三点需要你注意： Chart 名称必须是小写字母和数字，单词之间可以使用横杠-分隔，Chart 名称中不能用大写字母，也不能用下划线，.号也不行。 尽可能使用SemVer 2来表示版本号。 YAML 文件应该按照双空格的形式缩进 (一定不要使用 tab 键)。 第二步，编辑 iam 目录下的 Chart 文件。我们可以基于helm create生成的模板 Chart 来构建自己的 Chart 包。这里我们添加了创建 iam-apiserver、iam-authz-server、iam-pump、iamctl 服务需要的 YAML 格式的 Kubernetes 资源文件模板： $ ls -1 iam/templates/*.yaml iam/templates/hpa.yaml # Kubernetes HPA模板文件 iam/templates/iam-apiserver-deployment.yaml # iam-apiserver服务deployment模板文件 iam/templates/iam-apiserver-service.yaml # iam-apiserver服务service模板文件 iam/templates/iam-authz-server-deployment.yaml # iam-authz-server服务deployment模板文件 iam/templates/iam-authz-server-service.yaml # iam-authz-server服务service模板文件 iam/templates/iamctl-deployment.yaml # iamctl服务deployment模板文件 iam/templates/iam-pump-deployment.yaml # iam-pump服务deployment模板文件 iam/templates/iam-pump-service.yaml # iam-pump服务service模板文件 模板的具体内容，你可以查看deployments/iam/templates/。在编辑 Chart 时，我们可以通过 helm lint 验证格式是否正确，例如： $ helm lint iam ==\u003e Linting iam 1 chart(s) linted, 0 chart(s) failed 0 chart(s) failed 说明当前 Iam Chart 包是通过校验的。 第三步，修改 Chart 的配置文件，添加自定义配置。我们可以编辑deployments/iam/values.yaml文件，定制自己的配置。具体配置你可以参考deployments/iam/values.yaml。在修改 values.yaml 文件时，你可以参考下面这些最佳实践。 变量名称以小写字母开头，单词按驼峰区分，例如chickenNoodleSoup。 给所有字符串类型的值加上引号。 为了避免整数转换问题，将整型存储为字符串更好，并用 {{ int $value }} 在模板中将字符串转回整型。 values.yaml中定义的每个属性都应该文档化。文档字符串应该以它要描述的属性开头，并至少给出一句描述。例如： # serverHost is the host name for the webserverserverHost:example# serverPort is the HTTP listener port for the webserverserverPor","date":"2022-07-07 15:20:34","objectID":"/iam_service_deployment/:11:1","tags":["iam"],"title":"iam_service_deployment","uri":"/iam_service_deployment/"},{"categories":["iam"],"content":"IAM Chart 部署 上面，我们制作了 IAM 应用的 Chart 包，接下来我们就使用这个 Chart 包来一键创建 IAM 应用。IAM Chart 部署一共分为 10 个步骤，你可以跟着我一步步操作下。 第一步，配置scripts/install/environment.sh。scripts/install/environment.sh文件中包含了各类自定义配置，你主要配置下面这些跟数据库相关的就可以，其他配置使用默认值。 MariaDB 配置：environment.sh 文件中以MARIADB_开头的变量。Redis 配置：environment.sh 文件中以REDIS_开头的变量。MongoDB 配置：environment.sh 文件中以MONGO_开头的变量。 第二步，创建 IAM 应用的配置文件。 $ cd ${IAM_ROOT} $ make gen.defaultconfigs # 生成iam-apiserver、iam-authz-server、iam-pump、iamctl组件的默认配置文件 $ make gen.ca # 生成 CA 证书 上面的命令会将 IAM 的配置文件存放在目录${IAM_ROOT}/_output/configs/下。 第三步，创建 iam 命名空间。我们将 IAM 应用涉及到的各类资源都创建在iam命名空间中。将 IAM 资源创建在独立的命名空间中，不仅方便维护，还可以有效避免影响其他 Kubernetes 资源。 $ kubectl create namespace iam 第四步，将 IAM 各服务的配置文件，以 ConfigMap 资源的形式保存在 Kubernetes 集群中。 $ kubectl -n iam create configmap iam --from-file=${IAM_ROOT}/_output/configs/ $ kubectl -n iam get configmap iam NAME DATA AGE iam 4 13s 第五步，将 IAM 各服务使用的证书文件，以 ConfigMap 资源的形式保存在 Kubernetes 集群中。 $ kubectl -n iam create configmap iam-cert --from-file=${IAM_ROOT}/_output/cert $ kubectl -n iam get configmap iam-cert NAME DATA AGE iam-cert 14 12s 第六步，创建镜像仓库访问密钥。在准备阶段，我们开通了腾讯云镜像仓库服务，并创建了用户10000099``xxxx，密码为iam59!z$。 接下来，我们就可以创建 docker-registry secret 了。Kubernetes 在下载 Docker 镜像时，需要 docker-registry secret 来进行认证。创建命令如下： $ kubectl -n iam create secret docker-registry ccr-registry --docker-server=ccr.ccs.tencentyun.com --docker-username=10000099xxxx --docker-password='iam59!z$' 第七步，创建 Docker 镜像，并 Push 到镜像仓库。 $ make push REGISTRY_PREFIX=ccr.ccs.tencentyun.com/marmotedu VERSION=v1.1.0 第八步，安装 IAM Chart 包。在49 讲里，我介绍了 4 种安装 Chart 包的方法。这里，我们通过未打包的 IAM Chart 路径来安装，安装方法如下： $ cd ${IAM_ROOT} $ helm -n iam install iam deployments/iam NAME: iam LAST DEPLOYED: Sat Aug 21 17:46:56 2021 NAMESPACE: iam STATUS: deployed REVISION: 1 TEST SUITE: None 执行 helm install 后，Kubernetes 会自动部署应用，等到 IAM 应用的 Pod 都处在 Running 状态时，说明 IAM 应用已经成功安装： $ kubectl -n iam get pods|grep iam iam-apiserver-cb4ff955-hs827 1/1 Running 0 66s iam-authz-server-7fccc7db8d-chwnn 1/1 Running 0 66s iam-pump-78b57b4464-rrlbf 1/1 Running 0 66s iamctl-59fdc4995-xrzhn 1/1 Running 0 66s 第九步，测试 IAM 应用。我们通过helm install在iam命令空间下创建了一个测试 Deployment iamctl。你可以登陆iamctl Deployment 所创建出来的 Pod，执行一些运维操作和冒烟测试。登陆命令如下： $ kubectl -n iam exec -it `kubectl -n iam get pods -l app=iamctl | awk '/iamctl/{print $1}'` -- bash 登陆到iamctl-xxxxxxxxxx-xxxxx Pod 中后，你就可以执行运维操作和冒烟测试了。先来看运维操作。iamctl 工具以子命令的方式对外提供功能，你可以使用它提供的各类功能，如下图所示： 再来看冒烟测试： # cd /opt/iam/scripts/install # ./test.sh iam::test::smoke 如果./test.sh iam::test::smoke命令打印的输出中，最后一行为congratulations, smoke test passed!字符串，就说明 IAM 应用安装成功。如下图所示： 第十步，销毁 EKS 集群的资源。 $ kubectl delete namespace iam 你可以根据需要选择是否删除 EKS 集群，如果不需要了就可以选择删除。 ","date":"2022-07-07 15:20:34","objectID":"/iam_service_deployment/:11:2","tags":["iam"],"title":"iam_service_deployment","uri":"/iam_service_deployment/"},{"categories":["iam"],"content":"IAM 应用多环境部署 在实际的项目开发中，我们需要将 IAM 应用部署到不同的环境中，不同环境的配置文件是不同的，那么 IAM 项目是如何进行多环境部署的呢？ IAM 项目在configs目录下创建了多个 Helm values 文件（格式为values-{envName}-env.yaml）： values-test-env.yaml，测试环境 Helm values 文件。 values-pre-env.yaml，预发环境 Helm values 文件。 values-prod-env.yaml，生产环境 Helm values 文件。 在部署 IAM 应用时，我们在命令行指定-f参数，例如： $ helm -n iam install -f configs/values-test-env.yaml iam deployments/iam # 安装到测试环境。 ","date":"2022-07-07 15:20:34","objectID":"/iam_service_deployment/:11:3","tags":["iam"],"title":"iam_service_deployment","uri":"/iam_service_deployment/"},{"categories":["iam"],"content":"总结 这一讲，我们通过 helm create iam 创建了一个模板 Chart，并基于这个模板 Chart 包进行了二次开发，最终创建了 IAM 应用的 Helm Chart 包：deployments/iam。有了 Helm Chart 包，我们就可以通过 helm -n iam install iam deployments/iam 命令来一键部署好整个 IAM 应用。当 IAM 应用中的所有 Pod 都处在 Running 状态后，说明 IAM 应用被成功部署。 最后，我们可以登录 iamctl 容器，执行 test.sh iam::test::smoke 命令，来对 IAM 应用进行冒烟测试。 ","date":"2022-07-07 15:20:34","objectID":"/iam_service_deployment/:11:4","tags":["iam"],"title":"iam_service_deployment","uri":"/iam_service_deployment/"},{"categories":["iam"],"content":"51 | 基于 GitHub Actions 的 CI 实战 在 Go 项目开发中，我们要频繁地执行静态代码检查、测试、编译、构建等操作。如果每一步我们都手动执行，效率低不说，还容易出错。所以，我们通常借助 CI 系统来自动化执行这些操作。 当前业界有很多优秀的 CI 系统可供选择，例如 CircleCI、TravisCI、Jenkins、CODING、GitHub Actions 等。这些系统在设计上大同小异，为了减少你的学习成本，我选择了相对来说容易实践的 GitHub Actions，来给你展示如何通过 CI 来让工作自动化。这一讲，我会先介绍下 GitHub Actions 及其用法，再向你展示一个 CI 示例，最后给你演示下 IAM 是如何构建 CI 任务的。 ","date":"2022-07-07 15:20:34","objectID":"/iam_service_deployment/:12:0","tags":["iam"],"title":"iam_service_deployment","uri":"/iam_service_deployment/"},{"categories":["iam"],"content":"GitHub Actions 的基本用法 GitHub Actions 是 GitHub 为托管在 github.com 站点的项目提供的持续集成服务，于 2018 年 10 月推出。GitHub Actions 具有以下功能特性： 提供原子的 actions 配置和组合 actions 的 workflow 配置两种能力。 全局配置基于YAML 配置，兼容主流 CI/CD 工具配置。 Actions/Workflows 基于事件触发，包括 Event restrictions、Webhook events、Scheduled events、External events。 提供可供运行的托管容器服务，包括 Docker、VM，可运行 Linux、macOS、Windows 主流系统。 提供主流语言的支持，包括 Node.js、Python、Java、Ruby、PHP、Go、Rust、.NET。 提供实时日志流程，方便调试。 提供平台内置的 Actions与第三方提供的 Actions，开箱即用。 ","date":"2022-07-07 15:20:34","objectID":"/iam_service_deployment/:12:1","tags":["iam"],"title":"iam_service_deployment","uri":"/iam_service_deployment/"},{"categories":["iam"],"content":"GitHub Actions 的基本概念 在构建持续集成任务时，我们会在任务中心完成各种操作，比如克隆代码、编译代码、运行单元测试、构建和发布镜像等。GitHub 把这些操作称为 Actions。 Actions 在很多项目中是可以共享的，GitHub 允许开发者将这些可共享的 Actions 上传到GitHub 的官方 Actions 市场，开发者在 Actions 市场中可以搜索到他人提交的 Actions。另外，还有一个 awesome actions 的仓库，里面也有不少的 Action 可供开发者使用。如果你需要某个 Action，不必自己写复杂的脚本，直接引用他人写好的 Action 即可。整个持续集成过程，就变成了一个 Actions 的组合。 Action 其实是一个独立的脚本，可以将 Action 存放在 GitHub 代码仓库中，通过/的语法引用 Action。例如，actions/checkout@v2表示https://github.com/actions/checkout这个仓库，tag 是 v2。actions/checkout@v2也代表一个 Action，作用是安装 Go 编译环境。GitHub 官方的 Actions 都放在 github.com/actions 里面。 GitHub Actions 有一些自己的术语，下面我来介绍下。 workflow（工作流程）：一个 .yml 文件对应一个 workflow，也就是一次持续集成。一个 GitHub 仓库可以包含多个 workflow，只要是在 .github/workflow 目录下的 .yml 文件都会被 GitHub 执行。 job（任务）：一个 workflow 由一个或多个 job 构成，每个 job 代表一个持续集成任务。 step（步骤）：每个 job 由多个 step 构成，一步步完成。 action（动作）：每个 step 可以依次执行一个或多个命令（action）。 on：一个 workflow 的触发条件，决定了当前的 workflow 在什么时候被执行。 workflow 文件介绍 GitHub Actions 配置文件存放在代码仓库的.github/workflows目录下，文件后缀为.yml，支持创建多个文件，文件名可以任意取，比如iam.yml。GitHub 只要发现.github/workflows目录里面有.yml文件，就会自动运行该文件，如果运行过程中存在问题，会以邮件的形式通知到你。 workflow 文件的配置字段非常多，如果你想详细了解，可以查看官方文档。这里，我来介绍一些基本的配置字段。 name name字段是 workflow 的名称。如果省略该字段，默认为当前 workflow 的文件名。 name: GitHub Actions Demo on on字段指定触发 workflow 的条件，通常是某些事件。 on: push 上面的配置意思是，push事件触发 workflow。on字段也可以是事件的数组，例如: on: [push, pull_request] 上面的配置意思是，push事件或pull_request事件都可以触发 workflow。想了解完整的事件列表，你可以查看官方文档。除了代码库事件，GitHub Actions 也支持外部事件触发，或者定时运行。 on.\u003c push|pull_request\u003e.\u003c tags|branches\u003e 指定触发事件时，我们可以限定分支或标签。 on: push: branches: - master 上面的配置指定，只有master分支发生push事件时，才会触发 workflow。 jobs.\u003c job_id\u003e.name workflow 文件的主体是jobs字段，表示要执行的一项或多项任务。jobs字段里面，需要写出每一项任务的job_id，具体名称自定义。job_id里面的name字段是任务的说明。 jobs: my_first_job: name: My first job my_second_job: name: My second job 上面的代码中，jobs字段包含两项任务，job_id分别是my_first_job和my_second_job。 jobs.\u003c job_id\u003e.needs needs字段指定当前任务的依赖关系，即运行顺序。 jobs: job1: job2: needs: job1 job3: needs: [job1, job2] 上面的代码中，job1必须先于job2完成，而job3等待job1和job2完成后才能运行。因此，这个 workflow 的运行顺序为：job1、job2、job3。 jobs.\u003c job_id\u003e.runs-on runs-on字段指定运行所需要的虚拟机环境，它是必填字段。目前可用的虚拟机如下： ubuntu-latest、ubuntu-18.04 或 ubuntu-16.04。 windows-latest、windows-2019 或 windows-2016。 macOS-latest 或 macOS-10.14。 下面的配置指定虚拟机环境为ubuntu-18.04。 runs-on: ubuntu-18.04 jobs..steps steps字段指定每个 Job 的运行步骤，可以包含一个或多个步骤。每个步骤都可以指定下面三个字段。 jobs.\u003c job_id\u003e.steps.name：步骤名称。jobs.\u003c job_id\u003e.steps.run：该步骤运行的命令或者 action。jobs.\u003c job_id\u003e.steps.env：该步骤所需的环境变量。 下面是一个完整的 workflow 文件的范例： name: Greeting from Mona on: push jobs: my-job: name: My Job runs-on: ubuntu-latest steps: - name: Print a greeting env: MY_VAR: Hello! My name is FIRST_NAME: Lingfei LAST_NAME: Kong run: | echo $MY_VAR $FIRST_NAME $LAST_NAME. 上面的代码中，steps字段只包括一个步骤。该步骤先注入三个环境变量，然后执行一条 Bash 命令。 uses uses 可以引用别人已经创建的 actions，就是上面说的 actions 市场中的 actions。引用格式为userName/repoName@verison，例如uses: actions/setup-go@v1。 with with 指定 actions 的输入参数。每个输入参数都是一个键 / 值对。输入参数被设置为环境变量，该变量的前缀为 INPUT_，并转换为大写。 这里举个例子：我们定义 hello_world 操作所定义的三个输入参数（first_name、middle_name 和 last_name），这些输入变量将被 hello-world 操作作为 INPUT_FIRST_NAME、INPUT_MIDDLE_NAME 和 INPUT_LAST_NAME 环境变量使用。 jobs: my_first_job: steps: - name: My first step uses: actions/hello_world@master with: first_name: Lingfei middle_name: Go last_name: Kong run run指定执行的命令。可以有多个命令，例如： id id是 step 的唯一标识。 ","date":"2022-07-07 15:20:34","objectID":"/iam_service_deployment/:12:2","tags":["iam"],"title":"iam_service_deployment","uri":"/iam_service_deployment/"},{"categories":["iam"],"content":"GitHub Actions 的进阶用法 上面，我介绍了 GitHub Actions 的一些基本知识，这里我再介绍下 GitHub Actions 的进阶用法。 为工作流加一个 Badge 在 action 的面板中，点击Create status badge就可以复制 Badge 的 Markdown 内容到 README.md 中。之后，我们就可以直接在 README.md 中看到当前的构建结果： 使用构建矩阵 如果我们想在多个系统或者多个语言版本上测试构建，就需要设置构建矩阵。例如，我们想在多个操作系统、多个 Go 版本下跑测试，可以使用如下 workflow 配置： name: Go Test on: [push, pull_request] jobs: helloci-build: name: Test with go ${{ matrix.go_version }} on ${{ matrix.os }} runs-on: ${{ matrix.os }} strategy: matrix: go_version: [1.15, 1.16] os: [ubuntu-latest, macOS-latest] steps: - name: Set up Go ${{ matrix.go_version }} uses: actions/setup-go@v2 with: go-version: ${{ matrix.go_version }} id: go 上面的 workflow 配置，通过strategy.matrix配置了该工作流程运行的环境矩阵（格式为go_version.os）：ubuntu-latest.1.15、ubuntu-latest.1.16、macOS-latest.1.15、macOS-latest.1.16。也就是说，会在 4 台不同配置的服务器上执行该 workflow。 使用 Secrets 在构建过程中，我们可能需要用到ssh或者token等敏感数据，而我们不希望这些数据直接暴露在仓库中，此时就可以使用secrets。 我们在对应项目中选择Settings-\u003e Secrets，就可以创建secret，如下图所示： 配置文件中的使用方法如下： name: Go Test on: [push, pull_request] jobs: helloci-build: name: Test with go runs-on: [ubuntu-latest] environment: name: helloci steps: - name: use secrets env: super_secret: ${{ secrets.YourSecrets }} secret name 不区分大小写，所以如果新建 secret 的名字是 name，使用时用 secrets.name 或者 secrets.Name 都是可以的。而且，就算此时直接使用 echo 打印 secret , 控制台也只会打印出*来保护 secret。 这里要注意，你的 secret 是属于某一个环境变量的，所以要指明环境的名字：environment.name。上面的 workflow 配置中的secrets.YourSecrets属于helloci环境。 使用 Artifact 保存构建产物 在构建过程中，我们可能需要输出一些构建产物，比如日志文件、测试结果等。这些产物可以使用 Github Actions Artifact 来存储。你可以使用action/upload-artifact 和 download-artifact 进行构建参数的相关操作。 这里我以输出 Jest 测试报告为例来演示下如何保存 Artifact 产物。Jest 测试后的测试产物是 coverage： steps: - run: npm ci - run: npm test - name: Collect Test Coverage File uses: actions/upload-artifact@v1.0.0 with: name: coverage-output path: coverage 执行成功后，我们就能在对应 action 面板看到生成的 Artifact： ","date":"2022-07-07 15:20:34","objectID":"/iam_service_deployment/:12:3","tags":["iam"],"title":"iam_service_deployment","uri":"/iam_service_deployment/"},{"categories":["iam"],"content":"GitHub Actions 实战 上面，我介绍了 GitHub Actions 的用法，接下来我们就来实战下，看下使用 GitHub Actions 的 6 个具体步骤。 第一步，创建一个测试仓库。登陆GitHub 官网，点击 New repository 创建，如下图所示： 这里，我们创建了一个叫helloci的测试项目。 第二步，将新的仓库 clone 下来，并添加一些文件： $ git clone https://github.com/marmotedu/helloci 你可以克隆marmotedu/helloci，并将里面的文件拷贝到你创建的项目仓库中。 第三步，创建 GitHub Actions workflow 配置目录： $ mkdir -p .github/workflows 第四步，创建 GitHub Actions workflow 配置。在.github/workflows目录下新建helloci.yml文件，内容如下： name: Go Test on: [push, pull_request] jobs: helloci-build: name: Test with go ${{ matrix.go_version }} on ${{ matrix.os }} runs-on: ${{ matrix.os }} environment: name: helloci strategy: matrix: go_version: [1.16] os: [ubuntu-latest] steps: - name: Set up Go ${{ matrix.go_version }} uses: actions/setup-go@v2 with: go-version: ${{ matrix.go_version }} id: go - name: Check out code into the Go module directory uses: actions/checkout@v2 - name: Tidy run: | go mod tidy - name: Build run: | go build -v -o helloci . - name: Collect main.go file uses: actions/upload-artifact@v1.0.0 with: name: main-output path: main.go - name: Publish to Registry uses: elgohr/Publish-Docker-GitHub-Action@master with: name: ccr.ccs.tencentyun.com/marmotedu/helloci:beta # docker image 的名字 username: ${{ secrets.DOCKER_USERNAME}} # 用户名 password: ${{ secrets.DOCKER_PASSWORD }} # 密码 registry: ccr.ccs.tencentyun.com # 腾讯云Registry dockerfile: Dockerfile # 指定 Dockerfile 的位置 tag_names: true # 是否将 release 的 tag 作为 docker image 的 tag 上面的 workflow 文件定义了当 GitHub 仓库有push、pull_request事件发生时，会触发 GitHub Actions 工作流程，流程中定义了一个任务（Job）helloci-build，Job 中包含了多个步骤（Step），每个步骤又包含一些动作（Action）。 上面的 workflow 配置会按顺序执行下面的 6 个步骤。 准备一个 Go 编译环境。 从marmotedu/helloci下载源码。 添加或删除缺失的依赖包。 编译 Go 源码。 上传构建产物。 构建镜像，并将镜像 push 到ccr.ccs.tencentyun.com/marmotedu/helloci:beta。 第五步，在 push 代码之前，我们需要先创建DOCKER_USERNAME和DOCKER_PASSWORD secret。其中，DOCKER_USERNAME保存腾讯云镜像服务（CCR）的用户名，DOCKER_PASSWORD保存 CCR 的密码。我们将这两个 secret 保存在helloci Environments 中，如下图所示： 第六步，将项目 push 到 GitHub，触发 workflow 工作流： $ git add . $ git push origin master 打开我们的仓库 Actions 标签页，可以发现 GitHub Actions workflow 正在执行： 等 workflow 执行完，点击 Go Test 进入构建详情页面，在详情页面能够看到我们的构建历史： 然后，选择其中一个构建记录，查看其运行详情（具体可参考chore: update step name Go Test #10）： 你可以看到，Go Test工作流程执行了 6 个 Job，每个 Job 执行了下面这些自定义 Step： Set up Go 1.16。 Check out code into the Go module directory。 Tidy。 Build。 Collect main.go file。 Publish to Registry。 其他步骤是 GitHub Actions 自己添加的步骤：Setup Job、Post Check out code into the Go module directory、Complete job。点击每一个步骤，你都能看到它们的详细输出。 ","date":"2022-07-07 15:20:34","objectID":"/iam_service_deployment/:12:4","tags":["iam"],"title":"iam_service_deployment","uri":"/iam_service_deployment/"},{"categories":["iam"],"content":"IAM GitHub Actions 实战 接下来，我们再来看下 IAM 项目的 GitHub Actions 实战。假设 IAM 项目根目录为 ${IAM_ROOT}，它的 workflow 配置文件为： 接下来，我们再来看下 IAM 项目的 GitHub Actions 实战。假设 IAM 项目根目录为 ${IAM_ROOT}，它的 workflow 配置文件为： $ cat ${IAM_ROOT}/.github/workflows/iamci.yaml name: IamCI on: push: branchs: - '*' pull_request: types: [opened, reopened] jobs: iamci: name: Test with go ${{ matrix.go_version }} on ${{ matrix.os }} runs-on: ${{ matrix.os }} environment: name: iamci strategy: matrix: go_version: [1.16] os: [ubuntu-latest] steps: - name: Set up Go ${{ matrix.go_version }} uses: actions/setup-go@v2 with: go-version: ${{ matrix.go_version }} id: go - name: Check out code into the Go module directory uses: actions/checkout@v2 - name: Run go modules Tidy run: | make tidy - name: Generate all necessary files, such as error code files run: | make gen - name: Check syntax and styling of go sources run: | make lint - name: Run unit test and get test coverage run: | make cover - name: Build source code for host platform run: | make build - name: Collect Test Coverage File uses: actions/upload-artifact@v1.0.0 with: name: main-output path: _output/coverage.out - name: Set up Docker Buildx uses: docker/setup-buildx-action@v1 - name: Login to DockerHub uses: docker/login-action@v1 with: username: ${{ secrets.DOCKERHUB_USERNAME }} password: ${{ secrets.DOCKERHUB_TOKEN }} - name: Build docker images for host arch and push images to registry run: | make push 上面的 workflow 依次执行了以下步骤： 设置 Go 编译环境。 下载 IAM 项目源码。 添加 / 删除不需要的 Go 包。 生成所有的代码文件。 对 IAM 源码进行静态代码检查。 运行单元测试用例，并计算单元测试覆盖率是否达标。 编译代码。收集构建产物_output/coverage.out。 配置 Docker 构建环境。 登陆 DockerHub。 构建 Docker 镜像，并 push 到 DockerHub。 IamCI workflow 运行历史如下图所示： IamCI workflow 的其中一次工作流程运行结果如下图所示： ","date":"2022-07-07 15:20:34","objectID":"/iam_service_deployment/:12:5","tags":["iam"],"title":"iam_service_deployment","uri":"/iam_service_deployment/"},{"categories":["iam"],"content":"总结 在 Go 项目开发中，我们需要通过 CI 任务来将需要频繁操作的任务自动化，这不仅可以提高开发效率，还能减少手动操作带来的失误。这一讲，我选择了最易实践的 GitHub Actions，来给你演示如何构建 CI 任务。GitHub Actions 支持通过 push 事件来触发 CI 流程。一个 CI 流程其实就是一个 workflow，workflow 中包含多个任务，这些任务是可以并行执行的。一个任务又包含多个步骤，每一步又由多个动作组成。动作（Action）其实是一个命令 / 脚本，用来完成我们指定的任务，如编译等。因为 GitHub Actions 内容比较多，这一讲只介绍了一些核心的知识，更详细的 GitHub Actions 教程，你可以参考 官方中文文档。 ","date":"2022-07-07 15:20:34","objectID":"/iam_service_deployment/:12:6","tags":["iam"],"title":"iam_service_deployment","uri":"/iam_service_deployment/"},{"categories":["iam"],"content":"服务测试 ","date":"2022-07-07 15:20:30","objectID":"/iam_service_test/:0:0","tags":["iam"],"title":"iam_service_test","uri":"/iam_service_test/"},{"categories":["iam"],"content":"36 | 代码测试（上）：如何编写 Go 语言单元测试和性能测试用例？ 在 Go 项目开发中，我们不仅要开发功能，更重要的是确保这些功能稳定可靠，并且拥有一个不错的性能。要确保这些，就要对代码进行测试。开发人员通常会进行单元测试和性能测试，分别用来测试代码的功能是否正常和代码的性能是否满足需求。每种语言通常都有自己的测试包 / 模块，Go 语言也不例外。在 Go 中，我们可以通过testing包对代码进行单元测试和性能测试。这一讲，我会用一些示例来讲解如何编写单元测试和性能测试用例，下一讲则会介绍如何编写其他的测试类型，并介绍 IAM 项目的测试用例。 ","date":"2022-07-07 15:20:30","objectID":"/iam_service_test/:1:0","tags":["iam"],"title":"iam_service_test","uri":"/iam_service_test/"},{"categories":["iam"],"content":"如何测试 Go 代码？ Go 语言有自带的测试框架testing，可以用来实现单元测试（T 类型）和性能测试（B 类型），通过go test命令来执行单元测试和性能测试。 go test 执行测试用例时，是以 go 包为单位进行测试的。执行时需要指定包名，比如go test 包名，如果没有指定包名，默认会选择执行命令时所在的包。go test 在执行时，会遍历以_test.go结尾的源码文件，执行其中以Test、Benchmark、Example开头的测试函数。为了演示如何编写测试用例，我预先编写了 4 个函数。假设这些函数保存在 test 目录下的math.go文件中，包名为test，math.go 代码如下： package test import ( \"fmt\" \"math\" \"math/rand\" ) // Abs returns the absolute value of x. func Abs(x float64) float64 { return math.Abs(x) } // Max returns the larger of x or y. func Max(x, y float64) float64 { return math.Max(x, y) } // Min returns the smaller of x or y. func Min(x, y float64) float64 { return math.Min(x, y) } // RandInt returns a non-negative pseudo-random int from the default Source. func RandInt() int { return rand.Int() } 在这一讲后面的内容中，我会演示如何编写测试用例，来对这些函数进行单元测试和性能测试。下面让我们先来看下测试命名规范。 ","date":"2022-07-07 15:20:30","objectID":"/iam_service_test/:1:1","tags":["iam"],"title":"iam_service_test","uri":"/iam_service_test/"},{"categories":["iam"],"content":"测试命名规范 在我们对 Go 代码进行测试时，需要编写测试文件、测试函数、测试变量，它们都需要遵循一定的规范。这些规范有些来自于官方，有些则来自于社区。这里，我分别来介绍下测试文件、包、测试函数和测试变量的命名规范。 测试文件的命名规范 Go 的测试文件名必须以_test.go结尾。例如，如果我们有一个名为person.go的文件，那它的测试文件必须命名为person_test.go。这样做是因为，Go 需要区分哪些文件是测试文件。这些测试文件可以被 go test 命令行工具加载，用来测试我们编写的代码，但会被 Go 的构建程序忽略掉，因为 Go 程序的运行不需要这些测试代码。 包的命名规范 Go 的测试可以分为白盒测试和黑盒测试。 白盒测试：将测试和生产代码放在同一个 Go 包中，这使我们可以同时测试 Go 包中可导出和不可导出的标识符。当我们编写的单元测试需要访问 Go 包中不可导出的变量、函数和方法时，就需要编写白盒测试用例。 黑盒测试：将测试和生产代码放在不同的 Go 包中。这时，我们仅可以测试 Go 包的可导出标识符。这意味着我们的测试包将无法访问生产代码中的任何内部函数、变量或常量。 在白盒测试中，Go 的测试包名称需要跟被测试的包名保持一致，例如：person.go定义了一个person包，则person_test.go的包名也要为person，这也意味着person.go和person_test.go都要在同一个目录中。 在黑盒测试中，Go 的测试包名称需要跟被测试的包名不同，但仍然可以存放在同一个目录下。比如，person.go定义了一个person包，则person_test.go的包名需要跟person不同，通常我们命名为person_test。 如果不是需要使用黑盒测试，我们在做单元测试时要尽量使用白盒测试。一方面，这是 go test 工具的默认行为；另一方面，使用白盒测试，我们可以测试和使用不可导出的标识符。测试文件和包的命名规范，由 Go 语言及 go test 工具来强制约束。 函数的命名规范 测试用例函数必须以Test、Benchmark、Example开头，例如TestXxx、BenchmarkXxx、ExampleXxx，Xxx部分为任意字母数字的组合，首字母大写。这是由 Go 语言和 go test 工具来进行约束的，Xxx一般是需要测试的函数名。 除此之外，还有一些社区的约束，这些约束不是强制的，但是遵循这些约束会让我们的测试函数名更加易懂。例如，我们有以下函数： package main type Person struct { age int64 } func (p *Person) older(other *Person) bool { return p.age \u003e other.age } 很显然，我们可以把测试函数命名为TestOlder，这个名称可以很清晰地说明它是Older函数的测试用例。但是，如果我们想用多个测试用例来测试TestOlder函数，这些测试用例该如何命名呢？也许你会说，我们命名为TestOlder1、TestOlder2不就行了？ 其实，还有其他更好的命名方法。比如，这种情况下，我们可以将函数命名为TestOlderXxx，其中Xxx代表Older函数的某个场景描述。例如，strings.Compare函数有如下测试函数：TestCompare、TestCompareIdenticalString、TestCompareStrings。 变量的命名规范 Go 语言和 go test 没有对变量的命名做任何约束。但是，在编写单元测试用例时，还是有一些规范值得我们去遵守。 单元测试用例通常会有一个实际的输出，在单元测试中，我们会将预期的输出跟实际的输出进行对比，来判断单元测试是否通过。为了清晰地表达函数的实际输出和预期输出，可以将这两类输出命名为expected/actual，或者got/want。例如： if c.expected != actual { t.Fatalf(\"Expected User-Agent '%s' does not match '%s'\", c.expected, actual) } 或者： if got, want := diags[3].Description().Summary, undeclPlural; got != want { t.Errorf(\"wrong summary for diagnostic 3\\ngot: %s\\nwant: %s\", got, want) } 其他的变量命名，我们可以遵循 Go 语言推荐的变量命名方法，例如： Go 中的变量名应该短而不是长，对于范围有限的局部变量来说尤其如此。 变量离声明越远，对名称的描述性要求越高。 像循环、索引之类的变量，名称可以是单个字母（i）。如果是不常见的变量和全局变量，变量名就需要具有更多的描述性。 上面，我介绍了 Go 测试的一些基础知识。接下来，我们来看看如何编写单元测试用例和性能测试用例。 ","date":"2022-07-07 15:20:30","objectID":"/iam_service_test/:1:2","tags":["iam"],"title":"iam_service_test","uri":"/iam_service_test/"},{"categories":["iam"],"content":"单元测试 单元测试用例函数以 Test 开头，例如 TestXxx 或 Test_xxx （ Xxx 部分为任意字母数字组合，首字母大写）。函数参数必须是 *testing.T，可以使用该类型来记录错误或测试状态。 我们可以调用 testing.T 的 Error 、Errorf 、FailNow 、Fatal 、FatalIf 方法，来说明测试不通过；调用 Log 、Logf 方法来记录测试信息。函数列表和相关描述如下表所示： 下面的代码是两个简单的单元测试函数（函数位于文件math_test.go中）： func TestAbs(t *testing.T) { got := Abs(-1) if got != 1 { t.Errorf(\"Abs(-1) = %f; want 1\", got) } } func TestMax(t *testing.T) { got := Max(1, 2) if got != 2 { t.Errorf(\"Max(1, 2) = %f; want 2\", got) } } 执行go test命令来执行如上单元测试用例： $ go test PASS ok github.com/marmotedu/gopractise-demo/31/test 0.002s go test命令自动搜集所有的测试文件，也就是格式为*_test.go的文件，从中提取全部测试函数并执行。go test 还支持下面三个参数。 -v，显示所有测试函数的运行细节： $ go test -v === RUN TestAbs --- PASS: TestAbs (0.00s) === RUN TestMax --- PASS: TestMax (0.00s) PASS ok github.com/marmotedu/gopractise-demo/31/test 0.002s -run \u003c regexp\u003e，指定要执行的测试函数： $ go test -v -run='TestA.*' === RUN TestAbs --- PASS: TestAbs (0.00s) PASS ok github.com/marmotedu/gopractise-demo/31/test 0.001s 上面的例子中，我们只运行了以TestA开头的测试函数。 -count N，指定执行测试函数的次数： $ go test -v -run='TestA.*' -count=2 === RUN TestAbs --- PASS: TestAbs (0.00s) === RUN TestAbs --- PASS: TestAbs (0.00s) PASS ok github.com/marmotedu/gopractise-demo/31/test 0.002s 多个输入的测试用例 前面介绍的单元测试用例只有一个输入，但是很多时候，我们需要测试一个函数在多种不同输入下是否能正常返回。这时候，我们可以编写一个稍微复杂点的测试用例，用来支持多输入下的用例测试。例如，我们可以将TestAbs改造成如下函数： func TestAbs_2(t *testing.T) { tests := []struct { x float64 want float64 }{ {-0.3, 0.3}, {-2, 2}, {-3.1, 3.1}, {5, 5}, } for _, tt := range tests { if got := Abs(tt.x); got != tt.want { t.Errorf(\"Abs() = %f, want %v\", got, tt.want) } } } 上述测试用例函数中，我们定义了一个结构体数组，数组中的每一个元素代表一次测试用例。数组元素的的值包含输入和预期的返回值： tests := []struct { x float64 want float64 }{ {-0.3, 0.3}, {-2, 2}, {-3.1, 3.1}, {5, 5}, } 上述测试用例，将被测函数放在 for 循环中执行： for _, tt := range tests { if got := Abs(tt.x); got != tt.want { t.Errorf(\"Abs() = %f, want %v\", got, tt.want) } } 上面的代码将输入传递给被测函数，并将被测函数的返回值跟预期的返回值进行比较。如果相等，则说明此次测试通过，如果不相等则说明此次测试不通过。通过这种方式，我们就可以在一个测试用例中，测试不同的输入和输出，也就是不同的测试用例。如果要新增一个测试用例，根据需要添加输入和预期的返回值就可以了，这些测试用例都共享其余的测试代码。 上面的测试用例中，我们通过got != tt.want来对比实际返回结果和预期返回结果。我们也可以使用github.com/stretchr/testify/assert包中提供的函数来做结果对比，例如： func TestAbs_3(t *testing.T) { tests := []struct { x float64 want float64 }{ {-0.3, 0.3}, {-2, 2}, {-3.1, 3.1}, {5, 5}, } for _, tt := range tests { got := Abs(tt.x) assert.Equal(t, got, tt.want) } } 使用assert来对比结果，有下面这些好处： 友好的输出结果，易于阅读。 因为少了if got := Xxx(); got != tt.wang {}的判断，代码变得更加简洁。 可以针对每次断言，添加额外的消息说明，例如assert.Equal(t, got, tt.want, “Abs test”)。 assert 包还提供了很多其他函数，供开发者进行结果对比，例如Zero、NotZero、Equal、NotEqual、Less、True、Nil、NotNil等。如果想了解更多函数，你可以参考go doc github.com/stretchr/testify/assert。 自动生成单元测试用例 通过上面的学习，你也许可以发现，测试用例其实可以抽象成下面的模型： 用代码可表示为： func TestXxx(t *testing.T) { type args struct { // TODO: Add function input parameter definition. } type want struct { // TODO: Add function return parameter definition. } tests := []struct { name string args args want want }{ // TODO: Add test cases. } for _, tt := range tests { t.Run(tt.name, func(t *testing.T) { if got := Xxx(tt.args); got != tt.want { t.Errorf(\"Xxx() = %v, want %v\", got, tt.want) } }) } } 既然测试用例可以抽象成一些模型，那么我们就可以基于这些模型来自动生成测试代码。Go 社区中有一些优秀的工具可以自动生成测试代码，我推荐你使用gotests工具。 下面，我来讲讲 gotests 工具的使用方法，可以分成三个步骤。第一步，安装 gotests 工具： $ go get -u github.com/cweill/gotests/... gotests 命令执行格式为：gotests [options] [PATH] [FILE] …。gotests 可以为PATH下的所有 Go 源码文件中的函数生成测试代码，也可以只为某个FILE中的函数生成测试代码。 第二步，进入测试代码目录，执行 gotests 生成测试用例： $ gotests -all -w . 上面的命令会为当前目录下所有 Go 源码文件中的函数生成测试代码。 第三步，添加测试用例：生成完测试用例，你只需要添加需要测试的输入和预期的输出就可以了。下面的测试用例是通过 gotests 生成的： func TestUnpointer(t *testing.T) { type args struct { offset *int64 limit *int64 } tests := []struct { name string args args want *LimitAndOffset }{ // TODO: Add test cases. } for _, tt := range tests { t.Run(tt.name, func(t *testing.T) { if got := Unpointer(tt.args.offset, tt.args.limit); !reflect.DeepEqual(got, tt.want) { t.Errorf(\"Unpointer() = %v, want %v\", got, tt.want) } }) } } 我们只需要补全TODO位置的测试数据即可，补全后的测试用例见gorm_test.go文件。 ","date":"2022-07-07 15:20:30","objectID":"/iam_service_test/:1:3","tags":["iam"],"title":"iam_service_test","uri":"/iam_service_test/"},{"categories":["iam"],"content":"性能测试 上面，我讲了用来测试代码的功能是否正常的单元测试，接下来我们来看下性能测试，它是用来测试代码的性能是否满足需求的。 性能测试的用例函数必须以Benchmark开头，例如BenchmarkXxx或Benchmark_Xxx（ Xxx 部分为任意字母数字组合，首字母大写）。 函数参数必须是*testing.B，函数内以b.N作为循环次数，其中N会在运行时动态调整，直到性能测试函数可以持续足够长的时间，以便能够可靠地计时。下面的代码是一个简单的性能测试函数（函数位于文件math_test.go中）： func BenchmarkRandInt(b *testing.B) { for i := 0; i \u003c b.N; i++ { RandInt() } } go test命令默认不会执行性能测试函数，需要通过指定参数-bench 来运行性能测试函数。-bench后可以跟正则表达式，选择需要执行的性能测试函数，例如go test -bench=”.“表示执行所有的压力测试函数。执行go test -bench=”.“后输出如下： $ go test -bench=\".*\" goos: linux goarch: amd64 pkg: github.com/marmotedu/gopractise-demo/31/test BenchmarkRandInt-4 97384827 12.4 ns/op PASS ok github.com/marmotedu/gopractise-demo/31/test 1.223s 上面的结果只显示了性能测试函数的执行结果。BenchmarkRandInt性能测试函数的执行结果如下： BenchmarkRandInt-4 90848414 12.8 ns/op 每个函数的性能执行结果一共有 3 列，分别代表不同的意思，这里用上面的函数举例子： BenchmarkRandInt-4，BenchmarkRandInt表示所测试的测试函数名，4 表示有 4 个 CPU 线程参与了此次测试，默认是GOMAXPROCS的值。 90848414 ，说明函数中的循环执行了90848414次。 12.8 ns/op，说明每次循环的执行平均耗时是 12.8 纳秒，该值越小，说明代码性能越高。 如果我们的性能测试函数在执行循环前，需要做一些耗时的准备工作，我们就需要重置性能测试时间计数，例如： func BenchmarkBigLen(b *testing.B) { big := NewBig() b.ResetTimer() for i := 0; i \u003c b.N; i++ { big.Len() } } 当然，我们也可以先停止性能测试的时间计数，然后再开始时间计数，例如： func BenchmarkBigLen(b *testing.B) { b.StopTimer() // 调用该函数停止压力测试的时间计数 big := NewBig() b.StartTimer() // 重新开始时间 for i := 0; i \u003c b.N; i++ { big.Len() } } B 类型的性能测试还支持下面 4 个参数。 benchmem，输出内存分配统计： $ go test -bench=\".*\" -benchmem goos: linux goarch: amd64 pkg: github.com/marmotedu/gopractise-demo/31/test BenchmarkRandInt-4 96776823 12.8 ns/op 0 B/op 0 allocs/op PASS ok github.com/marmotedu/gopractise-demo/31/test 1.255s 指定了-benchmem参数后，执行结果中又多了两列： 0 B/op，表示每次执行分配了多少内存（字节），该值越小，说明代码内存占用越小；0 allocs/op，表示每次执行分配了多少次内存，该值越小，说明分配内存次数越少，意味着代码性能越高。 benchtime，指定测试时间和循环执行次数（格式需要为 Nx，例如 100x）： $ go test -bench=\".*\" -benchtime=10s # 指定测试时间 goos: linux goarch: amd64 pkg: github.com/marmotedu/gopractise-demo/31/test BenchmarkRandInt-4 910328618 13.1 ns/op PASS ok github.com/marmotedu/gopractise-demo/31/test 13.260s $ go test -bench=\".*\" -benchtime=100x # 指定循环执行次数 goos: linux goarch: amd64 pkg: github.com/marmotedu/gopractise-demo/31/test BenchmarkRandInt-4 100 16.9 ns/op PASS ok github.com/marmotedu/gopractise-demo/31/test 0.003s cpu，指定 GOMAXPROCS。 timeout，指定测试函数执行的超时时间： $ go test -bench=\".*\" -timeout=10s goos: linux goarch: amd64 pkg: github.com/marmotedu/gopractise-demo/31/test BenchmarkRandInt-4 97375881 12.4 ns/op PASS ok github.com/marmotedu/gopractise-demo/31/test 1.224s ","date":"2022-07-07 15:20:30","objectID":"/iam_service_test/:1:4","tags":["iam"],"title":"iam_service_test","uri":"/iam_service_test/"},{"categories":["iam"],"content":"总结 代码开发完成之后，我们需要为代码编写单元测试用例，并根据需要，给一些函数编写性能测试用例。Go 语言提供了 testing 包，供我们编写测试用例，并通过 go test 命令来执行这些测试用例。 go test 在执行测试用例时，会查找具有固定格式的 Go 源码文件名，并执行其中具有固定格式的函数，这些函数就是测试用例。这就要求我们的测试文件名、函数名要符合 go test 工具的要求：Go 的测试文件名必须以 _test.go 结尾；测试用例函数必须以 Test 、 Benchmark 、 Example 开头。此外，我们在编写测试用例时，还要注意包和变量的命名规范。 Go 项目开发中，编写得最多的是单元测试用例。单元测试用例函数以 Test 开头，例如 TestXxx 或 Test_xxx （Xxx 部分为任意字母数字组合，首字母大写）。函数参数必须是 *testing.T ，可以使用该类型来记录错误或测试状态。我们可以调用 testing.T 的 Error 、Errorf 、FailNow 、Fatal 、FatalIf 方法，来说明测试不通过；调用 Log 、Logf 方法来记录测试信息。 下面是一个简单的单元测试函数： func TestAbs(t *testing.T) { got := Abs(-1) if got != 1 { t.Errorf(\"Abs(-1) = %f; want 1\", got) } } 编写完测试用例之后，可以使用 go test 命令行工具来执行这些测试用例。此外，我们还可以使用gotests工具，来自动地生成单元测试用例，从而减少编写测试用例的工作量。 我们在 Go 项目开发中，还经常需要编写性能测试用例。性能测试用例函数必须以Benchmark开头，以*testing.B 作为函数入参，通过 go test -bench 运行。 ","date":"2022-07-07 15:20:30","objectID":"/iam_service_test/:1:5","tags":["iam"],"title":"iam_service_test","uri":"/iam_service_test/"},{"categories":["iam"],"content":"37 | 代码测试（下）：Go 语言其他测试类型及 IAM 测试介绍 上一讲，我介绍了 Go 中的两类测试：单元测试和性能测试。在 Go 中，还有一些其他的测试类型和测试方法，值得我们去了解和掌握。此外，IAM 项目也编写了大量测试用例，这些测试用例使用了不同的编写方法，你可以通过学习 IAM 的测试用例来验证你学到的测试知识。今天，我就来介绍下 Go 语言中的其他测试类型：示例测试、TestMain 函数、Mock 测试、Fake 测试等，并且介绍下 IAM 项目是如何编写和运行测试用例的。 ","date":"2022-07-07 15:20:30","objectID":"/iam_service_test/:2:0","tags":["iam"],"title":"iam_service_test","uri":"/iam_service_test/"},{"categories":["iam"],"content":"示例测试 示例测试以Example开头，没有输入和返回参数，通常保存在example_test.go文件中。示例测试可能包含以Output:或者Unordered output:开头的注释，这些注释放在函数的结尾部分。Unordered output:开头的注释会忽略输出行的顺序。 执行go test命令时，会执行这些示例测试，并且 go test 会将示例测试输出到标准输出的内容，跟注释作对比（比较时将忽略行前后的空格）。如果相等，则示例测试通过测试；如果不相等，则示例测试不通过测试。下面是一个示例测试（位于 example_test.go 文件中）： func ExampleMax() { fmt.Println(Max(1, 2)) // Output: // 2 } 执行 go test 命令，测试ExampleMax示例测试： $ go test -v -run='Example.*' === RUN ExampleMax --- PASS: ExampleMax (0.00s) PASS ok github.com/marmotedu/gopractise-demo/31/test 0.004s 可以看到ExampleMax测试通过。这里测试通过是因为fmt.Println(Max(1, 2))向标准输出输出了2，跟// Output:后面的2一致。当示例测试不包含Output:或者Unordered output:注释时，执行go test只会编译这些函数，但不会执行这些函数。 示例测试命名规范 示例测试需要遵循一些命名规范，因为只有这样，Godoc 才能将示例测试和包级别的标识符进行关联。例如，有以下示例测试（位于 example_test.go 文件中）： package stringutil_test import ( \"fmt\" \"github.com/golang/example/stringutil\" ) func ExampleReverse() { fmt.Println(stringutil.Reverse(\"hello\")) // Output: olleh } Godoc 将在Reverse函数的文档旁边提供此示例，如下图所示： 示例测试名以Example开头，后面可以不跟任何字符串，也可以跟函数名、类型名或者类型_方法名，中间用下划线_连接，例如： func Example() { ... } // 代表了整个包的示例 func ExampleF() { ... } // 函数F的示例 func ExampleT() { ... } // 类型T的示例 func ExampleT_M() { ... } // 方法T_M的示例 当某个函数 / 类型 / 方法有多个示例测试时，可以通过后缀来区分，后缀必须以小写字母开头，例如： func ExampleReverse() func ExampleReverse_second() func ExampleReverse_third() 大型示例 有时候，我们需要编写一个大型的示例测试，这时候我们可以编写一个整文件的示例（whole file example），它有这几个特点：文件名以_test.go结尾；只包含一个示例测试，文件中没有单元测试函数和性能测试函数；至少包含一个包级别的声明；当展示这类示例测试时，godoc 会直接展示整个文件。例如： package sort_test import ( \"fmt\" \"sort\" ) type Person struct { Name string Age int } func (p Person) String() string { return fmt.Sprintf(\"%s: %d\", p.Name, p.Age) } // ByAge implements sort.Interface for []Person based on // the Age field. type ByAge []Person func (a ByAge) Len() int { return len(a) } func (a ByAge) Swap(i, j int) { a[i], a[j] = a[j], a[i] } func (a ByAge) Less(i, j int) bool { return a[i].Age \u003c a[j].Age } func Example() { people := []Person{ {\"Bob\", 31}, {\"John\", 42}, {\"Michael\", 17}, {\"Jenny\", 26}, } fmt.Println(people) sort.Sort(ByAge(people)) fmt.Println(people) // Output: // [Bob: 31 John: 42 Michael: 17 Jenny: 26] // [Michael: 17 Jenny: 26 Bob: 31 John: 42] } 一个包可以包含多个 whole file example，一个示例一个文件，例如example_interface_test.go、example_keys_test.go、example_search_test.go等。 TestMain 函数 有时候，我们在做测试的时候，可能会在测试之前做些准备工作，例如创建数据库连接等；在测试之后做些清理工作，例如关闭数据库连接、清理测试文件等。这时，我们可以在_test.go文件中添加TestMain函数，其入参为*testing.M。 TestMain是一个特殊的函数（相当于 main 函数），测试用例在执行时，会先执行TestMain函数，然后可以在TestMain中调用m.Run()函数执行普通的测试函数。在m.Run()函数前面我们可以编写准备逻辑，在m.Run()后面我们可以编写清理逻辑。我们在示例测试文件math_test.go中添加如下 TestMain 函数： func TestMain(m *testing.M) { fmt.Println(\"do some setup\") m.Run() fmt.Println(\"do some cleanup\") } 执行 go test，输出如下： $ go test -v do some setup === RUN TestAbs --- PASS: TestAbs (0.00s) ... === RUN ExampleMax --- PASS: ExampleMax (0.00s) PASS do some cleanup ok github.com/marmotedu/gopractise-demo/31/test 0.006s 在执行测试用例之前，打印了do some setup，在测试用例运行完成之后，打印了do some cleanup。IAM 项目的测试用例中，使用 TestMain 函数在执行测试用例前连接了一个 fake 数据库，代码如下（位于internal/apiserver/service/v1/user_test.go文件中）： func TestMain(m *testing.M) { fakeStore, _ := fake.NewFakeStore() store.SetClient(fakeStore) os.Exit(m.Run()) } 单元测试、性能测试、示例测试、TestMain 函数是 go test 支持的测试类型。此外，为了测试在函数内使用了 Go Interface 的函数，我们还延伸出了 Mock 测试和 Fake 测试两种测试类型。 ","date":"2022-07-07 15:20:30","objectID":"/iam_service_test/:2:1","tags":["iam"],"title":"iam_service_test","uri":"/iam_service_test/"},{"categories":["iam"],"content":"Mock 测试 一般来说，单元测试中是不允许有外部依赖的，那么也就是说，这些外部依赖都需要被模拟。在 Go 中，一般会借助各类 Mock 工具来模拟一些依赖。 GoMock 是由 Golang 官方开发维护的测试框架，实现了较为完整的基于 interface 的 Mock 功能，能够与 Golang 内置的 testing 包良好集成，也能用于其他的测试环境中。GoMock 测试框架包含了 GoMock 包和 mockgen 工具两部分，其中 GoMock 包用来完成对象生命周期的管理，mockgen 工具用来生成 interface 对应的 Mock 类源文件。下面，我来分别详细介绍下 GoMock 包和 mockgen 工具，以及它们的使用方法。 安装 GoMock 要使用 GoMock，首先需要安装 GoMock 包和 mockgen 工具，安装方法如下: $ go get github.com/golang/mock/gomock $ go install github.com/golang/mock/mockgen 下面，我通过一个获取当前 Golang 最新版本的例子，来给你演示下如何使用 GoMock。示例代码目录结构如下（目录下的代码见gomock）： tree . . ├── go_version.go ├── main.go └── spider └── spider.go spider.go文件中定义了一个Spider接口，spider.go代码如下： package spider type Spider interface { GetBody() string } Spider接口中的 GetBody 方法可以抓取https://golang.org首页的Build version字段，来获取 Golang 的最新版本。我们在go_version.go文件中，调用Spider接口的GetBody方法，go_version.go代码如下： package gomock import ( \"github.com/marmotedu/gopractise-demo/gomock/spider\" ) func GetGoVersion(s spider.Spider) string { body := s.GetBody() return body } GetGoVersion函数直接返回表示版本的字符串。正常情况下，我们会写出如下的单元测试代码： func TestGetGoVersion(t *testing.T) { v := GetGoVersion(spider.CreateGoVersionSpider()) if v != \"go1.8.3\" { t.Error(\"Get wrong version %s\", v) } } 上面的测试代码，依赖spider.CreateGoVersionSpider()返回一个实现了Spider接口的实例（爬虫）。但很多时候，spider.CreateGoVersionSpider()爬虫可能还没有实现，或者在单元测试环境下不能运行（比如，在单元测试环境中连接数据库），这时候TestGetGoVersion测试用例就无法执行。 那么，如何才能在这种情况下运行TestGetGoVersion测试用例呢？这时候，我们就可以通过 Mock 工具，Mock 一个爬虫实例。接下来我讲讲具体操作。首先，用 GoMock 提供的 mockgen 工具，生成要 Mock 的接口的实现，我们在 gomock 目录下执行以下命令： $ mockgen -destination spider/mock/mock_spider.go -package spider github.com/marmotedu/gopractise-demo/gomock/spider Spider 上面的命令会在spider/mock目录下生成mock_spider.go文件： $ tree . . ├── go_version.go ├── go_version_test.go ├── go_version_test_traditional_method.go~ └── spider ├── mock │ └── mock_spider.go └── spider.go mock_spider.go文件中，定义了一些函数 / 方法，可以支持我们编写TestGetGoVersion测试函数。这时候，我们的单元测试代码如下（见go_version_test.go文件）： package gomock import ( \"testing\" \"github.com/golang/mock/gomock\" spider \"github.com/marmotedu/gopractise-demo/gomock/spider/mock\" ) func TestGetGoVersion(t *testing.T) { ctrl := gomock.NewController(t) defer ctrl.Finish() mockSpider := spider.NewMockSpider(ctrl) mockSpider.EXPECT().GetBody().Return(\"go1.8.3\") goVer := GetGoVersion(mockSpider) if goVer != \"go1.8.3\" { t.Errorf(\"Get wrong version %s\", goVer) } } 这一版本的TestGetGoVersion通过 GoMock， Mock 了一个Spider接口，而不用去实现一个Spider接口。这就大大降低了单元测试用例编写的复杂度。通过 Mock，很多不能测试的函数也变得可测试了。通过上面的测试用例，我们可以看到，GoMock 和上一讲介绍的 testing 单元测试框架可以紧密地结合起来工作。 mockgen 工具介绍 上面，我介绍了如何使用 GoMock 编写单元测试用例。其中，我们使用到了mockgen工具来生成 Mock 代码，mockgen工具提供了很多有用的功能，这里我来详细介绍下。 mockgen工具是 GoMock 提供的，用来 Mock 一个 Go 接口。它可以根据给定的接口，来自动生成 Mock 代码。这里，有两种模式可以生成 Mock 代码，分别是源码模式和反射模式。 源码模式 如果有接口文件，则可以通过以下命令来生成 Mock 代码： $ mockgen -destination spider/mock/mock_spider.go -package spider -source spider/spider.go 上面的命令，Mock 了spider/spider.go文件中定义的Spider接口，并将 Mock 代码保存在spider/mock/mock_spider.go文件中，文件的包名为spider。mockgen 工具的参数说明见下表： 反射模式 此外，mockgen 工具还支持通过使用反射程序来生成 Mock 代码。它通过传递两个非标志参数，即导入路径和逗号分隔的接口列表来启用，其他参数和源码模式共用，例如： $ mockgen -destination spider/mock/mock_spider.go -package spider github.com/marmotedu/gopractise-demo/gomock/spider Spider 通过注释使用 mockgen 如果有多个文件，并且分散在不同的位置，那么我们要生成 Mock 文件的时候，需要对每个文件执行多次 mockgen 命令（这里假设包名不相同）。这种操作还是比较繁琐的，mockgen 还提供了一种通过注释生成 Mock 文件的方式，此时需要借助go generate工具。 在接口文件的代码中，添加以下注释（具体代码见spider.go文件）： //go:generate mockgen -destination mock_spider.go -package spider github.com/cz-it/blog/blog/Go/testing/gomock/example/spider Spider 这时候，我们只需要在gomock目录下，执行以下命令，就可以自动生成 Mock 代码： $ go generate ./... 使用 Mock 代码编写单元测试用例 生成了 Mock 代码之后，我们就可以使用它们了。这里我们结合testing来编写一个使用了 Mock 代码的单元测试用例。 首先，需要在单元测试代码里创建一个 Mock 控制器： ctrl := gomock.NewController(t) 将*testing.T传递给 GoMock ，生成一个Controller对象，该对象控制了整个 Mock 的过程。在操作完后，还需要进行回收，所以一般会在NewController后面 defer 一个 Finish，代码如下： defer ctrl.Finish() 然后，就可以调用 Mock 的对象了： mockSpider := spider.NewMockSpider(ctrl) 这里的spider是 mockgen 命令里面传递的包名，后面是NewMockXxxx格式的对象创建函数，Xxx是接口名。这里，我们需要传递控制器对象进去，返回一个 Mock 实例。接着，有了 Mock 实","date":"2022-07-07 15:20:30","objectID":"/iam_service_test/:2:2","tags":["iam"],"title":"iam_service_test","uri":"/iam_service_test/"},{"categories":["iam"],"content":"Fake 测试 在 Go 项目开发中，对于比较复杂的接口，我们还可以 Fake 一个接口实现，来进行测试。所谓 Fake 测试，其实就是针对接口实现一个假（fake）的实例。至于如何实现 Fake 实例，需要你根据业务自行实现。例如：IAM 项目中 iam-apiserver 组件就实现了一个 fake store，代码见fake目录。因为这一讲后面的 IAM 项目测试实战部分有介绍，所以这里不再展开讲解。 ","date":"2022-07-07 15:20:30","objectID":"/iam_service_test/:2:3","tags":["iam"],"title":"iam_service_test","uri":"/iam_service_test/"},{"categories":["iam"],"content":"何时编写和执行单元测试用例？ 上面，我介绍了 Go 代码测试的基础知识，这里我再来分享下在做测试时一个比较重要的知识点：何时编写和执行单元测试用例。 编码前：TDD Test-Driven Development，也就是测试驱动开发，是敏捷开发的⼀项核心实践和技术，也是⼀种设计方法论。简单来说，TDD 原理就是：开发功能代码之前，先编写测试用例代码，然后针对测试用例编写功能代码，使其能够通过。这样做的好处在于，通过测试的执行代码肯定满足需求，而且有助于面向接口编程，降低代码耦合，也极大降低了 bug 的出现几率。 然而，TDD 的坏处也显而易见：由于测试用例是在进行代码设计之前写的，很有可能限制开发者对代码的整体设计；并且，由于 TDD 对开发⼈员要求非常高，体现的思想跟传统开发思维也不⼀样，因此实施起来比较困难；此外，因为要先编写测试用例，TDD 也可能会影响项目的研发进度。所以，在客观情况不满足的情况下，不应该盲目追求对业务代码使用 TDD 的开发模式。 与编码同步进行：增量 及时为增量代码写单测是一种良好的习惯。一方面是因为，此时我们对需求有一定的理解，能够更好地写出单元测试来验证正确性。并且，在单测阶段就发现问题，而不是等到联调测试中才发现，修复的成本也是最小的。 另一方面，在写单测的过程中，我们也能够反思业务代码的正确性、合理性，推动我们在实现的过程中更好地反思代码的设计，并及时调整。 编码后：存量 在完成业务需求后，我们可能会遇到这种情况：因为上线时间比较紧张、没有单测相关规划，开发阶段只手动测试了代码是否符合功能。 如果这部分存量代码出现较大的新需求，或者维护已经成为问题，需要大规模重构，这正是推动补全单测的好时机。为存量代码补充上单测，一方面能够推进重构者进一步理解原先的逻辑，另一方面也能够增强重构者重构代码后的信心，降低风险。 但是，补充存量单测可能需要再次回忆理解需求和逻辑设计等细节，而有时写单测的人并不是原编码的设计者，所以编码后编写和执行单元测试用例也有一定的不足。 测试覆盖率 我们写单元测试的时候应该想得很全面，能够覆盖到所有的测试用例，但有时也会漏过一些 case，Go 提供了 cover 工具来统计测试覆盖率。具体可以分为两大步骤。 第一步，生成测试覆盖率数据： $ go test -coverprofile=coverage.out do some setup PASS coverage: 40.0% of statements do some cleanup ok github.com/marmotedu/gopractise-demo/test 0.003s 上面的命令在当前目录下生成了coverage.out覆盖率数据文件。 第二步，分析覆盖率文件： $ go tool cover -func=coverage.out do some setup PASS coverage: 40.0% of statements do some cleanup ok github.com/marmotedu/gopractise-demo/test 0.003s [colin@dev test]$ go tool cover -func=coverage.out github.com/marmotedu/gopractise-demo/test/math.go:9: Abs 100.0% github.com/marmotedu/gopractise-demo/test/math.go:14: Max 100.0% github.com/marmotedu/gopractise-demo/test/math.go:19: Min 0.0% github.com/marmotedu/gopractise-demo/test/math.go:24: RandInt 0.0% github.com/marmotedu/gopractise-demo/test/math.go:29: Floor 0.0% total: (statements) 40.0% 在上述命令的输出中，我们可以查看到哪些函数没有测试，哪些函数内部的分支没有测试完全。cover 工具会根据被执行代码的行数与总行数的比例计算出覆盖率。可以看到，Abs 和 Max 函数的测试覆盖率为 100%，Min 和 RandInt 的测试覆盖率为 0。 我们还可以使用go tool cover -html生成HTML格式的分析文件，可以更加清晰地展示代码的测试情况： $ go tool cover -html=coverage.out -o coverage.html 上述命令会在当前目录下生成一个coverage.html文件，用浏览器打开coverage.html文件，可以更加清晰地看到代码的测试情况，如下图所示： 通过上图，我们可以知道红色部分的代码没有被测试到，可以让我们接下来有针对性地添加测试用例，而不是一头雾水，不知道需要为哪些代码编写测试用例。 在 Go 项目开发中，我们往往会把测试覆盖率作为代码合并的一个强制要求，所以需要在进行代码测试时，同时生成代码覆盖率数据文件。在进行代码测试时，可以通过分析该文件，来判断我们的代码测试覆盖率是否满足要求，如果不满足则代码测试失败。 ","date":"2022-07-07 15:20:30","objectID":"/iam_service_test/:2:4","tags":["iam"],"title":"iam_service_test","uri":"/iam_service_test/"},{"categories":["iam"],"content":"IAM 项目测试实战 接下来，我来介绍下 IAM 项目是如何编写和运行测试用例的，你可以通过 IAM 项目的测试用例，加深对上面内容的理解。 IAM 项目是如何运行测试用例的？ 首先，我们来看下 IAM 项目是如何执行测试用例的。在 IAM 项目的源码根目录下，可以通过运行make test执行测试用例，make test会执行iam/scripts/make-rules/golang.mk文件中的go.test伪目标，规则如下： .PHONY: go.test go.test: tools.verify.go-junit-report @echo \"===========\u003e Run unit test\" @set -o pipefail;$(GO) test -race -cover -coverprofile=$(OUTPUT_DIR)/coverage.out \\\\ -timeout=10m -short -v `go list ./...|\\ egrep -v $(subst $(SPACE),'|',$(sort $(EXCLUDE_TESTS)))` 2\u003e\u00261 | \\\\ tee \u003e(go-junit-report --set-exit-code \u003e$(OUTPUT_DIR)/report.xml) @sed -i '/mock_.*.go/d' $(OUTPUT_DIR)/coverage.out # remove mock_.*.go files from test coverage @$(GO) tool cover -html=$(OUTPUT_DIR)/coverage.out -o $(OUTPUT_DIR)/coverage.html 在上述规则中，我们执行go test时设置了超时时间、竞态检查，开启了代码覆盖率检查，覆盖率测试数据保存在了coverage.out文件中。在 Go 项目开发中，并不是所有的包都需要单元测试，所以上面的命令还过滤掉了一些不需要测试的包，这些包配置在EXCLUDE_TESTS变量中： EXCLUDE_TESTS=github.com/marmotedu/iam/test github.com/marmotedu/iam/pkg/log github.com/marmotedu/iam/third_party github.com/marmotedu/iam/internal/pump/storage github.com/marmotedu/iam/internal/pump github.com/marmotedu/iam/internal/pkg/logger 同时，也调用了go-junit-report将 go test 的结果转化成了 xml 格式的报告文件，该报告文件会被一些 CI 系统，例如 Jenkins 拿来解析并展示结果。上述代码也同时生成了 coverage.html 文件，该文件可以存放在制品库中，供我们后期分析查看。 这里需要注意，Mock 的代码是不需要编写测试用例的，为了避免影响项目的单元测试覆盖率，需要将 Mock 代码的单元测试覆盖率数据从coverage.out文件中删除掉，go.test规则通过以下命令删除这些无用的数据： sed -i '/mock_.*.go/d' $(OUTPUT_DIR)/coverage.out # remove mock_.*.go files from test coverage 另外，还可以通过make cover来进行单元测试覆盖率测试，make cover会执行iam/scripts/make-rules/golang.mk文件中的go.test.cover伪目标，规则如下： .PHONY: go.test.cover go.test.cover: go.test @$(GO) tool cover -func=$(OUTPUT_DIR)/coverage.out | \\\\ awk -v target=$(COVERAGE) -f $(ROOT_DIR)/scripts/coverage.awk 上述目标依赖go.test，也就是说执行单元测试覆盖率目标之前，会先进行单元测试，然后使用单元测试产生的覆盖率数据coverage.out计算出总的单元测试覆盖率，这里是通过coverage.awk脚本来计算的。如果单元测试覆盖率不达标，Makefile 会报错并退出。可以通过 Makefile 的COVERAGE变量来设置单元测试覆盖率阈值。 COVERAGE 的默认值为 60，我们也可以在命令行手动指定，例如： $ make cover COVERAGE=80 为了确保项目的单元测试覆盖率达标，需要设置单元测试覆盖率质量红线。一般来说，这些红线很难靠开发者的自觉性去保障，所以好的方法是将质量红线加入到 CICD 流程中。所以，在Makefile文件中，我将cover放在all目标的依赖中，并且位于 build 之前，也就是all: gen add-copyright format lint cover build。这样每次当我们执行 make 时，会自动进行代码测试，并计算单元测试覆盖率，如果覆盖率不达标，则停止构建；如果达标，继续进入下一步的构建流程。 IAM 项目测试案例分享 接下来，我会给你展示一些 IAM 项目的测试案例，因为这些测试案例的实现方法，我在36 讲 和这一讲的前半部分已有详细介绍，所以这里，我只列出具体的实现代码，不会再介绍这些代码的实现方法。 单元测试案例 我们可以手动编写单元测试代码，也可以使用 gotests 工具生成单元测试代码。先来看手动编写测试代码的案例。这里单元测试代码见Test_Option，代码如下： func Test_Option(t *testing.T) { fs := pflag.NewFlagSet(\"test\", pflag.ExitOnError) opt := log.NewOptions() opt.AddFlags(fs) args := []string{\"--log.level=debug\"} err := fs.Parse(args) assert.Nil(t, err) assert.Equal(t, \"debug\", opt.Level) } 上述代码中，使用了github.com/stretchr/testify/assert包来对比结果。再来看使用 gotests 工具生成单元测试代码的案例（Table-Driven 的测试模式）。出于效率上的考虑，IAM 项目的单元测试用例，基本都是使用 gotests 工具生成测试用例模板代码，并基于这些模板代码填充测试 Case 的。代码见service_test.go文件。 性能测试案例 IAM 项目的性能测试用例，见BenchmarkListUser测试函数。代码如下： func BenchmarkListUser(b *testing.B) { opts := metav1.ListOptions{ Offset: pointer.ToInt64(0), Limit: pointer.ToInt64(50), } storeIns, _ := fake.GetFakeFactoryOr() u := \u0026userService{ store: storeIns, } for i := 0; i \u003c b.N; i++ { _, _ = u.List(context.TODO(), opts) } } 示例测试案例 IAM 项目的示例测试用例见example_test.go文件。example_test.go中的一个示例测试代码如下： func ExampleNew() { err := New(\"whoops\") fmt.Println(err) // Output: whoops } TestMain 测试案例 IAM 项目的 TestMain 测试案例，见user_test.go文件中的TestMain函数： func TestMain(m *testing.M) { _, _ = fake.GetFakeFactoryOr() os.Exit(m.Run()) } TestMain函数初始化了 fake Factory，然后调用m.Run执行测试用例。 Mock 测试案例 Mock 代码见internal/apiserver/service/v1/mock_service.go，使用 Mock 的测试用例见internal/apiserver/controller/v1/user/create_test.go文件。因为代码比较多，这里建议你打开链接，查看测试用例的具体实现。我们可以在 IAM 项目的根目录下执行以下命令，来自动生成所有的 Mock 文件： $ go generate ./... Fake 测试案例 fake store 代码实现位于internal/apiserver/store/fake目录下。fake store 的使用方式，见user_test.go文件： func TestMain(m *testing.M) { _, _ = fake.GetFakeFactoryOr() os.Exit(m.Run()) } func BenchmarkListUser(b *testing.B) { opts := metav1.ListOptions{ Offset: pointer.ToIn","date":"2022-07-07 15:20:30","objectID":"/iam_service_test/:2:5","tags":["iam"],"title":"iam_service_test","uri":"/iam_service_test/"},{"categories":["iam"],"content":"其他测试工具 / 包 最后，我再来分享下 Go 项目测试中常用的工具 / 包，因为内容较多，我就不详细介绍了，如果感兴趣你可以点进链接自行学习。我将这些测试工具 / 包分为了两类，分别是测试框架和 Mock 工具。 测试框架 Testify 框架：Testify 是 Go test 的预判工具，它能让你的测试代码变得更优雅和高效，测试结果也变得更详细。 GoConvey 框架：GoConvey 是一款针对 Golang 的测试框架，可以管理和运行测试用例，同时提供了丰富的断言函数，并支持很多 Web 界面特性。 Mock 工具 这一讲里，我介绍了 Go 官方提供的 Mock 框架 GoMock，不过还有一些其他的优秀 Mock 工具可供我们使用。这些 Mock 工具分别用在不同的 Mock 场景中，我在 10 讲中已经介绍过。不过，为了使我们这一讲的测试知识体系更加完整，这里我还是再提一次，你可以复习一遍。 sqlmock：可以用来模拟数据库连接。数据库是项目中比较常见的依赖，在遇到数据库依赖时都可以用它。 httpmock：可以用来 Mock HTTP 请求。 bouk/monkey：猴子补丁，能够通过替换函数指针的方式来修改任意函数的实现。如果 golang/mock、sqlmock 和 httpmock 这几种方法都不能满足我们的需求，我们可以尝试用猴子补丁的方式来 Mock 依赖。可以这么说，猴子补丁提供了单元测试 Mock 依赖的最终解决方案。 ","date":"2022-07-07 15:20:30","objectID":"/iam_service_test/:2:6","tags":["iam"],"title":"iam_service_test","uri":"/iam_service_test/"},{"categories":["iam"],"content":"总结 这一讲，我介绍了除单元测试和性能测试之外的另一些测试方法。 除了示例测试和 TestMain 函数，我还详细介绍了 Mock 测试，也就是如何使用 GoMock 来测试一些在单元测试环境下不好实现的接口。绝大部分情况下，可以使用 GoMock 来 Mock 接口，但是对于一些业务逻辑比较复杂的接口，我们可以通过 Fake 一个接口实现，来对代码进行测试，这也称为 Fake 测试。此外，我还介绍了何时编写和执行测试用例。我们可以根据需要，选择在编写代码前、编写代码中、编写代码后编写测试用例。 为了保证单元测试覆盖率，我们还应该为整个项目设置单元测试覆盖率质量红线，并将该质量红线加入到 CICD 流程中。我们可以通过 go test -coverprofile=coverage.out 命令来生成测试覆盖率数据，通过go tool cover -func=coverage.out 命令来分析覆盖率文件。 IAM 项目中使用了大量的测试方法和技巧来测试代码，为了加深你对测试知识的理解，我也列举了一些测试案例，供你参考、学习和验证。具体的测试案例，你可以返回前面查看下。除此之外，我们还可以使用其他一些测试框架，例如 Testify 框架和 GoConvey 框架。在 Go 代码测试中，我们最常使用的是 Go 官方提供的 Mock 框架 GoMock，但仍然有其他优秀的 Mock 工具，可供我们在不同场景下使用，例如 sqlmock、httpmock、bouk/monkey 等。 ","date":"2022-07-07 15:20:30","objectID":"/iam_service_test/:2:7","tags":["iam"],"title":"iam_service_test","uri":"/iam_service_test/"},{"categories":["iam"],"content":"38｜性能分析（上）：如何分析 Go 语言代码的性能？ 作为开发人员，我们一般都局限在功能上的单元测试中，对一些性能上的细节往往不会太关注。但是，如果我们在上线的时候对项目的整体性能没有一个全面的了解，随着请求量越来越大，可能会出现各种各样的问题，比如 CPU 占用高、内存使用率高、请求延时高等。为了避免这些性能瓶颈，我们在开发的过程中需要通过一定的手段，来对程序进行性能分析。 Go 语言已经为开发者内置了很多性能调优、监控的工具和方法，这大大提升了我们 profile 分析的效率，借助这些工具，我们可以很方便地对 Go 程序进行性能分析。在 Go 语言开发中，开发者基本都是通过内置的pprof工具包来进行性能分析的。 在进行性能分析时，我们会先借助一些工具和包，生成性能数据文件，然后再通过pprof工具分析性能数据文件，从而分析代码的性能。那么接下来，我们就分别来看下如何执行这两步操作。 ","date":"2022-07-07 15:20:30","objectID":"/iam_service_test/:3:0","tags":["iam"],"title":"iam_service_test","uri":"/iam_service_test/"},{"categories":["iam"],"content":"生成性能数据文件 要查看性能数据，需要先生成性能数据文件。生成性能数据文件有三种方法，分别是通过命令行、通过代码和通过net/http/pprof包。这些工具和包会分别生成 CPU 和内存性能数据。接下来，我们就来看下这三种方法分别是如何生成性能数据文件的。 通过命令行生成性能数据文件 我们可以使用go test -cpuprofile来生成性能测试数据。进入internal/apiserver/service/v1目录，执行以下命令： $ go test -bench=\".*\" -cpuprofile cpu.profile -memprofile mem.profile goos: linux goarch: amd64 pkg: github.com/marmotedu/iam/internal/apiserver/service/v1 cpu: AMD EPYC Processor BenchmarkListUser-8 280 4283077 ns/op PASS ok github.com/marmotedu/iam/internal/apiserver/service/v1 1.798s 上面的命令会在当前目录下生成 3 个文件： v1.test，测试生成的二进制文件，进行性能分析时可以用来解析各种符号。 cpu.profile，CPU 性能数据文件。 mem.profile，内存性能数据文件。 通过代码生成性能数据文件 我们还可以使用代码来生成性能数据文件，例如pprof.go文件： package main import ( \"os\" \"runtime/pprof\" ) func main() { cpuOut, _ := os.Create(\"cpu.out\") defer cpuOut.Close() pprof.StartCPUProfile(cpuOut) defer pprof.StopCPUProfile() memOut, _ := os.Create(\"mem.out\") defer memOut.Close() defer pprof.WriteHeapProfile(memOut) Sum(3, 5) } func Sum(a, b int) int { return a + b } 运行pprof.go文件： $ go run pprof.go 运行pprof.go文件后，会在当前目录生成cpu.profile和mem.profile性能数据文件。 通过net/http/pprof生成性能数据文件 如果要分析 HTTP Server 的性能，我们可以使用net/http/pprof包来生成性能数据文件。IAM 项目使用 Gin 框架作为 HTTP 引擎，所以 IAM 项目使用了github.com/gin-contrib/pprof包来启用 HTTP 性能分析。github.com/gin-contrib/pprof包是net/http/pprof的一个简单封装，通过封装使 pprof 的功能变成了一个 Gin 中间件，这样可以根据需要加载 pprof 中间件。 github.com/gin-contrib/pprof包中的pprof.go文件中有以下代码： func Register(r *gin.Engine, prefixOptions ...string) { prefix := getPrefix(prefixOptions...) prefixRouter := r.Group(prefix) { ... prefixRouter.GET(\"/profile\", pprofHandler(pprof.Profile)) ... } } func pprofHandler(h http.HandlerFunc) gin.HandlerFunc { handler := http.HandlerFunc(h) return func(c *gin.Context) { handler.ServeHTTP(c.Writer, c.Request) } } 通过上面的代码，你可以看到github.com/gin-contrib/pprof包将net/http/pprof.Profile转换成了gin.HandlerFunc，也就是 Gin 中间件。 要开启 HTTP 性能分析，只需要在代码中注册 pprof 提供的 HTTP Handler 即可（位于internal/pkg/server/genericapiserver.go文件中）： // install pprof handler if s.enableProfiling { pprof.Register(s.Engine) } 上面的代码根据配置–feature.profiling来判断是否开启 HTTP 性能分析功能。我们开启完 HTTP 性能分析，启动 HTTP 服务 iam-apiserver 后，即可访问http:// x.x.x.x:8080/debug/pprof（x.x.x.x是 Linux 服务器的地址）来查看 profiles 信息。profiles 信息如下图所示： 我们可以通过以下命令，来获取 CPU 性能数据文件： $ curl http://127.0.0.1:8080/debug/pprof/profile -o cpu.profile 执行完上面的命令后，需要等待 30s，pprof 会采集这 30s 内的性能数据，我们需要在这段时间内向服务器连续发送多次请求，请求的频度可以根据我们的场景来决定。30s 之后，/debug/pprof/profile接口会生成 CPU profile 文件，被 curl 命令保存在当前目录下的 cpu.profile 文件中。 同样的，我们可以执行以下命令来生成内存性能数据文件： $ curl http://127.0.0.1:8080/debug/pprof/heap -o mem.profile 上面的命令会自动下载 heap 文件，并被 curl 命令保存在当前目录下的 mem.profile 文件中。 我们可以使用go tool pprof [mem|cpu].profile命令来分析 HTTP 接口的 CPU 和内存性能。我们也可以使用命令go tool pprof http://127.0.0.1:8080/debug/pprof/profile，或者go tool pprof http://127.0.0.1:8080/debug/pprof/heap，来直接进入 pprof 工具的交互 Shell 中。go tool pprof会首先下载并保存 CPU 和内存性能数据文件，然后再分析这些文件。 通过上面的三种方法，我们生成了 cpu.profile 和 mem.profile，接下来我们就可以使用go tool pprof来分析这两个性能数据文件，进而分析我们程序的 CPU 和内存性能了。下面，我来具体讲讲性能分析的过程。 性能分析 使用go tool pprof，来对性能进行分析的流程，你可以参考下图： 接下来，我先给你介绍下 pprof 工具，再介绍下如何生成性能数据，最后再分别介绍下 CPU 和内存性能分析方法。 pprof 工具介绍 pprof是一个 Go 程序性能分析工具，用它可以访问并分析性能数据文件，它还会根据我们的要求，提供高可读性的输出信息。Go 在语言层面上集成了 profile 采样工具，只需在代码中简单地引入runtime/pprof或者net/http/pprof包，即可获取程序的 profile 文件，并通过 profile 文件来进行性能分析。 net/http/pprof基于runtime/pprof包进行封装，并在 HTTP 端口上暴露出来。 生成性能数据 我们在做性能分析时，主要是对内存和 CPU 性能进行分析。为了分析内存和 CPU 的性能，我们需要先生成性能数据文件。在 IAM 源码中，也有包含性能测试的用例，下面我会借助 IAM 源码中的性能测试用例，来介绍如何分析程序的性能。 进入internal/apiserver/service/v1目录，user_test.go 文件包含了性能测试函数 BenchmarkListUser，执行以下命令来生成性能数据文件： $ go test -benchtime=30s -benchmem -bench=\".*\" -cpuprofile cpu.profile -memprofile mem.profile goos: linux goarch: amd64 pkg: github.com/marmotedu/iam/internal/apiserver/service/v1 cpu: AMD EPYC Processor BenchmarkListUser-8 175 204523677 ns/op 15331 B/op 268 allocs/op PASS ok github.com/marmotedu/iam/internal/apiserver/service/v1 56.514s 上面的命令会在当前目录下产生cpu.profile、mem.profile性能数据文件，以及v1.test二进制文件。接下来，我们基于cpu.profile、mem.profile、v1.test文件来分析代码的 CPU 和内存性能。为了获取足够的采样数据，我们将 benchmark 时间设置为30","date":"2022-07-07 15:20:30","objectID":"/iam_service_test/:3:1","tags":["iam"],"title":"iam_service_test","uri":"/iam_service_test/"},{"categories":["iam"],"content":"CPU 性能分析 在默认情况下，Go 语言的运行时系统会以 100 Hz 的的频率对 CPU 使用情况进行采样，也就是说每秒采样 100 次，每 10 毫秒采样一次。每次采样时，会记录正在运行的函数，并统计其运行时间，从而生成 CPU 性能数据。 上面我们已经生成了 CPU 性能数据文件cpu.profile，接下来会运用上面提到的三种方法来分析该性能文件，优化性能。 方法一：分析采样图 要分析性能，最直观的方式当然是看图，所以首先我们需要生成采样图，生成过程可以分为两个步骤。 第一步，确保系统安装了graphviz： $ sudo yum -y install graphviz.x86_64 第二步，执行go tool pprof生成调用图： $ go tool pprof -svg cpu.profile \u003e cpu.svg # svg 格式 $ go tool pprof -pdf cpu.profile \u003e cpu.pdf # pdf 格式 $ go tool pprof -png cpu.profile \u003e cpu.png # png 格式 以上命令会生成cpu.pdf、cpu.svg和cpu.png文件，文件中绘制了函数调用关系以及其他采样数据。如下图所示： 这张图片由有向线段和矩形组成。我们先来看有向线段的含义。 有向线段描述了函数的调用关系，矩形包含了 CPU 采样数据。从图中，我们看到没箭头的一端调用了有箭头的一端，可以知道v1.(*userService).List函数调用了fake.(*policies).List。线段旁边的数字90ms则说明，v1.(*userService).List调用fake.(*policies).List函数，在采样周期内，一共耗用了90ms。通过函数调用关系，我们可以知道某个函数调用了哪些函数，并且调用这些函数耗时多久。 这里，我们再次解读下图中调用关系中的重要信息： runtime.schedule的累积采样时间（140ms）中，有 10ms 来自于runtime.goschedImpl函数的直接调用，有 70ms 来自于runtime.park_m函数的直接调用。这些数据可以说明runtime.schedule函数分别被哪些函数调用，并且调用频率有多大。也因为这个原因，函数runtime.goschedImpl对函数runtime.schedule的调用时间必定小于等于函数runtime.schedule的累积采样时间。 我们再来看下矩形里的采样数据。这些矩形基本都包含了 3 类信息： 函数名 / 方法名，该类信息包含了包名、结构体名、函数名 / 方法名，方便我们快速定位到函数 / 方法，例如fake(*policies)List说明是 fake 包，policies 结构体的 List 方法。 本地采样时间，以及它在采样总数中所占的比例。本地采样时间是指采样点落在该函数中的总时间。 累积采样时间，以及它在采样总数中所占的比例。累积采样时间是指采样点落在该函数，以及被它直接或者间接调用的函数中的总时间。 我们可以通过OutDir函数来解释本地采样时间和累积采样时间这两个概念。OutDir函数如下图所示： 整个函数的执行耗时，我们可以认为是累积采样时间，包含了白色部分的代码耗时和红色部分的函数调用耗时。白色部分的代码耗时，可以认为是本地采样时间。 通过累积采样时间，我们可以知道函数的总调用时间，累积采样时间越大，说明调用它所花费的 CPU 时间越多。但你要注意，这并不一定说明这个函数本身是有问题的，也有可能是函数所调用的函数性能有瓶颈，这时候我们应该根据函数调用关系顺藤摸瓜，去寻找这个函数直接或间接调用的函数中最耗费 CPU 时间的那些。 如果函数的本地采样时间很大，就说明这个函数自身耗时（除去调用其他函数的耗时）很大，这时候需要我们分析这个函数自身的代码，而不是这个函数直接或者间接调用函数的代码。采样图中，矩形框面积越大，说明这个函数的累积采样时间越大。那么，如果一个函数分析采样图中的矩形框面积很大，这时候我们就要认真分析了，因为很可能这个函数就有需要优化性能的地方。 方法二：分析火焰图 上面介绍的采样图，其实在分析性能的时候还不太直观，这里我们可以通过生成火焰图，来更直观地查看性能瓶颈。火焰图是由 Brendan Gregg 大师发明的专门用来把采样到的堆栈轨迹（Stack Trace）转化为直观图片显示的工具，因整张图看起来像一团跳动的火焰而得名。 go tool pprof提供了-http参数，可以使我们通过浏览器浏览采样图和火焰图。执行以下命令： $ go tool pprof -http=\"0.0.0.0:8081\" v1.test cpu.profile 然后访问http://x.x.x.x:8081/（x.x.x.x是执行go tool pprof命令所在服务器的 IP 地址），则会在浏览器显示各类采样视图数据，如下图所示： 上面的 UI 页面提供了不同的采样数据视图： Top，类似于 linux top 的形式，从高到低排序。 Graph，默认弹出来的就是该模式，也就是上一个图的那种带有调用关系的图。 Flame Graph：pprof 火焰图。 Peek：类似于 Top 也是从高到底的排序。 Source：和交互命令式的那种一样，带有源码标注。 Disassemble：显示所有的总量。 接下来，我们主要来分析火焰图。在 UI 界面选择 Flame Graph（VIEW -\u003e Flame Graph），就会展示火焰图，如下图所示： 火焰图主要有下面这几个特征： 每一列代表一个调用栈，每一个格子代表一个函数。 纵轴展示了栈的深度，按照调用关系从上到下排列。最下面的格子代表采样时，正在占用 CPU 的函数。 调用栈在横向会按照字母排序，并且同样的调用栈会做合并，所以一个格子的宽度越大，说明这个函数越可能是瓶颈。 火焰图格子的颜色是随机的暖色调，方便区分各个调用信息。 查看火焰图时，格子越宽的函数，就越可能存在性能问题，这时候，我们就可以分析该函数的代码，找出问题所在。 方法三：用go tool pprof交互模式查看详细数据 我们可以执行go tool pprof命令，来查看 CPU 的性能数据文件： $ go tool pprof v1.test cpu.profile File: v1.test Type: cpu Time: Aug 17, 2021 at 2:17pm (CST) Duration: 56.48s, Total samples = 440ms ( 0.78%) Entering interactive mode (type \"help\" for commands, \"o\" for options) (pprof) go tool pprof输出了很多信息： File，二进制可执行文件名称。 Type，采样文件的类型，例如 cpu、mem 等。 Time，生成采样文件的时间。 Duration，程序执行时间。上面的例子中，程序总执行时间为37.43s，采样时间为42.37s。采样程序在采样时，会自动分配采样任务给多个核心，所以总采样时间可能会大于总执行时间。 (pprof)，命令行提示，表示当前在go tool的pprof工具命令行中，go tool还包括cgo、doc、pprof、trace等多种命令。 执行go tool pprof命令后，会进入一个交互 shell。在这个交互 shell 中，我们可以执行多个命令，最常用的命令有三个，如下表所示： 我们在交互界面中执行top命令，可以查看性能样本数据： (pprof) top Showing nodes accounting for 350ms, 79.55% of 440ms total Showing top 10 nodes out of 47 flat flat% sum% cum cum% 110ms 25.00% 25.00% 110ms 25.00% runtime.futex 70ms 15.91% 40.91% 90ms 20.45% github.com/marmotedu/iam/internal/apiserver/store/fake.(*policies).List 40ms 9.09% 50.00% 40ms 9.09% runtime.epollwait 40ms 9.09% 59.09% 180ms 40.91% runtime.findrunnable 30ms 6.82% 65.91% 30ms 6.82% runtime.write1 20ms 4.55% 70.45% 30ms 6.82% runtime.notesleep 10ms 2.27% 72.73% 100ms 22.73% github.com/marmotedu/iam/internal/apiserver/service/v1.(*userService).List 10ms 2.27% 75.00% 10ms 2.27% runtime.checkTimers 10ms 2.27% 77.27% 10ms 2.27% runtime.doaddtimer 10ms 2.27% 79.55% 10ms 2.27% runtime.mallocgc 上面的输出中，每一行表示一个函数的信息。pprof 程序中最重要的命令就是 topN，这个命令用来显示 profile 文件中最靠前的 N 个样本（s","date":"2022-07-07 15:20:30","objectID":"/iam_service_test/:3:2","tags":["iam"],"title":"iam_service_test","uri":"/iam_service_test/"},{"categories":["iam"],"content":"总结 在 Go 项目开发中，程序性能低下时，我们需要分析出问题所在的代码。Go 语言提供的 go tool pprof 工具可以支持我们分析代码的性能。我们可以通过两步来分析代码的性能，分别是生成性能数据文件和分析性能数据文件。Go 中可以用来生成性能数据文件的方式有三种：通过命令行生成性能数据文件、通过代码生成性能数据文件、通过 net/http/pprof 生成性能数据文件。 生成性能数据文件之后，就可以使用 go tool pprof 工具来分析性能数据文件了。我们可以分别获取到 CPU 和内存的性能数据，通过分析就可以找到性能瓶颈。有 3 种分析性能数据文件的方式，分别是分析采样图、分析火焰图和用 go tool pprof 交互模式查看详细数据。因为火焰图直观高效，所以我建议你多使用火焰图来分析性能。 ","date":"2022-07-07 15:20:30","objectID":"/iam_service_test/:3:3","tags":["iam"],"title":"iam_service_test","uri":"/iam_service_test/"},{"categories":["iam"],"content":"39｜性能分析（下）：API Server性能测试和调优实战 上一讲，我们学习了如何分析 Go 代码的性能。掌握了性能分析的基本知识之后，这一讲，我们再来看下如何分析 API 接口的性能。在 API 上线之前，我们需要知道 API 的性能，以便知道 API 服务器所能承载的最大请求量、性能瓶颈，再根据业务对性能的要求，来对 API 进行性能调优或者扩缩容。通过这些，可以使 API 稳定地对外提供服务，并且让请求在合理的时间内返回。这一讲，我就介绍如何用 wrk 工具来测试 API Server 接口的性能，并给出分析方法和结果。 ","date":"2022-07-07 15:20:30","objectID":"/iam_service_test/:4:0","tags":["iam"],"title":"iam_service_test","uri":"/iam_service_test/"},{"categories":["iam"],"content":"API 性能测试指标 API 性能测试，往大了说其实包括 API 框架的性能和指定 API 的性能。不过，因为指定 API 的性能跟该 API 具体的实现（比如有无数据库连接，有无复杂的逻辑处理等）有关，我认为脱离了具体实现来探讨单个 API 的性能是毫无意义的，所以这一讲只探讨 API 框架的性能。 用来衡量 API 性能的指标主要有 3 个： 并发数（Concurrent）：并发数是指某个时间范围内，同时在使用系统的用户个数。广义上的并发数是指同时使用系统的用户个数，这些用户可能调用不同的 API；严格意义上的并发数是指同时请求同一个 API 的用户个数。这一讲我们讨论的并发数是严格意义上的并发数。 每秒查询数（QPS）：每秒查询数 QPS 是对一个特定的查询服务器在规定时间内所处理流量多少的衡量标准。QPS = 并发数 / 平均请求响应时间。 请求响应时间（TTLB）：请求响应时间指的是从客户端发出请求到得到响应的整个时间。这个过程从客户端发起的一个请求开始，到客户端收到服务器端的响应结束。在一些工具中，请求响应时间通常会被称为 TTLB（Time to last byte，意思是从发送一个请求开始，到客户端收到最后一个字节的响应为止所消费的时间）。请求响应时间的单位一般为“秒”或“毫秒”。 这三个指标中，衡量 API 性能的最主要指标是 QPS，但是在说明 QPS 时，需要指明是多少并发数下的 QPS，否则毫无意义，因为不同并发数下的 QPS 是不同的。举个例子，单用户 100 QPS 和 100 用户 100 QPS 是两个不同的概念，前者说明 API 可以在一秒内串行执行 100 个请求，而后者说明在并发数为 100 的情况下，API 可以在一秒内处理 100 个请求。当 QPS 相同时，并发数越大，说明 API 性能越好，并发处理能力越强。 在并发数设置过大时，API 同时要处理很多请求，会频繁切换上下文，而真正用于处理请求的时间变少，反而使得 QPS 会降低。并发数设置过大时，请求响应时间也会变长。API 会有一个合适的并发数，在该并发数下，API 的 QPS 可以达到最大，但该并发数不一定是最佳并发数，还要参考该并发数下的平均请求响应时间。 此外，在有些 API 接口中，也会测试 API 接口的 TPS（Transactions Per Second，每秒事务数）。一个事务是指客户端向服务器发送请求，然后服务器做出反应的过程。客户端在发送请求时开始计时，收到服务器响应后结束计时，以此来计算使用的时间和完成的事务个数。 那么，TPS 和 QPS 有什么区别呢？如果是对一个查询接口（单场景）压测，且这个接口内部不会再去请求其他接口，那么 TPS=QPS，否则，TPS≠QPS。如果是对多个接口（混合场景）压测，假设 N 个接口都是查询接口，且这个接口内部不会再去请求其他接口，QPS=N*TPS。 ","date":"2022-07-07 15:20:30","objectID":"/iam_service_test/:4:1","tags":["iam"],"title":"iam_service_test","uri":"/iam_service_test/"},{"categories":["iam"],"content":"API 性能测试方法 Linux 下有很多 Web 性能测试工具，常用的有 Jmeter、AB、Webbench 和 wrk。每个工具都有自己的特点，IAM 项目使用 wrk 来对 API 进行性能测试。wrk 非常简单，安装方便，测试结果也相对专业，并且可以支持 Lua 脚本来创建更复杂的测试场景。下面，我来介绍下 wrk 的安装方法和使用方法。 wrk 安装方法 wrk 的安装很简单，一共可分为两步。第一步，Clone wrk repo： $ git clone https://github.com/wg/wrk 第二步，编译并安装： $ cd wrk $ make $ sudo cp ./wrk /usr/bin wrk 使用简介 这里我们来看下 wrk 的使用方法。wrk 使用起来不复杂，执行wrk –help可以看到 wrk 的所有运行参数： $ wrk --help Usage: wrk \u003coptions\u003e \u003curl\u003e Options: -c, --connections \u003cN\u003e Connections to keep open -d, --duration \u003cT\u003e Duration of test -t, --threads \u003cN\u003e Number of threads to use -s, --script \u003cS\u003e Load Lua script file -H, --header \u003cH\u003e Add header to request --latency Print latency statistics --timeout \u003cT\u003e Socket/request timeout -v, --version Print version details Numeric arguments may include a SI unit (1k, 1M, 1G) Time arguments may include a time unit (2s, 2m, 2h) 常用的参数有下面这些： -t，线程数（线程数不要太多，是核数的 2 到 4 倍就行，多了反而会因为线程切换过多造成效率降低）。 -c，并发数。 -d，测试的持续时间，默认为 10s。 -T，请求超时时间。 -H，指定请求的 HTTP Header，有些 API 需要传入一些 Header，可通过 wrk 的 -H 参数来传入。 –latency，打印响应时间分布。 -s，指定 Lua 脚本，Lua 脚本可以实现更复杂的请求。 然后，我们来看一个 wrk 的测试结果，并对结果进行解析。一个简单的测试如下（确保 iam-apiserver 已经启动，并且开启了健康检查）： $ wrk -t144 -c30000 -d30s -T30s --latency http://10.0.4.57:8080/healthz Running 30s test @ http://10.0.4.57:8080/healthz 144 threads and 30000 connections Thread Stats Avg Stdev Max +/- Stdev Latency 508.77ms 604.01ms 9.27s 81.59% Req/Sec 772.48 0.94k 10.45k 86.82% Latency Distribution 50% 413.35ms 75% 948.99ms 90% 1.33s 99% 2.44s 2276265 requests in 30.10s, 412.45MB read Socket errors: connect 1754, read 40, write 0, timeout 0 Requests/sec: 75613.16 Transfer/sec: 13.70MB 下面是对测试结果的解析。 144 threads and 30000 connections：用 144 个线程模拟 20000 个连接，分别对应 -t 和 -c 参数。 Thread Stats 是线程统计，包括 Latency 和 Req/Sec。 Latency：响应时间，有平均值、标准偏差、最大值、正负一个标准差占比。 Req/Sec：每个线程每秒完成的请求数, 同样有平均值、标准偏差、最大值、正负一个标准差占比。 Latency Distribution 是响应时间分布。 50%：50% 的响应时间为 413.35ms。 75%：75% 的响应时间为 948.99ms。 90%：90% 的响应时间为 1.33s。 99%：99% 的响应时间为 2.44s。 2276265 requests in 30.10s, 412.45MB read：30.10s 完成的总请求数（2276265）和数据读取量（412.45MB）。 Socket errors: connect 1754, read 40, write 0, timeout 0：错误统计，会统计 connect 连接失败请求个数（1754）、读失败请求个数、写失败请求个数、超时请求个数。 Requests/sec：QPS。 Transfer/sec：平均每秒读取 13.70MB 数据（吞吐量）。 ","date":"2022-07-07 15:20:30","objectID":"/iam_service_test/:4:2","tags":["iam"],"title":"iam_service_test","uri":"/iam_service_test/"},{"categories":["iam"],"content":"API Server 性能测试实践 接下来，我们就来测试下 API Server 的性能。影响 API Server 性能的因素有很多，除了 iam-apiserver 自身的原因之外，服务器的硬件和配置、测试方法、网络环境等都会影响。为了方便你对照性能测试结果，我给出了我的测试环境配置，你可以参考下。 客户端硬件配置：1 核 4G。 客户端软件配置：干净的CentOS Linux release 8.2.2004 (Core)。 服务端硬件配置：2 核 8G。 服务端软件配置：干净的CentOS Linux release 8.2.2004 (Core)。 测试网络环境：腾讯云 VPC 内访问，除了性能测试程序外，没有其他资源消耗型业务程序。 测试架构如下图所示： 性能测试脚本介绍 在做 API Server 的性能测试时，需要先执行 wrk，生成性能测试数据。为了能够更直观地查看性能数据，我们还需要以图表的方式展示这些性能数据。这一讲，我使用 gnuplot 工具来自动化地绘制这些性能图，为此我们需要确保 Linux 服务器已经安装了 gnuplot 工具。你可以通过以下方式安装： $ sudo yum -y install gnuplot 在这一讲的测试中，我会绘制下面这两张图，通过它们来观测和分析 API Server 的性能。 QPS \u0026 TTLB 图：X轴为并发数（Concurrent），Y轴为每秒查询数（QPS）和请求响应时间（TTLB）。 成功率图：X轴为并发数（Concurrent），Y轴为请求成功率。 为了方便你测试 API 接口性能，我将性能测试和绘图逻辑封装在scripts/wrktest.sh脚本中，你可以在 iam 源码根目录下执行如下命令，生成性能测试数据和性能图表： $ scripts/wrktest.sh http://10.0.4.57:8080/healthz 上面的命令会执行性能测试，记录性能测试数据，并根据这些性能测试数据绘制出 QPS 和成功率图。接下来，我再来介绍下 wrktest.sh 性能测试脚本，并给出一个使用示例。 wrktest.sh 性能测试脚本，用来测试 API Server 的性能，记录测试的性能数据，并根据性能数据使用 gnuplot 绘制性能图。wrktest.sh 也可以对比前后两次的性能测试结果，并将对比结果通过图表展示出来。wrktest.sh 会根据 CPU 的核数自动计算出适合的 wrk 启动线程数（-t）：CPU核数 * 3。 wrktest.sh 默认会测试多个并发下的 API 性能，默认测试的并发数为200 500 1000 3000 5000 10000 15000 20000 25000 50000。你需要根据自己的服务器配置选择测试的最大并发数，我因为服务器配置不高（主要是8G内存在高并发下，很容易就耗尽），最大并发数选择了50000。如果你的服务器配置够高，可以再依次尝试下测试 100000 、200000 、500000 、1000000 并发下的 API 性能。 wrktest.sh 的使用方法如下： $ scripts/wrktest.sh -h Usage: scripts/wrktest.sh [OPTION] [diff] URL Performance automation test script. URL HTTP request url, like: http://10.0.4.57:8080/healthz diff Compare two performance test results OPTIONS: -h Usage information -n Performance test task name, default: apiserver -d Directory used to store performance data and gnuplot graphic, default: _output/wrk Reprot bugs to \u003ccolin404@foxmail.com\u003e. wrktest.sh 提供的命令行参数介绍如下。 URL：需要测试的 API 接口。 diff：如果比较两次测试的结果，需要执行 wrktest.sh diff。 -n：本次测试的任务名，wrktest.sh 会根据任务名命名生成的文件。 -d：输出文件存放目录。 -h：打印帮助信息。 下面，我来展示一个 wrktest.sh 使用示例。wrktest.sh 的主要功能有两个，分别是运行性能测试并获取结果和对比性能测试结果。下面我就分别介绍下它们的具体使用方法。 运行性能测试并获取结果执行如下命令： $ scripts/wrktest.sh http://10.0.4.57:8080/healthz Running wrk command: wrk -t3 -d300s -T30s --latency -c 200 http://10.0.4.57:8080/healthz Running wrk command: wrk -t3 -d300s -T30s --latency -c 500 http://10.0.4.57:8080/healthz Running wrk command: wrk -t3 -d300s -T30s --latency -c 1000 http://10.0.4.57:8080/healthz Running wrk command: wrk -t3 -d300s -T30s --latency -c 3000 http://10.0.4.57:8080/healthz Running wrk command: wrk -t3 -d300s -T30s --latency -c 5000 http://10.0.4.57:8080/healthz Running wrk command: wrk -t3 -d300s -T30s --latency -c 10000 http://10.0.4.57:8080/healthz Running wrk command: wrk -t3 -d300s -T30s --latency -c 15000 http://10.0.4.57:8080/healthz Running wrk command: wrk -t3 -d300s -T30s --latency -c 20000 http://10.0.4.57:8080/healthz Running wrk command: wrk -t3 -d300s -T30s --latency -c 25000 http://10.0.4.57:8080/healthz Running wrk command: wrk -t3 -d300s -T30s --latency -c 50000 http://10.0.4.57:8080/healthz Now plot according to /home/colin/_output/wrk/apiserver.dat QPS graphic file is: /home/colin/_output/wrk/apiserver_qps_ttlb.png Success rate graphic file is: /home/colin/_output/wrk/apiserver_successrate.pngz 上面的命令默认会在_output/wrk/目录下生成 3 个文件： apiserver.dat，wrk 性能测试结果，每列含义分别为并发数、QPS 平均响应时间、成功率。 apiserver_qps_ttlb.png，QPS\u0026TTLB 图。 apiserver_successrate.png，成功率图。 这里要注意，请求 URL 中的 IP 地址应该是腾讯云 VPC 内网地址，因为通过内网访问，不仅网络延时最低，而且还最安全，所以真实的业务通常都是内网访问的。 对比性能测试结果 假设我们还有另外一次 API 性能测试，测试数据保存在 _output/wrk/http.dat 文件中。执行如下命令，对比两次测试结果： $ scripts/wrktest.sh diff _output/wrk/apiserver.dat _output/wrk/http.dat apiserver.dat和http.dat是两个需要对比的 Wrk 性能数据文件。上述命令默认会在_output/wrk目录下生成下面这两个文件： apiserver_http.qps.ttlb.diff.png，QPS \u0026 TTLB 对比图。 apiserver_http.success_rate.diff.png，成功率对比图。 关闭 Debug 配置选项 在测试之前，我们需要关闭一些 Debug 选项，以免影响性能测试。执行下面这两步操作，修改 iam-apiserver 的配置文件： 将server.mode设置为 release，server.middlewares去掉 dump、logger 中间件。 将log.level设置为 info，log.output-paths去掉 stdout。 因为我们要在执行压力测试时分析程序的性能，所以需要设置feature.profiling为 true，以开启性能分析。修改完之后，重新启动 iam-apiserver。 使用 wrktest.sh 测试 IAM API","date":"2022-07-07 15:20:30","objectID":"/iam_service_test/:4:3","tags":["iam"],"title":"iam_service_test","uri":"/iam_service_test/"},{"categories":["iam"],"content":"API Server 性能分析 上面，我们测试了 API 接口的性能，如果性能不合预期，我们还需要分析性能数据，并优化性能。 在分析前我们需要对 API Server 加压，在加压的情况下，API 接口的性能才更可能暴露出来，所以继续执行如下命令： $ scripts/wrktest.sh http://10.0.4.57:8080/healthz 在上述命令执行压力测试期间，可以打开另外一个 Linux 终端，使用go tool pprof工具分析 HTTP 的 profile 文件： $ go tool pprof http://10.0.4.57:8080/debug/pprof/profile 执行完go tool pprof后，因为需要采集性能数据，所以该命令会阻塞 30s。在 pprof 交互 shell 中，执行top -cum查看累积采样时间，我们执行top30 -cum，多观察一些函数： (pprof) top20 -cum Showing nodes accounting for 32.12s, 39.62% of 81.07s total Dropped 473 nodes (cum \u003c= 0.41s) Showing top 20 nodes out of 167 (pprof) top30 -cum Showing nodes accounting for 11.82s, 20.32% of 58.16s total Dropped 632 nodes (cum \u003c= 0.29s) Showing top 30 nodes out of 239 flat flat% sum% cum cum% 0.10s 0.17% 0.17% 51.59s 88.70% net/http.(*conn).serve 0.01s 0.017% 0.19% 42.86s 73.69% net/http.serverHandler.ServeHTTP 0.04s 0.069% 0.26% 42.83s 73.64% github.com/gin-gonic/gin.(*Engine).ServeHTTP 0.01s 0.017% 0.28% 42.67s 73.37% github.com/gin-gonic/gin.(*Engine).handleHTTPRequest 0.08s 0.14% 0.41% 42.59s 73.23% github.com/gin-gonic/gin.(*Context).Next (inline) 0.03s 0.052% 0.46% 42.58s 73.21% .../internal/pkg/middleware.RequestID.func1 0 0% 0.46% 41.02s 70.53% .../internal/pkg/middleware.Context.func1 0.01s 0.017% 0.48% 40.97s 70.44% github.com/gin-gonic/gin.CustomRecoveryWithWriter.func1 0.03s 0.052% 0.53% 40.95s 70.41% .../internal/pkg/middleware.LoggerWithConfig.func1 0.01s 0.017% 0.55% 33.46s 57.53% .../internal/pkg/middleware.NoCache 0.08s 0.14% 0.69% 32.58s 56.02% github.com/tpkeeper/gin-dump.DumpWithOptions.func1 0.03s 0.052% 0.74% 24.73s 42.52% github.com/tpkeeper/gin-dump.FormatToBeautifulJson 0.02s 0.034% 0.77% 22.73s 39.08% github.com/tpkeeper/gin-dump.BeautifyJsonBytes 0.08s 0.14% 0.91% 16.39s 28.18% github.com/tpkeeper/gin-dump.format 0.21s 0.36% 1.27% 16.38s 28.16% github.com/tpkeeper/gin-dump.formatMap 3.75s 6.45% 7.72% 13.71s 23.57% runtime.mallocgc ... 因为top30内容过多，这里只粘贴了耗时最多的一些关联函数。从上面的列表中，可以看到有 ServeHTTP 类的函数，这些函数是 gin/http 自带的函数，我们无需对此进行优化。还有这样一些函数： .../gin.(*Context).Next (inline) .../internal/pkg/middleware.RequestID.func1 .../internal/pkg/middleware.Context.func1 github.com/gin-gonic/gin.CustomRecoveryWithWriter.func1 .../internal/pkg/middleware.LoggerWithConfig.func1 .../internal/pkg/middleware.NoCache github.com/tpkeeper/gin-dump.DumpWithOptions.func1 可以看到，middleware.RequestID.func1、middleware.Context.func1、gin.CustomRecoveryWithWriter.func1、middleware.LoggerWithConfig.func1等，这些耗时较久的函数都是我们加载的 Gin 中间件。这些中间件消耗了大量的 CPU 时间，所以我们可以选择性加载这些中间件，删除一些不需要的中间件，来优化 API Server 的性能。 假如我们暂时不需要这些中间件，也可以通过配置 iam-apiserver 的配置文件，将server.middlewares设置为空或者注释掉，然后重启 iam-apiserver。重启后，再次执行wrktest.sh测试性能，并跟原生的 HTTP Server 性能进行对比，对比结果如下面 2 张图所示： 可以看到，删除无用的 Gin 中间件后，API Server 的性能有了很大的提升，并发数为200时性能最好，此时 QPS 为47812，响应时间为4.33ms，成功率为100.00%。在并发数为50000的时候，其 QPS 是原生 HTTP Server 的75.02%。 ","date":"2022-07-07 15:20:30","objectID":"/iam_service_test/:4:4","tags":["iam"],"title":"iam_service_test","uri":"/iam_service_test/"},{"categories":["iam"],"content":"API 接口性能参考 不同团队对 API 接口的性能要求不同，同一团队对每个 API 接口的性能要求也不同，所以并没有一个统一的数值标准来衡量 API 接口的性能，但可以肯定的是，性能越高越好。我根据自己的研发经验，在这里给出一个参考值（并发数可根据需要选择），如下表所示： ","date":"2022-07-07 15:20:30","objectID":"/iam_service_test/:4:5","tags":["iam"],"title":"iam_service_test","uri":"/iam_service_test/"},{"categories":["iam"],"content":"API Server 性能测试注意事项 在进行 API Server 性能测试时，要考虑到 API Server 的性能影响因素。影响 API Server 性能的因素很多，大致可以分为两类，分别是 Web 框架的性能和 API 接口的性能。另外，在做性能测试时，还需要确保测试环境是一致的，最好是一个干净的测试环境。 ","date":"2022-07-07 15:20:30","objectID":"/iam_service_test/:4:6","tags":["iam"],"title":"iam_service_test","uri":"/iam_service_test/"},{"categories":["iam"],"content":"Web 框架性能 Web 框架的性能至关重要，因为它会影响我们的每一个 API 接口的性能。在设计阶段，我们会确定所使用的 Web 框架，这时候我们需要对 Web 框架有个初步的测试，确保我们选择的 Web 框架在性能和稳定性上都足够优秀。当整个 Go 后端服务开发完成之后，在上线之前，我们还需要对 Web 框架再次进行测试，确保按照我们最终的使用方式，Web 框架仍然能够保持优秀的性能和稳定性。 我们通常会通过 API 接口来测试 Web 框架的性能，例如健康检查接口/healthz。我们需要保证该 API 接口足够简单，API 接口里面不应该掺杂任何逻辑，只需要象征性地返回一个很小的返回内容即可。比如，这一讲中我们通过/healthz接口来测试 Web 框架的性能： s.GET(\"/healthz\", func(c *gin.Context) { core.WriteResponse(c, nil, map[string]string{\"status\": \"ok\"}) }) 接口中只调用了core.WriteResponse函数，返回了{“status”:“ok”}。这里使用core.WriteResponse函数返回请求数据，而不是直接返回ok字符串，这样做是为了保持 API 接口返回格式统一。 ","date":"2022-07-07 15:20:30","objectID":"/iam_service_test/:4:7","tags":["iam"],"title":"iam_service_test","uri":"/iam_service_test/"},{"categories":["iam"],"content":"API 接口性能 除了测试 Web 框架的性能，我们还可能需要测试某些重要的 API 接口，甚至所有 API 接口的性能。为了测试 API 接口在真实场景下的接口性能，我们会使用 wrk 这类 HTTP 压力测试工具，来模拟多个 API 请求，进而分析 API 的性能。 因为会模拟大量的请求，这时候测试写类接口，例如Create、Update、Delete等会存在一些问题，比如可能在数据库中插入了很多数据，导致磁盘空间被写满或者数据库被压爆。所以，针对写类接口，我们可以借助单元测试，来测试其性能。根据我的开发经验，写类接口通常不会有性能问题，反而读类接口更可能遇到性能问题。针对读类接口，我们可以使用 wrk 这类 HTTP 压力测试工具来进行测试。 ","date":"2022-07-07 15:20:30","objectID":"/iam_service_test/:4:8","tags":["iam"],"title":"iam_service_test","uri":"/iam_service_test/"},{"categories":["iam"],"content":"测试环境 在做性能 / 压力测试时，为了不影响生产环境，要确保在测试环境进行压测，并且测试环境的网络不能影响到生产环境的网络。另外，为了更好地进行性能对比和分析，也要保证我们的测试方法和测试环境是一致的。这就要求我们最好将性能测试自动化，并且每次在同一个测试环境进行测试。 ","date":"2022-07-07 15:20:30","objectID":"/iam_service_test/:4:9","tags":["iam"],"title":"iam_service_test","uri":"/iam_service_test/"},{"categories":["iam"],"content":"总结 在项目上线前，我们需要对 API 接口进行性能测试。通常 API 接口的性能延时要小于 500ms ，如果大于这个值，需要考虑优化性能。在进行性能测试时，需要确保每次测试都有一个一致的测试环境，这样不同测试之间的数据才具有可对比性。这一讲中，我推荐了一个比较优秀的性能测试工具 wrk ，我们可以编写 shell 脚本，将 wrk 的性能测试数据自动绘制成图，方便我们查看、对比性能。 总结在项目上线前，我们需要对 API 接口进行性能测试。通常 API 接口的性能延时要小于 500ms ，如果大于这个值，需要考虑优化性能。在进行性能测试时，需要确保每次测试都有一个一致的测试环境，这样不同测试之间的数据才具有可对比性。这一讲中，我推荐了一个比较优秀的性能测试工具 wrk ，我们可以编写 shell 脚本，将 wrk 的性能测试数据自动绘制成图，方便我们查看、对比性能。 ","date":"2022-07-07 15:20:30","objectID":"/iam_service_test/:4:10","tags":["iam"],"title":"iam_service_test","uri":"/iam_service_test/"},{"categories":["iam"],"content":"服务开发 ","date":"2022-07-07 15:20:13","objectID":"/iam_service_develop/:0:0","tags":["iam"],"title":"iam_service_develop","uri":"/iam_service_develop/"},{"categories":["iam"],"content":"24 | Web 服务：Web 服务核心功能有哪些，如何实现？ 在 Go 项目开发中，绝大部分情况下，我们是在写能提供某种功能的后端服务，这些功能以 RPC API 接口或者 RESTful API 接口的形式对外提供，能提供这两种 API 接口的服务也统称为 Web 服务。今天这一讲，我就通过介绍 RESTful API 风格的 Web 服务，来给你介绍下如何实现 Web 服务的核心功能。 ","date":"2022-07-07 15:20:13","objectID":"/iam_service_develop/:1:0","tags":["iam"],"title":"iam_service_develop","uri":"/iam_service_develop/"},{"categories":["iam"],"content":"Web 服务的核心功能 Web 服务有很多功能，为了便于你理解，我将这些功能分成了基础功能和高级功能两大类，并总结在了下面这张图中： 下面，我就按图中的顺序，来串讲下这些功能。 要实现一个 Web 服务，首先我们要选择通信协议和通信格式。在 Go 项目开发中，有 HTTP+JSON 和 gRPC+Protobuf 两种组合可选。因为 iam-apiserver 主要提供的是 REST 风格的 API 接口，所以选择的是 HTTP+JSON 组合。 Web 服务最核心的功能是路由匹配。路由匹配其实就是根据(HTTP方法, 请求路径)匹配到处理这个请求的函数，最终由该函数处理这次请求，并返回结果，过程如下图所示： 一次 HTTP 请求经过路由匹配，最终将请求交由Delete(c *gin.Context)函数来处理。变量c中存放了这次请求的参数，在 Delete 函数中，我们可以进行参数解析、参数校验、逻辑处理，最终返回结果。对于大型系统，可能会有很多个 API 接口，API 接口随着需求的更新迭代，可能会有多个版本，为了便于管理，我们需要对路由进行分组。有时候，我们需要在一个服务进程中，同时开启 HTTP 服务的 80 端口和 HTTPS 的 443 端口，这样我们就可以做到：对内的服务，访问 80 端口，简化服务访问复杂度；对外的服务，访问更为安全的 HTTPS 服务。显然，我们没必要为相同功能启动多个服务进程，所以这时候就需要 Web 服务能够支持一进程多服务的功能。 我们开发 Web 服务最核心的诉求是：输入一些参数，校验通过后，进行业务逻辑处理，然后返回结果。所以 Web 服务还应该能够进行参数解析、参数校验、逻辑处理、返回结果。这些都是 Web 服务的业务处理功能。 上面这些是 Web 服务的基本功能，此外，我们还需要支持一些高级功能。在进行 HTTP 请求时，经常需要针对每一次请求都设置一些通用的操作，比如添加 Header、添加 RequestID、统计请求次数等，这就要求我们的 Web 服务能够支持中间件特性。为了保证系统安全，对于每一个请求，我们都需要进行认证。Web 服务中，通常有两种认证方式，一种是基于用户名和密码，一种是基于 Token。认证通过之后，就可以继续处理请求了。为了方便定位和跟踪某一次请求，需要支持 RequestID，定位和跟踪 RequestID 主要是为了排障。 最后，当前的软件架构中，很多采用了前后端分离的架构。在前后端分离的架构中，前端访问地址和后端访问地址往往是不同的，浏览器为了安全，会针对这种情况设置跨域请求，所以 Web 服务需要能够处理浏览器的跨域请求。 到这里，我就把 Web 服务的基础功能和高级功能串讲了一遍。当然，上面只介绍了 Web 服务的核心功能，还有很多其他的功能，你可以通过学习Gin 的官方文档来了解。 你可以看到，Web 服务有很多核心功能，这些功能我们可以基于 net/http 包自己封装。但在实际的项目开发中， 我们更多会选择使用基于 net/http 包进行封装的优秀开源 Web 框架。本实战项目选择了 Gin 框架。接下来，我们主要看下 Gin 框架是如何实现以上核心功能的，这些功能我们在实际的开发中可以直接拿来使用。 ","date":"2022-07-07 15:20:13","objectID":"/iam_service_develop/:1:1","tags":["iam"],"title":"iam_service_develop","uri":"/iam_service_develop/"},{"categories":["iam"],"content":"为什么选择 Gin 框架？ 优秀的 Web 框架有很多，我们为什么要选择 Gin 呢？在回答这个问题之前，我们先来看下选择 Web 框架时的关注点。 在选择 Web 框架时，我们可以关注如下几点：路由功能；是否具备 middleware/filter 能力；HTTP 参数（path、query、form、header、body）解析和返回；性能和稳定性；使用复杂度；社区活跃度。 按 GitHub Star 数来排名，当前比较火的 Go Web 框架有 Gin、Beego、Echo、Revel 、Martini。经过调研，我从中选择了 Gin 框架，原因是 Gin 具有如下特性： 轻量级，代码质量高，性能比较高；项目目前很活跃，并有很多可用的 Middleware；作为一个 Web 框架，功能齐全，使用起来简单。 那接下来，我就先详细介绍下 Gin 框架。Gin是用 Go 语言编写的 Web 框架，功能完善，使用简单，性能很高。Gin 核心的路由功能是通过一个定制版的HttpRouter来实现的，具有很高的路由性能。 Gin 有很多功能，这里我给你列出了它的一些核心功能： 支持 HTTP 方法：GET、POST、PUT、PATCH、DELETE、OPTIONS。 支持不同位置的 HTTP 参数：路径参数（path）、查询字符串参数（query）、表单参数（form）、HTTP 头参数（header）、消息体参数（body）。 支持 HTTP 路由和路由分组。 支持 middleware 和自定义 middleware。 支持自定义 Log。 支持 binding 和 validation， 支持自定义 validator。可以 bind 如下参数：query、path、body、header、form。 支持重定向。 支持 basic auth middleware。 支持自定义 HTTP 配置。 支持优雅关闭。 支持 HTTP2。 支持设置和获取 cookie。 ","date":"2022-07-07 15:20:13","objectID":"/iam_service_develop/:1:2","tags":["iam"],"title":"iam_service_develop","uri":"/iam_service_develop/"},{"categories":["iam"],"content":"Gin 是如何支持 Web 服务基础功能的？ 接下来，我们先通过一个具体的例子，看下 Gin 是如何支持 Web 服务基础功能的，后面再详细介绍这些功能的用法。我们创建一个 webfeature 目录，用来存放示例代码。因为要演示 HTTPS 的用法，所以需要创建证书文件。具体可以分为两步。 第一步，执行以下命令创建证书： cat \u003c\u003c 'EOF' \u003e ca.pem -----BEGIN CERTIFICATE----- MIICSjCCAbOgAwIBAgIJAJHGGR4dGioHMA0GCSqGSIb3DQEBCwUAMFYxCzAJBgNV BAYTAkFVMRMwEQYDVQQIEwpTb21lLVN0YXRlMSEwHwYDVQQKExhJbnRlcm5ldCBX aWRnaXRzIFB0eSBMdGQxDzANBgNVBAMTBnRlc3RjYTAeFw0xNDExMTEyMjMxMjla Fw0yNDExMDgyMjMxMjlaMFYxCzAJBgNVBAYTAkFVMRMwEQYDVQQIEwpTb21lLVN0 YXRlMSEwHwYDVQQKExhJbnRlcm5ldCBXaWRnaXRzIFB0eSBMdGQxDzANBgNVBAMT BnRlc3RjYTCBnzANBgkqhkiG9w0BAQEFAAOBjQAwgYkCgYEAwEDfBV5MYdlHVHJ7 +L4nxrZy7mBfAVXpOc5vMYztssUI7mL2/iYujiIXM+weZYNTEpLdjyJdu7R5gGUu g1jSVK/EPHfc74O7AyZU34PNIP4Sh33N+/A5YexrNgJlPY+E3GdVYi4ldWJjgkAd Qah2PH5ACLrIIC6tRka9hcaBlIECAwEAAaMgMB4wDAYDVR0TBAUwAwEB/zAOBgNV HQ8BAf8EBAMCAgQwDQYJKoZIhvcNAQELBQADgYEAHzC7jdYlzAVmddi/gdAeKPau sPBG/C2HCWqHzpCUHcKuvMzDVkY/MP2o6JIW2DBbY64bO/FceExhjcykgaYtCH/m oIU63+CFOTtR7otyQAWHqXa7q4SbCDlG7DyRFxqG0txPtGvy12lgldA2+RgcigQG Dfcog5wrJytaQ6UA0wE= -----END CERTIFICATE----- EOF cat \u003c\u003c 'EOF' \u003e server.key -----BEGIN PRIVATE KEY----- MIICdQIBADANBgkqhkiG9w0BAQEFAASCAl8wggJbAgEAAoGBAOHDFScoLCVJpYDD M4HYtIdV6Ake/sMNaaKdODjDMsux/4tDydlumN+fm+AjPEK5GHhGn1BgzkWF+slf 3BxhrA/8dNsnunstVA7ZBgA/5qQxMfGAq4wHNVX77fBZOgp9VlSMVfyd9N8YwbBY AckOeUQadTi2X1S6OgJXgQ0m3MWhAgMBAAECgYAn7qGnM2vbjJNBm0VZCkOkTIWm V10okw7EPJrdL2mkre9NasghNXbE1y5zDshx5Nt3KsazKOxTT8d0Jwh/3KbaN+YY tTCbKGW0pXDRBhwUHRcuRzScjli8Rih5UOCiZkhefUTcRb6xIhZJuQy71tjaSy0p dHZRmYyBYO2YEQ8xoQJBAPrJPhMBkzmEYFtyIEqAxQ/o/A6E+E4w8i+KM7nQCK7q K4JXzyXVAjLfyBZWHGM2uro/fjqPggGD6QH1qXCkI4MCQQDmdKeb2TrKRh5BY1LR 81aJGKcJ2XbcDu6wMZK4oqWbTX2KiYn9GB0woM6nSr/Y6iy1u145YzYxEV/iMwff DJULAkB8B2MnyzOg0pNFJqBJuH29bKCcHa8gHJzqXhNO5lAlEbMK95p/P2Wi+4Hd aiEIAF1BF326QJcvYKmwSmrORp85AkAlSNxRJ50OWrfMZnBgzVjDx3xG6KsFQVk2 ol6VhqL6dFgKUORFUWBvnKSyhjJxurlPEahV6oo6+A+mPhFY8eUvAkAZQyTdupP3 XEFQKctGz+9+gKkemDp7LBBMEMBXrGTLPhpEfcjv/7KPdnFHYmhYeBTBnuVmTVWe F98XJ7tIFfJq -----END PRIVATE KEY----- EOF cat \u003c\u003c 'EOF' \u003e server.pem -----BEGIN CERTIFICATE----- MIICnDCCAgWgAwIBAgIBBzANBgkqhkiG9w0BAQsFADBWMQswCQYDVQQGEwJBVTET MBEGA1UECBMKU29tZS1TdGF0ZTEhMB8GA1UEChMYSW50ZXJuZXQgV2lkZ2l0cyBQ dHkgTHRkMQ8wDQYDVQQDEwZ0ZXN0Y2EwHhcNMTUxMTA0MDIyMDI0WhcNMjUxMTAx MDIyMDI0WjBlMQswCQYDVQQGEwJVUzERMA8GA1UECBMISWxsaW5vaXMxEDAOBgNV BAcTB0NoaWNhZ28xFTATBgNVBAoTDEV4YW1wbGUsIENvLjEaMBgGA1UEAxQRKi50 ZXN0Lmdvb2dsZS5jb20wgZ8wDQYJKoZIhvcNAQEBBQADgY0AMIGJAoGBAOHDFSco LCVJpYDDM4HYtIdV6Ake/sMNaaKdODjDMsux/4tDydlumN+fm+AjPEK5GHhGn1Bg zkWF+slf3BxhrA/8dNsnunstVA7ZBgA/5qQxMfGAq4wHNVX77fBZOgp9VlSMVfyd 9N8YwbBYAckOeUQadTi2X1S6OgJXgQ0m3MWhAgMBAAGjazBpMAkGA1UdEwQCMAAw CwYDVR0PBAQDAgXgME8GA1UdEQRIMEaCECoudGVzdC5nb29nbGUuZnKCGHdhdGVy em9vaS50ZXN0Lmdvb2dsZS5iZYISKi50ZXN0LnlvdXR1YmUuY29thwTAqAEDMA0G CSqGSIb3DQEBCwUAA4GBAJFXVifQNub1LUP4JlnX5lXNlo8FxZ2a12AFQs+bzoJ6 hM044EDjqyxUqSbVePK0ni3w1fHQB5rY9yYC5f8G7aqqTY1QOhoUk8ZTSTRpnkTh y4jjdvTZeLDVBlueZUTDRmy2feY5aZIU18vFDK08dTG0A87pppuv1LNIR3loveU8 -----END CERTIFICATE----- EOF 第二步，创建 main.go 文件： package main import ( \"fmt\" \"log\" \"net/http\" \"sync\" \"time\" \"github.com/gin-gonic/gin\" \"golang.org/x/sync/errgroup\" ) type Product struct { Username string `json:\"username\" binding:\"required\"` Name string `json:\"name\" binding:\"required\"` Category string `json:\"category\" binding:\"required\"` Price int `json:\"price\" binding:\"gte=0\"` Description string `json:\"description\"` CreatedAt time.Time `json:\"createdAt\"` } type productHandler struct { sync.RWMutex products map[string]Product } func newProductHandler() *productHandler { return \u0026productHandler{ products: make(map[string]Product), } } func (u *productHandler) Create(c *gin.Context) { u.Lock() defer u.Unlock() // 1. 参数解析 var product Product if err := c.ShouldBindJSON(\u0026product); err != nil { c.JSON(http.StatusBadRequest, gin.H{\"error\": err.Error()}) return } // 2. 参数校验 if _, ok := u.products[product.Name]; ok { c.JSON(http.StatusBadRequest, gin.H{\"error\": fmt.Sprintf(\"product %s already exist\", product.Name)}) return } product.CreatedAt = ","date":"2022-07-07 15:20:13","objectID":"/iam_service_develop/:1:3","tags":["iam"],"title":"iam_service_develop","uri":"/iam_service_develop/"},{"categories":["iam"],"content":"Gin 是如何支持 Web 服务高级功能的？ 上面介绍了 Web 服务的基础功能，这里我再来介绍下高级功能。Web 服务可以具备多个高级功能，但比较核心的高级功能是中间件、认证、RequestID、跨域和优雅关停。 中间件 Gin 支持中间件，HTTP 请求在转发到实际的处理函数之前，会被一系列加载的中间件进行处理。在中间件中，可以解析 HTTP 请求做一些逻辑处理，例如：跨域处理或者生成 X-Request-ID 并保存在 context 中，以便追踪某个请求。处理完之后，可以选择中断并返回这次请求，也可以选择将请求继续转交给下一个中间件处理。当所有的中间件都处理完之后，请求才会转给路由函数进行处理。具体流程如下图： 通过中间件，可以实现对所有请求都做统一的处理，提高开发效率，并使我们的代码更简洁。但是，因为所有的请求都需要经过中间件的处理，可能会增加请求延时。对于中间件特性，我有如下建议： 中间件做成可加载的，通过配置文件指定程序启动时加载哪些中间件。 只将一些通用的、必要的功能做成中间件。 在编写中间件时，一定要保证中间件的代码质量和性能。 在 Gin 中，可以通过 gin.Engine 的 Use 方法来加载中间件。中间件可以加载到不同的位置上，而且不同的位置作用范围也不同，例如： router := gin.New() router.Use(gin.Logger(), gin.Recovery()) // 中间件作用于所有的HTTP请求 v1 := router.Group(\"/v1\").Use(gin.BasicAuth(gin.Accounts{\"foo\": \"bar\", \"colin\": \"colin404\"})) // 中间件作用于v1 group v1.POST(\"/login\", Login).Use(gin.BasicAuth(gin.Accounts{\"foo\": \"bar\", \"colin\": \"colin404\"})) //中间件只作用于/v1/login API接口 Gin 框架本身支持了一些中间件。 gin.Logger()：Logger 中间件会将日志写到 gin.DefaultWriter，gin.DefaultWriter 默认为 os.Stdout。 gin.Recovery()：Recovery 中间件可以从任何 panic 恢复，并且写入一个 500 状态码。 gin.CustomRecovery(handle gin.RecoveryFunc)：类似 Recovery 中间件，但是在恢复时还会调用传入的 handle 方法进行处理。 gin.BasicAuth()：HTTP 请求基本认证（使用用户名和密码进行认证）。 另外，Gin 还支持自定义中间件。中间件其实是一个函数，函数类型为 gin.HandlerFunc，HandlerFunc 底层类型为 func(*Context)。如下是一个 Logger 中间件的实现： package main import ( \"log\" \"time\" \"github.com/gin-gonic/gin\" ) func Logger() gin.HandlerFunc { return func(c *gin.Context) { t := time.Now() // 设置变量example c.Set(\"example\", \"12345\") // 请求之前 c.Next() // 请求之后 latency := time.Since(t) log.Print(latency) // 访问我们发送的状态 status := c.Writer.Status() log.Println(status) } } func main() { r := gin.New() r.Use(Logger()) r.GET(\"/test\", func(c *gin.Context) { example := c.MustGet(\"example\").(string) // it would print: \"12345\" log.Println(example) }) // Listen and serve on 0.0.0.0:8080 r.Run(\":8080\") } 另外，还有很多开源的中间件可供我们选择，我把一些常用的总结在了表格里： 认证、RequestID、跨域 认证、RequestID、跨域这三个高级功能，都可以通过 Gin 的中间件来实现，例如： router := gin.New() // 认证 router.Use(gin.BasicAuth(gin.Accounts{\"foo\": \"bar\", \"colin\": \"colin404\"})) // RequestID router.Use(requestid.New(requestid.Config{ Generator: func() string { return \"test\" }, })) // 跨域 // CORS for https://foo.com and https://github.com origins, allowing: // - PUT and PATCH methods // - Origin header // - Credentials share // - Preflight requests cached for 12 hours router.Use(cors.New(cors.Config{ AllowOrigins: []string{\"https://foo.com\"}, AllowMethods: []string{\"PUT\", \"PATCH\"}, AllowHeaders: []string{\"Origin\"}, ExposeHeaders: []string{\"Content-Length\"}, AllowCredentials: true, AllowOriginFunc: func(origin string) bool { return origin == \"https://github.com\" }, MaxAge: 12 * time.Hour, })) 优雅关停 Go 项目上线后，我们还需要不断迭代来丰富项目功能、修复 Bug 等，这也就意味着，我们要不断地重启 Go 服务。对于 HTTP 服务来说，如果访问量大，重启服务的时候可能还有很多连接没有断开，请求没有完成。如果这时候直接关闭服务，这些连接会直接断掉，请求异常终止，这就会对用户体验和产品口碑造成很大影响。因此，这种关闭方式不是一种优雅的关闭方式。 这时候，我们期望 HTTP 服务可以在处理完所有请求后，正常地关闭这些连接，也就是优雅地关闭服务。我们有两种方法来优雅关闭 HTTP 服务，分别是借助第三方的 Go 包和自己编码实现。 方法一：借助第三方的 Go 包如果使用第三方的 Go 包来实现优雅关闭，目前用得比较多的包是fvbock/endless。我们可以使用 fvbock/endless 来替换掉 net/http 的 ListenAndServe 方法，例如： router := gin.Default() router.GET(\"/\", handler) // [...] endless.ListenAndServe(\":4242\", router) 方法二：编码实现借助第三方包的好处是可以稍微减少一些编码工作量，但缺点是引入了一个新的依赖包，因此我更倾向于自己编码实现。Go 1.8 版本或者更新的版本，http.Server 内置的 Shutdown 方法，已经实现了优雅关闭。下面是一个示例： // +build go1.8 package main import ( \"context\" \"log\" \"net/http\" \"os\" \"os/signal\" \"syscall\" \"time\" \"github.com/gin-gonic/gin\" ) func main() { router := gin.Default() router.GET(\"/\", func(c *gin.Context) { time.Sleep(5 * time.Second) c.String(http.StatusOK, \"Welcome Gin Server\") }) srv := \u0026http.Server{ Addr: \":8080\", Handler: router, } // Initializing the server in a goroutine so that // it won't block the graceful shutdown handling below go func() { if err := srv.ListenAndServe(); err != nil \u0026\u0026 err != http.ErrServerClosed { log.Fatalf(\"listen: %s\\n\", err) } }() // Wait for interrupt signal to gracefully shutdown the server with // a timeout of 5 seconds. quit := make(chan os.Signal, 1) // kill (no param) default send syscall.SIGTERM // kill -2 is sys","date":"2022-07-07 15:20:13","objectID":"/iam_service_develop/:1:4","tags":["iam"],"title":"iam_service_develop","uri":"/iam_service_develop/"},{"categories":["iam"],"content":"总结 今天我们主要学习了 Web 服务的核心功能，以及如何开发这些功能。在实际的项目开发中， 我们一般会使用基于 net/http 包进行封装的优秀开源 Web 框架。当前比较火的 Go Web 框架有 Gin、Beego、Echo、Revel、Martini。你可以根据需要进行选择。我比较推荐 Gin，Gin 也是目前比较受欢迎的 Web 框架。Gin Web 框架支持 Web 服务的很多基础功能，例如 HTTP/HTTPS、JSON 格式的数据、路由分组和匹配、一进程多服务等。另外，Gin 还支持 Web 服务的一些高级功能，例如 中间件、认证、RequestID、跨域和优雅关停等。 ","date":"2022-07-07 15:20:13","objectID":"/iam_service_develop/:1:5","tags":["iam"],"title":"iam_service_develop","uri":"/iam_service_develop/"},{"categories":["iam"],"content":"25 | 认证机制：应用程序如何进行访问认证？ 保证应用的安全是软件开发的最基本要求，我们有多种途径来保障应用的安全，例如网络隔离、设置防火墙、设置 IP 黑白名单等。不过在我看来，这些更多是从运维角度来解决应用的安全问题。作为开发者，我们也可以从软件层面来保证应用的安全，这可以通过认证来实现。 这一讲，我以 HTTP 服务为例，来给你介绍下当前常见的四种认证方法：Basic、Digest、OAuth、Bearer。还有很多基于这四种方法的变种，这里就不再介绍了。IAM 项目使用了 Basic、Bearer 两种认证方法。这一讲，我先来介绍下这四种认证方法，下一讲，我会给你介绍下 IAM 项目是如何设计和实现访问认证功能的。 ","date":"2022-07-07 15:20:13","objectID":"/iam_service_develop/:2:0","tags":["iam"],"title":"iam_service_develop","uri":"/iam_service_develop/"},{"categories":["iam"],"content":"认证和授权有什么区别？ 在介绍四种基本的认证方法之前，我想先带你区分下认证和授权，这是很多开发者都容易搞混的两个概念。 认证（Authentication，英文缩写 authn）：用来验证某个用户是否具有访问系统的权限。如果认证通过，该用户就可以访问系统，从而创建、修改、删除、查询平台支持的资源。 授权（Authorization，英文缩写 authz）：用来验证某个用户是否具有访问某个资源的权限，如果授权通过，该用户就能对资源做增删改查等操作。 这里，我通过下面的图片，来让你明白二者的区别： 图中，我们有一个仓库系统，用户 james、colin、aaron 分别创建了 Product-A、Product-B、Product-C。现在用户 colin 通过用户名和密码（认证）成功登陆到仓库系统中，但他尝试访问 Product-A、Product-C 失败，因为这两个产品不属于他（授权失败），但他可以成功访问自己创建的资源 Product-B（授权成功）。由此可见：认证证明了你是谁，授权决定了你能做什么。 上面，我们介绍了认证和授权的区别。那么接下来，我们就回到这一讲的重心：应用程序如何进行访问认证。 ","date":"2022-07-07 15:20:13","objectID":"/iam_service_develop/:2:1","tags":["iam"],"title":"iam_service_develop","uri":"/iam_service_develop/"},{"categories":["iam"],"content":"四种基本的认证方式 常见的认证方式有四种，分别是 Basic、Digest、OAuth 和 Bearer。先来看下 Basic 认证。 Basic Basic 认证（基础认证），是最简单的认证方式。它简单地将用户名:密码进行 base64 编码后，放到 HTTP Authorization Header 中。HTTP 请求到达后端服务后，后端服务会解析出 Authorization Header 中的 base64 字符串，解码获取用户名和密码，并将用户名和密码跟数据库中记录的值进行比较，如果匹配则认证通过。例如： $ basic=`echo -n 'admin:Admin@2021'|base64` $ curl -XPOST -H\"Authorization: Basic ${basic}\" http://127.0.0.1:8080/login 通过 base64 编码，可以将密码以非明文的方式传输，增加一定的安全性。但是，base64 不是加密技术，入侵者仍然可以截获 base64 字符串，并反编码获取用户名和密码。另外，即使 Basic 认证中密码被加密，入侵者仍可通过加密后的用户名和密码进行重放攻击。 所以，Basic 认证虽然简单，但极不安全。使用 Basic 认证的唯一方式就是将它和 SSL 配合使用，来确保整个认证过程是安全的。 IAM 项目中，为了支持前端通过用户名和密码登录，仍然使用了 Basic 认证，但前后端使用 HTTPS 来通信，保证了认证的安全性。 这里需要注意，在设计系统时，要遵循一个通用的原则：不要在请求参数中使用明文密码，也不要在任何存储中保存明文密码。 Digest Digest 认证（摘要认证），是另一种 HTTP 认证协议，它与基本认证兼容，但修复了基本认证的严重缺陷。Digest 具有如下特点： 绝不会用明文方式在网络上发送密码。 可以有效防止恶意用户进行重放攻击。 可以有选择地防止对报文内容的篡改。 摘要认证的过程见下图： 在上图中，完成摘要认证需要下面这四步： 客户端请求服务端的资源。 在客户端能够证明它知道密码从而确认其身份之前，服务端认证失败，返回401 Unauthorized，并返回WWW-Authenticate头，里面包含认证需要的信息。 客户端根据WWW-Authenticate头中的信息，选择加密算法，并使用密码随机数 nonce，计算出密码摘要 response，并再次请求服务端。 服务器将客户端提供的密码摘要与服务器内部计算出的摘要进行对比。如果匹配，就说明客户端知道密码，认证通过，并返回一些与授权会话相关的附加信息，放在 Authorization-Info 中。 WWW-Authenticate头中包含的信息见下表： 虽然使用摘要可以避免密码以明文方式发送，一定程度上保护了密码的安全性，但是仅仅隐藏密码并不能保证请求是安全的。因为请求（包括密码摘要）仍然可以被截获，这样就可以重放给服务器，带来安全问题。 为了防止重放攻击，服务器向客户端发送了密码随机数 nonce，nonce 每次请求都会变化。客户端会根据 nonce 生成密码摘要，这种方式，可以使摘要随着随机数的变化而变化。服务端收到的密码摘要只对特定的随机数有效，而没有密码的话，攻击者就无法计算出正确的摘要，这样我们就可以防止重放攻击。 摘要认证可以保护密码，比基本认证安全很多。但摘要认证并不能保护内容，所以仍然要与 HTTPS 配合使用，来确保通信的安全。 OAuth OAuth（开放授权）是一个开放的授权标准，允许用户让第三方应用访问该用户在某一 Web 服务上存储的私密资源（例如照片、视频、音频等），而无需将用户名和密码提供给第三方应用。OAuth 目前的版本是 2.0 版。 OAuth2.0 一共分为四种授权方式，分别为密码式、隐藏式、凭借式和授权码模式。接下来，我们就具体介绍下每一种授权方式。 第一种，密码式。密码式的授权方式，就是用户把用户名和密码直接告诉给第三方应用，然后第三方应用使用用户名和密码换取令牌。所以，使用此授权方式的前提是无法采用其他授权方式，并且用户高度信任某应用。 认证流程如下： 网站 A 向用户发出获取用户名和密码的请求； 用户同意后，网站 A 凭借用户名和密码向网站 B 换取令牌； 网站 B 验证用户身份后，给出网站 A 令牌，网站 A 凭借令牌可以访问网站 B 对应权限的资源。 第二种，隐藏式。这种方式适用于前端应用。认证流程如下： A 网站提供一个跳转到 B 网站的链接，用户点击后跳转至 B 网站，并向用户请求授权； 用户登录 B 网站，同意授权后，跳转回 A 网站指定的重定向 redirect_url 地址，并携带 B 网站返回的令牌，用户在 B 网站的数据给 A 网站使用。 这个授权方式存在着“中间人攻击”的风险，因此只能用于一些安全性要求不高的场景，并且令牌的有效时间要非常短。 第三种，凭借式。这种方式是在命令行中请求授权，适用于没有前端的命令行应用。认证流程如下： 应用 A 在命令行向应用 B 请求授权，此时应用 A 需要携带应用 B 提前颁发的 secretID 和 secretKey，其中 secretKey 出于安全性考虑，需在后端发送； 应用 B 接收到 secretID 和 secretKey，并进行身份验证，验证通过后返回给应用 A 令牌。 第四种，授权码模式。这种方式就是第三方应用先提前申请一个授权码，然后再使用授权码来获取令牌。相对来说，这种方式安全性更高，前端传送授权码，后端存储令牌，与资源的通信都是在后端，可以避免令牌的泄露导致的安全问题。认证流程如下： A 网站提供一个跳转到 B 网站的链接 +redirect_url，用户点击后跳转至 B 网站； 用户携带向 B 网站提前申请的 client_id，向 B 网站发起身份验证请求； 用户登录 B 网站，通过验证，授予 A 网站权限，此时网站跳转回 redirect_url，其中会有 B 网站通过验证后的授权码附在该 url 后； 网站 A 携带授权码向网站 B 请求令牌，网站 B 验证授权码后，返回令牌即 access_token。 Bearer Bearer 认证，也称为令牌认证，是一种 HTTP 身份验证方法。Bearer 认证的核心是 bearer token。bearer token 是一个加密字符串，通常由服务端根据密钥生成。客户端在请求服务端时，必须在请求头中包含Authorization: Bearer 。服务端收到请求后，解析出 ，并校验 的合法性，如果校验通过，则认证通过。跟基本认证一样，Bearer 认证需要配合 HTTPS 一起使用，来保证认证安全性。 当前最流行的 token 编码方式是 JSON Web Token（JWT，音同 jot，详见 JWT RFC 7519）。接下来，我通过讲解 JWT 认证来帮助你了解 Bearer 认证的原理。 ","date":"2022-07-07 15:20:13","objectID":"/iam_service_develop/:2:2","tags":["iam"],"title":"iam_service_develop","uri":"/iam_service_develop/"},{"categories":["iam"],"content":"基于 JWT 的 Token 认证机制实现 在典型业务场景中，为了区分用户和保证安全，必须对 API 请求进行鉴权，但是不能要求每一个请求都进行登录操作。合理做法是，在第一次登录之后产生一个有一定有效期的 token，并将它存储在浏览器的 Cookie 或 LocalStorage 之中。之后的请求都携带这个 token ，请求到达服务器端后，服务器端用这个 token 对请求进行认证。在第一次登录之后，服务器会将这个 token 用文件、数据库或缓存服务器等方法存下来，用于之后请求中的比对。 或者也可以采用更简单的方法：直接用密钥来签发 Token。这样，就可以省下额外的存储，也可以减少每一次请求时对数据库的查询压力。这种方法在业界已经有一种标准的实现方式，就是 JWT。接下来，我就来具体介绍下 JWT。 JWT 简介 JWT 是 Bearer Token 的一个具体实现，由 JSON 数据格式组成，通过 HASH 散列算法生成一个字符串。该字符串可以用来进行授权和信息交换。使用 JWT Token 进行认证有很多优点，比如说无需在服务端存储用户数据，可以减轻服务端压力；而且采用 JSON 数据格式，比较易读。除此之外，使用 JWT Token 还有跨语言、轻量级等优点。 JWT 认证流程 使用 JWT Token 进行认证的流程如下图： 具体可以分为四步： 客户端使用用户名和密码请求登录。 服务端收到请求后，会去验证用户名和密码。如果用户名和密码跟数据库记录不一致，则验证失败；如果一致则验证通过，服务端会签发一个 Token 返回给客户端。 客户端收到请求后会将 Token 缓存起来，比如放在浏览器 Cookie 中或者 LocalStorage 中，之后每次请求都会携带该 Token。 服务端收到请求后，会验证请求中的 Token，验证通过则进行业务逻辑处理，处理完后返回处理后的结果。 JWT 格式 JWT 由三部分组成，分别是 Header、Payload 和 Signature，它们之间用圆点.连接，例如： eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhdWQiOiJpYW0uYXBpLm1hcm1vdGVkdS5jb20iLCJleHAiOjE2NDI4NTY2MzcsImlkZW50aXR5IjoiYWRtaW4iLCJpc3MiOiJpYW0tYXBpc2VydmVyIiwib3JpZ19pYXQiOjE2MzUwODA2MzcsInN1YiI6ImFkbWluIn0.Shw27RKENE_2MVBq7-c8OmgYdF92UmdwS8xE-Fts2FM JWT 中，每部分包含的信息见下图： 下面我来具体介绍下这三部分，以及它们包含的信息。 Header JWT Token 的 Header 中，包含两部分信息：一是 Token 的类型，二是 Token 所使用的加密算法。例如： { \"typ\": \"JWT\", \"alg\": \"HS256\" } 参数说明：typ：说明 Token 类型是 JWT。alg：说明 Token 的加密算法，这里是 HS256（alg 算法可以有多种）。 这里，我们将 Header 进行 base64 编码： $ echo -n '{\"typ\":\"JWT\",\"alg\":\"HS256\"}'|base64 eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9 在某些场景下，可能还会有 kid 选项，用来标识一个密钥 ID，例如： { \"alg\": \"HS256\", \"kid\": \"XhbY3aCrfjdYcP1OFJRu9xcno8JzSbUIvGE2\", \"typ\": \"JWT\" } Payload（载荷） Payload 中携带 Token 的具体内容由三部分组成：JWT 标准中注册的声明（可选）、公共的声明、私有的声明。下面来分别看下。 JWT 标准中注册的声明部分，有以下标准字段： 本例中的 payload 内容为： { \"aud\": \"iam.authz.marmotedu.com\", \"exp\": 1604158987, \"iat\": 1604151787, \"iss\": \"iamctl\", \"nbf\": 1604151787 } 这里，我们将 Payload 进行 base64 编码： $ echo -n '{\"aud\":\"iam.authz.marmotedu.com\",\"exp\":1604158987,\"iat\":1604151787,\"iss\":\"iamctl\",\"nbf\":1604151787}'|base64 eyJhdWQiOiJpYW0uYXV0aHoubWFybW90ZWR1LmNvbSIsImV4cCI6MTYwNDE1ODk4NywiaWF0Ijox NjA0MTUxNzg3LCJpc3MiOiJpYW1jdGwiLCJuYmYiOjE2MDQxNTE3ODd9 除此之外，还有公共的声明和私有的声明。公共的声明可以添加任何的需要的信息，一般添加用户的相关信息或其他业务需要的信息，注意不要添加敏感信息；私有声明是客户端和服务端所共同定义的声明，因为 base64 是对称解密的，所以一般不建议存放敏感信息。 Signature（签名） Signature 是 Token 的签名部分，通过如下方式生成：将 Header 和 Payload 分别 base64 编码后，用 . 连接。然后再使用 Header 中声明的加密方式，利用 secretKey 对连接后的字符串进行加密，加密后的字符串即为最终的 Signature。 secretKey 是密钥，保存在服务器中，一般通过配置文件来保存，例如： 这里要注意，密钥一定不能泄露。密钥泄露后，入侵者可以使用该密钥来签发 JWT Token，从而入侵系统。最后生成的 Token 如下： eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJhdWQiOiJpYW0uYXV0aHoubWFybW90ZWR1LmNvbSIsImV4cCI6MTYwNDE1ODk4NywiaWF0IjoxNjA0MTUxNzg3LCJpc3MiOiJpYW1jdGwiLCJuYmYiOjE2MDQxNTE3ODd9.LjxrK9DuAwAzUD8-9v43NzWBN7HXsSLfebw92DKd1JQ 签名后服务端会返回生成的 Token，客户端下次请求会携带该 Token。服务端收到 Token 后会解析出 header.payload，然后用相同的加密算法和密钥对 header.payload 再进行一次加密，得到 Signature。并且，对比加密后的 Signature 和收到的 Signature 是否相同，如果相同则验证通过，不相同则返回 HTTP 401 Unauthorized 的错误。 最后，关于 JWT 的使用，我还有两点建议： 不要存放敏感信息在 Token 里； Payload 中的 exp 值不要设置得太大，一般开发版本 7 天，线上版本 2 小时。当然，你也可以根据需要自行设置。 ","date":"2022-07-07 15:20:13","objectID":"/iam_service_develop/:2:3","tags":["iam"],"title":"iam_service_develop","uri":"/iam_service_develop/"},{"categories":["iam"],"content":"总结 在开发 Go 应用时，我们需要通过认证来保障应用的安全。认证，用来验证某个用户是否具有访问系统的权限，如果认证通过，该用户就可以访问系统，从而创建、修改、删除、查询平台支持的资源。业界目前有四种常用的认证方式：Basic、Digest、OAuth、Bearer。其中 Basic 和 Bearer 用得最多。 Basic 认证通过用户名和密码来进行认证，主要用在用户登录场景；Bearer 认证通过 Token 来进行认证，通常用在 API 调用场景。不管是 Basic 认证还是 Bearer 认证，都需要结合 HTTPS 来使用，来最大程度地保证请求的安全性。 Basic 认证简单易懂，但是 Bearer 认证有一定的复杂度，所以这一讲的后半部分通过 JWT Token，讲解了 Bearer Token 认证的原理。 JWT Token 是 Bearer 认证的一种比较好的实现，主要包含了 3 个部分： Header：包含了 Token 的类型、Token 使用的加密算法。在某些场景下，你还可以添加 kid 字段，用来标识一个密钥 ID。 Payload：Payload 中携带 Token 的具体内容，由 JWT 标准中注册的声明、公共的声明和私有的声明三部分组成。 Signature：Signature 是 Token 的签名部分，程序通过验证 Signature 是否合法，来决定认证是否通过。 ","date":"2022-07-07 15:20:13","objectID":"/iam_service_develop/:2:4","tags":["iam"],"title":"iam_service_develop","uri":"/iam_service_develop/"},{"categories":["iam"],"content":"26 | IAM项目是如何设计和实现访问认证功能的？ 上一讲，我们学习了应用认证常用的四种方式：Basic、Digest、OAuth、Bearer。这一讲，我们再来看下 IAM 项目是如何设计和实现认证功能的。IAM 项目用到了 Basic 认证和 Bearer 认证。其中，Basic 认证用在前端登陆的场景，Bearer 认证用在调用后端 API 服务的场景下。接下来，我们先来看下 IAM 项目认证功能的整体设计思路。 ","date":"2022-07-07 15:20:13","objectID":"/iam_service_develop/:3:0","tags":["iam"],"title":"iam_service_develop","uri":"/iam_service_develop/"},{"categories":["iam"],"content":"如何设计 IAM 项目的认证功能？ 在认证功能开发之前，我们要根据需求，认真考虑下如何设计认证功能，并在设计阶段通过技术评审。那么我们先来看下，如何设计 IAM 项目的认证功能。 首先，我们要梳理清楚认证功能的使用场景和需求。 IAM 项目的 iam-apiserver 服务，提供了 IAM 系统的管理流功能接口，它的客户端可以是前端（这里也叫控制台），也可以是 App 端。 为了方便用户在 Linux 系统下调用，IAM 项目还提供了 iamctl 命令行工具。 为了支持在第三方代码中调用 iam-apiserver 提供的 API 接口，还支持了 API 调用。 为了提高用户在代码中调用 API 接口的效率，IAM 项目提供了 Go SDK。 可以看到，iam-apiserver 有很多客户端，每种客户端适用的认证方式是有区别的。 控制台、App 端需要登录系统，所以需要使用用户名：密码这种认证方式，也即 Basic 认证。iamctl、API 调用、Go SDK 因为可以不用登录系统，所以可以采用更安全的认证方式：Bearer 认证。同时，Basic 认证作为 iam-apiserver 已经集成的认证方式，仍然可以供 iamctl、API 调用、Go SDK 使用。 这里有个地方需要注意：如果 iam-apiserver 采用 Bearer Token 的认证方式，目前最受欢迎的 Token 格式是 JWT Token。而 JWT Token 需要密钥（后面统一用 secretKey 来指代），因此需要在 iam-apiserver 服务中为每个用户维护一个密钥，这样会增加开发和维护成本。 业界有一个更好的实现方式：将 iam-apiserver 提供的 API 接口注册到 API 网关中，通过 API 网关中的 Token 认证功能，来实现对 iam-apiserver API 接口的认证。有很多 API 网关可供选择，例如腾讯云 API 网关、Tyk、Kong 等。这里需要你注意：通过 iam-apiserver 创建的密钥对是提供给 iam-authz-server 使用的。 另外，我们还需要调用 iam-authz-server 提供的 RESTful API 接口：/v1/authz，来进行资源授权。API 调用比较适合采用的认证方式是 Bearer 认证。当然，/v1/authz也可以直接注册到 API 网关中。在实际的 Go 项目开发中，也是我推荐的一种方式。但在这里，为了展示实现 Bearer 认证的过程，iam-authz-server 自己实现了 Bearer 认证。讲到 iam-authz-server Bearer 认证实现的时候，我会详细介绍这一点。 Basic 认证需要用户名和密码，Bearer 认证则需要密钥，所以 iam-apiserver 需要将用户名 / 密码、密钥等信息保存在后端的 MySQL 中，持久存储起来。 在进行认证的时候，需要获取密码或密钥进行反加密，这就需要查询密码或密钥。查询密码或密钥有两种方式。一种是在请求到达时查询数据库。因为数据库的查询操作延时高，会导致 API 接口延时较高，所以不太适合用在数据流组件中。另外一种是将密码或密钥缓存在内存中，这样请求到来时，就可以直接从内存中查询，从而提升查询速度，提高接口性能。但是，将密码或密钥缓存在内存中时，就要考虑内存和数据库的数据一致性，这会增加代码实现的复杂度。因为管控流组件对性能延时要求不那么敏感，而数据流组件则一定要实现非常高的接口性能，所以 iam-apiserver 在请求到来时查询数据库，而 iam-authz-server 则将密钥信息缓存在内存中。 那在这里，可以总结出一张 IAM 项目的认证设计图： 另外，为了将控制流和数据流区分开来，密钥的 CURD 操作也放在了 iam-apiserver 中，但是 iam-authz-server 需要用到这些密钥信息。为了解决这个问题，目前的做法是： iam-authz-server 通过 gRPC API 请求 iam-apiserver，获取所有的密钥信息； 当 iam-apiserver 有密钥更新时，会 Pub 一条消息到 Redis Channel 中。因为 iam-authz-server 订阅了同一个 Redis Channel，iam-authz-searver 监听到 channel 有新消息时，会获取、解析消息，并更新它缓存的密钥信息。这样，我们就能确保 iam-authz-server 内存中缓存的密钥和 iam-apiserver 中的密钥保持一致。 学到这里，你可能会问：将所有密钥都缓存在 iam-authz-server 中，那岂不是要占用很大的内存？别担心，这个问题我也想过，并且替你计算好了：8G 的内存大概能保存约 8 千万个密钥信息，完全够用。后期不够用的话，可以加大内存。不过这里还是有个小缺陷：如果 Redis down 掉，或者出现网络抖动，可能会造成 iam-apiserver 中和 iam-authz-server 内存中保存的密钥数据不一致，但这不妨碍我们学习认证功能的设计和实现。至于如何保证缓存系统的数据一致性，我会在新一期的特别放送里专门介绍下。 最后注意一点：Basic 认证请求和 Bearer 认证请求都可能被截获并重放。所以，为了确保 Basic 认证和 Bearer 认证的安全性，和服务端通信时都需要配合使用 HTTPS 协议。 ","date":"2022-07-07 15:20:13","objectID":"/iam_service_develop/:3:1","tags":["iam"],"title":"iam_service_develop","uri":"/iam_service_develop/"},{"categories":["iam"],"content":"IAM 项目是如何实现 Basic 认证的？ 我们已经知道，IAM 项目中主要用了 Basic 和 Bearer 这两种认证方式。我们要支持 Basic 认证和 Bearer 认证，并根据需要选择不同的认证方式，这很容易让我们想到使用设计模式中的策略模式来实现。所以，在 IAM 项目中，我将每一种认证方式都视作一个策略，通过选择不同的策略，来使用不同的认证方法。 IAM 项目实现了如下策略： auto 策略：该策略会根据 HTTP 头Authorization: Basic XX.YY.ZZ和Authorization: Bearer XX.YY.ZZ自动选择使用 Basic 认证还是 Bearer 认证。 basic 策略：该策略实现了 Basic 认证。 jwt 策略：该策略实现了 Bearer 认证，JWT 是 Bearer 认证的具体实现。 cache 策略：该策略其实是一个 Bearer 认证的实现，Token 采用了 JWT 格式，因为 Token 中的密钥 ID 是从内存中获取的，所以叫 Cache 认证。这一点后面会详细介绍。 iam-apiserver 通过创建需要的认证策略，并加载到需要认证的 API 路由上，来实现 API 认证。具体代码如下： jwtStrategy, _ := newJWTAuth().(auth.JWTStrategy) g.POST(\"/login\", jwtStrategy.LoginHandler) g.POST(\"/logout\", jwtStrategy.LogoutHandler) // Refresh time can be longer than token timeout g.POST(\"/refresh\", jwtStrategy.RefreshHandler) 上述代码中，我们通过newJWTAuth函数创建了auth.JWTStrategy类型的变量，该变量包含了一些认证相关函数。 LoginHandler：实现了 Basic 认证，完成登陆认证。 RefreshHandler：重新刷新 Token 的过期时间。 LogoutHandler：用户注销时调用。登陆成功后，如果在 Cookie 中设置了认证相关的信息，执行 LogoutHandler 则会清空这些信息。 下面，我来分别介绍下 LoginHandler、RefreshHandler 和 LogoutHandler。 LoginHandler这里，我们来看下 LoginHandler Gin 中间件，该函数定义位于github.com/appleboy/gin-jwt包的auth_jwt.go文件中。 func (mw *GinJWTMiddleware) LoginHandler(c *gin.Context) { if mw.Authenticator == nil { mw.unauthorized(c, http.StatusInternalServerError, mw.HTTPStatusMessageFunc(ErrMissingAuthenticatorFunc, c)) return } data, err := mw.Authenticator(c) if err != nil { mw.unauthorized(c, http.StatusUnauthorized, mw.HTTPStatusMessageFunc(err, c)) return } // Create the token token := jwt.New(jwt.GetSigningMethod(mw.SigningAlgorithm)) claims := token.Claims.(jwt.MapClaims) if mw.PayloadFunc != nil { for key, value := range mw.PayloadFunc(data) { claims[key] = value } } expire := mw.TimeFunc().Add(mw.Timeout) claims[\"exp\"] = expire.Unix() claims[\"orig_iat\"] = mw.TimeFunc().Unix() tokenString, err := mw.signedString(token) if err != nil { mw.unauthorized(c, http.StatusUnauthorized, mw.HTTPStatusMessageFunc(ErrFailedTokenCreation, c)) return } // set cookie if mw.SendCookie { expireCookie := mw.TimeFunc().Add(mw.CookieMaxAge) maxage := int(expireCookie.Unix() - mw.TimeFunc().Unix()) if mw.CookieSameSite != 0 { c.SetSameSite(mw.CookieSameSite) } c.SetCookie( mw.CookieName, tokenString, maxage, \"/\", mw.CookieDomain, mw.SecureCookie, mw.CookieHTTPOnly, ) } mw.LoginResponse(c, http.StatusOK, tokenString, expire) } 从 LoginHandler 函数的代码实现中，我们可以知道，LoginHandler 函数会执行Authenticator函数，来完成 Basic 认证。如果认证通过，则会签发 JWT Token，并执行 PayloadFunc函数设置 Token Payload。如果我们设置了 SendCookie=true ，还会在 Cookie 中添加认证相关的信息，例如 Token、Token 的生命周期等，最后执行 LoginResponse 方法返回 Token 和 Token 的过期时间。 Authenticator、PayloadFunc、LoginResponse这三个函数，是我们在创建 JWT 认证策略时指定的。下面我来分别介绍下。先来看下Authenticator函数。Authenticator 函数从 HTTP Authorization Header 中获取用户名和密码，并校验密码是否合法。 func authenticator() func(c *gin.Context) (interface{}, error) { return func(c *gin.Context) (interface{}, error) { var login loginInfo var err error // support header and body both if c.Request.Header.Get(\"Authorization\") != \"\" { login, err = parseWithHeader(c) } else { login, err = parseWithBody(c) } if err != nil { return \"\", jwt.ErrFailedAuthentication } // Get the user information by the login username. user, err := store.Client().Users().Get(c, login.Username, metav1.GetOptions{}) if err != nil { log.Errorf(\"get user information failed: %s\", err.Error()) return \"\", jwt.ErrFailedAuthentication } // Compare the login password with the user password. if err := user.Compare(login.Password); err != nil { return \"\", jwt.ErrFailedAuthentication } return user, nil } } Authenticator函数需要获取用户名和密码。它首先会判断是否有Authorization请求头，如果有，则调用parseWithHeader函数获取用户名和密码，否则调用parseWithBody从 Body 中获取用户名和密码。如果都获取失败，则返回认证失败错误。所以，IAM 项目的 Basic 支持以下两种请求方式： $ curl -XPOST -H\"Authorization: Basic YWRtaW46QWRtaW5AMjAyMQ==\" http://127.0.0.1:8080/login # 用户名:密码通过base64加码后，通过HTTP Authorization Header进行传递，因为密码非明文，建议使用这种方式。 $ curl -s -XPOST -H'Content-Type: application/json' -d'{\"username\":\"admin\",\"password\":\"Admin@2021\"}' http://127.0.0.1:8080/login # 用户名和密码在HTTP ","date":"2022-07-07 15:20:13","objectID":"/iam_service_develop/:3:2","tags":["iam"],"title":"iam_service_develop","uri":"/iam_service_develop/"},{"categories":["iam"],"content":"IAM 项目是如何实现 Bearer 认证的？ 上面我们介绍了 Basic 认证。这里，我再来介绍下 IAM 项目中 Bearer 认证的实现方式。 IAM 项目中有两个地方实现了 Bearer 认证，分别是 iam-apiserver 和 iam-authz-server。下面我来分别介绍下它们是如何实现 Bearer 认证的。 iam-authz-server Bearer 认证实现 先来看下 iam-authz-server 是如何实现 Bearer 认证的。 iam-authz-server 通过在 /v1 路由分组中加载 cache 认证中间件来使用 cache 认证策略： auth := newCacheAuth() apiv1 := g.Group(\"/v1\", auth.AuthFunc()) 来看下newCacheAuth函数： func newCacheAuth() middleware.AuthStrategy { return auth.NewCacheStrategy(getSecretFunc()) } func getSecretFunc() func(string) (auth.Secret, error) { return func(kid string) (auth.Secret, error) { cli, err := store.GetStoreInsOr(nil) if err != nil { return auth.Secret{}, errors.Wrap(err, \"get store instance failed\") } secret, err := cli.GetSecret(kid) if err != nil { return auth.Secret{}, err } return auth.Secret{ Username: secret.Username, ID: secret.SecretId, Key: secret.SecretKey, Expires: secret.Expires, }, nil } } newCacheAuth 函数调用auth.NewCacheStrategy创建了一个 cache 认证策略，创建时传入了getSecretFunc函数，该函数会返回密钥的信息。密钥信息包含了以下字段： type Secret struct { Username string ID string Key string Expires int64 } 再来看下 cache 认证策略实现的AuthFunc方法： func (cache CacheStrategy) AuthFunc() gin.HandlerFunc { return func(c *gin.Context) { header := c.Request.Header.Get(\"Authorization\") if len(header) == 0 { core.WriteResponse(c, errors.WithCode(code.ErrMissingHeader, \"Authorization header cannot be empty.\"), nil) c.Abort() return } var rawJWT string // Parse the header to get the token part. fmt.Sscanf(header, \"Bearer %s\", \u0026rawJWT) // Use own validation logic, see below var secret Secret claims := \u0026jwt.MapClaims{} // Verify the token parsedT, err := jwt.ParseWithClaims(rawJWT, claims, func(token *jwt.Token) (interface{}, error) { // Validate the alg is HMAC signature if _, ok := token.Method.(*jwt.SigningMethodHMAC); !ok { return nil, fmt.Errorf(\"unexpected signing method: %v\", token.Header[\"alg\"]) } kid, ok := token.Header[\"kid\"].(string) if !ok { return nil, ErrMissingKID } var err error secret, err = cache.get(kid) if err != nil { return nil, ErrMissingSecret } return []byte(secret.Key), nil }, jwt.WithAudience(AuthzAudience)) if err != nil || !parsedT.Valid { core.WriteResponse(c, errors.WithCode(code.ErrSignatureInvalid, err.Error()), nil) c.Abort() return } if KeyExpired(secret.Expires) { tm := time.Unix(secret.Expires, 0).Format(\"2006-01-02 15:04:05\") core.WriteResponse(c, errors.WithCode(code.ErrExpired, \"expired at: %s\", tm), nil) c.Abort() return } c.Set(CtxUsername, secret.Username) c.Next() } } // KeyExpired checks if a key has expired, if the value of user.SessionState.Expires is 0, it will be ignored. func KeyExpired(expires int64) bool { if expires \u003e= 1 { return time.Now().After(time.Unix(expires, 0)) } return false } AuthFunc 函数依次执行了以下四大步来完成 JWT 认证，每一步中又有一些小步骤，下面我们来一起看看。 第一步，从 Authorization: Bearer XX.YY.ZZ 请求头中获取 XX.YY.ZZ，XX.YY.ZZ 即为 JWT Token。第二步，调用 github.com/dgrijalva/jwt-go 包提供的 ParseWithClaims 函数，该函数会依次执行下面四步操作。 调用 ParseUnverified 函数，依次执行以下操作： 从 Token 中获取第一段 XX，base64 解码后得到 JWT Token 的 Header{“alg”:“HS256”,“kid”:“a45yPqUnQ8gljH43jAGQdRo0bXzNLjlU0hxa”,“typ”:“JWT”}。从 Token 中获取第二段 YY，base64 解码后得到 JWT Token 的 Payload{“aud”:“iam.authz.marmotedu.com”,“exp”:1625104314,“iat”:1625097114,“iss”:“iamctl”,“nbf”:1625097114}。根据 Token Header 中的 alg 字段，获取 Token 加密函数。最终 ParseUnverified 函数会返回 Token 类型的变量，Token 类型包含 Method、Header、Claims、Valid 这些重要字段，这些字段会用于后续的认证步骤中。调用传入的 keyFunc 获取密钥，这里来看下 keyFunc 的实现： func(token *jwt.Token) (interface{}, error) { // Validate the alg is HMAC signature if _, ok := token.Method.(*jwt.SigningMethodHMAC); !ok { return nil, fmt.Errorf(\"unexpected signing method: %v\", token.Header[\"alg\"]) } kid, ok := token.Header[\"kid\"].(string) if !ok { return nil, ErrMissingKID } var err error secret, err = cache.get(kid) if err != nil { return nil, ErrMissingSecret } return []byte(secret.Key), nil } 可以看到，keyFunc 接受 *Token 类型的变量，并获取 Token Header 中的 kid，kid 即为密钥 ID：secretID。接着，调用 cache.get(kid) 获取密钥 secretKey。cache.get 函数即为 getSecretFunc，getSecretFunc 函数会根据 kid，从内存中查找密钥信息，","date":"2022-07-07 15:20:13","objectID":"/iam_service_develop/:3:3","tags":["iam"],"title":"iam_service_develop","uri":"/iam_service_develop/"},{"categories":["iam"],"content":"IAM 项目认证功能设计技巧 我在设计 IAM 项目的认证功能时，也运用了一些技巧，这里分享给你。 技巧 1：面向接口编程 在使用NewAutoStrategy函数创建 auto 认证策略时，传入了middleware.AuthStrategy接口类型的参数，这意味着 Basic 认证和 Bearer 认证都可以有不同的实现，这样后期可以根据需要扩展新的认证方式。 技巧 2：使用抽象工厂模式 auth.go文件中，通过 newBasicAuth、newJWTAuth、newAutoAuth 创建认证策略时，返回的都是接口。通过返回接口，可以在不公开内部实现的情况下，让调用者使用你提供的各种认证功能。 技巧 3：使用策略模式 在 auto 认证策略中，我们会根据 HTTP 请求头Authorization: XXX X.Y.X中的 XXX 来选择并设置认证策略（Basic 或 Bearer）。具体可以查看AutoStrategy的AuthFunc函数： func (a AutoStrategy) AuthFunc() gin.HandlerFunc { return func(c *gin.Context) { operator := middleware.AuthOperator{} authHeader := strings.SplitN(c.Request.Header.Get(\"Authorization\"), \" \", 2) ... switch authHeader[0] { case \"Basic\": operator.SetStrategy(a.basic) case \"Bearer\": operator.SetStrategy(a.jwt) // a.JWT.MiddlewareFunc()(c) default: core.WriteResponse(c, errors.WithCode(code.ErrSignatureInvalid, \"unrecognized Authorization header.\"), nil) c.Abort() return } operator.AuthFunc()(c) c.Next() } } 上述代码中，如果是 Basic，则设置为 Basic 认证方法operator.SetStrategy(a.basic)；如果是 Bearer，则设置为 Bearer 认证方法operator.SetStrategy(a.jwt)。 SetStrategy方法的入参是 AuthStrategy 类型的接口，都实现了AuthFunc() gin.HandlerFunc函数，用来进行认证，所以最后我们调用operator.AuthFunc()(c)即可完成认证。 ","date":"2022-07-07 15:20:13","objectID":"/iam_service_develop/:3:4","tags":["iam"],"title":"iam_service_develop","uri":"/iam_service_develop/"},{"categories":["iam"],"content":"总结 在 IAM 项目中，iam-apiserver 实现了 Basic 认证和 Bearer 认证，iam-authz-server 实现了 Bearer 认证。这一讲重点介绍了 iam-apiserver 的认证实现。 用户要访问 iam-apiserver，首先需要通过 Basic 认证，认证通过之后，会返回 JWT Token 和 JWT Token 的过期时间。前端将 Token 缓存在 LocalStorage 或 Cookie 中，后续的请求都通过 Token 来认证。执行 Basic 认证时，iam-apiserver 会从 HTTP Authorization Header 中解析出用户名和密码，将密码再加密，并和数据库中保存的值进行对比。如果不匹配，则认证失败，否则认证成功。认证成功之后，会返回 Token，并在 Token 的 Payload 部分设置用户名，Key 为 username 。 执行 Bearer 认证时，iam-apiserver 会从 JWT Token 中解析出 Header 和 Payload，并从 Header 中获取加密算法。接着，用获取到的加密算法和从配置文件中获取到的密钥对 Header.Payload 进行再加密，得到 Signature，并对比两次的 Signature 是否相等。如果不相等，则返回 HTTP 401 Unauthorized 错误；如果相等，接下来会判断 Token 是否过期，如果过期则返回认证不通过，否则认证通过。认证通过之后，会将 Payload 中的 username 添加到 gin.Context 类型的变量中，供后面的业务逻辑使用。我绘制了整个流程的示意图，你可以对照着再回顾一遍。 ","date":"2022-07-07 15:20:13","objectID":"/iam_service_develop/:3:5","tags":["iam"],"title":"iam_service_develop","uri":"/iam_service_develop/"},{"categories":["iam"],"content":"27 | 权限模型：5大权限模型是如何进行资源授权的？ 在开始讲解如何开发服务之前，我先来介绍一个比较重要的背景知识：权限模型。在你的研发生涯中，应该会遇到这样一种恐怖的操作：张三因为误操作删除了李四的资源。你在刷新闻时，也可能会刷到这么一个爆款新闻：某某程序员删库跑路。操作之所以恐怖，新闻之所以爆款，是因为这些行为往往会带来很大的损失。 那么如何避免这些风险呢？答案就是对资源做好权限管控，这也是项目开发中绕不开的话题。腾讯云会强制要求所有的云产品都对接 访问管理（CAM） 服务（阿里云也有这种要求），之所以这么做，是因为保证资源的安全是一件非常非常重要的事情。 可以说，保证应用的资源安全，已经成为一个应用的必备能力。作为开发人员，你也一定要知道如何保障应用的资源安全。那么如何才能保障资源的安全呢？我认为你至少需要掌握下面这两点： 权限模型：你需要了解业界成熟的权限模型，以及这些模型的适用场景。只有具备足够宽广的知识面和视野，我们才能避免闭门造车，设计出优秀的资源授权方案。 编码实现：选择或设计出了优秀的资源授权方案后，你就要编写代码实现该方案。这门课的 IAM 应用，就是一个资源授权方案的落地项目。你可以通过对 IAM 应用的学习，来掌握如何实现一个资源授权系统。 无论是第一点还是第二点，都需要你掌握基本的权限模型知识。那么这一讲，我就来介绍下业界优秀的权限模型，以及这些模型的适用场景，以使你今后设计出更好的资源授权系统。 ","date":"2022-07-07 15:20:13","objectID":"/iam_service_develop/:4:0","tags":["iam"],"title":"iam_service_develop","uri":"/iam_service_develop/"},{"categories":["iam"],"content":"权限相关术语介绍 在介绍业界常见的权限模型前，我们先来看下在权限模型中出现的术语。我把常见的术语总结在了下面的表格里： 为了方便你理解，这一讲我分别用用户、操作和资源来替代 Subject、Action 和 Object。 ","date":"2022-07-07 15:20:13","objectID":"/iam_service_develop/:4:1","tags":["iam"],"title":"iam_service_develop","uri":"/iam_service_develop/"},{"categories":["iam"],"content":"权限模型介绍 接下来，我就详细介绍下一些常见的权限模型，让你今后在设计权限系统时，能够根据需求选择合适的权限模型。 不同的权限模型具有不同的特点，可以满足不同的需求。常见的权限模型有下面这 5 种： 权限控制列表（ACL，Access Control List）。 自主访问控制（DAC，Discretionary Access Control）。 强制访问控制（MAC，Mandatory Access Control）。 基于角色的访问控制（RBAC，Role-Based Access Control）。 基于属性的权限验证（ABAC，Attribute-Based Access Control）。 这里先简单介绍下这 5 种权限模型。ACL 是一种简单的权限模型；DAC 基于 ACL，将权限下放给具有此权限的主题；但 DAC 因为权限下放，导致它对权限的控制过于分散，为了弥补 DAC 的这个缺陷，诞生了 MAC 权限模型。 DAC 和 MAC 都是基于 ACL 的权限模型。ACL 及其衍生的权限模型可以算是旧时代的权限模型，灵活性和功能性都满足不了现代应用的权限需求，所以诞生了 RBAC。RBAC 也是迄今为止最为普及的权限模型。但是，随着组织和应用规模的增长，所需的角色数量越来越多，变得难以管理，进而导致角色爆炸和职责分离（SoD）失败。最后，引入了一种新的、更动态的访问控制形式，称为基于属性的访问控制，也就是 ABAC。ABAC 被一些人看作是权限系统设计的未来。腾讯云的 CAM、AWS 的 IAM、阿里云的 RAM 都是 ABAC 类型的权限访问服务。 接下来，我会详细介绍这些权限模型的基本概念。 简单的权限模型：权限控制列表（ACL） ACL（Access Control List，权限控制列表），用来判断用户是否可以对资源做特定的操作。例如，允许 Colin 创建文章的 ACL 策略为： Subject: Colin Action: Create Object: Article 在 ACL 权限模型下，权限管理是围绕资源 Object 来设定的，ACL 权限模型也是比较简单的一种模型。 基于 ACL 下放权限的权限模型：自主访问控制（DAC） DAC (Discretionary Access Control，自主访问控制)，是 ACL 的扩展模型，灵活性更强。使用这种模型，不仅可以判断 Subject 是否可以对 Object 做 Action 操作，同时也能让 Subject 将 Object、Action 的相同权限授权给其他的 Subject。例如，Colin 可以创建文章： Subject: Colin Action: Create Object: Article 因为 Colin 具有创建文章的权限，所以 Colin 也可以授予 James 创建文章的权限： Subject: James Action: Create Object: Article 经典的 ACL 模型权限集中在同一个 Subject 上，缺乏灵活性，为了加强灵活性，在 ACL 的基础上，DAC 模型将权限下放，允许拥有权限的 Subject 自主地将权限授予其他 Subject。 基于 ACL 且安全性更高的权限模型：强制访问控制（MAC） MAC (Mandatory Access Control，强制访问控制)，是 ACL 的扩展模型，安全性更高。MAC 权限模型下，Subject 和 Object 同时具有安全属性。在做授权时，需要同时满足两点才能授权通过： Subject 可以对 Object 做 Action 操作。 Object 可以被 Subject 做 Action 操作。 例如，我们设定了“Colin 和 James 可以创建文章”这个 MAC 策略： Subject: Colin Action: Create Object: Article Subject: James Action: Create Object: Article 我们还有另外一个 MAC 策略“文章可以被 Colin 创建”： Subject: Article Action: Create Object: Colin 在上述策略中，Colin 可以创建文章，但是 James 不能创建文章，因为第二条要求没有满足。这里你需要注意，在 ACL 及其扩展模型中，Subject 可以是用户，也可以是组或群组。ACL、DAC 和 MAC 是旧时代的权限控制模型，无法满足现代应用对权限控制的需求，于是诞生了新时代的权限模型：RBAC 和 ABAC。 最普及的权限模型：基于角色的访问控制（RBAC） RBAC (Role-Based Access Control，基于角色的访问控制)，引入了 Role（角色）的概念，并且将权限与角色进行关联。用户通过扮演某种角色，具有该角色的所有权限。具体如下图所示： 如图所示，每个用户关联一个或多个角色，每个角色关联一个或多个权限，每个权限又包含了一个或者多个操作，操作包含了对资源的操作集合。通过用户和权限解耦，可以实现非常灵活的权限管理。例如，可以满足以下两个权限场景： 第一，可以通过角色批量给一个用户授权。例如，公司新来了一位同事，需要授权虚拟机的生产、销毁、重启和登录权限。这时候，我们可以将这些权限抽象成一个运维角色。如果再有新同事来，就可以通过授权运维角色，直接批量授权这些权限，不用一个个地给用户授权这些权限。第二，可以批量修改用户的权限。例如，我们有很多用户，同属于运维角色，这时候对运维角色的任何权限变更，就相当于对运维角色关联的所有用户的权限变更，不用一个个去修改这些用户的权限。 RBAC 又分为 RBAC0、RBAC1、RBAC2、RBAC3。RBAC0 是 RBAC 的核心思想，RBAC1 是基于 RBAC 的角色分层模型，RBAC2 增加了 RBAC 的约束模型。而 RBAC3，其实相当于 RBAC1 + RBAC2。 下面我来详细介绍下这四种 RBAC。 RBAC0：基础模型，只包含核心的四要素，也就是用户（User）、角色（Role）、权限（Permission：Objects-Operations）、会话（Session）。用户和角色可以是多对多的关系，权限和角色也是多对多的关系。RBAC1：包括了 RBAC0，并且添加了角色继承。角色继承，即角色可以继承自其他角色，在拥有其他角色权限的同时，还可以关联额外的权限。RBAC2：包括 RBAC0，并且添加了约束。具有以下核心特性： 互斥约束：包括互斥用户、互斥角色、互斥权限。同一个用户不能拥有相互排斥的角色，两个互斥角色不能分配一样的权限集，互斥的权限不能分配给同一个角色，在 Session 中，同一个角色不能拥有互斥权限。 基数约束：一个角色被分配的用户数量受限，它指的是有多少用户能拥有这个角色。例如，一个角色是专门为公司 CEO 创建的，那这个角色的数量就是有限的。 先决条件角色：指要想获得较高的权限，要首先拥有低一级的权限。例如，先有副总经理权限，才能有总经理权限。 静态职责分离(Static Separation of Duty)：用户无法同时被赋予有冲突的角色。 动态职责分离(Dynamic Separation of Duty)：用户会话中，无法同时激活有冲突的角色。 RBAC3：全功能的 RBAC，合并了 RBAC0、RBAC1、RBAC2。此外，RBAC 也可以很方便地模拟出 DAC 和 MAC 的效果。这里举个例子，来协助你理解 RBAC。例如，我们有 write article 和 manage article 的权限： Permission: - Name: write_article - Effect: \"allow\" - Action: [\"Create\", \"Update\", \"Read\"] - Object: [\"Article\"] - Name: manage_article - Effect: \"allow\" - Action: [\"Delete\", \"Read\"] - Object: [\"Article\"] 同时，我们也有 Writer、Manager 和 CEO 3 个角色，Writer 具有 write_article 权限，Manager 具有 manage_article 权限，CEO 具有所有权限： Role: - Name: Writer Permissions: - write_article - Name: Manager Permissions: - manage_article - Name: CEO Permissions: - write_article - manage_article 接下来，我们对 Colin 用户授予 Writer 角色： Subject: Colin Roles: - Writer 那么现在 Colin 就具有 Writer 角色的所有权限 write_article，write_article 权限可以创建文章。接下来，再对 James 用户授予 Writer 和 Manager 角色： Subject: James Roles: - Writer - Manager 那么现在 James 就具有 Writer 角色和 Manager 角色的所有权限：write_article、manage_article，这些权限允许 James 创建和删除文章。 最强大的权限模型：基于属性的权限验证（","date":"2022-07-07 15:20:13","objectID":"/iam_service_develop/:4:2","tags":["iam"],"title":"iam_service_develop","uri":"/iam_service_develop/"},{"categories":["iam"],"content":"相关开源项目 上面我介绍了权限模型的相关知识，但是现在如果让你真正去实现一个权限系统，你可能还是不知从何入手。在这里，我列出了一些 GitHub 上比较优秀的开源项目，你可以学习这些项目是如何落地一个权限模型的，也可以基于这些项目进行二次开发，开发一个满足业务需求的权限系统。 Casbin Casbin 是一个用 Go 语言编写的访问控制框架，功能强大，支持 ACL、RBAC、ABAC 等访问模型，很多优秀的权限管理系统都是基于 Casbin 来构建的。Casbin 的核心功能都是围绕着访问控制来构建的，不负责身份认证。如果以后老板让你实现一个权限管理系统，Casbin 是一定要好好研究的开源项目。 keto keto 是一个云原生权限控制服务，通过提供 REST API 进行授权，支持 RBAC、ABAC、ACL、AWS IAM 策略、Kubernetes Roles 等权限模型，可以解决下面这些问题：是否允许某些用户修改此博客文章？是否允许某个服务打印该文档？是否允许 ACME 组织的成员修改其租户中的数据？是否允许在星期一的下午 4 点到下午 5 点，从 IP 10.0.0.2 发出的请求执行某个 Job？ go-admin go-admin 是一个基于 Gin + Vue + Element UI 的前后端分离权限管理系统脚手架，它的访问控制模型采用了 Casbin 的 RBAC 访问控制模型，功能强大，包含了如下功能： 基础用户管理功能；JWT 鉴权；代码生成器；RBAC 权限控制；表单构建；…… 该项目还支持 RESTful API 设计规范、Swagger 文档、GORM 库等。go-admin 不仅是一个优秀的权限管理系统，也是一个优秀的、功能齐全的 Go 开源项目。你在做项目开发时，也可以参考该项目的构建思路。go-admin 管理系统自带前端，如下图所示。 LyricTian/gin-admin gin-admin 类似于 go-admin，是一个基于 Gin+Gorm+Casbin+Wire 实现的权限管理脚手架，并自带前端，在做权限管理系统调研时，也非常值得参考。gin-admin 大量采用了 Go 后端开发常用的技术，比如 Gin、GORM、JWT 认证、RESTful API、Logrus 日志包、Swagger 文档等。因此，你在做 Go 后端服务开发时，也可以学习该项目的构建方法。 gin-vue-admin gin-vue-admin 是一个基于 Gin 和 Vue 开发的全栈前后端分离的后台管理系统，集成了 JWT 鉴权、动态路由、动态菜单、Casbin 鉴权、表单生成器、代码生成器等功能。gin-vue-admin 集成了 RBAC 权限管理模型，界面如下图所示： ","date":"2022-07-07 15:20:13","objectID":"/iam_service_develop/:4:3","tags":["iam"],"title":"iam_service_develop","uri":"/iam_service_develop/"},{"categories":["iam"],"content":"选择建议 介绍了那么多优秀的开源项目，最后我想给你一些选择建议。如果你想研究 ACL、RBAC、ABAC 等权限模型如何落地，我强烈建议你学习 Casbin 项目，Casbin 目前有近万的 GitHub star 数，处于活跃的开发状态。有很多项目在使用 Casbin，例如 go-admin、 gin-admin 、 gin-vue-admin 等。 keto 类似于 Casbin，主要通过 Go 包的方式，对外提供授权能力。keto 也是一个非常优秀的权限类项目，当你研究完 Casbin 后，如果还想再研究下其他授权类项目，建议你读下 keto 的源码。 go-admin、gin-vue-admin、gin-admin 这 3 个都是基于 Casbin 的 Go 项目。其中，gin-vue-admin 是后台管理系统框架，里面包含了 RBAC 权限管理模块；go-admin 和 gin-admin 都是 RBAC 权限管理脚手架。所以，如果你想找一个比较完整的 RBAC 授权系统（自带前后端），建议你优先研究下 go-admin，如果还有精力，可以再研究下 gin-admin、gin-vue-admin。 ","date":"2022-07-07 15:20:13","objectID":"/iam_service_develop/:4:4","tags":["iam"],"title":"iam_service_develop","uri":"/iam_service_develop/"},{"categories":["iam"],"content":"总结 这一讲，我介绍了 5 种常见的权限模型。其中，ACL 最简单，ABAC 最复杂，但是功能最强大，也最灵活。RBAC 则介于二者之间。对于一些云计算厂商来说，因为它们面临的授权场景复杂多样，需要一个非常强大的授权模型，所以腾讯云、阿里云和 AWS 等云厂商普遍采用了 ABAC 模型。 如果你的资源授权需求不复杂，可以考虑 RBAC；如果你需要一个能满足复杂场景的资源授权系统，建议选择 ABAC，ABAC 的设计思路可以参考下腾讯云的 CAM、阿里云的 RAM 和 AWS 的 IAM。另外，如果你想深入了解权限模型如何具体落地，建议你阅读 Casbin 源码。 ","date":"2022-07-07 15:20:13","objectID":"/iam_service_develop/:4:5","tags":["iam"],"title":"iam_service_develop","uri":"/iam_service_develop/"},{"categories":["iam"],"content":"28 | 控制流（上）：通过iam-apiserver设计，看Web服务的构建 前面我们讲了很多关于应用构建的内容，你一定迫不及待地想看下 IAM 项目的应用是如何构建的。那么接下来，我就讲解下 IAM 应用的源码。在讲解过程中，我不会去讲解具体如何 Code，但会讲解一些构建过程中的重点、难点，以及 Code 背后的设计思路、想法。我相信这是对你更有帮助的。IAM 项目有很多组件，这一讲，我先来介绍下 IAM 项目的门面服务：iam-apiserver（管理流服务）。我会先给你介绍下 iam-apiserver 的功能和使用方法，再介绍下 iam-apiserver 的代码实现。 ","date":"2022-07-07 15:20:13","objectID":"/iam_service_develop/:5:0","tags":["iam"],"title":"iam_service_develop","uri":"/iam_service_develop/"},{"categories":["iam"],"content":"iam-apiserver 服务介绍 iam-apiserver 是一个 Web 服务，通过一个名为 iam-apiserver 的进程，对外提供 RESTful API 接口，完成用户、密钥、策略三种 REST 资源的增删改查。接下来，我从功能和使用方法两个方面来具体介绍下。 ","date":"2022-07-07 15:20:13","objectID":"/iam_service_develop/:5:1","tags":["iam"],"title":"iam_service_develop","uri":"/iam_service_develop/"},{"categories":["iam"],"content":"iam-apiserver 功能介绍 这里，我们可以通过 iam-apiserver 提供的 RESTful API 接口，来看下 iam-apiserver 具体提供的功能。iam-apiserver 提供的 RESTful API 接口可以分为四类，具体如下： 认证相关接口 用户相关接口 密钥相关接口 策略相关接口 ","date":"2022-07-07 15:20:13","objectID":"/iam_service_develop/:5:2","tags":["iam"],"title":"iam_service_develop","uri":"/iam_service_develop/"},{"categories":["iam"],"content":"iam-apiserver 使用方法介绍 上面我介绍了 iam-apiserver 的功能，接下来就介绍下如何使用这些功能。 我们可以通过不同的客户端来访问 iam-apiserver，例如前端、API 调用、SDK、iamctl 等。这些客户端最终都会执行 HTTP 请求，调用 iam-apiserver 提供的 RESTful API 接口。所以，我们首先需要有一个顺手的 REST API 客户端工具来执行 HTTP 请求，完成开发测试。 因为不同的开发者执行 HTTP 请求的方式、习惯不同，为了方便讲解，这里我统一通过 cURL 工具来执行 HTTP 请求。接下来先介绍下 cURL 工具。 标准的 Linux 发行版都安装了 cURL 工具。cURL 可以很方便地完成 RESTful API 的调用场景，比如设置 Header、指定 HTTP 请求方法、指定 HTTP 消息体、指定权限认证信息等。通过-v选项，也能输出 REST 请求的所有返回信息。cURL 功能很强大，有很多参数，这里列出 cURL 工具常用的参数： -X/--request [GET|POST|PUT|DELETE|…] 指定请求的 HTTP 方法 -H/--header 指定请求的 HTTP Header -d/--data 指定请求的 HTTP 消息体（Body） -v/--verbose 输出详细的返回信息 -u/--user 指定账号、密码 -b/--cookie 读取 cookie 此外，如果你想使用带 UI 界面的工具，这里我推荐你使用 Insomnia 。Insomnia 是一个跨平台的 REST API 客户端，与 Postman、Apifox 是一类工具，用于接口管理、测试。Insomnia 功能强大，支持以下功能： 发送 HTTP 请求；创建工作区或文件夹；导入和导出数据；导出 cURL 格式的 HTTP 请求命令；支持编写 swagger 文档；快速切换请求；URL 编码和解码。… Insomnia 界面如下图所示： 当然了，也有很多其他优秀的带 UI 界面的 REST API 客户端，例如 Postman、Apifox 等，你可以根据需要自行选择。接下来，我用对 secret 资源的 CURD 操作，来给你演示下如何使用 iam-apiserver 的功能。你需要执行 6 步操作。 登录 iam-apiserver，获取 token。 创建一个名为 secret0 的 secret。 获取 secret0 的详细信息。 更新 secret0 的描述。 获取 secret 列表。 删除 secret0。 具体操作如下：登录 iam-apiserver，获取 token： $ curl -s -XPOST -H\"Authorization: Basic `echo -n 'admin:Admin@2021'|base64`\" http://127.0.0.1:8080/login | jq -r .token eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhdWQiOiJpYW0uYXBpLm1hcm1vdGVkdS5jb20iLCJleHAiOjE2MzUwNTk4NDIsImlkZW50aXR5IjoiYWRtaW4iLCJpc3MiOiJpYW0tYXBpc2VydmVyIiwib3JpZ19pYXQiOjE2MjcyODM4NDIsInN1YiI6ImFkbWluIn0.gTS0n-7njLtpCJ7mvSnct2p3TxNTUQaduNXxqqLwGfI 这里，为了便于使用，我们将 token 设置为环境变量： TOKEN=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhdWQiOiJpYW0uYXBpLm1hcm1vdGVkdS5jb20iLCJleHAiOjE2MzUwNTk4NDIsImlkZW50aXR5IjoiYWRtaW4iLCJpc3MiOiJpYW0tYXBpc2VydmVyIiwib3JpZ19pYXQiOjE2MjcyODM4NDIsInN1YiI6ImFkbWluIn0.gTS0n-7njLtpCJ7mvSnct2p3TxNTUQaduNXxqqLwGfI 创建一个名为 secret0 的 secret： $ curl -v -XPOST -H \"Content-Type: application/json\" -H\"Authorization: Bearer ${TOKEN}\" -d'{\"metadata\":{\"name\":\"secret0\"},\"expires\":0,\"description\":\"admin secret\"}' http://iam.api.marmotedu.com:8080/v1/secrets * About to connect() to iam.api.marmotedu.com port 8080 (#0) * Trying 127.0.0.1... * Connected to iam.api.marmotedu.com (127.0.0.1) port 8080 (#0) \u003e POST /v1/secrets HTTP/1.1 \u003e User-Agent: curl/7.29.0 \u003e Host: iam.api.marmotedu.com:8080 \u003e Accept: */* \u003e Content-Type: application/json \u003e Authorization: Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhdWQiOiJpYW0uYXBpLm1hcm1vdGVkdS5jb20iLCJleHAiOjE2MzUwNTk4NDIsImlkZW50aXR5IjoiYWRtaW4iLCJpc3MiOiJpYW0tYXBpc2VydmVyIiwib3JpZ19pYXQiOjE2MjcyODM4NDIsInN1YiI6ImFkbWluIn0.gTS0n-7njLtpCJ7mvSnct2p3TxNTUQaduNXxqqLwGfI \u003e Content-Length: 72 \u003e * upload completely sent off: 72 out of 72 bytes \u003c HTTP/1.1 200 OK \u003c Content-Type: application/json; charset=utf-8 \u003c X-Request-Id: ff825bea-53de-4020-8e68-4e87574bd1ba \u003c Date: Mon, 26 Jul 2021 07:20:26 GMT \u003c Content-Length: 313 \u003c * Connection #0 to host iam.api.marmotedu.com left intact {\"metadata\":{\"id\":60,\"instanceID\":\"secret-jedr3e\",\"name\":\"secret0\",\"createdAt\":\"2021-07-26T15:20:26.885+08:00\",\"updatedAt\":\"2021-07-26T15:20:26.907+08:00\"},\"username\":\"admin\",\"secretID\":\"U6CxKs0YVWyOp5GrluychYIRxDmMDFd1mOOD\",\"secretKey\":\"fubNIn8jLA55ktuuTpXM8Iw5ogdR2mlf\",\"expires\":0,\"description\":\"admin secret\"} 可以看到，请求返回头中返回了X-Request-Id Header，X-Request-Id唯一标识这次请求。如果这次请求失败，就可以将X-Request-Id提供给运维或者开发，通过X-Request-Id定位出失败的请求，进行排障。另外X-Request-Id在微服务场景中，也可以透传给其他服务，从而实现请求调用链。 获取 secret0 的详细信息： $ curl -XGET -H\"Authorization: Bearer ${TOKEN}\" http://iam.api.marmotedu.com:8080/v1/secrets/secret0 {\"metadata\":{\"id\":60,\"instanceID\":\"secret-jedr3e\",\"name\":\"secret0\",\"createdAt\":\"2021-07-26T15:20:26+08:00\",\"updatedAt\":\"2021-07-26T15:20:26+08:00\"},\"username\":\"admin\",\"secretID\":\"U6CxKs0YVWyOp5GrluychYIRxDmMDFd1mOOD\",\"secretKey\":\"fubNIn8jLA55ktuuTpXM8Iw5ogdR2mlf\",\"expires\":0,\"description\":\"admin secret\"} 更新 secret0 的描述： $ curl -XPUT -H\"Authorization: Bearer ${TOKEN}\" -d'{\"metadata\":{\"name\":\"secret\"},\"expires\":0,\"description\":\"admin secret(modify)\"}' http://iam.api.marmotedu.com:8080/v1/sec","date":"2022-07-07 15:20:13","objectID":"/iam_service_develop/:5:3","tags":["iam"],"title":"iam_service_develop","uri":"/iam_service_develop/"},{"categories":["iam"],"content":"iam-apiserver 代码实现 上面，我介绍了 iam-apiserver 的功能和使用方法，这里我们再来看下 iam-apiserver 具体的代码实现。我会从配置处理、启动流程、请求处理流程、代码架构 4 个方面来讲解。 iam-apiserver 配置处理 iam-apiserver 服务的 main 函数位于apiserver.go文件中，你可以跟读代码，了解 iam-apiserver 的代码实现。这里，我来介绍下 iam-apiserver 服务的一些设计思想。 首先，来看下 iam-apiserver 中的 3 种配置： Options 配置、应用配置和 HTTP/GRPC 服务配置。Options 配置：用来构建命令行参数，它的值来自于命令行选项或者配置文件（也可能是二者 Merge 后的配置）。Options 可以用来构建应用框架，Options 配置也是应用配置的输入。 应用配置：iam-apiserver 组件中需要的一切配置。有很多地方需要配置，例如，启动 HTTP/GRPC 需要配置监听地址和端口，初始化数据库需要配置数据库地址、用户名、密码等。 HTTP/GRPC 服务配置：启动 HTTP 服务或者 GRPC 服务需要的配置。 这三种配置的关系如下图： Options 配置接管命令行选项，应用配置接管整个应用的配置，HTTP/GRPC 服务配置接管跟 HTTP/GRPC 服务相关的配置。这 3 种配置独立开来，可以解耦命令行选项、应用和应用内的服务，使得这 3 个部分可以独立扩展，又不相互影响。 iam-apiserver 根据 Options 配置来构建命令行参数和应用配置。 我们通过github.com/marmotedu/iam/pkg/app包的buildCommand方法来构建命令行参数。这里的核心是，通过NewApp函数构建 Application 实例时，传入的Options实现了Flags() (fss cliflag.NamedFlagSets)方法，通过 buildCommand 方法中的以下代码，将 option 的 Flag 添加到 cobra 实例的 FlagSet 中： if a.options != nil { namedFlagSets = a.options.Flags() fs := cmd.Flags() for _, f := range namedFlagSets.FlagSets { fs.AddFlagSet(f) } ... } 通过CreateConfigFromOptions函数来构建应用配置： cfg, err := config.CreateConfigFromOptions(opts) if err != nil { return err } 根据应用配置来构建 HTTP/GRPC 服务配置。例如，以下代码根据应用配置，构建了 HTTP 服务器的 Address 参数： func (s *InsecureServingOptions) ApplyTo(c *server.Config) error { c.InsecureServing = \u0026server.InsecureServingInfo{ Address: net.JoinHostPort(s.BindAddress, strconv.Itoa(s.BindPort)), } return nil } 其中，c *server.Config是 HTTP 服务器的配置，s *InsecureServingOptions是应用配置。 iam-apiserver 启动流程设计 接下来，我们来详细看下 iam-apiserver 的启动流程设计。启动流程如下图所示： 首先，通过opts := options.NewOptions()创建带有默认值的 Options 类型变量 opts。opts 变量作为github.com/marmotedu/iam/pkg/app包的NewApp函数的输入参数，最终在 App 框架中，被来自于命令行参数或配置文件的配置（也可能是二者 Merge 后的配置）所填充，opts 变量中各个字段的值会用来创建应用配置。 接着，会注册run函数到 App 框架中。run 函数是 iam-apiserver 的启动函数，里面封装了我们自定义的启动逻辑。run 函数中，首先会初始化日志包，这样我们就可以根据需要，在后面的代码中随时记录日志了。 然后，会创建应用配置。应用配置和 Options 配置其实是完全独立的，二者可能完全不同，但在 iam-apiserver 中，二者配置项是相同的。之后，根据应用配置，创建 HTTP/GRPC 服务器所使用的配置。在创建配置后，会先分别进行配置补全，再使用补全后的配置创建 Web 服务实例，例如： genericServer, err := genericConfig.Complete().New() if err != nil { return nil, err } extraServer, err := extraConfig.complete().New() if err != nil { return nil, err } ... func (c *ExtraConfig) complete() *completedExtraConfig { if c.Addr == \"\" { c.Addr = \"127.0.0.1:8081\" } return \u0026completedExtraConfig{c} } 上面的代码中，首先调用Complete/complete函数补全配置，再基于补全后的配置，New 一个 HTTP/GRPC 服务实例。 这里有个设计技巧：complete函数返回的是一个*completedExtraConfig类型的实例，在创建 GRPC 实例时，是调用completedExtraConfig结构体提供的New方法，这种设计方法可以确保我们创建的 GRPC 实例一定是基于 complete 之后的配置（completed）。 在实际的 Go 项目开发中，我们需要提供一种机制来处理或补全配置，这在 Go 项目开发中是一个非常有用的步骤。 最后，调用PrepareRun方法，进行 HTTP/GRPC 服务器启动前的准备。在准备函数中，我们可以做各种初始化操作，例如初始化数据库，安装业务相关的 Gin 中间件、RESTful API 路由等。完成 HTTP/GRPC 服务器启动前的准备之后，调用Run方法启动 HTTP/GRPC 服务。在Run方法中，分别启动了 GRPC 和 HTTP 服务。 可以看到，整个 iam-apiserver 的软件框架是比较清晰的。服务启动后，就可以处理请求了。所以接下来，我们再来看下 iam-apiserver 的 RESTAPI 请求处理流程。 iam-apiserver 的 REST API 请求处理流程 iam-apiserver 的请求处理流程也是清晰、规范的，具体流程如下图所示： 结合上面这张图，我们来看下 iam-apiserver 的 REST API 请求处理流程，来帮你更好地理解 iam-apiserver 是如何处理 HTTP 请求的。 首先，我们通过 API 调用（ + ）请求 iam-apiserver 提供的 RESTful API 接口。接着，Gin Web 框架接收到 HTTP 请求之后，会通过认证中间件完成请求的认证，iam-apiserver 提供了 Basic 认证和 Bearer 认证两种认证方式。认证通过后，请求会被我们加载的一系列中间件所处理，例如跨域、RequestID、Dump 等中间件。最后，根据 + 进行路由匹配。 举个例子，假设我们请求的 RESTful API 是POST + /v1/secrets，Gin Web 框架会根据 HTTP Method 和 HTTP Request Path，查找注册的 Controllers，最终匹配到secretController.CreateController。在 Create Controller 中，我们会依次执行请求参数解析、请求参数校验、调用业务层的方法创建 Secret、处理业务层的返回结果，最后返回最终的 HTTP 请求结果。 iam-apiserver 代码架构 iam-apiserver 代码设计遵循简洁架构设计，一个简洁架构具有以下 5 个特性： 独立于框架：该架构不会依赖于某些功能强大的软件库存在。这可以让你使用这样的框架作为工具，而不是让你的系统陷入到框架的约束中。 可测试性：业务规则可以在没有 UI、数据库、Web 服务或其他外部元素的情况下进行测试，在实际的开发中，我们通过 Mock 来解耦这些依赖。 独立于 UI ：在无需改变系统其他部分的情况下，UI 可以轻松地改变。例如，在没有改变业务规则的情况下，Web UI 可以替换为控制台 UI。 独立于数据库：你可以用 Mongo、Oracle、Etcd 或者其他数据库来替换 MariaDB，你的业务规则不要绑定到数据库。 独立于外部媒介：实际上，你的业务规则可以简单到根本不去了解外部世界。 所以，基于这些约束，每一层都必须是独立的和可测试的。iam-apiserver 代码架构分为 4 层：模型层（Models）、控制层（Controller）、业务层 （Service）、仓库层（Repository）。从控制层、业务层到仓库层，","date":"2022-07-07 15:20:13","objectID":"/iam_service_develop/:5:4","tags":["iam"],"title":"iam_service_develop","uri":"/iam_service_develop/"},{"categories":["iam"],"content":"总结 这一讲，我主要介绍了 iam-apiserver 的功能和使用方法，以及它的代码实现。iam-apiserver 是一个 Web 服务，提供了 REST API 来完成用户、密钥、策略三种 REST 资源的增删改查。我们可以通过 cURL、Insomnia 等工具，来完成 REST API 请求。 iam-apiserver 包含了 3 种配置：Options 配置、应用配置、HTTP/GRPC 服务配置。这三种配置分别用来构建命令行参数、应用和 HTTP/GRPC 服务。 iam-apiserver 在启动时，会先构建应用框架，接着会设置应用选项，然后对应用进行初始化，最后创建 HTTP/GRPC 服务的配置和实例，最终启动 HTTP/GRPC 服务。服务启动之后，就可以接收 HTTP 请求了。一个 HTTP 请求会先进行认证，接着会被注册的中间件处理，然后，会根据(HTTP Method, HTTP Request Path)匹配到处理函数。在处理函数中，会解析请求参数、校验参数、调用业务逻辑处理函数，最终返回请求结果。 iam-apiserver 采用了简洁架构，整个应用分为 4 层：模型层、控制层、业务层和仓库层。模型层存储对象的结构和它的方法；仓库层用来跟数据库 / 第三方服务进行 CURD 交互；业务层主要用来完成业务逻辑处理；控制层接收 HTTP 请求，并进行参数解析、参数校验、逻辑分发处理、请求返回操作。控制层、业务层、仓库层之间通过接口通信，通过接口通信可以使相同的功能支持不同的实现，并使每一层的代码变得可测试。 ","date":"2022-07-07 15:20:13","objectID":"/iam_service_develop/:5:5","tags":["iam"],"title":"iam_service_develop","uri":"/iam_service_develop/"},{"categories":["iam"],"content":"29｜控制流（下）：iam-apiserver服务核心功能实现讲解 上一讲，我介绍了 iam-apiserver 是如何构建 Web 服务的。这一讲，我们再来看下 iam-apiserver 中的核心功能实现。在对这些核心功能的讲解中，我会向你传达我的程序设计思路。 iam-apiserver 中包含了很多优秀的设计思想和实现，这些点可能比较零碎，但我觉得很值得分享给你。我将这些关键代码设计分为 3 类，分别是应用框架相关的特性、编程规范相关的特性和其他特性。接下来，我们就来详细看看这些设计点，以及它们背后的设计思想。 ","date":"2022-07-07 15:20:13","objectID":"/iam_service_develop/:6:0","tags":["iam"],"title":"iam_service_develop","uri":"/iam_service_develop/"},{"categories":["iam"],"content":"应用框架相关的特性 应用框架相关的特性包括三个，分别是优雅关停、健康检查和插件化加载中间件。 优雅关停 在讲优雅关停之前，先来看看不优雅的停止服务方式是什么样的。当我们需要重启服务时，首先需要停止服务，这时可以通过两种方式来停止我们的服务： 在 Linux 终端键入 Ctrl + C（其实是发送 SIGINT 信号）。 发送 SIGTERM 信号，例如 kill 或者 systemctl stop 等。 当我们使用以上两种方式停止服务时，都会产生下面两个问题： 有些请求正在处理，如果服务端直接退出，会造成客户端连接中断，请求失败。 我们的程序可能需要做一些清理工作，比如等待进程内任务队列的任务执行完成，或者拒绝接受新的消息等。 这些问题都会对业务造成影响，所以我们需要一种优雅的方式来关停我们的应用。在 Go 开发中，通常通过拦截 SIGINT 和 SIGTERM 信号，来实现优雅关停。当收到这两个信号时，应用进程会做一些清理工作，然后结束阻塞状态，继续执行余下的代码，最后自然退出进程。 先来看一个简单的优雅关停的示例代码： package main import ( \"context\" \"log\" \"net/http\" \"os\" \"os/signal\" \"time\" \"github.com/gin-gonic/gin\" ) func main() { router := gin.Default() router.GET(\"/\", func(c *gin.Context) { time.Sleep(5 * time.Second) c.String(http.StatusOK, \"Welcome Gin Server\") }) srv := \u0026http.Server{ Addr: \":8080\", Handler: router, } go func() { // 将服务在 goroutine 中启动 if err := srv.ListenAndServe(); err != nil \u0026\u0026 err != http.ErrServerClosed { log.Fatalf(\"listen: %s\\n\", err) } }() quit := make(chan os.Signal) signal.Notify(quit, os.Interrupt) \u003c-quit // 阻塞等待接收 channel 数据 log.Println(\"Shutdown Server ...\") ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second) // 5s 缓冲时间处理已有请求 defer cancel() if err := srv.Shutdown(ctx); err != nil { // 调用 net/http 包提供的优雅关闭函数：Shutdown log.Fatal(\"Server Shutdown:\", err) } log.Println(\"Server exiting\") } 上面的代码实现优雅关停的思路如下： 将 HTTP 服务放在 goroutine 中运行，程序不阻塞，继续执行。 创建一个无缓冲的 channel quit，调用 signal.Notify(quit, os.Interrupt)。通过 signal.Notify 函数调用，可以将进程收到的 os.Interrupt（SIGINT）信号，发送给 channel quit。 \u003c-quit 阻塞当前 goroutine（也就是 main 函数所在的 goroutine），等待从 channel quit 接收关停信号。通过以上步骤，我们成功启动了 HTTP 服务，并且 main 函数阻塞，防止启动 HTTP 服务的 goroutine 退出。当我们键入 Ctrl + C时，进程会收到 SIGINT 信号，并将该信号发送到 channel quit 中，这时候\u003c-quit收到了 channel 另一端传来的数据，结束阻塞状态，程序继续执行。这里，\u003c-quit唯一目的是阻塞当前的 goroutine，所以对收到的数据直接丢弃。 打印退出消息，提示准备退出当前服务。 调用 net/http 包提供的 Shutdown 方法，Shutdown 方法会在指定的时间内处理完现有请求，并返回。 最后，程序执行完 log.Println(“Server exiting”) 代码后，退出 main 函数。 iam-apiserver 也实现了优雅关停，优雅关停思路跟上面的代码类似。具体可以分为三个步骤，流程如下： 第一步，创建 channel 用来接收 os.Interrupt（SIGINT）和 syscall.SIGTERM（SIGKILL）信号。代码见 internal/pkg/server/signal.go 。 var onlyOneSignalHandler = make(chan struct{}) var shutdownHandler chan os.Signal func SetupSignalHandler() \u003c-chan struct{} { close(onlyOneSignalHandler) // panics when called twice shutdownHandler = make(chan os.Signal, 2) stop := make(chan struct{}) signal.Notify(shutdownHandler, shutdownSignals...) go func() { \u003c-shutdownHandler close(stop) \u003c-shutdownHandler os.Exit(1) // second signal. Exit directly. }() return stop } SetupSignalHandler 函数中，通过 close(onlyOneSignalHandler)来确保 iam-apiserver 组件的代码只调用一次 SetupSignalHandler 函数。否则，可能会因为信号传给了不同的 shutdownHandler，而造成信号丢失。SetupSignalHandler 函数还实现了一个功能：收到一次 SIGINT/ SIGTERM 信号，程序优雅关闭。收到两次 SIGINT/ SIGTERM 信号，程序强制关闭。实现代码如下： go func() { \u003c-shutdownHandler close(stop) \u003c-shutdownHandler os.Exit(1) // second signal. Exit directly. }() 这里要注意：signal.Notify(c chan\u003c- os.Signal, sig …os.Signal)函数不会为了向 c 发送信息而阻塞。也就是说，如果发送时 c 阻塞了，signal 包会直接丢弃信号。为了不丢失信号，我们创建了有缓冲的 channel shutdownHandler。最后，SetupSignalHandler 函数会返回 stop，后面的代码可以通过关闭 stop 来结束代码的阻塞状态。 第二步，将 channel stop 传递给启动 HTTP（S）、gRPC 服务的函数，在函数中以 goroutine 的方式启动 HTTP（S）、gRPC 服务，然后执行 \u003c-stop 阻塞 goroutine。 第三步，当 iam-apiserver 进程收到 SIGINT/SIGTERM 信号后，关闭 stop channel，继续执行 \u003c-stop 后的代码，在后面的代码中，我们可以执行一些清理逻辑，或者调用 google.golang.org/grpc和 net/http包提供的优雅关停函数 GracefulStop 和 Shutdown。例如下面这个代码（位于 internal/apiserver/grpc.go 文件中）： func (s *grpcAPIServer) Run(stopCh \u003c-chan struct{}) { listen, err := net.Listen(\"tcp\", s.address) if err != nil { log.Fatalf(\"failed to listen: %s\", err.Error()) } log.Infof(\"Start grpc server at %s\", s.address) go func() { if err := s.Serve(listen); err != nil { log.Fatalf(\"failed to start grpc server: %s\", err.Error()) } }() \u003c-stopCh log.Infof(\"Grpc server on %s stopped\", s.address) s.GracefulStop() } 除了上面说的方法，iam-apiserver 还通过 github.com/marmotedu/iam/pkg/shutdown 包，实现了另外一种优雅关停方法，这个方法更加友好、更加灵活。实现代码见 PrepareRun 函数。github.com/marmotedu/iam/pkg/shutdown 包的使用方法如下： package main import ( \"fmt\" \"time\" \"github.com/marmote","date":"2022-07-07 15:20:13","objectID":"/iam_service_develop/:6:1","tags":["iam"],"title":"iam_service_develop","uri":"/iam_service_develop/"},{"categories":["iam"],"content":"编程规范相关的特性 编程规范相关的特性有四个，分别是 API 版本、统一的资源元数据、统一的返回、并发处理模板。 API 版本 RESTful API 为了方便以后扩展，都需要支持 API 版本。在 12 讲 中，我们介绍了 API 版本号的 3 种标识方法，iam-apiserver 选择了将 API 版本号放在 URL 中，例如/v1/secrets。放在 URL 中的好处是很直观，看 API 路径就知道版本号。另外，API 的路径也可以很好地跟控制层、业务层、模型层的代码路径相映射。例如，密钥资源相关的代码存放位置如下： internal/apiserver/controller/v1/secret/ # 控制几层代码存放位置 internal/apiserver/service/v1/secret.go # 业务层代码存放位置 github.com/marmotedu/api/apiserver/v1/secret.go # 模型层代码存放位置 关于代码存放路径，我还有一些地方想跟你分享。对于 Secret 资源，通常我们需要提供 CRUD 接口。 C：Create（创建 Secret）。R：Get（获取详情）、List（获取 Secret 资源列表）。U：Update（更新 Secret）。D：Delete（删除指定的 Secret）、DeleteCollection（批量删除 Secret）。 每个接口相互独立，为了减少更新 A 接口代码时因为误操作影响到 B 接口代码的情况，这里建议 CRUD 接口每个接口一个文件，从物理上将不同接口的代码隔离开。这种接口还可以方便我们查找 A 接口的代码所在位置。例如，Secret 控制层相关代码的存放方式如下： $ ls internal/apiserver/controller/v1/secret/ create.go delete_collection.go delete.go doc.go get.go list.go secret.go update.go 业务层和模型层的代码也可以这么组织。iam-apiserver 中，因为 Secret 的业务层和模型层代码比较少，所以我放在了 internal/apiserver/service/v1/secret.go和 github.com/marmotedu/api/apiserver/v1/secret.go文件中。如果后期 Secret 业务代码增多，我们也可以修改成下面这种方式： $ ls internal/apiserver/service/v1/secret/ create.go delete_collection.go delete.go doc.go get.go list.go secret.go update.go 这里再说个题外话：/v1/secret/和/secret/v1/这两种目录组织方式都可以，你选择一个自己喜欢的就行。当我们需要升级 API 版本时，相关代码可以直接放在 v2 目录下，例如： internal/apiserver/controller/v2/secret/ # v2 版本控制几层代码存放位置 internal/apiserver/service/v2/secret.go # v2 版本业务层代码存放位置 github.com/marmotedu/api/apiserver/v2/secret.go # v2 版本模型层代码存放位置 这样既能够跟 v1 版本的代码物理隔离开，互不影响，又方便查找 v2 版本的代码。 统一的资源元数据 iam-apiserver 设计的一大亮点是，像 Kubernetes REST 资源一样，支持统一的资源元数据。 iam-apiserver 中所有的资源都是 REST 资源，iam-apiserver 将 REST 资源的属性也进一步规范化了，这里的规范化是指所有的 REST 资源均支持两种属性：公共属性。资源自有的属性。 例如，Secret 资源的定义方式如下： type Secret struct { // May add TypeMeta in the future. // metav1.TypeMeta `json:\",inline\"` // Standard object's metadata. metav1.ObjectMeta `json:\"metadata,omitempty\"` Username string `json:\"username\" gorm:\"column:username\" validate:\"omitempty\"` SecretID string `json:\"secretID\" gorm:\"column:secretID\" validate:\"omitempty\"` SecretKey string `json:\"secretKey\" gorm:\"column:secretKey\" validate:\"omitempty\"` // Required: true Expires int64 `json:\"expires\" gorm:\"column:expires\" validate:\"omitempty\"` Description string `json:\"description\" gorm:\"column:description\" validate:\"description\"` } 资源自有的属性，会因资源不同而不同。这里，我们来重点看一下公共属性 ObjectMeta ，它的定义如下： type ObjectMeta struct { ID uint64 `json:\"id,omitempty\" gorm:\"primary_key;AUTO_INCREMENT;column:id\"` InstanceID string `json:\"instanceID,omitempty\" gorm:\"unique;column:instanceID;type:varchar(32);not null\"` Name string `json:\"name,omitempty\" gorm:\"column:name;type:varchar(64);not null\" validate:\"name\"` Extend Extend `json:\"extend,omitempty\" gorm:\"-\" validate:\"omitempty\"` ExtendShadow string `json:\"-\" gorm:\"column:extendShadow\" validate:\"omitempty\"` CreatedAt time.Time `json:\"createdAt,omitempty\" gorm:\"column:createdAt\"` UpdatedAt time.Time `json:\"updatedAt,omitempty\" gorm:\"column:updatedAt\"` } 接下来，我来详细介绍公共属性中每个字段的含义及作用。 ID 这里的 ID，映射为 MariaDB 数据库中的 id 字段。id 字段在一些应用中，会作为资源的唯一标识。但 iam-apiserver 中没有使用 ID 作为资源的唯一标识，而是使用了 InstanceID。iam-apiserver 中 ID 唯一的作用是跟数据库 id 字段进行映射，代码中并没有使用到 ID。 InstanceID InstanceID 是资源的唯一标识，格式为-xxxxxx。其中，是资源的英文标识符号，xxxxxx是随机字符串。字符集合为 abcdefghijklmnopqrstuvwxyz1234567890，长度 \u003e=6，例如 secret-yj8m30、user-j4lz3g、policy-3v18jq。 **腾讯云、阿里云、华为云也都是采用这种格式的字符串作为资源唯一标识的。**InstanceID 的生成和更新都是自动化的，通过 gorm 提供的 AfterCreate Hooks 在记录插入数据库之后，生成并更新到数据库的 instanceID字段： func (s *Secret) AfterCreate(tx *gorm.DB) (err error) { s.InstanceID = idutil.GetInstanceID(s.ID, \"secret-\") return tx.Save(s).Error } 上面的代码，在 Secret 记录插入到 iam 数据库的 secret 表之后，调用 idutil.GetInstanceID生成 InstanceID，并通过 tx.Save(s)更新到数据库 secret 表的 instanceID字段。 因为通常情况下，应用中的 REST 资源只会保存到数据库中的一张表里，这样就能保证应用中每个资源的数据库 ID 是唯一的。所以 GetInstanceID(uid uint64, prefix string) string函数使用 github.com/speps/go-hashids包提供的方法，对这个数据库 ID 进行哈希，最终得到一个数据库级别的唯一的字符串（例如：3v18jq），并根据传入的 prefix，得到资源的 InstanceID。 使用这种方式生成资源的唯一标识，有下面这两个优点： 数据库级别唯一。 InstanceID 是长度可控的字符串，长度最小是 6 个字符，但会根据表中的记录个数动态变长。根","date":"2022-07-07 15:20:13","objectID":"/iam_service_develop/:6:2","tags":["iam"],"title":"iam_service_develop","uri":"/iam_service_develop/"},{"categories":["iam"],"content":"其他特性 除了上面那两大类，这里我还想给你介绍下关键代码设计中的其他特性，包括插件化选择 JSON 库、调用链实现、数据一致性。 插件化选择 JSON 库 Golang 提供的标准 JSON 解析库 encoding/json，在开发高性能、高并发的网络服务时会产生性能问题。所以很多开发者在实际的开发中，往往会选用第三方的高性能 JSON 解析库，例如 jsoniter 、 easyjson 、 jsonparser 等。 我见过的很多开发者选择了 jsoniter，也有一些开发者使用了 easyjson。jsoniter 的性能略高于 encoding/json。但随着 go 版本的迭代，encoding/json 库的性能也越来越高，jsoniter 的性能优势也越来越有限。所以，IAM 项目使用了 jsoniter 库，并准备随时切回 encoding/json 库。 为了方便切换不同的 JSON 包，iam-apiserver 采用了一种插件化的机制来使用不同的 JSON 包。具体是通过使用 go 的标签编译选择运行的解析库来实现的。 标签编译就是在源代码里添加标注，通常称之为编译标签（build tag）。编译标签通过注释的方式在靠近源代码文件顶部的地方添加。go build 在构建一个包的时候，会读取这个包里的每个源文件并且分析编译便签，这些标签决定了这个源文件是否参与本次编译。例如： // +build jsoniter package json import jsoniter \"github.com/json-iterator/go\" +build jsoniter就是编译标签。这里要注意，一个源文件可以有多个编译标签，多个编译标签之间是逻辑“与”的关系；一个编译标签可以包括由空格分割的多个标签，这些标签是逻辑“或”的关系。例如： // +build linux darwin // +build 386 这里要注意，编译标签和包的声明之间应该使用空行隔开，否则编译标签会被当作包声明的注释，而不是编译标签。那具体来说，我们是如何实现插件化选择 JSON 库的呢？ 首先，我自定义了一个 github.com/marmotedu/component-base/pkg/json json 包，来适配 encoding/json 和 json-iterator。github.com/marmotedu/component-base/pkg/json 包中有两个文件： json.go：映射了 encoding/json 包的 Marshal、Unmarshal、MarshalIndent、NewDecoder、NewEncoder 方法。 jsoniter.go：映射了 github.com/json-iterator/go 包的 Marshal、Unmarshal、MarshalIndent、NewDecoder、NewEncoder。 json.go 和 jsoniter.go 通过编译标签，让 Go 编译器在构建代码时选择使用哪一个 json 文件。接着，通过在执行 go build时指定 -tags 参数，来选择编译哪个 json 文件。 json/json.go、json/jsoniter.go 这两个 Go 文件的顶部，都有一行注释： // +build !jsoniter // +build jsoniter // +build !jsoniter表示，tags 不是 jsoniter 的时候编译这个 Go 文件。// +build jsoniter表示，tags 是 jsoniter 的时候编译这个 Go 文件。也就是说，这两种条件是互斥的，只有当 tags=jsoniter 的时候，才会使用 json-iterator，其他情况使用 encoding/json。 例如，如果我们想使用包，可以这么编译项目： $ go build -tags=jsoniter 在实际开发中，我们需要根据场景来选择合适的 JSON 库。这里我给你一些建议。 场景一：结构体序列化和反序列化场景 在这个场景中，我个人首推的是官方的 JSON 库。可能你会比较意外，那我就来说说我的理由： 首先，虽然 easyjson 的性能压倒了其他所有开源项目，但它有一个最大的缺陷，那就是需要额外使用工具来生成这段代码，而对额外工具的版本控制就增加了运维成本。当然，如果你的团队已经能够很好地处理 protobuf 了，也是可以用同样的思路来管理 easyjson 的。其次，虽然 Go 1.8 之前，官方 JSON 库的性能总是被大家吐槽，但现在（1.16.3）官方 JSON 库的性能已不可同日而语。此外，作为使用最为广泛，而且没有之一的 JSON 库，官方库的 bug 是最少的，兼容性也是最好的最后，jsoniter 的性能虽然依然优于官方，但没有达到逆天的程度。如果你追求的是极致的性能，那么你应该选择 easyjson 而不是 jsoniter。jsoniter 近年已经不活跃了，比如说，我前段时间提了一个 issue 没人回复，于是就上去看了下 issue 列表，发现居然还遗留着一些 2018 年的 issue。 场景二：非结构化数据的序列化和反序列化场景 这个场景下，我们要分高数据利用率和低数据利用率两种情况来看。你可能对数据利用率的高低没啥概念，那我举个例子：JSON 数据的正文中，如果说超过四分之一的数据都是业务需要关注和处理的，那就算是高数据利用率。 在高数据利用率的情况下，我推荐使用 jsonvalue。至于低数据利用率的情况，还可以根据 JSON 数据是否需要重新序列化，分成两种情况。 如果无需重新序列化，这个时候选择 jsonparser 就行了，因为它的性能实在是耀眼。如果需要重新序列化，这种情况下你有两种选择：如果对性能要求相对较低，可以使用 jsonvalue；如果对性能的要求高，并且只需要往二进制序列中插入一条数据，那么可以采用 jsoniter 的 Set 方法。 实际操作中，超大 JSON 数据量，并且同时需要重新序列化的情况非常少，往往是在代理服务器、网关、overlay 中继服务等，同时又需要往原数据中注入额外信息的时候。换句话说，jsoniter 的适用场景比较有限。下面是从 10% 到 60% 数据覆盖率下，不同库的操作效率对比（纵坐标单位：μs/op）： 可以看到，当 jsoniter 的数据利用率达到 25% 时，和 jsonvalue、jsonparser 相比就已经没有任何优势；至于 jsonvalue，由于对数据做了一次性的全解析，因此解析后的数据存取耗时极少，因此在不同数据覆盖率下的耗时都很稳定。 调用链实现 调用链对查日志、排障帮助非常大。所以，在 iam-apiserver 中也实现了调用链，通过 requestID来串联整个调用链。 具体是通过以下两步来实现的：第一步，将 ctx context.Context 类型的变量作为函数的第一个参数，在函数调用时传递。第二步，不同函数中，通过 log.L(ctx context.Context)来记录日志。 在请求到来时，请求会通过 Context 中间件处理： func Context() gin.HandlerFunc { return func(c *gin.Context) { c.Set(log.KeyRequestID, c.GetString(XRequestIDKey)) c.Set(log.KeyUsername, c.GetString(UsernameKey)) c.Next() } } 在 Context 中间件中，会在 gin.Context 类型的变量中设置 log.KeyRequestID键，其值为 36 位的 UUID。UUID 通过 RequestID 中间件来生成，并设置在 gin 请求的 Context 中。 RequestID 中间件在 Context 中间件之前被加载，所以在 Context 中间件被执行时，能够获取到 RequestID 生成的 UUID。log.L(ctx context.Context)函数在记录日志时，会从头 ctx 中获取到 log.KeyRequestID，并作为一个附加字段随日志打印。 通过以上方式，我们最终可以形成 iam-apiserver 的请求调用链，日志示例如下： 2021-07-19 19:41:33.472 INFO apiserver apiserver/auth.go:205 user `admin` is authenticated. {\"requestID\": \"b6c56cd3-d095-4fd5-a928-291a2e33077f\", \"username\": \"admin\"} 2021-07-19 19:41:33.472 INFO apiserver policy/create.go:22 create policy function called. {\"requestID\": \"b6c56cd3-d095-4fd5-a928-291a2e33077f\", \"username\": \"admin\"} ... 另外，ctx context.Context作为函数 / 方法的第一个参数，还有一个好处是方便后期扩展。例如，如果我们有以下调用关系： package main import \"fmt\" func B(name, address string) string { ret","date":"2022-07-07 15:20:13","objectID":"/iam_service_develop/:6:3","tags":["iam"],"title":"iam_service_develop","uri":"/iam_service_develop/"},{"categories":["iam"],"content":"总结 今天，我和你分享了 iam-apiserver 的一些关键功能实现，并介绍了我的设计思路。这里我再简要梳理下。 为了保证进程关停时，HTTP 请求执行完后再断开连接，进程中的任务正常完成，iam-apiserver 实现了优雅关停功能。 为了避免进程存在，但服务没成功启动的异常场景，iam-apiserver 实现了健康检查机制。 Gin 中间件可通过配置文件配置，从而实现按需加载的特性。 为了能够直接辨别出 API 的版本，iam-apiserver 将 API 的版本标识放在 URL 路径中，例如 /v1/secrets。 为了能够最大化地共享功能代码，iam-apiserver 抽象出了统一的元数据，每个 REST 资源都具有这些元数据。 因为 API 接口都是通过同一个函数来返回的，其返回格式自然是统一的。 因为程序中经常需要处理并发逻辑，iam-apiserver 抽象出了一个通用的并发模板。 为了方便根据需要切换 JSON 库，我们实现了插件化选择 JSON 库的功能。 为了实现调用链功能，iam-apiserver 不同函数之间通过 ctx context.Context 来传递 RequestID。 iam-apiserver 通过 Redis 的 Sub/Pub 机制来保证数据一致性。 ","date":"2022-07-07 15:20:13","objectID":"/iam_service_develop/:6:4","tags":["iam"],"title":"iam_service_develop","uri":"/iam_service_develop/"},{"categories":["iam"],"content":"30 | ORM：CURD 神器 GORM 包介绍及实战 在用 Go 开发项目时，我们免不了要和数据库打交道。每种语言都有优秀的 ORM 可供选择，在 Go 中也不例外，比如gorm、xorm、gorose等。目前，GitHub 上 star 数最多的是 GORM，它也是当前 Go 项目中使用最多的 ORM。 IAM 项目也使用了 GORM。这一讲，我就来详细讲解下 GORM 的基础知识，并介绍 iam-apiserver 是如何使用 GORM，对数据进行 CURD 操作的。 ","date":"2022-07-07 15:20:13","objectID":"/iam_service_develop/:7:0","tags":["iam"],"title":"iam_service_develop","uri":"/iam_service_develop/"},{"categories":["iam"],"content":"GORM 基础知识介绍 GORM 是 Go 语言的 ORM 包，功能强大，调用方便。像腾讯、华为、阿里这样的大厂，都在使用 GORM 来构建企业级的应用。GORM 有很多特性，开发中常用的核心特性如下： 功能全。使用 ORM 操作数据库的接口，GORM 都有，可以满足我们开发中对数据库调用的各类需求。支持钩子方法。这些钩子方法可以应用在 Create、Save、Update、Delete、Find 方法中。开发者友好，调用方便。支持 Auto Migration。支持关联查询。支持多种关系数据库，例如 MySQL、Postgres、SQLite、SQLServer 等。 GORM 有两个版本，V1和V2。遵循用新不用旧的原则，IAM 项目使用了最新的 V2 版本。 ","date":"2022-07-07 15:20:13","objectID":"/iam_service_develop/:7:1","tags":["iam"],"title":"iam_service_develop","uri":"/iam_service_develop/"},{"categories":["iam"],"content":"通过示例学习 GORM 接下来，我们先快速看一个使用 GORM 的示例，通过该示例来学习 GORM。示例代码存放在marmotedu/gopractise-demo/gorm/main.go文件中。因为代码比较长，你可以使用以下命令克隆到本地查看： $ mkdir -p $GOPATH/src/github.com/marmotedu $ cd $GOPATH/src/github.com/marmotedu $ git clone https://github.com/marmotedu/gopractise-demo $ cd gopractise-demo/gorm/ 假设我们有一个 MySQL 数据库，连接地址和端口为 127.0.0.1:3306 ，用户名为 iam ，密码为 iam1234 。创建完 main.go 文件后，执行以下命令来运行： $ go run main.go -H 127.0.0.1:3306 -u iam -p iam1234 -d test 2020/10/17 15:15:50 totalcount: 1 2020/10/17 15:15:50 code: D42, price: 100 2020/10/17 15:15:51 totalcount: 1 2020/10/17 15:15:51 code: D42, price: 200 2020/10/17 15:15:51 totalcount: 0 在企业级 Go 项目开发中，使用 GORM 库主要用来完成以下数据库操作： 连接和关闭数据库。连接数据库时，可能需要设置一些参数，比如最大连接数、最大空闲连接数、最大连接时长等。 插入表记录。可以插入一条记录，也可以批量插入记录。 更新表记录。可以更新某一个字段，也可以更新多个字段。 查看表记录。可以查看某一条记录，也可以查看符合条件的记录列表。 删除表记录。可以删除某一个记录，也可以批量删除。删除还支持永久删除和软删除。 在一些小型项目中，还会用到 GORM 的表结构自动迁移功能。 GORM 功能强大，上面的示例代码展示的是比较通用的一种操作方式。 上述代码中，首先定义了一个 GORM 模型（Models），Models 是标准的 Go struct，用来代表数据库中的一个表结构。我们可以给 Models 添加 TableName 方法，来告诉 GORM 该 Models 映射到数据库中的哪张表。Models 定义如下： type Product struct { gorm.Model Code string `gorm:\"column:code\"` Price uint `gorm:\"column:price\"` } // TableName maps to mysql table name. func (p *Product) TableName() string { return \"product\" } 如果没有指定表名，则 GORM 使用结构体名的蛇形复数作为表名。例如：结构体名为 DockerInstance ，则表名为 dockerInstances 。 在之后的代码中，使用 Pflag 来解析命令行的参数，通过命令行参数指定数据库的地址、用户名、密码和数据库名。之后，使用这些参数生成建立 MySQL 连接需要的配置文件，并调用 gorm.Open 建立数据库连接： var ( host = pflag.StringP(\"host\", \"H\", \"127.0.0.1:3306\", \"MySQL service host address\") username = pflag.StringP(\"username\", \"u\", \"root\", \"Username for access to mysql service\") password = pflag.StringP(\"password\", \"p\", \"root\", \"Password for access to mysql, should be used pair with password\") database = pflag.StringP(\"database\", \"d\", \"test\", \"Database name to use\") help = pflag.BoolP(\"help\", \"h\", false, \"Print this help message\") ) func main() { // Parse command line flags pflag.CommandLine.SortFlags = false pflag.Usage = func() { pflag.PrintDefaults() } pflag.Parse() if *help { pflag.Usage() return } dsn := fmt.Sprintf(`%s:%s@tcp(%s)/%s?charset=utf8\u0026parseTime=%t\u0026loc=%s`, *username, *password, *host, *database, true, \"Local\") db, err := gorm.Open(mysql.Open(dsn), \u0026gorm.Config{}) if err != nil { panic(\"failed to connect database\") } } 创建完数据库连接之后，会返回数据库实例 db ，之后就可以调用 db 实例中的方法，完成数据库的 CURD 操作。具体操作如下，一共可以分为六个操作： 第一个操作，自动迁移表结构。 // 1. Auto migration for given models db.AutoMigrate(\u0026Product{}) **我不建议你在正式的代码中自动迁移表结构。**因为变更现网数据库是一个高危操作，现网数据库字段的添加、类型变更等，都需要经过严格的评估才能实施。这里将变更隐藏在代码中，在组件发布时很难被研发人员感知到，如果组件启动，就可能会自动修改现网表结构，也可能会因此引起重大的现网事故。 GORM 的 AutoMigrate 方法，只对新增的字段或索引进行变更，理论上是没有风险的。在实际的 Go 项目开发中，也有很多人使用 AutoMigrate 方法自动同步表结构。但我更倾向于规范化、可感知的操作方式，所以我在实际开发中，都是手动变更表结构的。当然，具体使用哪种方法，你可以根据需要自行选择。 第二个操作，插入表记录。 // 2. Insert the value into database if err := db.Create(\u0026Product{Code: \"D42\", Price: 100}).Error; err != nil { log.Fatalf(\"Create error: %v\", err) } PrintProducts(db) 通过 db.Create 方法创建了一条记录。插入记录后，通过调用 PrintProducts 方法打印当前表中的所有数据记录，来测试是否成功插入。第三个操作，获取符合条件的记录。 // 3. Find first record that match given conditions product := \u0026Product{} if err := db.Where(\"code= ?\", \"D42\").First(\u0026product).Error; err != nil { log.Fatalf(\"Get product error: %v\", err) } First 方法只会返回符合条件的记录列表中的第一条，你可以使用 First 方法来获取某个资源的详细信息。第四个操作，更新表记录。 // 4. Update value in database, if the value doesn't have primary key, will insert it product.Price = 200 if err := db.Save(product).Error; err != nil { log.Fatalf(\"Update product error: %v\", err) } PrintProducts(db) 通过 Save 方法，可以把 product 变量中所有跟数据库不一致的字段更新到数据库中。具体操作是：先获取某个资源的详细信息，再通过 product.Price = 200 这类赋值语句，对其中的一些字段重新赋值。最后，调用 Save 方法更新这些字段。你可以将这些操作看作一种更新数据库的更新模式。 第五个操作，删除表记录。通过 Delete 方法删除表记录，代码如下： // 5. Delete value match given conditions if err := db.Where(\"code = ?\", \"D42\").Delete(\u0026Product{}).Error; err != nil { log.Fatalf(\"Delete product error: %v\", err) } PrintProducts(db) 这里需要注意，因为 Product 中有 gorm.DeletedAt 字段，所以，上述删除操作不会真正把记录从数据库表中删除掉，而是通过设置数据库 product 表 deletedAt 字段为当前时间的方法来删除。第六个操作，获取表记录列表。 products := make([]*Prod","date":"2022-07-07 15:20:13","objectID":"/iam_service_develop/:7:2","tags":["iam"],"title":"iam_service_develop","uri":"/iam_service_develop/"},{"categories":["iam"],"content":"GORM 常用操作讲解 看完上面的示例，我想你已经初步掌握了 GORM 的使用方法。接下来，我再来给你详细介绍下 GORM 所支持的数据库操作。 模型定义 GORM 使用模型（Models）来映射一个数据库表。默认情况下，使用 ID 作为主键，使用结构体名的 snake_cases 作为表名，使用字段名的 snake_case 作为列名，并使用 CreatedAt、UpdatedAt、DeletedAt 字段追踪创建、更新和删除时间。 使用 GORM 的默认规则，可以减少代码量，但**我更喜欢的方式是直接指明字段名和表名。**例如，有以下模型： type Animal struct { AnimalID int64 // 列名 `animal_id` Birthday time.Time // 列名 `birthday` Age int64 // 列名 `age` } 上述模型对应的表名为 animals ，列名分别为 animal_id 、 birthday 和 age 。我们可以通过以下方式来重命名表名和列名，并将 AnimalID 设置为表的主键： type Animal struct { AnimalID int64 `gorm:\"column:animalID;primarykey\"` // 将列名设为 `animalID` Birthday time.Time `gorm:\"column:birthday\"` // 将列名设为 `birthday` Age int64 `gorm:\"column:age\"` // 将列名设为 `age` } func (a *Animal) TableName() string { return \"animal\" } 上面的代码中，通过 primaryKey 标签指定主键，使用 column 标签指定列名，通过给 Models 添加 TableName 方法指定表名。 数据库表通常会包含 4 个字段。ID：自增字段，也作为主键。CreatedAt：记录创建时间。UpdatedAt：记录更新时间。DeletedAt：记录删除时间（软删除时有用）。 GORM 也预定义了包含这 4 个字段的 Models，在我们定义自己的 Models 时，可以直接内嵌到结构体内，例如： type Animal struct { gorm.Model AnimalID int64 `gorm:\"column:animalID\"` // 将列名设为 `animalID` Birthday time.Time `gorm:\"column:birthday\"` // 将列名设为 `birthday` Age int64 `gorm:\"column:age\"` // 将列名设为 `age` } Models 中的字段能支持很多 GORM 标签，但如果我们不使用 GORM 自动创建表和迁移表结构的功能，很多标签我们实际上是用不到的。在开发中，用得最多的是 column 标签。 连接数据库 在进行数据库的 CURD 操作之前，我们首先需要连接数据库。你可以通过以下代码连接 MySQL 数据库： import ( \"gorm.io/driver/mysql\" \"gorm.io/gorm\" ) func main() { // 参考 https://github.com/go-sql-driver/mysql#dsn-data-source-name 获取详情 dsn := \"user:pass@tcp(127.0.0.1:3306)/dbname?charset=utf8mb4\u0026parseTime=True\u0026loc=Local\" db, err := gorm.Open(mysql.Open(dsn), \u0026gorm.Config{}) } 如果需要 GORM 正确地处理 time.Time 类型，在连接数据库时需要带上 parseTime 参数。如果要支持完整的 UTF-8 编码，可将charset=utf8更改为charset=utf8mb4。 GORM 支持连接池，底层是用 database/sql 包来维护连接池的，连接池设置如下： sqlDB, err := db.DB() sqlDB.SetMaxIdleConns(100) // 设置MySQL的最大空闲连接数（推荐100） sqlDB.SetMaxOpenConns(100) // 设置MySQL的最大连接数（推荐100） sqlDB.SetConnMaxLifetime(time.Hour) // 设置MySQL的空闲连接最大存活时间（推荐10s） 上面这些设置，也可以应用在大型后端项目中。 创建记录 我们可以通过 db.Create 方法来创建一条记录： type User struct { gorm.Model Name string Age uint8 Birthday *time.Time } user := User{Name: \"Jinzhu\", Age: 18, Birthday: time.Now()} result := db.Create(\u0026user) // 通过数据的指针来创建 db.Create 函数会返回如下 3 个值：user.ID：返回插入数据的主键，这个是直接赋值给 user 变量。result.Error：返回 error。result.RowsAffected：返回插入记录的条数。 当需要插入的数据量比较大时，可以批量插入，以提高插入性能： var users = []User{{Name: \"jinzhu1\"}, {Name: \"jinzhu2\"}, {Name: \"jinzhu3\"}} DB.Create(\u0026users) for _, user := range users { user.ID // 1,2,3 } 删除记录 我们可以通过 Delete 方法删除记录： // DELETE from users where id = 10 AND name = \"jinzhu\"; db.Where(\"name = ?\", \"jinzhu\").Delete(\u0026user) GORM 也支持根据主键进行删除，例如： // DELETE FROM users WHERE id = 10; db.Delete(\u0026User{}, 10) 不过，我更喜欢使用 db.Where 的方式进行删除，这种方式有两个优点。 第一个优点是删除方式更通用。使用 db.Where 不仅可以根据主键删除，还能够随意组合条件进行删除。第二个优点是删除方式更显式，这意味着更易读。如果使用db.Delete(\u0026User{}, 10)，你还需要确认 User 的主键，如果记错了主键，还可能会引入 Bug。 此外，GORM 也支持批量删除： db.Where(\"name in (?)\", []string{\"jinzhu\", \"colin\"}).Delete(\u0026User{}) GORM 支持两种删除方法：软删除和永久删除。下面我来分别介绍下。 软删除 软删除是指执行 Delete 时，记录不会被从数据库中真正删除。GORM 会将 DeletedAt 设置为当前时间，并且不能通过正常的方式查询到该记录。如果模型包含了一个 gorm.DeletedAt 字段，GORM 在执行删除操作时，会软删除该记录。 下面的删除方法就是一个软删除： // UPDATE users SET deleted_at=\"2013-10-29 10:23\" WHERE age = 20; db.Where(\"age = ?\", 20).Delete(\u0026User{}) // SELECT * FROM users WHERE age = 20 AND deleted_at IS NULL; db.Where(\"age = 20\").Find(\u0026user) 可以看到，GORM 并没有真正把记录从数据库删除掉，而是只更新了 deleted_at 字段。在查询时，GORM 查询条件中新增了AND deleted_at IS NULL条件，所以这些被设置过 deleted_at 字段的记录不会被查询到。对于一些比较重要的数据，我们可以通过软删除的方式删除记录，软删除可以使这些重要的数据后期能够被恢复，并且便于以后的排障。 我们可以通过下面的方式查找被软删除的记录： // SELECT * FROM users WHERE age = 20; db.Unscoped().Where(\"age = 20\").Find(\u0026users) 永久删除 如果想永久删除一条记录，可以使用 Unscoped： // DELETE FROM orders WHERE id=10; db.Unscoped().Delete(\u0026order) 或者，你也可以在模型中去掉 gorm.DeletedAt。 更新记录 GORM 中，最常用的更新方法如下： db.First(\u0026user) user.Name = \"jinzhu 2\" user.Age = 100 // UPDATE users SET name='jinzhu 2', age=100, birthday='2016-01-01', updated_at = '2013-11-17 21:34:10' WHERE id=111; db.Save(\u0026user) 上述方法会保留所有字段，所以执行 Save 时，需要先执行 First，获取某个记录的所有列的值，然后再对需要更新的字段设置值","date":"2022-07-07 15:20:13","objectID":"/iam_service_develop/:7:3","tags":["iam"],"title":"iam_service_develop","uri":"/iam_service_develop/"},{"categories":["iam"],"content":"iam-apiserver 中的 CURD 操作实战 接下来，我来介绍下 iam-apiserver 是如何使用 GORM，对数据进行 CURD 操作的。 首先，我们需要配置连接 MySQL 的各类参数。iam-apiserver 通过NewMySQLOptions函数创建了一个带有默认值的MySQLOptions类型的变量，将该变量传给NewApp函数。在 App 框架中，最终会调用 MySQLOptions 提供的 AddFlags 方法，将 MySQLOptions 提供的命令行参数添加到 Cobra 命令行中。 接着，在PrepareRun函数中，调用GetMySQLFactoryOr函数，初始化并获取仓库层的实例mysqlFactory。实现了仓库层store.Factory接口： type Factory interface { Users() UserStore Secrets() SecretStore Policies() PolicyStore Close() error } GetMySQLFactoryOr 函数采用了我们在 11 讲 中提过的单例模式，确保 iam-apiserver 进程中只有一个仓库层的实例，这样可以减少内存开支和系统的性能开销。 GetMySQLFactoryOr 函数中，使用github.com/marmotedu/iam/pkg/db包提供的 New 函数，创建了 MySQL 实例。New 函数代码如下： func New(opts *Options) (*gorm.DB, error) { dsn := fmt.Sprintf(`%s:%s@tcp(%s)/%s?charset=utf8\u0026parseTime=%t\u0026loc=%s`, opts.Username, opts.Password, opts.Host, opts.Database, true, \"Local\") db, err := gorm.Open(mysql.Open(dsn), \u0026gorm.Config{ Logger: logger.New(opts.LogLevel), }) if err != nil { return nil, err } sqlDB, err := db.DB() if err != nil { return nil, err } // SetMaxOpenConns sets the maximum number of open connections to the database. sqlDB.SetMaxOpenConns(opts.MaxOpenConnections) // SetConnMaxLifetime sets the maximum amount of time a connection may be reused. sqlDB.SetConnMaxLifetime(opts.MaxConnectionLifeTime) // SetMaxIdleConns sets the maximum number of connections in the idle connection pool. sqlDB.SetMaxIdleConns(opts.MaxIdleConnections) return db, nil } 上述代码中，我们先创建了一个 *gorm.DB 类型的实例，并对该实例进行了如下设置： 通过 SetMaxOpenConns 方法，设置了 MySQL 的最大连接数（推荐 100）。 通过 SetConnMaxLifetime 方法，设置了 MySQL 的空闲连接最大存活时间（推荐 10s）。 通过 SetMaxIdleConns 方法，设置了 MySQL 的最大空闲连接数（推荐 100）。 GetMySQLFactoryOr 函数最后创建了 datastore 类型的变量 mysqlFactory，该变量是仓库层的变量。mysqlFactory 变量中，又包含了 *gorm.DB 类型的字段 db 。 最终，我们通过仓库层的变量 mysqlFactory，调用其 db 字段提供的方法来完成数据库的 CURD 操作。例如，创建密钥、更新密钥、删除密钥、获取密钥详情、查询密钥列表，具体代码如下（代码位于secret.go文件中）： // Create creates a new secret. func (s *secrets) Create(ctx context.Context, secret *v1.Secret, opts metav1.CreateOptions) error { return s.db.Create(\u0026secret).Error } // Update updates an secret information by the secret identifier. func (s *secrets) Update(ctx context.Context, secret *v1.Secret, opts metav1.UpdateOptions) error { return s.db.Save(secret).Error } // Delete deletes the secret by the secret identifier. func (s *secrets) Delete(ctx context.Context, username, name string, opts metav1.DeleteOptions) error { if opts.Unscoped { s.db = s.db.Unscoped() } err := s.db.Where(\"username = ? and name = ?\", username, name).Delete(\u0026v1.Secret{}).Error if err != nil \u0026\u0026 !errors.Is(err, gorm.ErrRecordNotFound) { return errors.WithCode(code.ErrDatabase, err.Error()) } return nil } // Get return an secret by the secret identifier. func (s *secrets) Get(ctx context.Context, username, name string, opts metav1.GetOptions) (*v1.Secret, error) { secret := \u0026v1.Secret{} err := s.db.Where(\"username = ? and name= ?\", username, name).First(\u0026secret).Error if err != nil { if errors.Is(err, gorm.ErrRecordNotFound) { return nil, errors.WithCode(code.ErrSecretNotFound, err.Error()) } return nil, errors.WithCode(code.ErrDatabase, err.Error()) } return secret, nil } // List return all secrets. func (s *secrets) List(ctx context.Context, username string, opts metav1.ListOptions) (*v1.SecretList, error) { ret := \u0026v1.SecretList{} ol := gormutil.Unpointer(opts.Offset, opts.Limit) if username != \"\" { s.db = s.db.Where(\"username = ?\", username) } selector, _ := fields.ParseSelector(opts.FieldSelector) name, _ := selector.RequiresExactMatch(\"name\") d := s.db.Where(\" name like ?\", \"%\"+name+\"%\"). Offset(ol.Offset). Limit(ol.Limit). Order(\"id desc\"). Find(\u0026ret.Items). Offset(-1). Limit(-1). Count(\u0026ret.TotalCount) return ret, d.Error } 上面的代码中， s.db 就是 *gorm.DB 类型的字段。 上面的代码段执行了以下操作： 通过 s.db.Save 来更新数据库表的各字段； 通过 s.db.Unscoped 来永久性从表中删除一行记录。对于支持软删除的资源，我们还可以通过 opts.Unscoped 选项来控制是否永久删除记录。 true 永久删除， false 软删除，默认软删除。 通过 errors.Is(err, gorm.ErrRecordNotFound) 来判断 GORM 返回的错误是否是没有找到记录的错误类型。 通过下面两行代码，来获取查询条件 name 的值： selector, _ := fields.ParseSelector(opts.FieldS","date":"2022-07-07 15:20:13","objectID":"/iam_service_develop/:7:4","tags":["iam"],"title":"iam_service_develop","uri":"/iam_service_develop/"},{"categories":["iam"],"content":"总结 在 Go 项目中，我们需要使用 ORM 来进行数据库的 CURD 操作。在 Go 生态中，当前最受欢迎的 ORM 是 GORM，IAM 项目也使用了 GORM。GORM 有很多功能，常用的功能有模型定义、连接数据库、创建记录、删除记录、更新记录和查询数据。这些常用功能的常见使用方式如下： package main import ( \"fmt\" \"log\" \"github.com/spf13/pflag\" \"gorm.io/driver/mysql\" \"gorm.io/gorm\" ) type Product struct { gorm.Model Code string `gorm:\"column:code\"` Price uint `gorm:\"column:price\"` } // TableName maps to mysql table name. func (p *Product) TableName() string { return \"product\" } var ( host = pflag.StringP(\"host\", \"H\", \"127.0.0.1:3306\", \"MySQL service host address\") username = pflag.StringP(\"username\", \"u\", \"root\", \"Username for access to mysql service\") password = pflag.StringP(\"password\", \"p\", \"root\", \"Password for access to mysql, should be used pair with password\") database = pflag.StringP(\"database\", \"d\", \"test\", \"Database name to use\") help = pflag.BoolP(\"help\", \"h\", false, \"Print this help message\") ) func main() { // Parse command line flags pflag.CommandLine.SortFlags = false pflag.Usage = func() { pflag.PrintDefaults() } pflag.Parse() if *help { pflag.Usage() return } dsn := fmt.Sprintf(`%s:%s@tcp(%s)/%s?charset=utf8\u0026parseTime=%t\u0026loc=%s`, *username, *password, *host, *database, true, \"Local\") db, err := gorm.Open(mysql.Open(dsn), \u0026gorm.Config{}) if err != nil { panic(\"failed to connect database\") } // 1. Auto migration for given models db.AutoMigrate(\u0026Product{}) // 2. Insert the value into database if err := db.Create(\u0026Product{Code: \"D42\", Price: 100}).Error; err != nil { log.Fatalf(\"Create error: %v\", err) } PrintProducts(db) // 3. Find first record that match given conditions product := \u0026Product{} if err := db.Where(\"code= ?\", \"D42\").First(\u0026product).Error; err != nil { log.Fatalf(\"Get product error: %v\", err) } // 4. Update value in database, if the value doesn't have primary key, will insert it product.Price = 200 if err := db.Save(product).Error; err != nil { log.Fatalf(\"Update product error: %v\", err) } PrintProducts(db) // 5. Delete value match given conditions if err := db.Where(\"code = ?\", \"D42\").Delete(\u0026Product{}).Error; err != nil { log.Fatalf(\"Delete product error: %v\", err) } PrintProducts(db) } // List products func PrintProducts(db *gorm.DB) { products := make([]*Product, 0) var count int64 d := db.Where(\"code like ?\", \"%D%\").Offset(0).Limit(2).Order(\"id desc\").Find(\u0026products).Offset(-1).Limit(-1).Count(\u0026count) if d.Error != nil { log.Fatalf(\"List products error: %v\", d.Error) } log.Printf(\"totalcount: %d\", count) for _, product := range products { log.Printf(\"\\tcode: %s, price: %d\\n\", product.Code, product.Price) } } 此外，GORM 还支持原生查询 SQL 和原生执行 SQL，可以满足更加复杂的 SQL 场景。GORM 中，还有一个非常有用的功能是支持 Hooks。Hooks 可以在执行某个 CURD 操作前被调用。在 Hook 中，可以添加一些非常有用的功能，例如生成唯一 ID。目前，GORM 支持 BeforeXXX 、 AfterXXX 和 AfterFind Hook，其中 XXX 可以是 Save、Create、Delete、Update。最后，我还介绍了 IAM 项目的 GORM 实战，具体使用方式跟总结中的示例代码大体保持一致，你可以返回文稿查看。 ","date":"2022-07-07 15:20:13","objectID":"/iam_service_develop/:7:5","tags":["iam"],"title":"iam_service_develop","uri":"/iam_service_develop/"},{"categories":["iam"],"content":"31 | 数据流：通过iam-authz-server设计，看数据流服务的设计 在 28 讲 和 29 讲 ，我介绍了 IAM 的控制流服务 iam-apiserver 的设计和实现。这一讲，我们再来看下 IAM 数据流服务 iam-authz-server 的设计和实现。因为 iam-authz-server 是数据流服务，对性能要求较高，所以采用了一些机制来最大化 API 接口的性能。另外，为了提高开发效率，避免重复造轮子，iam-authz-server 和 iam-apiserver 共享了大部分的功能代码。接下来，我们就来看下，iam-authz-server 是如何跟 iam-apiserver 共享代码的，以及 iam-authz-server 是如何保证 API 接口性能的。 ","date":"2022-07-07 15:20:13","objectID":"/iam_service_develop/:8:0","tags":["iam"],"title":"iam_service_develop","uri":"/iam_service_develop/"},{"categories":["iam"],"content":"iam-authz-server 的功能介绍 iam-authz-server 目前的唯一功能，是通过提供 /v1/authz RESTful API 接口完成资源授权。 /v1/authz 接口是通过github.com/ory/ladon来完成资源授权的。 因为 iam-authz-server 承载了数据流的请求，需要确保 API 接口具有较高的性能。为了保证 API 接口的性能，iam-authz-server 在设计上使用了大量的缓存技术。 ","date":"2022-07-07 15:20:13","objectID":"/iam_service_develop/:8:1","tags":["iam"],"title":"iam_service_develop","uri":"/iam_service_develop/"},{"categories":["iam"],"content":"github.com/ory/ladon 包介绍 因为 iam-authz-server 资源授权是通过 github.com/ory/ladon 来完成的，为了让你更好地理解 iam-authz-server 的授权策略，在这里我先介绍下 github.com/ory/ladon 包。 Ladon 是用 Go 语言编写的用于实现访问控制策略的库，类似于 RBAC（基于角色的访问控制系统，Role Based Access Control）和 ACL（访问控制列表，Access Control Lists）。但是与 RBAC 和 ACL 相比，Ladon 可以实现更细粒度的访问控制，并且能够在更为复杂的环境中（例如多租户、分布式应用程序和大型组织）工作。 Ladon 解决了这个问题：在特定的条件下，谁能够 / 不能够对哪些资源做哪些操作。为了解决这个问题，Ladon 引入了授权策略。授权策略是一个有语法规范的文档，这个文档描述了谁在什么条件下能够对哪些资源做哪些操作。Ladon 可以用请求的上下文，去匹配设置的授权策略，最终判断出当前授权请求是否通过。下面是一个 Ladon 的授权策略样例： { \"description\": \"One policy to rule them all.\", \"subjects\": [\"users:\u003cpeter|ken\u003e\", \"users:maria\", \"groups:admins\"], \"actions\" : [\"delete\", \"\u003ccreate|update\u003e\"], \"effect\": \"allow\", \"resources\": [ \"resources:articles:\u003c.*\u003e\", \"resources:printer\" ], \"conditions\": { \"remoteIP\": { \"type\": \"CIDRCondition\", \"options\": { \"cidr\": \"192.168.0.1/16\" } } } } 策略（Policy）由若干元素构成，用来描述授权的具体信息，你可以把它们看成一组规则。核心元素包括主题（Subject）、操作（Action）、效力（Effect）、资源（Resource）以及生效条件（Condition）。元素保留字仅支持小写，它们在描述上没有顺序要求。对于没有特定约束条件的策略，Condition 元素是可选项。一条策略包含下面 6 个元素： 主题（Subject），主题名是唯一的，代表一个授权主题。例如，“ken” or “printer-service.mydomain.com”。 操作（Action），描述允许或拒绝的操作。 效力（Effect），描述策略产生的结果是“允许”还是“拒绝”，包括 allow（允许）和 deny（拒绝）。 资源（Resource），描述授权的具体数据。 生效条件（Condition），描述策略生效的约束条件。 描述（Description），策略的描述。 有了授权策略，我们就可以传入请求上下文，由 Ladon 来决定请求是否能通过授权。下面是一个请求示例： { \"subject\": \"users:peter\", \"action\" : \"delete\", \"resource\": \"resources:articles:ladon-introduction\", \"context\": { \"remoteIP\": \"192.168.0.5\" } } 可以看到，在 remoteIP=\"192.168.0.5” 生效条件（Condition）下，针对主题（Subject） users:peter 对资源（Resource） resources:articles:ladon-introduction 的 delete 操作（Action），授权策略的效力（Effect）是 allow 的。所以 Ladon 会返回如下结果： { \"allowed\": true } Ladon 支持很多 Condition，具体见下表： 至于如何使用这些 Condition，你可以参考 Ladon Condition 使用示例。此外，Ladon 还支持自定义 Condition。 另外，Ladon 还支持授权审计，用来记录授权历史。我们可以通过在 ladon.Ladon 中附加一个 ladon.AuditLogger 来实现： import \"github.com/ory/ladon\" import manager \"github.com/ory/ladon/manager/memory\" func main() { warden := ladon.Ladon{ Manager: manager.NewMemoryManager(), AuditLogger: \u0026ladon.AuditLoggerInfo{} } // ... } 在上面的示例中，我们提供了 ladon.AuditLoggerInfo，该 AuditLogger 会在授权时打印调用的策略到标准错误。AuditLogger 是一个 interface： // AuditLogger tracks denied and granted authorizations. type AuditLogger interface { LogRejectedAccessRequest(request *Request, pool Policies, deciders Policies) LogGrantedAccessRequest(request *Request, pool Policies, deciders Policies) } 要实现一个新的 AuditLogger，你只需要实现 AuditLogger 接口就可以了。比如，我们可以实现一个 AuditLogger，将授权日志保存到 Redis 或者 MySQL 中。 Ladon 支持跟踪一些授权指标，比如 deny、allow、not match、error。你可以通过实现 ladon.Metric 接口，来对这些指标进行处理。ladon.Metric 接口定义如下： // Metric is used to expose metrics about authz type Metric interface { // RequestDeniedBy is called when we get explicit deny by policy RequestDeniedBy(Request, Policy) // RequestAllowedBy is called when a matching policy has been found. RequestAllowedBy(Request, Policies) // RequestNoMatch is called when no policy has matched our request RequestNoMatch(Request) // RequestProcessingError is called when unexpected error occured RequestProcessingError(Request, Policy, error) } 例如，你可以通过下面的示例，将这些指标暴露给 prometheus： type prometheusMetrics struct{} func (mtr *prometheusMetrics) RequestDeniedBy(r ladon.Request, p ladon.Policy) {} func (mtr *prometheusMetrics) RequestAllowedBy(r ladon.Request, policies ladon.Policies) {} func (mtr *prometheusMetrics) RequestNoMatch(r ladon.Request) {} func (mtr *prometheusMetrics) RequestProcessingError(r ladon.Request, err error) {} func main() { warden := ladon.Ladon{ Manager: manager.NewMemoryManager(), Metric: \u0026prometheusMetrics{}, } // ... } 在使用 Ladon 的过程中，有两个地方需要你注意： 所有检查都区分大小写，因为主题值可能是区分大小写的 ID。 如果 ladon.Ladon 无法将策略与请求匹配，会默认授权结果为拒绝，并返回错误。 ","date":"2022-07-07 15:20:13","objectID":"/iam_service_develop/:8:2","tags":["iam"],"title":"iam_service_develop","uri":"/iam_service_develop/"},{"categories":["iam"],"content":"iam-authz-server 使用方法介绍 上面，我介绍了 iam-authz-server 的资源授权功能，这里介绍下如何使用 iam-authz-server，也就是如何调用 /v1/authz 接口完成资源授权。你可以通过下面的 3 大步骤，来完成资源授权请求。 第一步，登陆 iam-apiserver，创建授权策略和密钥。 这一步又分为 3 个小步骤。登陆 iam-apiserver 系统，获取访问令牌： $ token=`curl -s -XPOST -H'Content-Type: application/json' -d'{\"username\":\"admin\",\"password\":\"Admin@2021\"}' http://127.0.0.1:8080/login | jq -r .token` 创建授权策略： $ curl -s -XPOST -H\"Content-Type: application/json\" -H\"Authorization: Bearer $token\" -d'{\"metadata\":{\"name\":\"authztest\"},\"policy\":{\"description\":\"One policy to rule them all.\",\"subjects\":[\"users:\u003cpeter|ken\u003e\",\"users:maria\",\"groups:admins\"],\"actions\":[\"delete\",\"\u003ccreate|update\u003e\"],\"effect\":\"allow\",\"resources\":[\"resources:articles:\u003c.*\u003e\",\"resources:printer\"],\"conditions\":{\"remoteIP\":{\"type\":\"CIDRCondition\",\"options\":{\"cidr\":\"192.168.0.1/16\"}}}}}' http://127.0.0.1:8080/v1/policies 创建密钥，并从请求结果中提取 secretID 和 secretKey： $ curl -s -XPOST -H\"Content-Type: application/json\" -H\"Authorization: Bearer $token\" -d'{\"metadata\":{\"name\":\"authztest\"},\"expires\":0,\"description\":\"admin secret\"}' http://127.0.0.1:8080/v1/secrets {\"metadata\":{\"id\":23,\"name\":\"authztest\",\"createdAt\":\"2021-04-08T07:24:50.071671422+08:00\",\"updatedAt\":\"2021-04-08T07:24:50.071671422+08:00\"},\"username\":\"admin\",\"secretID\":\"ZuxvXNfG08BdEMqkTaP41L2DLArlE6Jpqoox\",\"secretKey\":\"7Sfa5EfAPIwcTLGCfSvqLf0zZGCjF3l8\",\"expires\":0,\"description\":\"admin secret\"} 第二步，生成访问 iam-authz-server 的 token。 iamctl 提供了 jwt sigin 子命令，可以根据 secretID 和 secretKey 签发 Token，方便使用。 $ iamctl jwt sign ZuxvXNfG08BdEMqkTaP41L2DLArlE6Jpqoox 7Sfa5EfAPIwcTLGCfSvqLf0zZGCjF3l8 # iamctl jwt sign $secretID $secretKey eyJhbGciOiJIUzI1NiIsImtpZCI6Ilp1eHZYTmZHMDhCZEVNcWtUYVA0MUwyRExBcmxFNkpwcW9veCIsInR5cCI6IkpXVCJ9.eyJhdWQiOiJpYW0uYXV0aHoubWFybW90ZWR1LmNvbSIsImV4cCI6MTYxNzg0NTE5NSwiaWF0IjoxNjE3ODM3OTk1LCJpc3MiOiJpYW1jdGwiLCJuYmYiOjE2MTc4Mzc5OTV9.za9yLM7lHVabPAlVQLCqXEaf8sTU6sodAsMXnmpXjMQ 你可以通过 iamctl jwt show 来查看 Token 的内容： $ iamctl jwt show eyJhbGciOiJIUzI1NiIsImtpZCI6Ilp1eHZYTmZHMDhCZEVNcWtUYVA0MUwyRExBcmxFNkpwcW9veCIsInR5cCI6IkpXVCJ9.eyJhdWQiOiJpYW0uYXV0aHoubWFybW90ZWR1LmNvbSIsImV4cCI6MTYxNzg0NTE5NSwiaWF0IjoxNjE3ODM3OTk1LCJpc3MiOiJpYW1jdGwiLCJuYmYiOjE2MTc4Mzc5OTV9.za9yLM7lHVabPAlVQLCqXEaf8sTU6sodAsMXnmpXjMQ Header: { \"alg\": \"HS256\", \"kid\": \"ZuxvXNfG08BdEMqkTaP41L2DLArlE6Jpqoox\", \"typ\": \"JWT\" } Claims: { \"aud\": \"iam.authz.marmotedu.com\", \"exp\": 1617845195, \"iat\": 1617837995, \"iss\": \"iamctl\", \"nbf\": 1617837995 } 我们生成的 Token 包含了下面这些信息。 Header alg：生成签名的算法。kid：密钥 ID。typ：Token 的类型，这里是 JWT。 Claims aud：JWT Token 的接受者。exp：JWT Token 的过期时间（UNIX 时间格式）。iat：JWT Token 的签发时间（UNIX 时间格式）。iss：签发者，因为我们是用 iamctl 工具签发的，所以这里的签发者是 iamctl。nbf：JWT Token 的生效时间（UNIX 时间格式），默认是签发时间。 第三步，调用/v1/authz接口，完成资源授权请求。请求方法如下： $ curl -s -XPOST -H'Content-Type: application/json' -H'Authorization: Bearer eyJhbGciOiJIUzI1NiIsImtpZCI6Ilp1eHZYTmZHMDhCZEVNcWtUYVA0MUwyRExBcmxFNkpwcW9veCIsInR5cCI6IkpXVCJ9.eyJhdWQiOiJpYW0uYXV0aHoubWFybW90ZWR1LmNvbSIsImV4cCI6MTYxNzg0NTE5NSwiaWF0IjoxNjE3ODM3OTk1LCJpc3MiOiJpYW1jdGwiLCJuYmYiOjE2MTc4Mzc5OTV9.za9yLM7lHVabPAlVQLCqXEaf8sTU6sodAsMXnmpXjMQ' -d'{\"subject\":\"users:maria\",\"action\":\"delete\",\"resource\":\"resources:articles:ladon-introduction\",\"context\":{\"remoteIP\":\"192.168.0.5\"}}' http://127.0.0.1:9090/v1/authz {\"allowed\":true} 如果授权通过，会返回：{“allowed”:true} 。 如果授权失败，则返回： {\"allowed\":false,\"denied\":true,\"reason\":\"Request was denied by default\"} ","date":"2022-07-07 15:20:13","objectID":"/iam_service_develop/:8:3","tags":["iam"],"title":"iam_service_develop","uri":"/iam_service_develop/"},{"categories":["iam"],"content":"iam-authz-server 的代码实现 接下来，我们来看下 iam-authz-server 的具体实现，我会从配置处理、启动流程、请求处理流程和代码架构 4 个方面来讲解。 iam-authz-server 的配置处理 iam-authz-server 服务的 main 函数位于authzserver.go文件中，你可以跟读代码，了解 iam-authz-server 的代码实现。iam-authz-server 的服务框架设计跟 iam-apiserver 的服务框架设计保持一致，也是有 3 种配置：Options 配置、组件配置和 HTTP 服务配置。 Options 配置见options.go文件： type Options struct { RPCServer string ClientCA string GenericServerRunOptions *genericoptions.ServerRunOptions InsecureServing *genericoptions.InsecureServingOptions SecureServing *genericoptions.SecureServingOptions RedisOptions *genericoptions.RedisOptions FeatureOptions *genericoptions.FeatureOptions Log *log.Options AnalyticsOptions *analytics.AnalyticsOptions } 和 iam-apiserver 相比，iam-authz-server 多了 AnalyticsOptions，用来配置 iam-authz-server 内的 Analytics 服务，Analytics 服务会将授权日志异步写入到 Redis 中。 iam-apiserver 和 iam-authz-server 共用了 GenericServerRunOptions、InsecureServing、SecureServing、FeatureOptions、RedisOptions、Log 这些配置。所以，我们只需要用简单的几行代码，就可以将很多配置项都引入到 iam-authz-server 的命令行参数中，这也是命令行参数分组带来的好处：批量共享。 iam-authz-server 启动流程设计 接下来，我们来详细看下 iam-authz-server 的启动流程。 iam-authz-server 的启动流程也和 iam-apiserver 基本保持一致。二者比较大的不同在于 Options 参数配置和应用初始化内容。另外，和 iam-apiserver 相比，iam-authz-server 只提供了 REST API 服务。启动流程如下图所示： iam-authz-server 的 RESTful API 请求处理流程 iam-authz-server 的请求处理流程也是清晰、规范的，具体流程如下图所示： 首先，我们通过 API 调用（ + ）请求 iam-authz-server 提供的 RESTful API 接口 POST /v1/authz 。接着，Gin Web 框架接收到 HTTP 请求之后，会通过认证中间件完成请求的认证，iam-authz-server 采用了 Bearer 认证方式。 然后，请求会被我们加载的一系列中间件所处理，例如跨域、RequestID、Dump 等中间件。最后，根据 + 进行路由匹配。 比如，我们请求的 RESTful API 是POST /v1/authz，Gin Web 框架会根据 HTTP Method 和 HTTP Request Path，查找注册的 Controllers，最终匹配到 authzController.Authorize Controller。在 Authorize Controller 中，会先解析请求参数，接着校验请求参数、调用业务层的方法进行资源授权，最后处理业务层的返回结果，返回最终的 HTTP 请求结果。 iam-authz-server 的代码架构 iam-authz-server 的代码设计和 iam-apiserver 一样，遵循简洁架构设计。 iam-authz-server 的代码架构也分为 4 层，分别是模型层（Models）、控制层（Controller）、业务层 （Service）和仓库层（Repository）。从控制层、业务层到仓库层，从左到右层级依次加深。模型层独立于其他层，可供其他层引用。如下图所示： iam-authz-server 和 iam-apiserver 的代码架构有这三点不同：iam-authz-server 客户端不支持前端和命令行。iam-authz-server 仓库层对接的是 iam-apiserver 微服务，而非数据库。iam-authz-server 业务层的代码存放在目录authorization中。 ","date":"2022-07-07 15:20:13","objectID":"/iam_service_develop/:8:4","tags":["iam"],"title":"iam_service_develop","uri":"/iam_service_develop/"},{"categories":["iam"],"content":"iam-authz-server 关键代码分析 和 iam-apiserver 一样，iam-authz-server 也包含了一些优秀的设计思路和关键代码，这里我来一一介绍下。 资源授权 先来看下，iam-authz-server 是如何实现资源授权的。 我们可以调用 iam-authz-server 的 /v1/authz API 接口，实现资源的访问授权。 /v1/authz 对应的 controller 方法是Authorize： func (a *AuthzController) Authorize(c *gin.Context) { var r ladon.Request if err := c.ShouldBind(\u0026r); err != nil { core.WriteResponse(c, errors.WithCode(code.ErrBind, err.Error()), nil) return } auth := authorization.NewAuthorizer(authorizer.NewAuthorization(a.store)) if r.Context == nil { r.Context = ladon.Context{} } r.Context[\"username\"] = c.GetString(\"username\") rsp := auth.Authorize(\u0026r) core.WriteResponse(c, nil, rsp) } 该函数使用 github.com/ory/ladon 包进行资源访问授权，授权流程如下图所示： 具体分为以下几个步骤：第一步，在 Authorize 方法中调用 c.ShouldBind(\u0026r) ，将 API 请求参数解析到 ladon.Request 类型的结构体变量中。第二步，调用authorization.NewAuthorizer函数，该函数会创建并返回包含 Manager 和 AuditLogger 字段的Authorizer类型的变量。 Manager 包含一些函数，比如 Create、Update 和 FindRequestCandidates 等，用来对授权策略进行增删改查。AuditLogger 包含 LogRejectedAccessRequest 和 LogGrantedAccessRequest 函数，分别用来记录被拒绝的授权请求和被允许的授权请求，将其作为审计数据使用。 第三步，调用auth.Authorize函数，对请求进行访问授权。auth.Authorize 函数内容如下： func (a *Authorizer) Authorize(request *ladon.Request) *authzv1.Response { log.Debug(\"authorize request\", log.Any(\"request\", request)) if err := a.warden.IsAllowed(request); err != nil { return \u0026authzv1.Response{ Denied: true, Reason: err.Error(), } } return \u0026authzv1.Response{ Allowed: true, } } 该函数会调用 a.warden.IsAllowed(request) 完成资源访问授权。IsAllowed 函数会调用 FindRequestCandidates(r) 查询所有的策略列表，这里要注意，我们只需要查询请求用户的 policy 列表。在 Authorize 函数中，我们将 username 存入 ladon Request 的 context 中： r.Context[\"username\"] = c.GetHeader(\"username\") 在FindRequestCandidates函数中，我们可以从 Request 中取出 username，并根据 username 查询缓存中的 policy 列表，FindRequestCandidates 实现如下： func (m *PolicyManager) FindRequestCandidates(r *ladon.Request) (ladon.Policies, error) { username := \"\" if user, ok := r.Context[\"username\"].(string); ok { username = user } policies, err := m.client.List(username) if err != nil { return nil, errors.Wrap(err, \"list policies failed\") } ret := make([]ladon.Policy, 0, len(policies)) for _, policy := range policies { ret = append(ret, policy) } return ret, nil } IsAllowed 函数代码如下： func (l *Ladon) IsAllowed(r *Request) (err error) { policies, err := l.Manager.FindRequestCandidates(r) if err != nil { go l.metric().RequestProcessingError(*r, nil, err) return err } return l.DoPoliciesAllow(r, policies) } IsAllowed 会调用 DoPoliciesAllow(r, policies) 函数进行权限校验。如果权限校验不通过（请求在指定条件下不能够对资源做指定操作），就调用 LogRejectedAccessRequest 函数记录拒绝的请求，并返回值为非 nil 的 error，error 中记录了授权失败的错误信息。如果权限校验通过，则调用 LogGrantedAccessRequest 函数记录允许的请求，并返回值为 nil 的 error。 为了降低请求延时，LogRejectedAccessRequest 和 LogGrantedAccessRequest 会将授权记录存储在 Redis 中，之后由 iam-pump 进程读取 Redis，并将授权记录持久化存储在 MongoDB 中。 缓存设计 iam-authz-server 主要用来做资源访问授权，属于数据流的组件，对接口访问性能有比较高的要求，所以该组件采用了缓存的机制。如下图所示： iam-authz-server 组件通过缓存密钥和授权策略信息到内存中，加快密钥和授权策略的查询速度。通过缓存授权记录到内存中，提高了授权数据的写入速度，从而大大降低了授权请求接口的延时。 上面的缓存机制用到了 Redis key-value 存储，所以在 iam-authz-server 初始化阶段，需要先建立 Redis 连接（位于initialize函数中）： go storage.ConnectToRedis(ctx, s.buildStorageConfig()) 这个代码会维护一个 Redis 连接，如果 Redis 连接断掉，会尝试重连。这种方式可以使我们在调用 Redis 接口进行数据读写时，不用考虑连接断开的问题。 接下来，我们就来详细看看，iam-authz-server 是如何实现缓存机制的。 **先来看下密钥和策略缓存。**iam-authz-server 通过load包来完成密钥和策略的缓存。 在 iam-authz-server 进程启动时，会创建并启动一个 Load 服务（位于initialize函数中）： load.NewLoader(ctx, cacheIns).Start() 先来看创建 Load 服务。创建 Load 服务时，传入了 cacheIns 参数，cacheIns 是一个实现了Loader接口的实例： type Loader interface { Reload() error } 然后看启动 Load 服务。通过 Load 实例的 Start 方法来启动 Load 服务： func (l *Load) Start() { go startPubSubLoop() go l.reloadQueueLoop() go l.reloadLoop() l.DoReload() } Start 函数先启动了 3 个协程，再调用 l.DoReload() 完成一次密钥和策略的同步： func (l *Load) DoReload() { l.lock.Lock() defer l.lock.Unlock() if err := l.loader.Reload(); err != nil { log.Errorf(\"faild to refresh target storage: %s\", err.Error()) } log.Debug(\"refresh target storage succ\") } 上面我们说了，创建 Load 服务时，传入的 cacheIns 实例是一个实现了 Loader 接口的实例，所以在DoReload方法中，可以直接调用 Reload 方法。cacheIns 的 Reload 方法会从 iam-apiserver 中","date":"2022-07-07 15:20:13","objectID":"/iam_service_develop/:8:5","tags":["iam"],"title":"iam_service_develop","uri":"/iam_service_develop/"},{"categories":["iam"],"content":"总结 这一讲中，我介绍了 IAM 数据流服务 iam-authz-server 的设计和实现。iam-authz-server 提供了 /v1/authz RESTful API 接口，供第三方用户完成资源授权功能，具体是使用 Ladon 包来完成资源授权的。Ladon 包解决了“在特定的条件下，谁能够 / 不能够对哪些资源做哪些操作”的问题。 iam-authz-server 的配置处理、启动流程和请求处理流程跟 iam-apiserver 保持一致。此外，iam-authz-server 也实现了简洁架构。iam-authz-server 通过缓存密钥和策略信息、缓存授权日志来提高 /v1/authz 接口的性能。 在缓存密钥和策略信息时，为了和 iam-apiserver 中的密钥和策略信息保持一致，使用了 Redis Pub/Sub 机制。当 iam-apiserver 有密钥 / 策略变更时，会往指定的 Redis channel Pub 一条消息。iam-authz-server 订阅相同的 channel，在收到新消息时，会解析消息，并重新从 iam-apiserver 中获取密钥和策略信息，缓存在内存中。 iam-authz-server 执行完资源授权之后，会将授权日志存放在一个带缓冲的 channel 中。后端有多个 worker 消费 channel 中的数据，并进行批量投递。可以设置批量投递的条件，例如最大投递日志数和最大投递时间间隔。 ","date":"2022-07-07 15:20:13","objectID":"/iam_service_develop/:8:6","tags":["iam"],"title":"iam_service_develop","uri":"/iam_service_develop/"},{"categories":["iam"],"content":"32 | 数据处理：如何高效处理应用程序产生的数据？ 我们来聊聊，如何更好地进行异步数据处理。一个大型应用为了后期的排障、运营等，会将一些请求数据保存在存储系统中，供日后使用。例如：应用将请求日志保存到 Elasticsearch 中，方便排障；网关将 API 请求次数、请求消息体等数据保存在数据库中，供控制台查询展示。为了满足这些需求，我们需要进行数据采集，数据采集在大型应用中很常见，但我发现不少开发者设计的数据采集服务，通常会存在下面这些问题： 采集服务只针对某个采集需求开发，如果采集需求有变，需要修改主代码逻辑，代码改动势必会带来潜在的 Bug，增加开发测试工作量。 数据采集服务会导致已有的服务请求延时变高。 采集数据性能差，需要较长时间才能采集完一批数据。 启停服务时，会导致采集的数据丢失。 这一讲，我就来详细教你如何设计和落地一个数据采集服务，解决上面这些问题。 ","date":"2022-07-07 15:20:13","objectID":"/iam_service_develop/:9:0","tags":["iam"],"title":"iam_service_develop","uri":"/iam_service_develop/"},{"categories":["iam"],"content":"数据采集方式的分类 首先，你需要知道当前数据采集有哪些方式，以便更好地理解异步数据处理方案。 目前，数据采集主要有两种方式，分别是同步采集和异步采集。二者的概念和优缺点如下表所示： 现代应用对性能的要求越来越高，而异步采集对应用程序的性能影响更小，因此异步采集更受开发者欢迎，得到了大规模的应用。接下来，我要介绍的 IAM Pump Server 服务，采用的就是异步采集的方式。 ","date":"2022-07-07 15:20:13","objectID":"/iam_service_develop/:9:1","tags":["iam"],"title":"iam_service_develop","uri":"/iam_service_develop/"},{"categories":["iam"],"content":"数据采集系统设计 这一讲，我采用理论 + 实战的方式来展示如何设计一个数据采集服务，这里先来介绍下关于数据采集的理论知识，后面会有具体的实战案例。 在过往的项目开发中，我发现很多开发人员添加了数据采集功能后，因为同步上报数据、单线程、上报逻辑不对等原因，让整个应用程序的性能受到了严重影响。那么，如何在采集过程中不影响程序的性能？答案就是让数据采集模型化。通过模型化，可以使设计出来的采集系统功能更加通用，能够满足未来的很多同类需求，我们也就不需要重复开发相同的系统了。我今天就来给你详细介绍下，如何将数据采集功能模型化，以及该模型是如何解决上面说的的各种问题的。 设计数据采集系统时需要解决的核心问题 采集系统首先需要一个数据源 Input，Input 可以是一个或者多个，Input 中的数据来自于应用程序上报。采集后的数据通常需要经过处理，比如格式化、增删字段、过滤无用的数据等，然后将处理后的数据存储到下游系统（Output）中，如下图所示： 这里，我们需要解决这 3 个核心问题： 进行数据采集，就需要在正常流程中多加一个上报数据环节，这势必会影响程序的性能。那么，如何让程序的性能损失最小化？ 如果 Input 产生数据的速度大于 Output 的消费能力，产生数据堆积怎么办？ 数据采集后需要存储到下游系统。在存储之前，我们需要对数据进行不同的处理，并可能会存储到不同的下游系统，这种可变的需求如何满足？ 对于让程序性能损失最小化这一点，最好的方法是异步上报。如果是异步，我们需要先把数据缓存在内存中，然后再异步上报到目标系统中。当然，为了提高上报的效率，可以采用批量上报的方式。对于数据堆积这个问题，比较好的解决方法是，将采集的数据先上报到一些具有高吞吐量、可以存储大量数据的中间组件，比如 Kafka、Redis 中。这种方式也是业界标准的处理方式。对于采集需求多样化这个问题，我们可以将采集程序做成插件化、可扩展的，满足可变的需求。要解决这 3 个问题，其实就涉及到了数据采集系统中的两个功能点的设计，它们分别是数据上报功能和数据采集功能。接下来我们就来看下，如何设计这两个功能点。 数据上报功能设计 为了提高异步上报的吞吐量，你可以将数据缓存在内存中（Go 中可以使用有缓冲 channel），并使用多个 worker 去消费内存中的数据。使用多个 worker ，可以充分发挥 CPU 的多核能力。另外，上报给下游系统时，你也可以采用批量上报的方式。 数据采集功能设计 现代应用程序越来越讲究插件化、扩展性，在设计采集系统时，也应该考虑到未来的需求。比如，未来你可能需要将数据从上报到 MongoDB 切换到 HBase 中，或者同时将数据上报到 MongoDB 和 HBase 中。因此，上报给下游的程序逻辑要具有插件化的能力，并能通过配置选择需要的插件。 为了提高程序性能，会先把数据缓存在内存中。但是这样有个缺点：在关停程序时，内存中的数据就会丢失。所以，在程序结束之前，我们需要确保内存中的数据能够上报成功，也就是说采集程序需要实现优雅关停功能。优雅关停不仅要确保缓存中的数据被成功上报，还要确保正在处理的数据被成功上报。当然了，既然是数据采集，还要能够配置采集的频率。最后，因为采集程序通常是非 API 类型的，所以还需要对外暴露一个特殊的 API，用来返回采集程序的健康状态。 数据采集应用模型 通过上面的分析和设计，可以绘制出下面这个采集模型： 异步上报需要额外的异步逻辑，会增加开发工作量和程序复杂度，所以，对于一些 Input 数据生产速度小于 Output 消费速度，并且 Output 具有高吞吐量、低延时特性的场景，也可以采用同步上报，例如同步上报给 Redis。 ","date":"2022-07-07 15:20:13","objectID":"/iam_service_develop/:9:2","tags":["iam"],"title":"iam_service_develop","uri":"/iam_service_develop/"},{"categories":["iam"],"content":"数据采集系统落地项目：iam-authz-server + iam-pump 上面，我介绍了数据采集系统的架构，但是只有模型和理论，肯定还不足以解决你对数据采集程序的开发需求。所以，接下来我来介绍下如何落地上面的数据采集架构。整个架构包括两个部分，分别由不同的服务实现： iam-authz-server：实现数据上报功能。iam-pump：实现数据采集功能。 整个采集系统的架构，跟上面描述的数据采集架构完全一致，这里就不重复说明了。 iam-authz-server：数据上报 数据上报的最大难点，就是如何减少上报逻辑对应用性能的影响。对此，我们主要的解决思路就是异步上报数据。 接下来我会介绍 iam-authz-server 的数据上报设计。这是一个非常成熟的设计，在我所开发和了解的项目中被大量采用，有些项目可以承载十亿级 / 天的请求量。通过介绍这个设计，我们来看看异步上报的具体方法，以及上报过程中要考虑的因素。iam-authz-server 的数据上报架构如下图所示： iam-authz-server 服务中的数据上报功能可以选择性开启，开启代码见 internal/authzserver/server.go ，代码如下： if s.analyticsOptions.Enable { analyticsStore := storage.RedisCluster{KeyPrefix: RedisKeyPrefix} analyticsIns := analytics.NewAnalytics(s.analyticsOptions, \u0026analyticsStore) analyticsIns.Start() s.gs.AddShutdownCallback(shutdown.ShutdownFunc(func(string) error { analyticsIns.Stop() return nil })) } 上面的代码中，当 s.analyticsOptions.Enable 为 true 时，开启数据上报功能。因为数据上报会影响程序的性能，而且在未来可能会存在禁掉数据上报功能的场景，所以在设计 iam-authz-server 时，就把数据上报功能做成了可配置的，也就是说可以通过配置文件来启用 / 禁用数据上报功能。配置方式也很简单：将 iam-authz-server.yaml 的 analytics.enable 设置为 true，代表开启数据上报功能；设置为 false ，则代表关闭数据上报功能。 这里，我建议你在设计程序时，将未来的可能变量考虑进去，并将这些变量做成可配置的。这样，如果哪天需求变化，我们就能通过修改配置文件，而不是修改代码的方式来满足需求。这种方式可以将应用程序的变动局限在配置文件中，从而大大减小现网服务出现故障的概率，做到只变更配置文件就可以缩短发布变更的周期。 在上面的代码中，通过 NewAnalytics 创建一个数据上报服务，代码如下： func NewAnalytics(options *AnalyticsOptions, store storage.AnalyticsHandler) *Analytics { ps := options.PoolSize recordsBufferSize := options.RecordsBufferSize workerBufferSize := recordsBufferSize / uint64(ps) log.Debug(\"Analytics pool worker buffer size\", log.Uint64(\"workerBufferSize\", workerBufferSize)) recordsChan := make(chan *AnalyticsRecord, recordsBufferSize) return \u0026Analytics{ store: store, poolSize: ps, recordsChan: recordsChan, workerBufferSize: workerBufferSize, recordsBufferFlushInterval: options.FlushInterval, } } 这里的代码根据传入的参数，创建 Analytics 类型的变量并返回，变量中有 5 个字段需要你关注： store： storage.AnalyticsHandler 接口类型，提供了 Connect() bool和 AppendToSetPipelined(string, byte)函数，分别用来连接 storage 和上报数据给 storage。iam-authz-server 用了 redis storage。 recordsChan：授权日志会缓存在 recordsChan 带缓冲 channel 中，其长度可以通过 iam-authz-server.yaml 配置文件中的 analytics.records-buffer-size 配置。 poolSize：指定开启 worker 的个数，也就是开启多少个 Go 协程来消费 recordsChan 中的消息。 workerBufferSize：批量投递给下游系统的的消息数。通过批量投递，可以进一步提高消费能力、减少 CPU 消耗。 recordsBufferFlushInterval：设置最迟多久投递一次，也就是投递数据的超时时间。 analytics.ecords-buffer-size 和 analytics.pool-size 建议根据部署机器的 CPU 和内存来配置。在应用真正上线前，我建议你通过压力和负载测试，来配置一个合适的值。 Analytics 提供了 3 种方法： Start()，用来启动数据上报服务。 Stop()，用来关停数据上报服务。主程序在收到系统的终止命令后，调用 Stop 方法优雅关停数据上报服务，确保缓存中的数据都上报成功。 RecordHit(record *AnalyticsRecord) error，用来记录 AnalyticsRecord 的数据。 通过 NewXxx （NewAnalytics）返回一个 Xxx （Analytics）类型的结构体，Xxx（Analytics） 类型带有一些方法，如下： func NewAnalytics(options) *Analytics { ... } func (r *Analytics) Start() { ... } func (r *Analytics) Stop() { ... } func (r *Analytics) RecordHit(record *AnalyticsRecord) error { ... } 其实，上述代码段是一种常见的 Go 代码编写方式 / 设计模式。你在以后的开发生涯中，会经常遇到这种设计方式。使用上述代码设计方式有下面两个好处。 功能模块化：将数据上报的功能封装成一个服务模块，数据和方法都围绕着 Xxx 结构体来展开。这和 C++、Java、Python 的类有相似的地方，你可以这么理解：Xxx 相当于类，NewXxx 相当于初始化一个类实例，Start、Stop、RecordHit 是这个类提供的方法。功能模块化可以使程序逻辑更加清晰，功能更独立、更好维护，也可以供其他应用使用。 方便数据传递：可以将数据存放在 Xxx 结构体字段中，供不同的方法共享使用，如果有并发，数据共享时，注意要给非并发安全的类型加锁，例如 recordsChan。 接下来，我会介绍 iam-authz-server 服务中跟数据上报相关的 3 部分核心代码，分别是启动数据上报服务、异步上报授权日志和优雅关停数据上报。 启动服务：启动数据上报服务 在服务启动时，首先要启动数据上报功能模块。我们通过调用 analyticsIns.Start() 启动数据上报服务。Start 代码如下： func (r *Analytics) Start() { analytics = r r.store.Connect() // start worker pool atomic.SwapUint32(\u0026r.shouldStop, 0) for i := 0; i \u003c r.poolSize; i++ { r.poolWg.Add(1) go r.recordWorker() } // stop analytics workers go r.Stop() } 这里有一点需要你注意，数据上报和数据采集都大量应用了 Go 协程来并发地执行操作，为了防止潜在的并发读写引起的 Bug，建议你的测试程序编译时加上 -race，例如 go build -race cmd/iam-authz-server/authzserver.go。然后，在测试过程中，观察程序日志，看有无并发问题出现。 Start 中会开启 poolSize 个数的 worker 协程，这些协程共同消费 recordsChan 中的消息，消费逻辑见 recordWorker() ，代码如下： func (r *Analytics) recordWorker() { defer r.poolWg.Done() // this is buffer to send one pipelined command to redis // use r.recordsBufferSize as cap to reduce slice re-alloc","date":"2022-07-07 15:20:13","objectID":"/iam_service_develop/:9:3","tags":["iam"],"title":"iam_service_develop","uri":"/iam_service_develop/"},{"categories":["iam"],"content":"iam-pump：数据采集 iam-authz-server 将数据上报到 Redis，iam-pump 消费 Redis 中的数据，并保存在 MongoDB 中做持久化存储。 iam-pump 的设计要点是：插件化、可配置地将 Redis 中的数据处理后存储到下游系统中，并且实现优雅关停功能，这些也是设计数据采集程序的要点和难点所在。下面，我们就来看下 iam-pump 是如何插件化地实现一个数据采集程序的。这个数据采集程序的设计思路，在我开发的大型企业应用中有实际的落地验证，你可以放心使用。 iam-pump 数据采集架构如下图所示： 在 iam-pump 服务启动时，要启动数据采集功能，启动代码见 internal/pump/server.go。接下来，我会介绍下 iam-pump 服务中的 5 部分核心代码： 数据采集插件定义。初始化数据采集插件。健康检查。启动 Loop 周期性消费 Redis 数据。优雅关停数据采集服务。 初始化服务：数据采集插件定义 数据采集组件设计的核心是插件化，这里我将需要上报的系统抽象成一个个的 pump，那么如何定义 pump 接口呢？接口定义需要参考实际的采集需求，通常来说，至少需要下面这几个函数。 New：创建一个 pump。 Init：初始化一个 pump，例如，可以在 Init 中创建下游系统的网络连接。 WriteData：往下游系统写入数据。为了提高性能，最好支持批量写入。 SetFilters：设置是否过滤某条数据，这也是一个非常常见的需求，因为不是所有的数据都是需要的。 SetTimeout：设置超时时间。我就在开发过程中遇到过一个坑，连接 Kafka 超时，导致整个采集程序超时。所以这里需要有超时处理，通过超时处理，可以保证整个采集框架正常运行。 我之前开发过公有云的网关服务，网关服务需要把网关的请求数据转存到 MongoDB 中。我们的网关服务曾经遇到一个比较大的坑：有些用户会通过网关上传非常大的文件（百 M 级别），这些数据转存到 MongoDB 中，快速消耗了 MongoDB 的存储空间（500G 存储空间）。为了避免这个问题，在转存数据时，需要过滤掉一些比较详细的数据，所以 iam-pump 添加了 SetOmitDetailedRecording 来过滤掉详细的数据。 所以，最后 iam-pump 的插件接口定义为 internal/pump/pumps/pump.go ： type Pump interface { GetName() string New() Pump Init(interface{}) error WriteData(context.Context, []interface{}) error SetFilters(analytics.AnalyticsFilters) GetFilters() analytics.AnalyticsFilters SetTimeout(timeout int) GetTimeout() int SetOmitDetailedRecording(bool) GetOmitDetailedRecording() bool } 你在实际开发中，如果有更多的需求，可以在 Pump interface 定义中继续添加需要的处理函数。 初始化服务：初始化数据采集插件 定义好插件之后，需要初始化插件。在 initialize 函数中初始化 pumps： func (s *pumpServer) initialize() { pmps = make([]pumps.Pump, len(s.pumps)) i := 0 for key, pmp := range s.pumps { pumpTypeName := pmp.Type if pumpTypeName == \"\" { pumpTypeName = key } pmpType, err := pumps.GetPumpByName(pumpTypeName) if err != nil { log.Errorf(\"Pump load error (skipping): %s\", err.Error()) } else { pmpIns := pmpType.New() initErr := pmpIns.Init(pmp.Meta) if initErr != nil { log.Errorf(\"Pump init error (skipping): %s\", initErr.Error()) } else { log.Infof(\"Init Pump: %s\", pmpIns.GetName()) pmpIns.SetFilters(pmp.Filters) pmpIns.SetTimeout(pmp.Timeout) pmpIns.SetOmitDetailedRecording(pmp.OmitDetailedRecording) pmps[i] = pmpIns } } i++ } } initialize 会创建、初始化，并调用 SetFilters、SetTimeout、SetOmitDetailedRecording 来设置这些 pump。Filters、Timeout、OmitDetailedRecording 等信息在 pump 的配置文件中指定。 这里有个技巧你也可以注意下：pump 配置文件支持通用的配置，也支持自定义的配置，配置结构为 PumpConfig ： type PumpConfig struct { Type string Filters analytics.AnalyticsFilters Timeout int OmitDetailedRecording bool Meta map[string]interface{} } pump 自定义的配置可以存放在 map 类型的变量 Meta 中。通用配置可以使配置共享，减少开发和维护工作量，自定义配置可以适配不同 pump 的差异化配置。 初始化服务：健康检查 因为 iam-pump 是一个非 API 服务，为了监控其运行状态，这里也设置了一个健康检查接口。iam-pump 组件通过调用 server.ServeHealthCheck 函数启动一个 HTTP 服务，ServeHealthCheck 函数代码如下： func ServeHealthCheck(healthPath string, healthAddress string) { http.HandleFunc(\"/\"+healthPath, func(w http.ResponseWriter, r *http.Request) { w.Header().Set(\"Content-type\", \"application/json\") w.WriteHeader(http.StatusOK) _, _ = w.Write([]byte(`{\"status\": \"ok\"}`)) }) if err := http.ListenAndServe(healthAddress, nil); err != nil { log.Fatalf(\"Error serving health check endpoint: %s\", err.Error()) } } 该函数启动了一个 HTTP 服务，服务监听地址通过 health-check-address 配置，健康检查路径通过 health-check-path 配置。如果请求 http:///返回{“status”: “ok”}，说明 iam-pump 可以正常工作。 这里的健康检查只是简单返回了一个字符串，实际开发中，可以封装更复杂的逻辑。比如，检查进程是否可以成功 ping 通数据库，进程内的工作进程是否处于 worker 状态等。 iam-pump 默认的健康检查请求地址为http://127.0.0.1:7070/healthz 。 运行服务：启动 Loop 周期性消费 Redis 数据 初始化 pumps 之后，就可以通过 Run 函数启动消费逻辑了。在 Run 函数中，会定期（通过配置 purge-delay 设置轮训时间）从 Redis 中获取所有数据，经过 msgpack.Unmarshal 解压后，传给 writeToPumps 处理： func (s preparedPumpServer) Run(stopCh \u003c-chan struct{}) error { ticker := time.NewTicker(time.Duration(s.secInterval) * time.Second) defer ticker.Stop() for { select { case \u003c-ticker.C: analyticsValues := s.analyticsStore.GetAndDeleteSet(storage.AnalyticsKeyName) if len(analyticsValues) \u003e 0 { // Convert to something clean keys := make([]interface{}, len(analyticsValues)) for i, v := range analyticsValues { decoded := analytics.AnalyticsRecord{} err := msgpack.Unmarshal([]byte(v.(st","date":"2022-07-07 15:20:13","objectID":"/iam_service_develop/:9:4","tags":["iam"],"title":"iam_service_develop","uri":"/iam_service_develop/"},{"categories":["iam"],"content":"总结 这一讲，我主要介绍了如何将数据采集需求转化成一个数据采集模型，并从这个模型出发，设计出一个可扩展、高性能的数据采集服务，并通过 iam-pump 组件来落地该采集模型。最后，我还想给你一个建议：在开发中，你也可以将一些功能抽象成一些通用的模型，并为该模型实现基本框架（引擎），然后将一些需要定制化的部分插件化。通过这种方式，可以设计出一个高扩展的服务，使得服务不仅能够满足现在的需求，还能够满足未来的需求。 ","date":"2022-07-07 15:20:13","objectID":"/iam_service_develop/:9:5","tags":["iam"],"title":"iam_service_develop","uri":"/iam_service_develop/"},{"categories":["iam"],"content":"33 | SDK 设计（上）：如何设计出一个优秀的 Go SDK？ 后端服务通过 API 接口对外提供应用的功能，但是用户直接调用 API 接口，需要编写 API 接口调用的逻辑，并且需要构造入参和解析返回的数据包，使用起来效率低，而且有一定的开发工作量。 在实际的项目开发中，通常会提供对开发者更友好的 SDK 包，供客户端调用。很多大型服务在发布时都会伴随着 SDK 的发布，例如腾讯云很多产品都提供了 SDK： 既然 SDK 如此重要，那么如何设计一个优秀的 Go SDK 呢？这一讲我就来详细介绍一下。 ","date":"2022-07-07 15:20:13","objectID":"/iam_service_develop/:10:0","tags":["iam"],"title":"iam_service_develop","uri":"/iam_service_develop/"},{"categories":["iam"],"content":"什么是 SDK？首先，我们来看下什么是 SDK。 对于 SDK（Software Development Kit，软件开发工具包），不同场景下有不同的解释。但是对于一个 Go 后端服务来说，SDK 通常是指封装了 Go 后端服务 API 接口的软件包，里面通常包含了跟软件相关的库、文档、使用示例、封装好的 API 接口和工具。 调用 SDK 跟调用本地函数没有太大的区别，所以可以极大地提升开发者的开发效率和体验。SDK 可以由服务提供者提供，也可以由其他组织或个人提供。为了鼓励开发者使用其系统或语言，SDK 通常都是免费提供的。 通常，服务提供者会提供不同语言的 SDK，比如针对 Python 开发者会提供 Python 版的 SDK，针对 Go 开发者会提供 Go 版的 SDK。一些比较专业的团队还会有 SDK 自动生成工具，可以根据 API 接口定义，自动生成不同语言的 SDK。例如，Protocol Buffers 的编译工具 protoc，就可以基于 Protobuf 文件生成 C++、Python、Java、JavaScript、PHP 等语言版本的 SDK。阿里云、腾讯云这些一线大厂，也可以基于 API 定义，生成不同编程语言的 SDK。 ","date":"2022-07-07 15:20:13","objectID":"/iam_service_develop/:10:1","tags":["iam"],"title":"iam_service_develop","uri":"/iam_service_develop/"},{"categories":["iam"],"content":"SDK 设计方法 那么，我们如何才能设计一个好的 SDK 呢？对于 SDK，不同团队会有不同的设计方式，我调研了一些优秀 SDK 的实现，发现这些 SDK 有一些共同点。根据我的调研结果，结合我在实际开发中的经验，我总结出了一套 SDK 设计方法，接下来就分享给你。 ","date":"2022-07-07 15:20:13","objectID":"/iam_service_develop/:10:2","tags":["iam"],"title":"iam_service_develop","uri":"/iam_service_develop/"},{"categories":["iam"],"content":"如何给 SDK 命名？ 在讲设计方法之前，我先来介绍两个重要的知识点：SDK 的命名方式和 SDK 的目录结构。SDK 的名字目前没有统一的规范，但比较常见的命名方式是 xxx-sdk-go / xxx-sdk-python / xxx-sdk-java 。其中， xxx 可以是项目名或者组织名，例如腾讯云在 GitHub 上的组织名为 tencentcloud，那它的 SDK 命名如下图所示： ","date":"2022-07-07 15:20:13","objectID":"/iam_service_develop/:10:3","tags":["iam"],"title":"iam_service_develop","uri":"/iam_service_develop/"},{"categories":["iam"],"content":"SDK 的目录结构 不同项目 SDK 的目录结构也不相同，但一般需要包含下面这些文件或目录。目录名可能会有所不同，但目录功能是类似的。 README.md：SDK 的帮助文档，里面包含了安装、配置和使用 SDK 的方法。 examples/sample/：SDK 的使用示例。 sdk/：SDK 共享的包，里面封装了最基础的通信功能。如果是 HTTP 服务，基本都是基于 net/http 包进行封装。 api：如果 xxx-sdk-go 只是为某一个服务提供 SDK，就可以把该服务的所有 API 接口封装代码存放在 api 目录下。 services/{iam, tms} ：如果 xxx-sdk-go 中， xxx 是一个组织，那么这个 SDK 很可能会集成该组织中很多服务的 API，就可以把某类服务的 API 接口封装代码存放在 services/\u003c服务名\u003e下，例如 AWS 的Go SDK。 一个典型的目录结构如下： ├── examples # 示例代码存放目录 │ └── authz.go ├── README.md # SDK使用文档 ├── sdk # 公共包，封装了SDK配置、API请求、认证等代码 │ ├── client.go │ ├── config.go │ ├── credential.go │ └── ... └── services # API封装 ├── common │ └── model ├── iam # iam服务的API接口 │ ├── authz.go │ ├── client.go │ └── ... └── tms # tms服务的API接口 ","date":"2022-07-07 15:20:13","objectID":"/iam_service_develop/:10:4","tags":["iam"],"title":"iam_service_develop","uri":"/iam_service_develop/"},{"categories":["iam"],"content":"SDK 设计方法 SDK 的设计方法如下图所示： 我们可以通过 Config 配置创建客户端 Client，例如 func NewClient(config sdk.Config) (Client, error)，配置中可以指定下面的信息。 服务的后端地址：服务的后端地址可以通过配置文件来配置，也可以直接固化在 SDK 中，推荐后端服务地址可通过配置文件配置。 认证信息：最常用的认证方式是通过密钥认证，也有一些是通过用户名和密码认证。 其他配置：例如超时时间、重试次数、缓存时间等。 创建的 Client 是一个结构体或者 Go interface。这里我建议你使用 interface 类型，这样可以将定义和具体实现解耦。Client 具有一些方法，例如 CreateUser、DeleteUser 等，每一个方法对应一个 API 接口，下面是一个 Client 定义： type Client struct { client *sdk.Request } func (c *Client) CreateUser(req *CreateUserRequest) (*CreateUserResponse, error) { // normal code resp := \u0026CreateUserResponse{} err := c.client.Send(req, resp) return resp, err } 调用 client.CreateUser(req) 会执行 HTTP 请求，在 req 中可以指定 HTTP 请求的方法 Method、路径 Path 和请求 Body。 CreateUser 函数中，会调用 c.client.Send(req) 执行具体的 HTTP 请求。 c.client 是 *Request 类型的变量， *Request 类型的变量具有一些方法，可以根据传入的请求参数 req 和 config 配置构造出请求路径、认证头和请求 Body，并调用 net/http 包完成最终的 HTTP 请求，最后将返回结果 Unmarshal 到传入的 resp 结构体中。 根据我的调研，目前有两种 SDK 设计方式可供参考，一种是各大公有云厂商采用的 SDK 设计方式，一种是 Kubernetes client-go 的设计方式。IAM 项目分别实现了这两种 SDK 设计方式，但我还是更倾向于对外提供 client-go 方式的 SDK，我会在下一讲详细介绍它。这两种设计方式的设计思路跟上面介绍的是一致的。 ","date":"2022-07-07 15:20:13","objectID":"/iam_service_develop/:10:5","tags":["iam"],"title":"iam_service_develop","uri":"/iam_service_develop/"},{"categories":["iam"],"content":"公有云厂商采用的 SDK 设计方式 这里，我先来简单介绍下公有云厂商采用的 SDK 设计模式。SDK 架构如下图所示： SDK 框架分为两层，分别是 API 层和基础层。API 层主要用来构建客户端实例，并调用客户端实例提供的方法来完成 API 请求，每一个方法对应一个 API 接口。API 层最终会调用基础层提供的能力，来完成 REST API 请求。基础层通过依次执行构建请求参数（Builder）、签发并添加认证头（Signer）、执行 HTTP 请求（Request）三大步骤，来完成具体的 REST API 请求。 为了让你更好地理解公有云 SDK 的设计方式，接下来我会结合一些真实的代码，给你讲解 API 层和基础层的具体设计，SDK 代码见medu-sdk-go。 ","date":"2022-07-07 15:20:13","objectID":"/iam_service_develop/:10:6","tags":["iam"],"title":"iam_service_develop","uri":"/iam_service_develop/"},{"categories":["iam"],"content":"API 层：创建客户端实例 客户端在使用服务 A 的 SDK 时，首先需要根据 Config 配置创建一个服务 A 的客户端 Client，Client 实际上是一个 struct，定义如下： type Client struct { sdk.Client } 在创建客户端时，需要传入认证（例如密钥、用户名 / 密码）、后端服务地址等配置信息。例如，可以通过NewClientWithSecret方法来构建一个带密钥对的客户端： func NewClientWithSecret(secretID, secretKey string) (client *Client, err error) { client = \u0026Client{} config := sdk.NewConfig().WithEndpoint(defaultEndpoint) client.Init(serviceName).WithSecret(secretID, secretKey).WithConfig(config) return } 这里要注意，上面创建客户端时，传入的密钥对最终会在基础层中被使用，用来签发 JWT Token。 Client 有多个方法（Sender），例如 Authz 等，每个方法代表一个 API 接口。Sender 方法会接收 AuthzRequest 等结构体类型的指针作为输入参数。我们可以调用 client.Authz(req) 来执行 REST API 调用。可以在 client.Authz 方法中添加一些业务逻辑处理。client.Authz 代码如下： type AuthzRequest struct { *request.BaseRequest Resource *string `json:\"resource\"` Action *string `json:\"action\"` Subject *string `json:\"subject\"` Context *ladon.Context } func (c *Client) Authz(req *AuthzRequest) (resp *AuthzResponse, err error) { if req == nil { req = NewAuthzRequest() } resp = NewAuthzResponse() err = c.Send(req, resp) return } 请求结构体中的字段都是指针类型的，使用指针的好处是可以判断入参是否有被指定，如果req.Subject == nil 就说明传参中没有 Subject 参数，如果req.Subject != nil就说明参数中有传 Subject 参数。根据某个参数是否被传入，执行不同的业务逻辑，这在 Go API 接口开发中非常常见。 另外，因为 Client 通过匿名的方式继承了基础层中的Client： type Client struct { sdk.Client } 所以，API 层创建的 Client 最终可以直接调用基础层中的 Client 提供的Send(req, resp) 方法，来执行 RESTful API 调用，并将结果保存在 resp 中。为了方便和 API 层的 Client 进行区分，我下面统一将基础层中的 Client 称为 sdk.Client。最后，一个完整的客户端调用示例代码如下： package main import ( \"fmt\" \"github.com/ory/ladon\" \"github.com/marmotedu/medu-sdk-go/sdk\" iam \"github.com/marmotedu/medu-sdk-go/services/iam/authz\" ) func main() { client, _ := iam.NewClientWithSecret(\"XhbY3aCrfjdYcP1OFJRu9xcno8JzSbUIvGE2\", \"bfJRvlFwsoW9L30DlG87BBW0arJamSeK\") req := iam.NewAuthzRequest() req.Resource = sdk.String(\"resources:articles:ladon-introduction\") req.Action = sdk.String(\"delete\") req.Subject = sdk.String(\"users:peter\") ctx := ladon.Context(map[string]interface{}{\"remoteIPAddress\": \"192.168.0.5\"}) req.Context = \u0026ctx resp, err := client.Authz(req) if err != nil { fmt.Println(\"err1\", err) return } fmt.Printf(\"get response body: `%s`\\n\", resp.String()) fmt.Printf(\"allowed: %v\\n\", resp.Allowed) } ","date":"2022-07-07 15:20:13","objectID":"/iam_service_develop/:10:7","tags":["iam"],"title":"iam_service_develop","uri":"/iam_service_develop/"},{"categories":["iam"],"content":"基础层：构建并执行 HTTP 请求 上面我们创建了客户端实例，并调用了它的 Send 方法来完成最终的 HTTP 请求。这里，我们来看下 Send 方法具体是如何构建 HTTP 请求的。 sdk.Client 通过 Send 方法，完成最终的 API 调用，代码如下： func (c *Client) Send(req request.Request, resp response.Response) error { method := req.GetMethod() builder := GetParameterBuilder(method, c.Logger) jsonReq, _ := json.Marshal(req) encodedUrl, err := builder.BuildURL(req.GetURL(), jsonReq) if err != nil { return err } endPoint := c.Config.Endpoint if endPoint == \"\" { endPoint = fmt.Sprintf(\"%s/%s\", defaultEndpoint, c.ServiceName) } reqUrl := fmt.Sprintf(\"%s://%s/%s%s\", c.Config.Scheme, endPoint, req.GetVersion(), encodedUrl) body, err := builder.BuildBody(jsonReq) if err != nil { return err } sign := func(r *http.Request) error { signer := NewSigner(c.signMethod, c.Credential, c.Logger) _ = signer.Sign(c.ServiceName, r, strings.NewReader(body)) return err } rawResponse, err := c.doSend(method, reqUrl, body, req.GetHeaders(), sign) if err != nil { return err } return response.ParseFromHttpResponse(rawResponse, resp) } 上面的代码大体上可以分为四个步骤。 第一步，Builder：构建请求参数。 根据传入的 AuthzRequest 和客户端配置 Config，构造 HTTP 请求参数，包括请求路径和请求 Body。接下来，我们来看下如何构造 HTTP 请求参数。 HTTP 请求路径构建 在创建客户端时，我们通过NewAuthzRequest函数创建了 /v1/authz REST API 接口请求结构体 AuthzRequest，代码如下： func NewAuthzRequest() (req *AuthzRequest) { req = \u0026AuthzRequest{ BaseRequest: \u0026request.BaseRequest{ URL: \"/authz\", Method: \"POST\", Header: nil, Version: \"v1\", }, } return } 可以看到，我们创建的 req 中包含了 API 版本（Version）、API 路径（URL）和请求方法（Method）。这样，我们就可以在 Send 方法中，构建出请求路径： endPoint := c.Config.Endpoint if endPoint == \"\" { endPoint = fmt.Sprintf(\"%s/%s\", defaultEndpoint, c.ServiceName) } reqUrl := fmt.Sprintf(\"%s://%s/%s%s\", c.Config.Scheme, endPoint, req.GetVersion(), encodedUrl) 上述代码中，c.Config.Scheme=http/https、endPoint=iam.api.marmotedu.com:8080、req.GetVersion()=v1 和 encodedUrl，我们可以认为它们等于 /authz。所以，最终构建出的请求路径为http://iam.api.marmotedu.com:8080/v1/authz 。 HTTP 请求 Body 构建 在BuildBody方法中构建请求 Body。BuildBody 会将 req Marshal 成 JSON 格式的 string。HTTP 请求会以该字符串作为 Body 参数。 第二步，Signer：签发并添加认证头。 访问 IAM 的 API 接口需要进行认证，所以在发送 HTTP 请求之前，还需要给 HTTP 请求添加认证 Header。 medu-sdk-go 代码提供了 JWT 和 HMAC 两种认证方式，最终采用了 JWT 认证方式。JWT 认证签发方法为Sign，代码如下： func (v1 SignatureV1) Sign(serviceName string, r *http.Request, body io.ReadSeeker) http.Header { tokenString := auth.Sign(v1.Credentials.SecretID, v1.Credentials.SecretKey, \"medu-sdk-go\", serviceName+\".marmotedu.com\") r.Header.Set(\"Authorization\", fmt.Sprintf(\"Bearer %s\", tokenString)) return r.Header } auth.Sign 方法根据 SecretID 和 SecretKey 签发 JWT Token。接下来，我们就可以调用doSend方法来执行 HTTP 请求了。调用代码如下： rawResponse, err := c.doSend(method, reqUrl, body, req.GetHeaders(), sign) if err != nil { return err } 可以看到，我们传入了 HTTP 请求方法 method 、HTTP 请求 URL reqUrl 、HTTP 请求 Body body，以及用来签发 JWT Token 的 sign 方法。我们在调用 NewAuthzRequest 创建 req 时，指定了 HTTP Method，所以这里的 method := req.GetMethod() 、reqUrl 和请求 Body 都是通过 Builder 来构建的。 第三步，Request：执行 HTTP请求。 调用doSend方法执行 HTTP 请求，doSend 通过调用 net/http 包提供的 http.NewRequest 方法来发送 HTTP 请求，执行完 HTTP 请求后，会返回 *http.Response 类型的 Response。代码如下： func (c *Client) doSend(method, url, data string, header map[string]string, sign SignFunc) (*http.Response, error) { client := \u0026http.Client{Timeout: c.Config.Timeout} req, err := http.NewRequest(method, url, strings.NewReader(data)) if err != nil { c.Logger.Errorf(\"%s\", err.Error()) return nil, err } c.setHeader(req, header) err = sign(req) if err != nil { return nil, err } return client.Do(req) } 第四步，处理 HTTP请求返回结果。 调用 doSend 方法返回 *http.Response 类型的 Response 后，Send 方法会调用ParseFromHttpResponse函数来处理 HTTP Response，ParseFromHttpResponse 函数代码如下： func ParseFromHttpResponse(rawResponse *http.Response, response Response) error { defer rawResponse.Body.Close() body, err := ioutil.ReadAll(rawResponse.Body) if err != nil { return err } if rawResponse.StatusCode != 200 { return fmt.Errorf(\"request fail with status: %s, with body: %s\", rawResponse.Status, body) } if err := response.ParseErrorFromHTTPResponse(body); err != nil { return err } return json.Unmarshal(body, \u0026response) } 可以看到，在 Pars","date":"2022-07-07 15:20:13","objectID":"/iam_service_develop/:10:8","tags":["iam"],"title":"iam_service_develop","uri":"/iam_service_develop/"},{"categories":["iam"],"content":"总结 这一讲，我主要介绍了如何设计一个优秀的 Go SDK。通过提供 SDK，可以提高 API 调用效率，减少 API 调用难度，所以大型应用通常都会提供 SDK。不同团队有不同的 SDK 设计方法，但目前比较好的实现是公有云厂商采用的 SDK 设计方式。公有云厂商的 SDK 设计方式中，SDK 按调用顺序从上到下可以分为 3 个模块，如下图所示： Client 构造 SDK 客户端，在构造客户端时，会创建请求参数 req ， req 中会指定 API 版本、HTTP 请求方法、API 请求路径等信息。Client 会请求 Builder 和 Signer 来构建 HTTP 请求的各项参数：HTTP 请求方法、HTTP 请求路径、HTTP 认证头、HTTP 请求 Body。Builder 和 Signer 是根据 req 配置来构造这些 HTTP 请求参数的。构造完成之后，会请求 Request 模块，Request 模块通过调用 net/http 包，来执行 HTTP 请求，并返回请求结果。 ","date":"2022-07-07 15:20:13","objectID":"/iam_service_develop/:10:9","tags":["iam"],"title":"iam_service_develop","uri":"/iam_service_develop/"},{"categories":["iam"],"content":"34 | SDK 设计（下）：IAM项目Go SDK设计和实现 上一讲，我介绍了公有云厂商普遍采用的 SDK 设计方式。其实，还有一些比较优秀的 SDK 设计方式，比如 Kubernetes 的 client-go SDK 设计方式。IAM 项目参考 client-go，也实现了 client-go 风格的 SDK：marmotedu-sdk-go。 和 33 讲 介绍的 SDK 设计方式相比，client-go 风格的 SDK 具有以下优点：大量使用了 Go interface 特性，将接口的定义和实现解耦，可以支持多种实现方式。接口调用层级跟资源的层级相匹配，调用方式更加友好。多版本共存。 所以，我更推荐你使用 marmotedu-sdk-go。接下来，我们就来看下 marmotedu-sdk-go 是如何设计和实现的。 ","date":"2022-07-07 15:20:13","objectID":"/iam_service_develop/:11:0","tags":["iam"],"title":"iam_service_develop","uri":"/iam_service_develop/"},{"categories":["iam"],"content":"marmotedu-sdk-go 设计 和 medu-sdk-go 相比，marmotedu-sdk-go 的设计和实现要复杂一些，但功能更强大，使用体验也更好。 这里，我们先来看一个使用 SDK 调用 iam-authz-server /v1/authz 接口的示例，代码保存在 marmotedu-sdk-go/examples/authz_clientset/main.go文件中： package main import ( \"context\" \"flag\" \"fmt\" \"path/filepath\" \"github.com/ory/ladon\" metav1 \"github.com/marmotedu/component-base/pkg/meta/v1\" \"github.com/marmotedu/component-base/pkg/util/homedir\" \"github.com/marmotedu/marmotedu-sdk-go/marmotedu\" \"github.com/marmotedu/marmotedu-sdk-go/tools/clientcmd\" ) func main() { var iamconfig *string if home := homedir.HomeDir(); home != \"\" { iamconfig = flag.String( \"iamconfig\", filepath.Join(home, \".iam\", \"config\"), \"(optional) absolute path to the iamconfig file\", ) } else { iamconfig = flag.String(\"iamconfig\", \"\", \"absolute path to the iamconfig file\") } flag.Parse() // use the current context in iamconfig config, err := clientcmd.BuildConfigFromFlags(\"\", *iamconfig) if err != nil { panic(err.Error()) } // create the clientset clientset, err := marmotedu.NewForConfig(config) if err != nil { panic(err.Error()) } request := \u0026ladon.Request{ Resource: \"resources:articles:ladon-introduction\", Action: \"delete\", Subject: \"users:peter\", Context: ladon.Context{ \"remoteIP\": \"192.168.0.5\", }, } // Authorize the request fmt.Println(\"Authorize request...\") ret, err := clientset.Iam().AuthzV1().Authz().Authorize(context.TODO(), request, metav1.AuthorizeOptions{}) if err != nil { panic(err.Error()) } fmt.Printf(\"Authorize response: %s.\\n\", ret.ToString()) } 在上面的代码示例中，包含了下面的操作。首先，调用 BuildConfigFromFlags 函数，创建出 SDK 的配置实例 config；接着，调用 marmotedu.NewForConfig(config) 创建了 IAM 项目的客户端 clientset ;最后，调用以下代码请求 /v1/authz 接口执行资源授权请求： ret, err := clientset.Iam().AuthzV1().Authz().Authorize(context.TODO(), request, metav1.AuthorizeOptions{}) if err != nil { panic(err.Error()) } fmt.Printf(\"Authorize response: %s.\\n\", ret.ToString()) 调用格式为项目客户端.应用客户端.服务客户端.资源名.接口 。所以，上面的代码通过创建项目级别的客户端、应用级别的客户端和服务级别的客户端，来调用资源的 API 接口。接下来，我们来看下如何创建这些客户端。 ","date":"2022-07-07 15:20:13","objectID":"/iam_service_develop/:11:1","tags":["iam"],"title":"iam_service_develop","uri":"/iam_service_develop/"},{"categories":["iam"],"content":"marmotedu-sdk-go 客户端设计 在讲客户端创建之前，我们先来看下客户端的设计思路。 Go 项目的组织方式是有层级的：Project -\u003e Application -\u003e Service。marmotedu-sdk-go 很好地体现了这种层级关系，使得 SDK 的调用更加易懂、易用。marmotedu-sdk-go 的层级关系如下图所示： marmotedu-sdk-go 定义了 3 类接口，分别代表了项目、应用和服务级别的 API 接口： // 项目级别的接口 type Interface interface { Iam() iam.IamInterface Tms() tms.TmsInterface } // 应用级别的接口 type IamInterface interface { APIV1() apiv1.APIV1Interface AuthzV1() authzv1.AuthzV1Interface } // 服务级别的接口 type APIV1Interface interface { RESTClient() rest.Interface SecretsGetter UsersGetter PoliciesGetter } // 资源级别的客户端 type SecretsGetter interface { Secrets() SecretInterface } // 资源的接口定义 type SecretInterface interface { Create(ctx context.Context, secret *v1.Secret, opts metav1.CreateOptions) (*v1.Secret, error) Update(ctx context.Context, secret *v1.Secret, opts metav1.UpdateOptions) (*v1.Secret, error) Delete(ctx context.Context, name string, opts metav1.DeleteOptions) error DeleteCollection(ctx context.Context, opts metav1.DeleteOptions, listOpts metav1.ListOptions) error Get(ctx context.Context, name string, opts metav1.GetOptions) (*v1.Secret, error) List(ctx context.Context, opts metav1.ListOptions) (*v1.SecretList, error) SecretExpansion } Interface 代表了项目级别的接口，里面包含了 Iam 和 Tms 两个应用； IamInterface 代表了应用级别的接口，里面包含了 api（iam-apiserver）和 authz（iam-authz-server）两个服务级别的接口。api 和 authz 服务中，又包含了各自服务中 REST 资源的 CURD 接口。 marmotedu-sdk-go 通过 XxxV1 这种命名方式来支持不同版本的 API 接口，好处是可以在程序中同时调用同一个 API 接口的不同版本，例如：clientset.Iam().AuthzV1().Authz().Authorize() 、clientset.Iam().AuthzV2().Authz().Authorize() 分别调用了 /v1/authz 和 /v2/authz 两个版本的 API 接口。 上述关系也可以从目录结构中反映出来，marmotedu-sdk-go 目录设计如下（只列出了一些重要的文件）： ├── examples # 存放SDK的使用示例 ├── Makefile # 管理SDK源码，静态代码检查、代码格式化、测试、添加版权信息等 ├── marmotedu │ ├── clientset.go # clientset实现，clientset中包含多个应用，多个服务的API接口 │ ├── fake # clientset的fake实现，主要用于单元测试 │ └── service # 按应用进行分类，存放应用中各服务API接口的具体实现 │ ├── iam # iam应用的API接口实现，包含多个服务 │ │ ├── apiserver # iam应用中，apiserver服务的API接口，包含多个版本 │ │ │ └── v1 # apiserver v1版本API接口 │ │ ├── authz # iam应用中，authz服务的API接口 │ │ │ └── v1 # authz服务v1版本接口 │ │ └── iam_client.go # iam应用的客户端，包含了apiserver和authz 2个服务的客户端 │ └── tms # tms应用的API接口实现 ├── pkg # 存放一些共享包，可对外暴露 ├── rest # HTTP请求的底层实现 ├── third_party # 存放修改过的第三方包，例如：gorequest └── tools └── clientcmd # 一些函数用来帮助创建rest.Config配置 每种类型的客户端，都可以通过以下相似的方式来创建： config, err := clientcmd.BuildConfigFromFlags(\"\", \"/root/.iam/config\") clientset, err := xxxx.NewForConfig(config) /root/.iam/config 为配置文件，里面包含了服务的地址和认证信息。BuildConfigFromFlags 函数加载配置文件，创建并返回 rest.Config 类型的配置变量，并通过 xxxx.NewForConfig 函数创建需要的客户端。xxxx 是所在层级的 client 包，例如 iam、tms。 marmotedu-sdk-go 客户端定义了 3 类接口，这可以带来两个好处。 第一，API 接口调用格式规范，层次清晰，可以使 API 接口调用更加清晰易记。第二，可以根据需要，自行选择客户端类型，调用灵活。举个例子，在 A 服务中需要同时用到 iam-apiserver 和 iam-authz-server 提供的接口，就可以创建应用级别的客户端 IamClient，然后通过 iamclient.APIV1() 和 iamclient.AuthzV1() ，来切换调用不同服务的 API 接口。 接下来，我们来看看如何创建三个不同级别的客户端。 项目级别客户端创建 Interface 对应的客户端实现为Clientset，所在的包为 marmotedu-sdk-go/marmotedu，Clientset 客户端的创建方式为： config, err := clientcmd.BuildConfigFromFlags(\"\", \"/root/.iam/config\") clientset, err := marmotedu.NewForConfig(config) 调用方式为 clientset.应用.服务.资源名.接口 ，例如： rsp, err := clientset.Iam().AuthzV1().Authz().Authorize() 参考示例为 marmotedu-sdk-go/examples/authz_clientset/main.go。 应用级别客户端创建 IamInterface 对应的客户端实现为IamClient，所在的包为 marmotedu-sdk-go/marmotedu/service/iam，IamClient 客户端的创建方式为： config, err := clientcmd.BuildConfigFromFlags(\"\", \"/root/.iam/config\") iamclient,, err := iam.NewForConfig(config) 调用方式为 iamclient.服务.资源名.接口 ，例如： rsp, err := iamclient.AuthzV1().Authz().Authorize() 参考示例为 marmotedu-sdk-go/examples/authz_iam/main.go。 服务级别客户端创建 AuthzV1Interface 对应的客户端实现为AuthzV1Client，所在的包为 marmotedu-sdk-go/marmotedu/service/iam/authz/v1，AuthzV1Client 客户端的创建方式为： config, err := clientcmd.BuildConfigFromFlags(\"\", \"/root/.iam/config\") client, err := v1.NewForConfig(config) 调用方式为 client.资源名.接口 ，例如： rsp, err := client.Authz().Authorize() 参考示例为 marmotedu-sdk-go/examples/authz/main.go。上面我介绍了 marmotedu-sdk-go 的客户端创建方法，接下来我们再来看下，这些客户端具体是如何执行 REST API 请求的。 ","date":"2022-07-07 15:20:13","objectID":"/iam_service_develop/:11:2","tags":["iam"],"title":"iam_service_develop","uri":"/iam_service_develop/"},{"categories":["iam"],"content":"marmotedu-sdk-go 的实现 marmotedu-sdk-go 的实现和 medu-sdk-go 一样，也是采用分层结构，分为 API 层和基础层。如下图所示： RESTClient是整个 SDK 的核心，RESTClient 向下通过调用Request模块，来完成 HTTP 请求方法、请求路径、请求体、认证信息的构建。Request 模块最终通过调用gorequest包提供的方法，完成 HTTP 的 POST、PUT、GET、DELETE 等请求，获取 HTTP 返回结果，并解析到指定的结构体中。RESTClient 向上提供 Post() 、 Put() 、 Get() 、 Delete() 等方法来供客户端完成 HTTP 请求。 marmotedu-sdk-go 提供了两类客户端，分别是 RESTClient 客户端和基于 RESTClient 封装的客户端。 RESTClient：Raw 类型的客户端，可以通过指定 HTTP 的请求方法、请求路径、请求参数等信息，直接发送 HTTP 请求，例如 client.Get().AbsPath(\"/version”).Do().Into() 。 基于 RESTClient 封装的客户端：例如 AuthzV1Client、APIV1Client 等，执行特定 REST 资源、特定 API 接口的请求，方便开发者调用。 接下来，我们具体看下如何创建 RESTClient 客户端，以及 Request 模块的实现。 ","date":"2022-07-07 15:20:13","objectID":"/iam_service_develop/:11:3","tags":["iam"],"title":"iam_service_develop","uri":"/iam_service_develop/"},{"categories":["iam"],"content":"RESTClient 客户端实现 我通过下面两个步骤，实现了 RESTClient 客户端。 第一步，创建rest.Config类型的变量。 BuildConfigFromFlags函数通过加载 yaml 格式的配置文件，来创建 rest.Config 类型的变量，加载的 yaml 格式配置文件内容为： apiVersion: v1 user: #token: # JWT Token username: admin # iam 用户名 password: Admin@2020 # iam 密码 #secret-id: # 密钥 ID #secret-key: # 密钥 Key client-certificate: /home/colin/.iam/cert/admin.pem # 用于 TLS 的客户端证书文件路径 client-key: /home/colin/.iam/cert/admin-key.pem # 用于 TLS 的客户端 key 文件路径 #client-certificate-data: #client-key-data: server: address: https://127.0.0.1:8443 # iam api-server 地址 timeout: 10s # 请求 api-server 超时时间 #max-retries: # 最大重试次数，默认为 0 #retry-interval: # 重试间隔，默认为 1s #tls-server-name: # TLS 服务器名称 #insecure-skip-tls-verify: # 设置为 true 表示跳过 TLS 安全验证模式，将使得 HTTPS 连接不安全 certificate-authority: /home/colin/.iam/cert/ca.pem # 用于 CA 授权的 cert 文件路径 #certificate-authority-data: 在配置文件中，我们可以指定服务的地址、用户名 / 密码、密钥、TLS 证书、超时时间、重试次数等信息。创建方法如下： config, err := clientcmd.BuildConfigFromFlags(\"\", *iamconfig) if err != nil { panic(err.Error()) } 这里的代码中，*iamconfig 是 yaml 格式的配置文件路径。BuildConfigFromFlags 函数中，调用LoadFromFile函数来解析 yaml 配置文件。LoadFromFile 最终是通过 yaml.Unmarshal 的方式来解析 yaml 格式的配置文件的。 第二步，根据 rest.Config 类型的变量，创建 RESTClient 客户端。 通过RESTClientFor函数来创建 RESTClient 客户端： func RESTClientFor(config *Config) (*RESTClient, error) { ... baseURL, versionedAPIPath, err := defaultServerURLFor(config) if err != nil { return nil, err } // Get the TLS options for this client config tlsConfig, err := TLSConfigFor(config) if err != nil { return nil, err } // Only retry when get a server side error. client := gorequest.New().TLSClientConfig(tlsConfig).Timeout(config.Timeout). Retry(config.MaxRetries, config.RetryInterval, http.StatusInternalServerError) // NOTICE: must set DoNotClearSuperAgent to true, or the client will clean header befor http.Do client.DoNotClearSuperAgent = true ... clientContent := ClientContentConfig{ Username: config.Username, Password: config.Password, SecretID: config.SecretID, SecretKey: config.SecretKey, ... } return NewRESTClient(baseURL, versionedAPIPath, clientContent, client) } RESTClientFor 函数调用defaultServerURLFor(config)生成基本的 HTTP 请求路径：baseURL=http://127.0.0.1:8080，versionedAPIPath=/v1。然后，通过TLSConfigFor函数生成 TLS 配置，并调用 gorequest.New() 创建 gorequest 客户端，将客户端配置信息保存在变量中。最后，调用NewRESTClient函数创建 RESTClient 客户端。 RESTClient 客户端提供了以下方法，来供调用者完成 HTTP 请求： func (c *RESTClient) APIVersion() scheme.GroupVersion func (c *RESTClient) Delete() *Request func (c *RESTClient) Get() *Request func (c *RESTClient) Post() *Request func (c *RESTClient) Put() *Request func (c *RESTClient) Verb(verb string) *Request 可以看到，RESTClient 提供了 Delete 、 Get 、 Post 、 Put 方法，分别用来执行 HTTP 的 DELETE、GET、POST、PUT 方法，提供的 Verb 方法可以灵活地指定 HTTP 方法。这些方法都返回了 Request 类型的变量。Request 类型的变量提供了一些方法，用来完成具体的 HTTP 请求，例如： type Response struct { Allowed bool `json:\"allowed\"` Denied bool `json:\"denied,omitempty\"` Reason string `json:\"reason,omitempty\"` Error string `json:\"error,omitempty\"` } func (c *authz) Authorize(ctx context.Context, request *ladon.Request, opts metav1.AuthorizeOptions) (result *Response, err error) { result = \u0026Response{} err = c.client.Post(). Resource(\"authz\"). VersionedParams(opts). Body(request). Do(ctx). Into(result) return } 上面的代码中， c.client 是 RESTClient 客户端，通过调用 RESTClient 客户端的 Post 方法，返回了 *Request 类型的变量。 *Request 类型的变量提供了 Resource 和 VersionedParams 方法，来构建请求 HTTP URL 中的路径 /v1/authz ；通过 Body 方法，指定了 HTTP 请求的 Body。 到这里，我们分别构建了 HTTP 请求需要的参数：HTTP Method、请求 URL、请求 Body。所以，之后就可以调用 Do 方法来执行 HTTP 请求，并将返回结果通过 Into 方法保存在传入的 result 变量中。 ","date":"2022-07-07 15:20:13","objectID":"/iam_service_develop/:11:4","tags":["iam"],"title":"iam_service_develop","uri":"/iam_service_develop/"},{"categories":["iam"],"content":"Request 模块实现 RESTClient 客户端的方法会返回 Request 类型的变量，Request 类型的变量提供了一系列的方法用来构建 HTTP 请求参数，并执行 HTTP 请求。 所以，Request 模块可以理解为最底层的通信层，我们来看下 Request 模块具体是如何完成 HTTP 请求的。我们先来看下Request 结构体的定义： type RESTClient struct { // base is the root URL for all invocations of the client base *url.URL // group stand for the client group, eg: iam.api, iam.authz group string // versionedAPIPath is a path segment connecting the base URL to the resource root versionedAPIPath string // content describes how a RESTClient encodes and decodes responses. content ClientContentConfig Client *gorequest.SuperAgent } type Request struct { c *RESTClient timeout time.Duration // generic components accessible via method setters verb string pathPrefix string subpath string params url.Values headers http.Header // structural elements of the request that are part of the IAM API conventions // namespace string // namespaceSet bool resource string resourceName string subresource string // output err error body interface{} } 再来看下 Request 结构体提供的方法： func (r *Request) AbsPath(segments ...string) *Request func (r *Request) Body(obj interface{}) *Request func (r *Request) Do(ctx context.Context) Result func (r *Request) Name(resourceName string) *Request func (r *Request) Param(paramName, s string) *Request func (r *Request) Prefix(segments ...string) *Request func (r *Request) RequestURI(uri string) *Request func (r *Request) Resource(resource string) *Request func (r *Request) SetHeader(key string, values ...string) *Request func (r *Request) SubResource(subresources ...string) *Request func (r *Request) Suffix(segments ...string) *Request func (r *Request) Timeout(d time.Duration) *Request func (r *Request) URL() *url.URL func (r *Request) Verb(verb string) *Request func (r *Request) VersionedParams(v interface{}) *Request 通过 Request 结构体的定义和使用方法，我们不难猜测出：Request 模块通过 Name 、 Resource 、 Body 、 SetHeader 等方法来设置 Request 结构体中的各个字段。这些字段最终用来构建出一个 HTTP 请求，并通过 Do 方法来执行 HTTP 请求。那么，如何构建并执行一个 HTTP 请求呢？我们可以通过以下 5 步，来构建并执行 HTTP 请求： 构建 HTTP URL；构建 HTTP Method；构建 HTTP Body；执行 HTTP 请求；保存 HTTP 返回结果。 接下来，我们就来具体看下 Request 模块是如何构建这些请求参数，并发送 HTTP 请求的。 第一步，构建 HTTP URL。 首先，通过defaultServerURLFor函数返回了http://iam.api.marmotedu.com:8080 和 /v1 ，并将二者分别保存在了 Request 类型结构体变量中 c 字段的 base 字段和 versionedAPIPath 字段中。 通过 Do 方法执行 HTTP 时，会调用r.URL()方法来构建请求 URL。 r.URL 方法中，通过以下代码段构建了 HTTP 请求 URL： func (r *Request) URL() *url.URL { p := r.pathPrefix if len(r.resource) != 0 { p = path.Join(p, strings.ToLower(r.resource)) } if len(r.resourceName) != 0 || len(r.subpath) != 0 || len(r.subresource) != 0 { p = path.Join(p, r.resourceName, r.subresource, r.subpath) } finalURL := \u0026url.URL{} if r.c.base != nil { *finalURL = *r.c.bas } finalURL.Path = p ... } p := r.pathPrefix 和 r.c.base ，是通过 defaultServerURLFor 调用返回的 v1 和 http://iam.api.marmotedu.com:8080 来构建的。resourceName 通过 func (r *Request) Resource(resource string) *Request 来指定，例如 authz 。所以，最终我们构建的请求 URL 为 http://iam.api.marmotedu.com:8080/v1/authz 。 第二步，构建 HTTP Method。 HTTP Method 通过 RESTClient 提供的 Post 、Delete 、Get 等方法来设置，例如： func (c *RESTClient) Post() *Request { return c.Verb(\"POST\") } func (c *RESTClient) Verb(verb string) *Request { return NewRequest(c).Verb(verb) } NewRequest(c).Verb(verb) 最终设置了 Request 结构体的 verb 字段，供 Do 方法使用。 第三步，构建 HTTP Body。 HTTP Body 通过 Request 结构体提供的 Body 方法来指定： func (r *Request) Body(obj interface{}) *Request { if v := reflect.ValueOf(obj); v.Kind() == reflect.Struct { r.SetHeader(\"Content-Type\", r.c.content.ContentType) } r.body = obj return r } 第四步，执行 HTTP 请求。 通过 Request 结构体提供的 Do 方法来执行具体的 HTTP 请求，代码如下： func (r *Request) Do(ctx context.Context) Result { client := r.c.Client client.Header = r.headers if r.timeout \u003e 0 { var cancel context.CancelFunc ctx, cancel = context.WithTimeout(ctx, r.timeout) defer cancel() } client.WithContext(ctx) resp, body, errs := client.CustomMethod(r.verb, r.URL().String()).Send(r.body).EndBytes() if err := combineErr(resp, body, errs); err != nil { return Result{ response: \u0026resp, err: err, body: body, } } decoder, err ","date":"2022-07-07 15:20:13","objectID":"/iam_service_develop/:11:5","tags":["iam"],"title":"iam_service_develop","uri":"/iam_service_develop/"},{"categories":["iam"],"content":"请求认证 接下来，我再来介绍下 marmotedu-sdk-go 另外一个比较核心的功能：请求认证。 marmotedu-sdk-go 支持两种认证方式： Basic 认证：通过给请求添加 Authorization: Basic xxxx 来实现。 Bearer 认证：通过给请求添加 Authorization: Bearer xxxx 来实现。这种方式又支持直接指定 JWT Token，或者通过指定密钥对由 SDK 自动生成 JWT Token。 Basic 认证和 Bearer 认证，我在 25 讲介绍过，你可以返回查看下。认证头是 RESTClient 客户端发送 HTTP 请求时指定的，具体实现位于NewRequest函数中： switch { case c.content.HasTokenAuth(): r.SetHeader(\"Authorization\", fmt.Sprintf(\"Bearer %s\", c.content.BearerToken)) case c.content.HasKeyAuth(): tokenString := auth.Sign(c.content.SecretID, c.content.SecretKey, \"marmotedu-sdk-go\", c.group+\".marmotedu.com\") r.SetHeader(\"Authorization\", fmt.Sprintf(\"Bearer %s\", tokenString)) case c.content.HasBasicAuth(): // TODO: get token and set header r.SetHeader(\"Authorization\", \"Basic \"+basicAuth(c.content.Username, c.content.Password)) } 上面的代码会根据配置信息，自动判断使用哪种认证方式。 ","date":"2022-07-07 15:20:13","objectID":"/iam_service_develop/:11:6","tags":["iam"],"title":"iam_service_develop","uri":"/iam_service_develop/"},{"categories":["iam"],"content":"总结 这一讲中，我介绍了 Kubernetes client-go 风格的 SDK 实现方式。和公有云厂商的 SDK 设计相比，client-go 风格的 SDK 设计有很多优点。marmotedu-sdk-go 在设计时，通过接口实现了 3 类客户端，分别是项目级别的客户端、应用级别的客户端和服务级别的客户端。开发人员可以根据需要，自行创建客户端类型。 marmotedu-sdk-go 通过RESTClientFor，创建了 RESTClient 类型的客户端，RESTClient 向下通过调用Request模块，来完成 HTTP 请求方法、请求路径、请求体、认证信息的构建。Request 模块最终通过调用gorequest包提供的方法，完成 HTTP 的 POST、PUT、GET、DELETE 等请求，获取 HTTP 返回结果，并解析到指定的结构体中。RESTClient 向上提供 Post() 、 Put() 、 Get() 、 Delete() 等方法，来供客户端完成 HTTP 请求。 ","date":"2022-07-07 15:20:13","objectID":"/iam_service_develop/:11:7","tags":["iam"],"title":"iam_service_develop","uri":"/iam_service_develop/"},{"categories":["iam"],"content":"35 | 效率神器：如何设计和实现一个命令行客户端工具？ 如果你用过 Kubernetes、Istio、etcd，那你一定用过这些开源项目所提供的命令行工具：kubectl、istioctl、etcdctl。一个 xxx 项目，伴随着一个 xxxctl 命令行工具，这似乎已经成为一种趋势，在一些大型系统中更是常见。提供 xxxctl 命令行工具有这两个好处： 实现自动化：可以通过在脚本中调用 xxxctl 工具，实现自动化。 提高效率：通过将应用的功能封装成命令和参数，方便运维、开发人员在 Linux 服务器上调用。 其中，kubectl 命令设计的功能最为复杂，也是非常优秀的命令行工具，IAM 项目的 iamctl 客户端工具就是仿照 kubectl 来实现的。这一讲，我就通过剖析 iamctl 命令行工具的实现，来介绍下如何实现一个优秀的客户端工具。 ","date":"2022-07-07 15:20:13","objectID":"/iam_service_develop/:12:0","tags":["iam"],"title":"iam_service_develop","uri":"/iam_service_develop/"},{"categories":["iam"],"content":"常见客户端介绍 在介绍 iamctl 命令行工具的实现之前，我们先来看下常见的客户端。 客户端又叫用户端，与后端服务相对应，安装在客户机上，用户可以使用这些客户端访问后端服务。不同的客户端面向的人群不同，所能提供的访问能力也有差异。常见的客户端有下面这几种： 前端，包括浏览器、手机应用；SDK；命令行工具；其他终端。 接下来，我就来分别介绍下。 浏览器和手机应用提供一个交互界面供用户访问后端服务，使用体验最好，面向的人群是最终的用户。这两类客户端也称为前端。前端由前端开发人员进行开发，并通过 API 接口，调用后端的服务。后端开发人员不需要关注这两类客户端，只需要关注如何提供 API 接口即可。SDK（Software Development Kit）也是一个客户端，供开发者调用。开发者调用 API 时，如果是通过 HTTP 协议，需要编写 HTTP 的调用代码、HTTP 请求包的封装和返回包的解封，还要处理 HTTP 的状态码，使用起来不是很方便。SDK 其实是封装了 API 接口的一系列函数集合，开发者通过调用 SDK 中的函数调用 API 接口，提供 SDK 主要是方便开发者调用，减少工作量。命令行工具是可以在操作系统上执行的一个二进制程序，提供了一种比 SDK 和 API 接口更方便快捷的访问后端服务的途径，供运维或者开发人员在服务器上直接执行使用，或者在自动化脚本中调用。 还有其他各类客户端，这里我列举一些常见的。终端设备：POS 机、学习机、智能音箱等。第三方应用程序：通过调用 API 接口或者 SDK，调用我们提供的后端服务，从而实现自身的功能。脚本：脚本中通过 API 接口或者命令行工具，调用我们提供的后端服务，实现自动化。 这些其他的各类客户端，都是通过调用 API 接口使用后端服务的，它们跟前端一样，也不需要后台开发人员开发。需要后台开发人员投入工作量进行研发的客户端是 SDK 和命令行工具。这两类客户端工具有个调用和被调用的顺序，如下图所示： 你可以看到，命令行工具和 SDK 最终都是通过 API 接口调用后端服务的，通过这种方式可以保证服务的一致性，并减少为适配多个客户端所带来的额外开发工作量。 ","date":"2022-07-07 15:20:13","objectID":"/iam_service_develop/:12:1","tags":["iam"],"title":"iam_service_develop","uri":"/iam_service_develop/"},{"categories":["iam"],"content":"大型系统客户端（xxxctl）的特点 通过学习 kubectl、istioctl、etcdctl 这些优秀的命令行工具，可以发现一个大型系统的命令行工具，通常具有下面这些特点： 支持命令和子命令，命令 / 子命名有自己独有的命令行参数。 支持一些特殊的命令。比如支持 completion 命令，completion 命令可以输出 bash/zsh 自动补全脚本，实现命令行及参数的自动补全。还支持 version 命令，version 命令不仅可以输出客户端的版本，还可以输出服务端的版本（如果有需要）。 支持全局 option，全局 option 可以作为所有命令及子命令的命令行参数。 支持 -h/help，-h/help 可以打印 xxxctl 的帮助信息，例如： $ iamctl -h iamctl controls the iam platform, is the client side tool for iam platform. Find more information at: https://github.com/marmotedu/iam/blob/master/docs/guide/en-US/cmd/iamctl/iamctl.md Basic Commands: info Print the host information color Print colors supported by the current terminal new Generate demo command code jwt JWT command-line tool Identity and Access Management Commands: user Manage users on iam platform secret Manage secrets on iam platform policy Manage authorization policies on iam platform Troubleshooting and Debugging Commands: validate Validate the basic environment for iamctl to run Settings Commands: set Set specific features on objects completion Output shell completion code for the specified shell (bash or zsh) Other Commands: version Print the client and server version information Usage: iamctl [flags] [options] Use \"iamctl \u003ccommand\u003e --help\" for more information about a given command. Use \"iamctl options\" for a list of global command-line options (applies to all commands). 支持 xxxctl help [command | command subcommand] [command | command subcommand] -h ，打印命令 / 子命令的帮助信息，格式通常为 命令描述 + 使用方法 。例如： $ istioctl help register Registers a service instance (e.g. VM) joining the mesh Usage: istioctl register \u003csvcname\u003e \u003cip\u003e [name1:]port1 [name2:]port2 ... [flags] 除此之外，一个大型系统的命令行工具还可以支持一些更高阶的功能，例如：支持命令分组，支持配置文件，支持命令的使用 example，等等。 在 Go 生态中，如果我们要找一个符合上面所有特点的命令行工具，那非kubectl莫属。因为我今天要重点讲的 iamctl 客户端工具，就是仿照它来实现的，所以这里就不展开介绍 kubectl 了，不过还是建议你认真研究下 kubectl 的实现。 ","date":"2022-07-07 15:20:13","objectID":"/iam_service_develop/:12:2","tags":["iam"],"title":"iam_service_develop","uri":"/iam_service_develop/"},{"categories":["iam"],"content":"iamctl 的核心实现 接下来，我就来介绍 IAM 系统自带的 iamctl 客户端工具，它是仿照 kubectl 来实现的，能够满足一个大型系统客户端工具的需求。我会从 iamctl 的功能、代码结构、命令行选项和配置文件解析 4 个方面来介绍。 iamctl 的功能 iamctl 将命令进行了分类。这里，我也建议你对命令进行分类，因为通过分类，不仅可以协助你理解命令的用途，还能帮你快速定位某类命令。另外，当命令很多时，分类也可以使命令看起来更规整。iamctl 实现的命令如下： 更详细的功能，你可以参考 iamctl -h 。我建议你在实现 xxxctl 工具时，考虑实现下面这几个功能。 API 功能：平台具有的 API 功能，都能通过 xxxctl 方便地进行调用。 工具：一些使用 IAM 系统时有用的功能，比如签发 JWT Token。 version、completion、validate 命令。 代码结构 iamctl 工具的 main 函数位于iamctl.go文件中。命令的实现存放在internal/iamctl/cmd/cmd.go文件中。iamctl 的命令统一存放在internal/iamctl/cmd目录下，每个命令都是一个 Go 包，包名即为命令名，具体实现存放在 internal/iamctl/cmd/\u003c命令\u003e/\u003c命令\u003e.go 文件中。如果命令有子命令，则子命令的实现存放在 internal/iamctl/cmd/\u003c命令\u003e/\u003c命令\u003e_\u003c子命令\u003e.go 文件中。 使用这种代码组织方式，即使是在命令很多的情况下，也能让代码井然有序，方便定位和维护代码。 命令行选项 添加命令行选项的代码在NewIAMCtlCommand函数中，核心代码为： flags := cmds.PersistentFlags() ... iamConfigFlags := genericclioptions.NewConfigFlags(true).WithDeprecatedPasswordFlag().WithDeprecatedSecretFlag() iamConfigFlags.AddFlags(flags) matchVersionIAMConfigFlags := cmdutil.NewMatchVersionFlags(iamConfigFlags) matchVersionIAMConfigFlags.AddFlags(cmds.PersistentFlags()) NewConfigFlags(true) 返回带有默认值的参数，并通过 iamConfigFlags.AddFlags(flags) 添加到 cobra 的命令行 flag 中。NewConfigFlags(true) 返回结构体类型的值都是指针类型，这样做的好处是：程序可以判断出是否指定了某个参数，从而可以根据需要添加参数。例如：可以通过 WithDeprecatedPasswordFlag() 和 WithDeprecatedSecretFlag() 添加密码和密钥认证参数。 NewMatchVersionFlags 指定是否需要服务端版本和客户端版本一致。如果不一致，在调用服务接口时会报错。 配置文件解析 iamctl 需要连接 iam-apiserver，来完成用户、策略和密钥的增删改查，并且需要进行认证。要完成这些功能，需要有比较多的配置项。这些配置项如果每次都在命令行选项指定，会很麻烦，也容易出错。 最好的方式是保存到配置文件中，并加载配置文件。加载配置文件的代码位于 NewIAMCtlCommand 函数中，代码如下： _ = viper.BindPFlags(cmds.PersistentFlags()) cobra.OnInitialize(func() { genericapiserver.LoadConfig(viper.GetString(genericclioptions.FlagIAMConfig), \"iamctl\") }) iamctl 会按以下优先级加载配置文件： 命令行参 –iamconfig 指定的配置文件。 当前目录下的 iamctl.yaml 文件。 $HOME/.iam/iamctl.yaml 文件。 这种加载方式具有两个好处。首先是可以手动指定不同的配置文件，这在多环境、多配置下尤为重要。其次是方便使用，可以把配置存放在默认的加载路径中，在执行命令时，就不用再指定 –iamconfig 参数。加载完配置文件之后，就可以通过 viper.Get() 函数来获取配置。例如，iamctl 使用了以下 viper.Get 方法： ","date":"2022-07-07 15:20:13","objectID":"/iam_service_develop/:12:3","tags":["iam"],"title":"iam_service_develop","uri":"/iam_service_develop/"},{"categories":["iam"],"content":"iamctl 中子命令是如何构建的？ 讲完了 iamctl 命令行工具的核心实现，我们再来看看 iamctl 命令行工具中，子命令是如何构建的。命令行工具的核心是命令，有很多种方法可以构建一个命令，但还是有一些比较好的构建方法，值得我们去参考。接下来，我来介绍下如何用比较好的方式去构建命令。 命令构建 命令行工具的核心能力是提供各类命令，来完成不同功能，每个命令构建的方式可以完全不同，但最好能按相同的方式去构建，并抽象成一个模型。如下图所示： 你可以将一个命令行工具提供的命令进行分组。每个分组包含多个命令，每个命令又可以具有多个子命令，子命令和父命令在构建方式上完全一致。每个命令可以按下面的四种方式构建。具体代码你可以参考internal/iamctl/cmd/user/user_update.go。 通过 NewCmdXyz 函数创建命令框架。 NewCmdXyz 函数通过创建一个 cobra.Command 类型的变量来创建命令；通过指定 cobra.Command 结构体类型的 Short、Long、Example 字段，来指定该命令的使用文档iamctl -h 、详细使用文档iamctl xyz -h 和使用示例。 通过 cmd.Flags().XxxxVar 来给该命令添加命令行选项。 为了在不指定命令行参数时，能够按照默认的方式执行命令，可以通过 NewXyzOptions 函数返回一个设置了默认选项的 XyzOptions 类型的变量。 XyzOptions 选项具有 Complete 、Validate 和 Run 三个方法，分别完成选项补全、选项验证和命令执行。命令的执行逻辑可以在 func (o *XyzOptions) Run(args []string) error 函数中编写。 按相同的方式去构建命令，抽象成一个通用模型，这种方式有下面四个好处。 减少理解成本：理解一个命令的构建方式，就可以理解其他命令的构建方式。 提高新命令的开发效率：可以复用其他命令的开发框架，新命令只需填写业务逻辑即可。 自动生成命令：可以按照规定的命令模型，自动生成新的命令。 易维护：因为所有的命令都来自于同一个命令模型，所以可以保持一致的代码风格，方便后期维护。 自动生成命令 上面讲到，自动生成命令模型的好处之一是可以自动生成命令，下面让我们来具体看下。iamctl 自带了命令生成工具，下面我们看看生成方法，一共可以分成 5 步。这里假设生成 xyz 命令。 第一步，新建一个 xyz 目录，用来存放 xyz 命令源码： $ mkdir internal/iamctl/cmd/xyz 第二步，在 xyz 目录下，使用 iamctl new 命令生成 xyz 命令源码： $ cd internal/iamctl/cmd/xyz/ $ iamctl new xyz Command file generated: xyz.go 第三步，将 xyz 命令添加到 root 命令中，假设 xyz 属于 Settings Commands 命令分组。 在 NewIAMCtlCommand 函数中，找到 Settings Commands 分组，将 NewCmdXyz 追加到 Commands 数组后面： { Message: \"Settings Commands:\", Commands: []*cobra.Command{ set.NewCmdSet(f, ioStreams), completion.NewCmdCompletion(ioStreams.Out, \"\"), xyz.NewCmdXyz(f, ioStreams), }, }, 第四步，编译 iamctl： $ make build BINS=iamctl 第五步，测试： $ iamctl xyz -h A longer description that spans multiple lines and likely contains examples and usage of using your command. For example: Cobra is a CLI library for Go that empowers applications. This application is a tool to generate the needed files to quickly create a Cobra application. Examples: # Print all option values for xyz iamctl xyz marmotedu marmotedupass Options: -b, --bool=false: Bool option. -i, --int=0: Int option. --slice=[]: String slice option. --string='default': String option. Usage: iamctl xyz USERNAME PASSWORD [options] Use \"iamctl options\" for a list of global command-line options (applies to all commands). $ iamctl xyz marmotedu marmotedupass The following is option values: ==\u003e --string: default(complete) ==\u003e --slice: [] ==\u003e --int: 0 ==\u003e --bool: false The following is args values: ==\u003e username: marmotedu ==\u003e password: marmotedupass 你可以看到，经过短短的几步，就添加了一个新的命令 xyz 。 iamctl new 命令不仅可以生成不带子命令的命令，还可以生成带有子命令的命令，生成方式如下： $ iamctl new -g xyz Command file generated: xyz.go Command file generated: xyz_subcmd1.go Command file generated: xyz_subcmd2.go 命令自动补全 cobra 会根据注册的命令自动生成补全脚本，可以补全父命令、子命令和选项参数。在 bash 下，可以按下面的方式配置自动补全功能。 第一步，生成自动补全脚本： $ iamctl completion bash \u003e ~/.iam/completion.bash.inc 第二步，登陆时加载 bash，自动补全脚本： $ echo \"source '$HOME/.iam/completion.bash.inc'\" \u003e\u003e $HOME/.bash_profile $ source $HOME/.bash_profile 第三步，测试自动补全功能： $ iamctl xy\u003cTAB\u003e # 按TAB键，自动补全为：iamctl xyz $ iamctl xyz --b\u003cTAB\u003e # 按TAB键，自动补全为：iamctl xyz --bool 更友好的输出 在开发命令时，可以通过一些技巧来提高使用体验。我经常会在输出中打印一些彩色输出，或者将一些输出以表格的形式输出，如下图所示： 这里，使用 github.com/olekukonko/tablewriter 包来实现表格功能，使用 github.com/fatih/color 包来打印带色彩的字符串。具体使用方法，你可以参考internal/iamctl/cmd/validate/validate.go文件。 github.com/fatih/color 包可以给字符串标示颜色，字符串和颜色的对应关系可通过 iamctl color 来查看，如下图所示： ","date":"2022-07-07 15:20:13","objectID":"/iam_service_develop/:12:4","tags":["iam"],"title":"iam_service_develop","uri":"/iam_service_develop/"},{"categories":["iam"],"content":"iamctl 是如何进行 API 调用的？ 上面我介绍了 iamctl 命令的构建方式，那么这里我们再来看下 iamctl 是如何请求服务端 API 接口的。 Go 后端服务的功能通常通过 API 接口来对外暴露，一个后端服务可能供很多个终端使用，比如浏览器、命令行工具、手机等。为了保持功能的一致性，这些终端都会调用同一套 API 来完成相同的功能，如下图所示： 如果命令行工具需要用到后端服务的功能，也需要通过 API 调用的方式。理想情况下，Go 后端服务对外暴露的所有 API 功能，都可以通过命令行工具来完成。一个 API 接口对应一个命令，API 接口的参数映射到命令的参数。 要调用服务端的 API 接口，最便捷的方法是通过 SDK 来调用，对于一些没有实现 SDK 的接口，也可以直接调用。所以，在命令行工具中，需要支持以下两种调用方式： 通过 SDK 调用服务端 API 接口。直接调用服务端的 API 接口（本专栏是 REST API 接口）。 iamctl 通过cmdutil.NewFactory创建一个 Factory 类型的变量 f ， Factory 定义为： type Factory interface { genericclioptions.RESTClientGetter IAMClientSet() (*marmotedu.Clientset, error) RESTClient() (*restclient.RESTClient, error) } 将变量 f 传入到命令中，在命令中使用 Factory 接口提供的 RESTClient() 和 IAMClientSet() 方法，分别返回 RESTful API 客户端和 SDK 客户端，从而使用客户端提供的接口函数。代码可参考internal/iamctl/cmd/version/version.go。 ","date":"2022-07-07 15:20:13","objectID":"/iam_service_develop/:12:5","tags":["iam"],"title":"iam_service_develop","uri":"/iam_service_develop/"},{"categories":["iam"],"content":"客户端配置文件 如果要创建 RESTful API 客户端和 SDK 的客户端，需要调用 f.ToRESTConfig() 函数返回 *github.com/marmotedu/marmotedu-sdk-go/rest.Config 类型的配置变量，然后再基于 rest.Config 类型的配置变量创建客户端。 f.ToRESTConfig 函数最终是调用toRawIAMConfigLoader函数来生成配置的，代码如下： func (f *ConfigFlags) toRawIAMConfigLoader() clientcmd.ClientConfig { config := clientcmd.NewConfig() if err := viper.Unmarshal(\u0026config); err != nil { panic(err) } return clientcmd.NewClientConfigFromConfig(config) } toRawIAMConfigLoader 返回 clientcmd.ClientConfig 类型的变量， clientcmd.ClientConfig 类型提供了 ClientConfig 方法，用来返回*rest.Config类型的变量。 在 toRawIAMConfigLoader 函数内部，通过 viper.Unmarshal 将 viper 中存储的配置解析到 clientcmd.Config 类型的结构体变量中。viper 中存储的配置，是在 cobra 命令启动时通过 LoadConfig 函数加载的，代码如下（位于 NewIAMCtlCommand 函数中）： cobra.OnInitialize(func() { genericapiserver.LoadConfig(viper.GetString(genericclioptions.FlagIAMConfig), \"config\") }) 你可以通过 –config 选项，指定配置文件的路径。 ","date":"2022-07-07 15:20:13","objectID":"/iam_service_develop/:12:6","tags":["iam"],"title":"iam_service_develop","uri":"/iam_service_develop/"},{"categories":["iam"],"content":"SDK 调用 通过IAMClient返回 SDK 客户端，代码如下： func (f *factoryImpl) IAMClient() (*iam.IamClient, error) { clientConfig, err := f.ToRESTConfig() if err != nil { return nil, err } return iam.NewForConfig(clientConfig) } marmotedu.Clientset 提供了 iam-apiserver 的所有接口。 ","date":"2022-07-07 15:20:13","objectID":"/iam_service_develop/:12:7","tags":["iam"],"title":"iam_service_develop","uri":"/iam_service_develop/"},{"categories":["iam"],"content":"REST API 调用 通过RESTClient()返回 RESTful API 客户端，代码如下： func (f *factoryImpl) RESTClient() (*restclient.RESTClient, error) { clientConfig, err := f.ToRESTConfig() if err != nil { return nil, err } setIAMDefaults(clientConfig) return restclient.RESTClientFor(clientConfig) } 可以通过下面的方式访问 RESTful API 接口： serverVersion *version.Info client, _ := f.RESTClient() if err := client.Get().AbsPath(\"/version\").Do(context.TODO()).Into(\u0026serverVersion); err != nil { return err } 上面的代码请求了 iam-apiserver 的 /version 接口，并将返回结果保存在 serverVersion 变量中。 ","date":"2022-07-07 15:20:13","objectID":"/iam_service_develop/:12:8","tags":["iam"],"title":"iam_service_develop","uri":"/iam_service_develop/"},{"categories":["iam"],"content":"总结 这一讲，我主要剖析了 iamctl 命令行工具的实现，进而向你介绍了如何实现一个优秀的客户端工具。 对于一个大型系统 xxx 来说，通常需要有一个 xxxctl 命令行工具， xxxctl 命令行工具可以方便开发、运维使用系统功能，并能实现功能自动化。 IAM 项目参考 kubectl，实现了命令行工具 iamctl。iamctl 集成了很多功能，我们可以通过 iamctl 子命令来使用这些功能。例如，我们可以通过 iamctl 对用户、密钥和策略进行 CURD 操作；可以设置 iamctl 自动补全脚本；可以查看 IAM 系统的版本信息。甚至，你还可以使用 iamctl new 命令，快速创建一个 iamctl 子命令模板。 iamctl 使用了 cobra、pflag、viper 包来构建，每个子命令又包含了一些基本的功能，例如短描述、长描述、使用示例、命令行选项、选项校验等。iamctl 命令可以加载不同的配置文件，来连接不同的客户端。iamctl 通过 SDK 调用、REST API 调用两种方式来调用服务端 API 接口。 ","date":"2022-07-07 15:20:13","objectID":"/iam_service_develop/:12:9","tags":["iam"],"title":"iam_service_develop","uri":"/iam_service_develop/"},{"categories":["iam"],"content":"12 | API 风格（上）：如何设计RESTful API？ 绝大部分的 Go 后端服务需要编写 API 接口，对外提供服务。所以在开发之前，我们需要确定一种 API 风格。API 风格也可以理解为 API 类型，目前业界常用的 API 风格有三种：REST、RPC 和 GraphQL。我们需要根据项目需求，并结合 API 风格的特点，确定使用哪种 API 风格，这对以后的编码实现、通信方式和通信效率都有很大的影响。 在 Go 项目开发中，用得最多的是 REST 和 RPC，我们在 IAM 实战项目中也使用了 REST 和 RPC 来构建示例项目。接下来的两讲，我会详细介绍下 REST 和 RPC 这两种风格，如果你对 GraphQL 感兴趣，GraphQL 中文官网有很多文档和代码示例，你可以自行学习。 ","date":"2022-07-07 15:19:49","objectID":"/iam_basic_func/:1:0","tags":["iam"],"title":"iam_basic_func","uri":"/iam_basic_func/"},{"categories":["iam"],"content":"RESTful API 介绍 在回答“RESTful API 是什么”之前，我们先来看下 REST 是什么意思：REST 代表的是表现层状态转移（REpresentational State Transfer），由 Roy Fielding 在他的论文《Architectural Styles and the Design of Network-based Software Architectures》里提出。REST 本身并没有创造新的技术、组件或服务，它只是一种软件架构风格，是一组架构约束条件和原则，而不是技术框架。 REST 有一系列规范，满足这些规范的 API 均可称为 RESTful API。REST 规范把所有内容都视为资源，也就是说网络上一切皆资源。REST 架构对资源的操作包括获取、创建、修改和删除，这些操作正好对应 HTTP 协议提供的 GET、POST、PUT 和 DELETE 方法。HTTP 动词与 REST 风格 CRUD 的对应关系见下表：REST 风格虽然适用于很多传输协议，但在实际开发中，由于 REST 天生和 HTTP 协议相辅相成，因此 HTTP 协议已经成了实现 RESTful API 事实上的标准。所以，REST 具有以下核心特点： 以资源 (resource) 为中心，所有的东西都抽象成资源，所有的行为都应该是在资源上的 CRUD 操作。 资源对应着面向对象范式里的对象，面向对象范式以对象为中心。 资源使用 URI 标识，每个资源实例都有一个唯一的 URI 标识。例如，如果我们有一个用户，用户名是 admin，那么它的 URI 标识就可以是 /users/admin。 资源是有状态的，使用 JSON/XML 等在 HTTP Body 里表征资源的状态。 客户端通过四个 HTTP 动词，对服务器端资源进行操作，实现“表现层状态转化”。 无状态，这里的无状态是指每个 RESTful API 请求都包含了所有足够完成本次操作的信息，服务器端无须保持 session。无状态对于服务端的弹性扩容是很重要的。 因为怕你弄混概念，这里强调下 REST 和 RESTful API 的区别：REST 是一种规范，而 RESTful API 则是满足这种规范的 API 接口。 ","date":"2022-07-07 15:19:49","objectID":"/iam_basic_func/:1:1","tags":["iam"],"title":"iam_basic_func","uri":"/iam_basic_func/"},{"categories":["iam"],"content":"RESTful API 设计原则 上面我们说了，RESTful API 就是满足 REST 规范的 API，由此看来，RESTful API 的核心是规范，那么具体有哪些规范呢？ 接下来，我就从 URI 设计、API 版本管理等七个方面，给你详细介绍下 RESTful API 的设计原则，然后再通过一个示例来帮助你快速启动一个 RESTful API 服务。希望你学完这一讲之后，对如何设计 RESTful API 有一个清楚的认知。 URI 设计 资源都是使用 URI 标识的，我们应该按照一定的规范来设计 URI，通过规范化可以使我们的 API 接口更加易读、易用。以下是 URI 设计时，应该遵循的一些规范： 资源名使用名词而不是动词，并且用名词复数表示。资源分为 Collection 和 Member 两种。 Collection：一堆资源的集合。例如我们系统里有很多用户（User）, 这些用户的集合就是 Collection。Collection 的 URI 标识应该是 域名/资源名复数, 例如https:// iam.api.marmotedu.com/users。 Member：单个特定资源。例如系统中特定名字的用户，就是 Collection 里的一个 Member。Member 的 URI 标识应该是 域名/资源名复数/资源名称, 例如https:// iam.api.marmotedu/users/admin。 URI 结尾不应包含/。 URI 中不能出现下划线 _，必须用中杠线 -代替（有些人推荐用 _，有些人推荐用 -，统一使用一种格式即可，我比较推荐用 -）。 URI 路径用小写，不要用大写。 避免层级过深的 URI。超过 2 层的资源嵌套会很乱，建议将其他资源转化为?参数，比如： /schools/tsinghua/classes/rooma/students/zhang # 不**推荐** /students?school=qinghua\u0026class=rooma # **推荐** 这里有个地方需要注意：在实际的 API 开发中，可能你会发现有些操作不能很好地映射为一个 REST 资源，这时候，你可以参考下面的做法。 将一个操作变成资源的一个属性，比如想在系统中暂时禁用某个用户，可以这么设计 URI：/users/zhangsan?active=false。 将操作当作是一个资源的嵌套资源，比如一个 GitHub 的加星操作： PUT /gists/:id/star # github star action DELETE /gists/:id/star # github unstar action 如果以上都不能解决问题，有时可以打破这类规范。比如登录操作，登录不属于任何一个资源，URI 可以设计为：/login。 在设计 URI 时，如果你遇到一些不确定的地方，推荐你参考 GitHub 标准 RESTful API。 REST 资源操作映射为 HTTP 方法 基本上 RESTful API 都是使用 HTTP 协议原生的 GET、PUT、POST、DELETE 来标识对资源的 CRUD 操作的，形成的规范如下表所示：对资源的操作应该满足安全性和幂等性： 安全性：不会改变资源状态，可以理解为只读的。 幂等性：执行 1 次和执行 N 次，对资源状态改变的效果是等价的。 使用不同 HTTP 方法时，资源操作的安全性和幂等性对照见下表：在使用 HTTP 方法的时候，有以下两点需要你注意： GET 返回的结果，要尽量可用于 PUT、POST 操作中。例如，用 GET 方法获得了一个 user 的信息，调用者修改 user 的邮件，然后将此结果再用 PUT 方法更新。这要求 GET、PUT、POST 操作的资源属性是一致的。 如果对资源进行状态 / 属性变更，要用 PUT 方法，POST 方法仅用来创建或者批量删除这两种场景。 在设计 API 时，经常会有批量删除的需求，需要在请求中携带多个需要删除的资源名，但是 HTTP 的 DELETE 方法不能携带多个资源名，这时候可以通过下面三种方式来解决： 发起多个 DELETE 请求。 操作路径中带多个 id，id 之间用分隔符分隔, 例如：DELETE /users?ids=1,2,3 。 直接使用 POST 方式来批量删除，body 中传入需要删除的资源列表。 其中，第二种是我最推荐的方式，因为使用了匹配的 DELETE 动词，并且不需要发送多次 DELETE 请求。 你需要注意的是，这三种方式都有各自的使用场景，你可以根据需要自行选择。如果选择了某一种方式，那么整个项目都需要统一用这种方式。 统一的返回格式 一般来说，一个系统的 RESTful API 会向外界开放多个资源的接口，每个接口的返回格式要保持一致。另外，每个接口都会返回成功和失败两种消息，这两种消息的格式也要保持一致。不然，客户端代码要适配不同接口的返回格式，每个返回格式又要适配成功和失败两种消息格式，会大大增加用户的学习和使用成本。 返回的格式没有强制的标准，你可以根据实际的业务需要返回不同的格式。本专栏 第 19 讲 中会推荐一种返回格式，它也是业界最常用和推荐的返回格式。 API 版本管理 随着时间的推移、需求的变更，一个 API 往往满足不了现有的需求，这时候就需要对 API 进行修改。对 API 进行修改时，不能影响其他调用系统的正常使用，这就要求 API 变更做到向下兼容，也就是新老版本共存。 **但在实际场景中，很可能会出现同一个 API 无法向下兼容的情况。这时候最好的解决办法是从一开始就引入 API 版本机制，当不能向下兼容时，就引入一个新的版本，老的版本则保留原样。**这样既能保证服务的可用性和安全性，同时也能满足新需求。 API 版本有不同的标识方法，在 RESTful API 开发中，通常将版本标识放在如下 3 个位置： URL 中，比如/v1/users。 HTTP Header 中，比如Accept: vnd.example-com.foo+json; version=1.0。 Form 参数中，比如/users?version=v1。 我们这门课中的版本标识是放在 URL 中的，比如/v1/users，这样做的好处是很直观，GitHub、Kubernetes、Etcd 等很多优秀的 API 均采用这种方式。 这里要注意，有些开发人员不建议将版本放在 URL 中，因为他们觉得不同的版本可以理解成同一种资源的不同表现形式，所以应该采用同一个 URI。对于这一点，没有严格的标准，根据项目实际需要选择一种方式即可。 API 命名 API 通常的命名方式有三种，分别是驼峰命名法 (serverAddress)、蛇形命名法 (server_address) 和脊柱命名法 (server-address)。 驼峰命名法和蛇形命名法都需要切换输入法，会增加操作的复杂性，也容易出错，所以这里建议用脊柱命名法。GitHub API 用的就是脊柱命名法，例如 selected-actions。 统一分页 / 过滤 / 排序 / 搜索功能 REST 资源的查询接口，通常情况下都需要实现分页、过滤、排序、搜索功能，因为这些功能是每个 REST 资源都能用到的，所以可以实现为一个公共的 API 组件。下面来介绍下这些功能。 分页：在列出一个 Collection 下所有的 Member 时，应该提供分页功能，例如/users?offset=0\u0026limit=20（limit，指定返回记录的数量；offset，指定返回记录的开始位置）。引入分页功能可以减少 API 响应的延时，同时可以避免返回太多条目，导致服务器 / 客户端响应特别慢，甚至导致服务器 / 客户端 crash 的情况。 过滤：如果用户不需要一个资源的全部状态属性，可以在 URI 参数里指定返回哪些属性，例如/users?fields=email,username,address。 排序：用户很多时候会根据创建时间或者其他因素，列出一个 Collection 中前 100 个 Member，这时可以在 URI 参数中指明排序参数，例如/users?sort=age,desc。 搜索：当一个资源的 Member 太多时，用户可能想通过搜索，快速找到所需要的 Member，或着想搜下有没有名字为 xxx 的某类资源，这时候就需要提供搜索功能。搜索建议按模糊匹配来搜索。 域名 API 的域名设置主要有两种方式： https://marmotedu.com/api，这种方式适合 API 将来不会有进一步扩展的情况，比如刚开始 marmotedu.com 域名下只有一套 API 系统，未来也只有这一套 API 系统。 https://iam.api.marmotedu.com，如果 marmotedu.com 域名下未来会新增另一个系统 API，这时候最好的方式是每个系统的 API 拥有专有的 API 域名，比如：storage.api.marmotedu.com，network.api.marmotedu.com。腾讯云的域名就是采用这种方式。 到这里，我们就将 REST 设计原则中的核心原则讲完了，这里有个需要注意的点：不同公司、不同团队、不同项目可能采取不同的 REST 设计原则，以上所列的基本上都是大家公认的原则。 REST 设计原则中，还有一些原则因为内容比较多，并且可以独立成模块，所以放在后面来讲。比如 RESTful API 安全性、状态返回码和认证等。 R","date":"2022-07-07 15:19:49","objectID":"/iam_basic_func/:1:2","tags":["iam"],"title":"iam_basic_func","uri":"/iam_basic_func/"},{"categories":["iam"],"content":"总结 这一讲，我介绍了两种常用 API 风格中的一种，RESTful API。REST 是一种 API 规范，而 RESTful API 则是满足这种规范的 API 接口，RESTful API 的核心是规范。 在 REST 规范中，资源通过 URI 来标识，资源名使用名词而不是动词，并且用名词复数表示，资源都是分为 Collection 和 Member 两种。RESTful API 中，分别使用 POST、DELETE、PUT、GET 来表示 REST 资源的增删改查，HTTP 方法、Collection、Member 不同组合会产生不同的操作，具体的映射你可以看下 REST 资源操作映射为 HTTP 方法 部分的表格。 为了方便用户使用和理解，每个 RESTful API 的返回格式、错误和正确消息的返回格式，都应该保持一致。RESTful API 需要支持 API 版本，并且版本应该能够向前兼容，我们可以将版本号放在 URL 中、HTTP Header 中、Form 参数中，但这里我建议将版本号放在 URL 中，例如 /v1/users，这种形式比较直观。 另外，我们可以通过脊柱命名法来命名 API 接口名。对于一个 REST 资源，其查询接口还应该支持分页 / 过滤 / 排序 / 搜索功能，这些功能可以用同一套机制来实现。 API 的域名可以采用 https://marmotedu.com/api 和 https://iam.api.marmotedu.com 两种格式。 最后，在 Go 中我们可以使用 net/http 包来快速启动一个 RESTful API 服务。 ","date":"2022-07-07 15:19:49","objectID":"/iam_basic_func/:1:3","tags":["iam"],"title":"iam_basic_func","uri":"/iam_basic_func/"},{"categories":["iam"],"content":"13 | API 风格（下）：RPC API介绍 在 Go 项目开发中，如果业务对性能要求比较高，并且需要提供给多种编程语言调用，这时候就可以考虑使用 RPC API 接口。RPC 在 Go 项目开发中用得也非常多，需要我们认真掌握。 为什么要用rpc框架： 在采用微服务架构之前，我们需要思考为什么采用微服务架构，并不是所有的开发团队和发展阶段都适合采用微服务架构。通常，采用微服务架构可以解决以下问题：首先，开发团队具有一定的规模，所有成员共同开发一个单体应用的内耗太高，如果采用微服务架构，每个服务可以由单个或者少数成员独立负责。第二，业务系统的功能模块很多，耦合在一起会增加测试和部署的成本，任何一个模块故障也会导致整个系统故障。第三，功能模块之间的负载无法隔离，容易互相影响，没有办法针对热点模块的计算层或者存储层进行扩容。 如果我们采用微服务架构，单个服务是⾮常简单的，但是，分布式服务之间的功能调用远⽐单体应用内部更加复杂。在单体应用中，⼀个函数可以调⽤其他任何一个公共函数。在微服务架构中，一个函数只可以调⽤同⼀个微服务的函数。如何实现分布式服务之间的通信是微服务架构的首要问题，构建高性能、高可用的远程调用能力并不容易。值得庆幸的是，已经有 grpc、thrift、tars、go-zero、GoFrame、cloudwego/kitex 和 spring cloud 等大量开源的分布式服务开发框架，这些框架可以帮助终端用户快速地构建微服务。不幸的是，仅仅把服务开发出来并且跑通是不够的，保障大规模服务的稳定运营还需要考虑诸多问题，例如：在分布式架构中如何处理基础设施以及应用层的各种异常、如何实现大规模服务的无损发布和流量调度，如何定位和分析复杂调用链路中出现的问题等。对于中大型企业来说，还存在异构的开发技术栈和运行时环境，存在跨地域和混合云的架构要求，如何在更加复杂的应用场景中解决上述问题，面临更多的挑战。 ","date":"2022-07-07 15:19:49","objectID":"/iam_basic_func/:2:0","tags":["iam"],"title":"iam_basic_func","uri":"/iam_basic_func/"},{"categories":["iam"],"content":"RPC 介绍 根据维基百科的定义，RPC（Remote Procedure Call），即远程过程调用，是一个计算机通信协议。该协议允许运行于一台计算机的程序调用另一台计算机的子程序，而程序员不用额外地为这个交互作用编程。 **通俗来讲，就是服务端实现了一个函数，客户端使用 RPC 框架提供的接口，像调用本地函数一样调用这个函数，并获取返回值。**RPC 屏蔽了底层的网络通信细节，使得开发人员无需关注网络编程的细节，可以将更多的时间和精力放在业务逻辑本身的实现上，从而提高开发效率。 RPC 的调用过程如下图所示：RPC 调用具体流程如下： Client 通过本地调用，调用 Client Stub。 Client Stub 将参数打包（也叫 Marshalling）成一个消息，然后发送这个消息。 Client 所在的 OS 将消息发送给 Server。 Server 端接收到消息后，将消息传递给 Server Stub。 Server Stub 将消息解包（也叫 Unmarshalling）得到参数。 Server Stub 调用服务端的子程序（函数），处理完后，将最终结果按照相反的步骤返回给 Client。 这里需要注意，Stub 负责调用参数和返回值的流化（serialization）、参数的打包和解包，以及网络层的通信。Client 端一般叫 Stub，Server 端一般叫 Skeleton。 目前，业界有很多优秀的 RPC 协议，例如腾讯的 Tars、阿里的 Dubbo、微博的 Motan、Facebook 的 Thrift、RPCX，等等。但使用最多的还是gRPC，这也是本专栏所采用的 RPC 框架，所以接下来我会重点介绍 gRPC 框架。 ","date":"2022-07-07 15:19:49","objectID":"/iam_basic_func/:2:1","tags":["iam"],"title":"iam_basic_func","uri":"/iam_basic_func/"},{"categories":["iam"],"content":"gRPC 介绍 gRPC 是由 Google 开发的高性能、开源、跨多种编程语言的通用 RPC 框架，基于 HTTP 2.0 协议开发，默认采用 Protocol Buffers 数据序列化协议。gRPC 具有如下特性： 支持多种语言，例如 Go、Java、C、C++、C#、Node.js、PHP、Python、Ruby 等。 基于 IDL（Interface Definition Language）文件定义服务，通过 proto3 工具生成指定语言的数据结构、服务端接口以及客户端 Stub。通过这种方式，也可以将服务端和客户端解耦，使客户端和服务端可以并行开发。 通信协议基于标准的 HTTP/2 设计，支持双向流、消息头压缩、单 TCP 的多路复用、服务端推送等特性。 支持 Protobuf 和 JSON 序列化数据格式。Protobuf 是一种语言无关的高性能序列化框架，可以减少网络传输流量，提高通信效率。 这里要注意的是，gRPC 的全称不是 golang Remote Procedure Call，而是 google Remote Procedure Call。 gRPC 的调用如下图所示：在 gRPC 中，客户端可以直接调用部署在不同机器上的 gRPC 服务所提供的方法，调用远端的 gRPC 方法就像调用本地的方法一样，非常简单方便，通过 gRPC 调用，我们可以非常容易地构建出一个分布式应用。 像很多其他的 RPC 服务一样，gRPC 也是通过 IDL 语言，预先定义好接口（接口的名字、传入参数和返回参数等）。在服务端，gRPC 服务实现我们所定义的接口。在客户端，gRPC 存根提供了跟服务端相同的方法。 gRPC 支持多种语言，比如我们可以用 Go 语言实现 gRPC 服务，并通过 Java 语言客户端调用 gRPC 服务所提供的方法。通过多语言支持，我们编写的 gRPC 服务能满足客户端多语言的需求。 gRPC API 接口通常使用的数据传输格式是 Protocol Buffers。接下来，我们就一起了解下 Protocol Buffers。 ","date":"2022-07-07 15:19:49","objectID":"/iam_basic_func/:2:2","tags":["iam"],"title":"iam_basic_func","uri":"/iam_basic_func/"},{"categories":["iam"],"content":"Protocol Buffers 介绍 Protocol Buffers（ProtocolBuffer/ protobuf）是 Google 开发的一套对数据结构进行序列化的方法，可用作（数据）通信协议、数据存储格式等，也是一种更加灵活、高效的数据格式，与 XML、JSON 类似。它的传输性能非常好，所以常被用在一些对数据传输性能要求比较高的系统中，作为数据传输格式。Protocol Buffers 的主要特性有下面这几个。 更快的数据传输速度：protobuf 在传输时，会将数据序列化为二进制数据，和 XML、JSON 的文本传输格式相比，这可以节省大量的 IO 操作，从而提高数据传输速度。 跨平台多语言：protobuf 自带的编译工具 protoc 可以基于 protobuf 定义文件，编译出不同语言的客户端或者服务端，供程序直接调用，因此可以满足多语言需求的场景。 具有非常好的扩展性和兼容性，可以更新已有的数据结构，而不破坏和影响原有的程序。 基于 IDL 文件定义服务，通过 proto3 工具生成指定语言的数据结构、服务端和客户端接口。 在 gRPC 的框架中，Protocol Buffers 主要有三个作用。 第一，可以用来定义数据结构。举个例子，下面的代码定义了一个 SecretInfo 数据结构： // SecretInfo contains secret details. message SecretInfo { string name = 1; string secret_id = 2; string username = 3; string secret_key = 4; int64 expires = 5; string description = 6; string created_at = 7; string updated_at = 8; } 第二，可以用来定义服务接口。下面的代码定义了一个 Cache 服务，服务包含了 ListSecrets 和 ListPolicies 两个 API 接口。 // Cache implements a cache rpc service. service Cache{ rpc ListSecrets(ListSecretsRequest) returns (ListSecretsResponse) {} rpc ListPolicies(ListPoliciesRequest) returns (ListPoliciesResponse) {} } 第三，可以通过 protobuf 序列化和反序列化，提升传输效率。 ","date":"2022-07-07 15:19:49","objectID":"/iam_basic_func/:2:3","tags":["iam"],"title":"iam_basic_func","uri":"/iam_basic_func/"},{"categories":["iam"],"content":"gRPC 示例 我们已经对 gRPC 这一通用 RPC 框架有了一定的了解，但是你可能还不清楚怎么使用 gRPC 编写 API 接口。接下来，我就通过 gRPC 官方的一个示例来快速给大家展示下。运行本示例需要在 Linux 服务器上安装 Go 编译器、Protocol buffer 编译器（protoc，v3）和 protoc 的 Go 语言插件，在 02 讲 中我们已经安装过，这里不再讲具体的安装方法。 这个示例分为下面几个步骤：定义 gRPC 服务。生成客户端和服务器代码。实现 gRPC 服务。实现 gRPC 客户端。 示例代码存放在gopractise-demo/apistyle/greeter目录下。代码结构如下： $ tree ├── client │ └── main.go ├── helloworld │ ├── helloworld.pb.go │ └── helloworld.proto └── server └── main.go client 目录存放 Client 端的代码，helloworld 目录用来存放服务的 IDL 定义，server 目录用来存放 Server 端的代码。 下面我具体介绍下这个示例的四个步骤。 定义 gRPC 服务。 首先，需要定义我们的服务。进入 helloworld 目录，新建文件 helloworld.proto： $ cd helloworld $ vi helloworld.proto 内容如下： syntax = \"proto3\"; option go_package = \"github.com/marmotedu/gopractise-demo/apistyle/greeter/helloworld\"; package helloworld; // The greeting service definition. service Greeter { // Sends a greeting rpc SayHello (HelloRequest) returns (HelloReply) {} } // The request message containing the user's name. message HelloRequest { string name = 1; } // The response message containing the greetings message HelloReply { string message = 1; } 在 helloworld.proto 定义文件中，option 关键字用来对.proto 文件进行一些设置，其中 go_package 是必需的设置，而且 go_package 的值必须是包导入的路径。package 关键字指定生成的.pb.go 文件所在的包名。我们通过 service 关键字定义服务，然后再指定该服务拥有的 RPC 方法，并定义方法的请求和返回的结构体类型： service Greeter { // Sends a greeting rpc SayHello (HelloRequest) returns (HelloReply) {} } gRPC 支持定义 4 种类型的服务方法，分别是简单模式、服务端数据流模式、客户端数据流模式和双向数据流模式。 简单模式（Simple RPC）：是最简单的 gRPC 模式。客户端发起一次请求，服务端响应一个数据。定义格式为 rpc SayHello (HelloRequest) returns (HelloReply) {}。 服务端数据流模式（Server-side streaming RPC）：客户端发送一个请求，服务器返回数据流响应，客户端从流中读取数据直到为空。定义格式为 rpc SayHello (HelloRequest) returns (stream HelloReply) {}。 客户端数据流模式（Client-side streaming RPC）：客户端将消息以流的方式发送给服务器，服务器全部处理完成之后返回一次响应。定义格式为 rpc SayHello (stream HelloRequest) returns (HelloReply) {}。 双向数据流模式（Bidirectional streaming RPC）：客户端和服务端都可以向对方发送数据流，这个时候双方的数据可以同时互相发送，也就是可以实现实时交互 RPC 框架原理。定义格式为 rpc SayHello (stream HelloRequest) returns (stream HelloReply) {}。 本示例使用了简单模式。.proto 文件也包含了 Protocol Buffers 消息的定义，包括请求消息和返回消息。例如请求消息： // The request message containing the user's name. message HelloRequest { string name = 1; } 生成客户端和服务器代码。 接下来，我们需要根据.proto 服务定义生成 gRPC 客户端和服务器接口。我们可以使用 protoc 编译工具，并指定使用其 Go 语言插件来生成： $ protoc -I. --go_out=plugins=grpc:$GOPATH/src helloworld.proto $ ls helloworld.pb.go helloworld.proto 你可以看到，新增了一个 helloworld.pb.go 文件。 实现 gRPC 服务。 接着，我们就可以实现 gRPC 服务了。进入 server 目录，新建 main.go 文件： $ cd ../server $ vi main.go main.go 内容如下： // Package main implements a server for Greeter service. package main import ( \"context\" \"log\" \"net\" pb \"github.com/marmotedu/gopractise-demo/apistyle/greeter/helloworld\" \"google.golang.org/grpc\" ) const ( port = \":50051\" ) // server is used to implement helloworld.GreeterServer. type server struct { pb.UnimplementedGreeterServer } // SayHello implements helloworld.GreeterServer func (s *server) SayHello(ctx context.Context, in *pb.HelloRequest) (*pb.HelloReply, error) { log.Printf(\"Received: %v\", in.GetName()) return \u0026pb.HelloReply{Message: \"Hello \" + in.GetName()}, nil } func main() { lis, err := net.Listen(\"tcp\", port) if err != nil { log.Fatalf(\"failed to listen: %v\", err) } s := grpc.NewServer() pb.RegisterGreeterServer(s, \u0026server{}) if err := s.Serve(lis); err != nil { log.Fatalf(\"failed to serve: %v\", err) } } 上面的代码实现了我们上一步根据服务定义生成的 Go 接口。 我们先定义了一个 Go 结构体 server，并为 server 结构体添加SayHello(context.Context, pb.HelloRequest) (pb.HelloReply, error)方法，也就是说 server 是 GreeterServer 接口（位于 helloworld.pb.go 文件中）的一个实现。 在我们实现了 gRPC 服务所定义的方法之后，就可以通过 net.Listen(…) 指定监听客户端请求的端口；接着，通过 grpc.NewServer() 创建一个 gRPC Server 实例，并通过 pb.RegisterGreeterServer(s, \u0026server{}) 将该服务注册到 gRPC 框架中；最后，通过 s.Serve(lis) 启动 gRPC 服务。 创建完 main.go 文件后，在当前目录下执行 go run main.go ，启动 gRPC 服务。 实现 gRPC 客户端。 打开一个新的 Linux 终端，进入 client 目录，新建 main.go 文件： $ cd ../client $ vi main.go main.go 内容如下： // Package main implements a client for Greeter service. package main import ( \"context\" \"log\" \"os\" \"time\" pb \"github.com/marmotedu/gopractise-demo/apistyle/greeter/hell","date":"2022-07-07 15:19:49","objectID":"/iam_basic_func/:2:4","tags":["iam"],"title":"iam_basic_func","uri":"/iam_basic_func/"},{"categories":["iam"],"content":"RESTful VS gRPC 到这里，今天我们已经介绍完了 gRPC API。回想一下我们昨天学习的 RESTful API，你可能想问：这两种 API 风格分别有什么优缺点，适用于什么场景呢？我把这个问题的答案放在了下面这张表中，你可以对照着它，根据自己的需求在实际应用时进行选择。当然，更多的时候，RESTful API 和 gRPC API 是一种合作的关系，对内业务使用 gRPC API，对外业务使用 RESTful API，如下图所示：在 Go 项目开发中，我们可以选择使用 RESTful API 风格和 RPC API 风格，这两种服务都用得很多。其中，RESTful API 风格因为规范、易理解、易用，所以适合用在需要对外提供 API 接口的场景中。而 RPC API 因为性能比较高、调用方便，更适合用在内部业务中。 RESTful API 使用的是 HTTP 协议，而 RPC API 使用的是 RPC 协议。目前，有很多 RPC 协议可供你选择，而我推荐你使用 gRPC，因为它很轻量，同时性能很高、很稳定，是一个优秀的 RPC 框架。所以目前业界用的最多的还是 gRPC 协议，腾讯、阿里等大厂内部很多核心的线上服务用的就是 gRPC。 除了使用 gRPC 协议，在进行 Go 项目开发前，你也可以了解业界一些其他的优秀 Go RPC 框架，比如腾讯的 tars-go、阿里的 dubbo-go、Facebook 的 thrift、rpcx 等，你可以在项目开发之前一并调研，根据实际情况进行选择。 ","date":"2022-07-07 15:19:49","objectID":"/iam_basic_func/:2:5","tags":["iam"],"title":"iam_basic_func","uri":"/iam_basic_func/"},{"categories":["iam"],"content":"14 | 项目管理：如何编写高质量的Makefile？ 要写出一个优雅的 Go 项目，不仅仅是要开发一个优秀的 Go 应用，而且还要能够高效地管理项目。有效手段之一，就是通过 Makefile 来管理我们的项目，这就要求我们要为项目编写 Makefile 文件。 在和其他开发同学交流时，我发现大家都认可 Makefile 强大的项目管理能力，也会自己编写 Makefile。但是其中的一些人项目管理做得并不好，我和他们进一步交流后发现，这些同学在用 Makefile 简单的语法重复编写一些低质量 Makefile 文件，根本没有把 Makefile 的功能充分发挥出来。 下面给你举个例子，你就会理解低质量的 Makefile 文件是什么样的了。 build: clean vet @mkdir -p ./Role @export GOOS=linux \u0026\u0026 go build -v . vet: go vet ./... fmt: go fmt ./... clean: rm -rf dashboard 上面这个 Makefile 存在不少问题。例如：功能简单，只能完成最基本的编译、格式化等操作，像构建镜像、自动生成代码等一些高阶的功能都没有；扩展性差，没法编译出可在 Mac 下运行的二进制文件；没有 Help 功能，使用难度高；单 Makefile 文件，结构单一，不适合添加一些复杂的管理功能。所以，我们不光要编写 Makefile，还要编写高质量的 Makefile。那么如何编写一个高质量的 Makefile 呢？我觉得，可以通过以下 4 个方法来实现： 打好基础，也就是熟练掌握 Makefile 的语法。做好准备工作，也就是提前规划 Makefile 要实现的功能。进行规划，设计一个合理的 Makefile 结构。掌握方法，用好 Makefile 的编写技巧。 ","date":"2022-07-07 15:19:49","objectID":"/iam_basic_func/:3:0","tags":["iam"],"title":"iam_basic_func","uri":"/iam_basic_func/"},{"categories":["iam"],"content":"熟练掌握 Makefile 语法 工欲善其事，必先利其器。编写高质量 Makefile 的第一步，便是熟练掌握 Makefile 的核心语法。 因为 Makefile 的语法比较多，我把一些建议你重点掌握的语法放在了近期会更新的特别放送中，包括 Makefile 规则语法、伪目标、变量赋值、条件语句和 Makefile 常用函数等等。如果你想更深入、全面地学习 Makefile 的语法，我推荐你学习陈皓老师编写的《跟我一起写 Makefile》 (PDF 重制版)。 ","date":"2022-07-07 15:19:49","objectID":"/iam_basic_func/:3:1","tags":["iam"],"title":"iam_basic_func","uri":"/iam_basic_func/"},{"categories":["iam"],"content":"规划 Makefile 要实现的功能 接着，我们需要规划 Makefile 要实现的功能。提前规划好功能，有利于你设计 Makefile 的整体结构和实现方法。 不同项目拥有不同的 Makefile 功能，这些功能中一小部分是通过目标文件来实现的，但更多的功能是通过伪目标来实现的。对于 Go 项目来说，虽然不同项目集成的功能不一样，但绝大部分项目都需要实现一些通用的功能。接下来，我们就来看看，在一个大型 Go 项目中 Makefile 通常可以实现的功能。下面是 IAM 项目的 Makefile 所集成的功能，希望会对你日后设计 Makefile 有一些帮助。 $ make help Usage: make \u003cTARGETS\u003e \u003cOPTIONS\u003e ... Targets: # 代码生成类命令 gen Generate all necessary files, such as error code files. # 格式化类命令 format Gofmt (reformat) package sources (exclude vendor dir if existed). # 静态代码检查 lint Check syntax and styling of go sources. # 测试类命令 test Run unit test. cover Run unit test and get test coverage. # 构建类命令 build Build source code for host platform. build.multiarch Build source code for multiple platforms. See option PLATFORMS. # Docker镜像打包类命令 image Build docker images for host arch. image.multiarch Build docker images for multiple platforms. See option PLATFORMS. push Build docker images for host arch and push images to registry. push.multiarch Build docker images for multiple platforms and push images to registry. # 部署类命令 deploy Deploy updated components to development env. # 清理类命令 clean Remove all files that are created by building. # 其他命令，不同项目会有区别 release Release iam verify-copyright Verify the boilerplate headers for all files. ca Generate CA files for all iam components. install Install iam system with all its components. swagger Generate swagger document. tools install dependent tools. # 帮助命令 help Show this help info. # 选项 Options: DEBUG Whether to generate debug symbols. Default is 0. BINS The binaries to build. Default is all of cmd. This option is available when using: make build/build.multiarch Example: make build BINS=\"iam-apiserver iam-authz-server\" ... 更详细的命令，你可以在 IAM 项目仓库根目录下执行make help查看。通常而言，Go 项目的 Makefile 应该实现以下功能：格式化代码、静态代码检查、单元测试、代码构建、文件清理、帮助等等。如果通过 docker 部署，还需要有 docker 镜像打包功能。因为 Go 是跨平台的语言，所以构建和 docker 打包命令，还要能够支持不同的 CPU 架构和平台。为了能够更好地控制 Makefile 命令的行为，还需要支持 Options。 为了方便查看 Makefile 集成了哪些功能，我们需要支持 help 命令。help 命令最好通过解析 Makefile 文件来输出集成的功能，例如： ## help: Show this help info. .PHONY: help help: Makefile @echo -e \"\\nUsage: make \u003cTARGETS\u003e \u003cOPTIONS\u003e ...\\n\\nTargets:\" @sed -n 's/^##//p' $\u003c | column -t -s ':' | sed -e 's/^/ /' @echo \"$$USAGE_OPTIONS\" 上面的 help 命令，通过解析 Makefile 文件中的##注释，获取支持的命令。通过这种方式，我们以后新加命令时，就不用再对 help 命令进行修改了。你可以参考上面的 Makefile 管理功能，结合自己项目的需求，整理出一个 Makefile 要实现的功能列表，并初步确定实现思路和方法。做完这些，你的编写前准备工作就基本完成了。 ","date":"2022-07-07 15:19:49","objectID":"/iam_basic_func/:3:2","tags":["iam"],"title":"iam_basic_func","uri":"/iam_basic_func/"},{"categories":["iam"],"content":"设计合理的 Makefile 结构 设计完 Makefile 需要实现的功能，接下来我们就进入 Makefile 编写阶段。编写阶段的第一步，就是设计一个合理的 Makefile 结构。 对于大型项目来说，需要管理的内容很多，所有管理功能都集成在一个 Makefile 中，可能会导致 Makefile 很大，难以阅读和维护，所以****建议采用分层的设计方法，根目录下的 Makefile 聚合所有的 Makefile 命令，具体实现则按功能分类，放在另外的 Makefile 中。 我们经常会在 Makefile 命令中集成 shell 脚本，但如果 shell 脚本过于复杂，也会导致 Makefile 内容过多，难以阅读和维护。并且在 Makefile 中集成复杂的 shell 脚本，编写体验也很差。对于这种情况，可以将复杂的 shell 命令封装在 shell 脚本中，供 Makefile 直接调用，而一些简单的命令则可以直接集成在 Makefile 中。所以，最终我推荐的 Makefile 结构如下： 在上面的 Makefile 组织方式中，根目录下的 Makefile 聚合了项目所有的管理功能，这些管理功能通过 Makefile 伪目标的方式实现。同时，还将这些伪目标进行分类，把相同类别的伪目标放在同一个 Makefile 中，这样可以使得 Makefile 更容易维护。对于复杂的命令，则编写成独立的 shell 脚本，并在 Makefile 命令中调用这些 shell 脚本。举个例子，下面是 IAM 项目的 Makefile 组织结构： ├── Makefile ├── scripts │ ├── gendoc.sh │ ├── make-rules │ │ ├── gen.mk │ │ ├── golang.mk │ │ ├── image.mk │ │ └── ... └── ... 我们将相同类别的操作统一放在 scripts/make-rules 目录下的 Makefile 文件中。Makefile 的文件名参考分类命名，例如 golang.mk。最后，在 /Makefile 中 include 这些 Makefile。为了跟 Makefile 的层级相匹配，golang.mk 中的所有目标都按go.xxx这种方式命名。通过这种命名方式，我们可以很容易分辨出某个目标完成什么功能，放在什么文件里，这在复杂的 Makefile 中尤其有用。以下是 IAM 项目根目录下，Makefile 的内容摘录，你可以看一看，作为参考： include scripts/make-rules/golang.mk include scripts/make-rules/image.mk include scripts/make-rules/gen.mk include scripts/make-rules/... ## build: Build source code for host platform. .PHONY: build build: @$(MAKE) go.build ## build.multiarch: Build source code for multiple platforms. See option PLATFORMS. .PHONY: build.multiarch build.multiarch: @$(MAKE) go.build.multiarch ## image: Build docker images for host arch. .PHONY: image image: @$(MAKE) image.build ## push: Build docker images for host arch and push images to registry. .PHONY: push push: @$(MAKE) image.push ## ca: Generate CA files for all iam components. .PHONY: ca ca: @$(MAKE) gen.ca 另外，一个合理的 Makefile 结构应该具有前瞻性。也就是说，要在不改变现有结构的情况下，接纳后面的新功能。这就需要你整理好 Makefile 当前要实现的功能、即将要实现的功能和未来可能会实现的功能，然后基于这些功能，利用 Makefile 编程技巧，编写可扩展的 Makefile。 这里需要你注意：上面的 Makefile 通过 .PHONY 标识定义了大量的伪目标，定义伪目标一定要加 .PHONY 标识，否则当有同名的文件时，伪目标可能不会被执行。 ","date":"2022-07-07 15:19:49","objectID":"/iam_basic_func/:3:3","tags":["iam"],"title":"iam_basic_func","uri":"/iam_basic_func/"},{"categories":["iam"],"content":"掌握 Makefile 编写技巧 最后，在编写过程中，你还需要掌握一些 Makefile 的编写技巧，这些技巧可以使你编写的 Makefile 扩展性更强，功能更强大。接下来，我会把自己长期开发过程中积累的一些 Makefile 编写经验分享给你。这些技巧，你需要在实际编写中多加练习，并形成编写习惯。 技巧 1：善用通配符和自动变量 Makefile 允许对目标进行类似正则运算的匹配，主要用到的通配符是%。通过使用通配符，可以使不同的目标使用相同的规则，从而使 Makefile 扩展性更强，也更简洁。我们的 IAM 实战项目中，就大量使用了通配符%，例如：go.build.%、ca.gen.%、deploy.run.%、tools.verify.%、tools.install.%等。这里，我们来看一个具体的例子，tools.verify.%（位于scripts/make-rules/tools.mk文件中）定义如下： tools.verify.%: @if ! which $* \u0026\u003e/dev/null; then $(MAKE) tools.install.$*; fi make tools.verify.swagger, make tools.verify.mockgen等均可以使用上面定义的规则，%分别代表了swagger和mockgen。如果不使用%，则我们需要分别为tools.verify.swagger和tools.verify.mockgen定义规则，很麻烦，后面修改也困难。 另外，这里也能看出tools.verify.%这种命名方式的好处：tools 说明依赖的定义位于scripts/make-rules/tools.mk Makefile 中；verify说明tools.verify.%伪目标属于 verify 分类，主要用来验证工具是否安装。通过这种命名方式，你可以很容易地知道目标位于哪个 Makefile 文件中，以及想要完成的功能。另外，上面的定义中还用到了自动变量$*，用来指代被匹配的值swagger、mockgen。 技巧 2：善用函数 Makefile 自带的函数能够帮助我们实现很多强大的功能。所以，在我们编写 Makefile 的过程中，如果有功能需求，可以优先使用这些函数。我把常用的函数以及它们实现的功能整理在了 Makefile 常用函数列表 中，你可以参考下。 IAM 的 Makefile 文件中大量使用了上述函数，如果你想查看这些函数的具体使用方法和场景，可以参考 IAM 项目的 Makefile 文件 make-rules。 技巧 3：依赖需要用到的工具 如果 Makefile 某个目标的命令中用到了某个工具，可以将该工具放在目标的依赖中。这样，当执行该目标时，就可以指定检查系统是否安装该工具，如果没有安装则自动安装，从而实现更高程度的自动化。例如，/Makefile 文件中，format 伪目标，定义如下： .PHONY: format format: tools.verify.golines tools.verify.goimports @echo \"===========\u003e Formating codes\" @$(FIND) -type f -name '*.go' | $(XARGS) gofmt -s -w @$(FIND) -type f -name '*.go' | $(XARGS) goimports -w -local $(ROOT_PACKAGE) @$(FIND) -type f -name '*.go' | $(XARGS) golines -w --max-len=120 --reformat-tags --shorten-comments --ignore-generated . 你可以看到，format 依赖tools.verify.golines tools.verify.goimports。我们再来看下tools.verify.golines的定义： tools.verify.%: @if ! which $* \u0026\u003e/dev/null; then $(MAKE) tools.install.$*; fi 再来看下tools.install.$*规则： .PHONY: install.golines install.golines: @$(GO) get -u github.com/segmentio/golines 通过tools.verify.%规则定义，我们可以知道，tools.verify.%会先检查工具是否安装，如果没有安装，就会执行tools.install.$*来安装。如此一来，当我们执行tools.verify.%目标时，如果系统没有安装 golines 命令，就会自动调用go get安装，提高了 Makefile 的自动化程度。 技巧 4：把常用功能放在 /Makefile 中，不常用的放在分类 Makefile 中 一个项目，尤其是大型项目，有很多需要管理的地方，其中大部分都可以通过 Makefile 实现自动化操作。不过，为了保持 /Makefile 文件的整洁性，我们不能把所有的命令都添加在 /Makefile 文件中。 一个比较好的建议是，将常用功能放在 /Makefile 中，不常用的放在分类 Makefile 中，并在 /Makefile 中 include 这些分类 Makefile。例如，IAM 项目的 /Makefile 集成了format、lint、test、build等常用命令，而将gen.errcode.code、gen.errcode.doc这类不常用的功能放在 scripts/make-rules/gen.mk 文件中。当然，我们也可以直接执行 make gen.errcode.code来执行gen.errcode.code伪目标。通过这种方式，既可以保证 /Makefile 的简洁、易维护，又可以通过make命令来运行伪目标，更加灵活。 技巧 5：编写可扩展的 Makefile 什么叫可扩展的 Makefile 呢？在我看来，可扩展的 Makefile 包含两层含义： 可以在不改变 Makefile 结构的情况下添加新功能。扩展项目时，新功能可以自动纳入到 Makefile 现有逻辑中。 其中的第一点，我们可以通过设计合理的 Makefile 结构来实现。要实现第二点，就需要我们在编写 Makefile 时采用一定的技巧，例如多用通配符、自动变量、函数等。这里我们来看一个例子，可以让你更好地理解。 在我们 IAM 实战项目的golang.mk中，执行 make go.build 时能够构建 cmd/ 目录下的所有组件，也就是说，当有新组件添加时， make go.build 仍然能够构建新增的组件，这就实现了上面说的第二点。具体实现方法如下： COMMANDS ?= $(filter-out %.md, $(wildcard ${ROOT_DIR}/cmd/*)) BINS ?= $(foreach cmd,${COMMANDS},$(notdir ${cmd})) .PHONY: go.build go.build: go.build.verify $(addprefix go.build., $(addprefix $(PLATFORM)., $(BINS))) .PHONY: go.build.% go.build.%: $(eval COMMAND := $(word 2,$(subst ., ,$*))) $(eval PLATFORM := $(word 1,$(subst ., ,$*))) $(eval OS := $(word 1,$(subst _, ,$(PLATFORM)))) $(eval ARCH := $(word 2,$(subst _, ,$(PLATFORM)))) @echo \"===========\u003e Building binary $(COMMAND)$(VERSION)for $(OS)$(ARCH)\" @mkdir -p $(OUTPUT_DIR)/platforms/$(OS)/$(ARCH) @CGO_ENABLED=0 GOOS=$(OS) GOARCH=$(ARCH) $(GO) build $(GO_BUILD_FLAGS) -o $(OUTPUT_DIR)/platforms/$(OS)/$(ARCH)/$(COMMAND)$(GO_OUT_EXT) $(ROOT_PACKAGE)/cmd/$(COMMAND) 当执行make go.build 时，会执行 go.build 的依赖 $(addprefix go.build., $(addprefix $(PLATFORM)., $(BINS))) ,addprefix函数最终返回字符串 go.build.linux_amd64.iamctl go.build.linux_amd64.iam-authz-server go.build.linux_amd64.iam-apiserver … ，这时候就会执行 go.build.% 伪目标。 在 go.build.% 伪目标中，通过 eval、word、subst 函数组合，算出了 COMMAND 的值 iamctl/iam-apiserver/iam-authz-server/…，最终通过 $(ROOT_PACKAGE)/cmd/$(COMMAND) 定位到需要构建的组件的 ","date":"2022-07-07 15:19:49","objectID":"/iam_basic_func/:3:4","tags":["iam"],"title":"iam_basic_func","uri":"/iam_basic_func/"},{"categories":["iam"],"content":"总结 如果你想要高效管理项目，使用 Makefile 来管理是目前的最佳实践。我们可以通过下面的几个方法，来编写一个高质量的 Makefile。首先，你需要熟练掌握 Makefile 的语法。我建议你重点掌握以下语法：Makefile 规则语法、伪目标、变量赋值、特殊变量、自动化变量。接着，我们需要提前规划 Makefile 要实现的功能。一个大型 Go 项目通常需要实现以下功能：代码生成类命令、格式化类命令、静态代码检查、 测试类命令、构建类命令、Docker 镜像打包类命令、部署类命令、清理类命令，等等。然后，我们还需要通过 Makefile 功能分类、文件分层、复杂命令脚本化等方式，来设计一个合理的 Makefile 结构。最后，我们还需要掌握一些 Makefile 编写技巧，例如：善用通配符、自动变量和函数；编写可扩展的 Makefile；使用带层级的命名方式，等等。通过这些技巧，可以进一步保证我们编写出一个高质量的 Makefile。 ","date":"2022-07-07 15:19:49","objectID":"/iam_basic_func/:3:5","tags":["iam"],"title":"iam_basic_func","uri":"/iam_basic_func/"},{"categories":["iam"],"content":"15 | 研发流程实战：IAM项目是如何进行研发流程管理的？ 在这一讲中，我会重点介绍这两个阶段中的 Makefile 项目管理功能，并且穿插一些我的 Makefile 的设计思路。 为了向你演示流程，这里先假设一个场景。我们有一个需求：给 IAM 客户端工具 iamctl 增加一个 helloworld 命令，该命令向终端打印 hello world。接下来，我们就来看下如何具体去执行研发流程中的每一步。首先，我们进入开发阶段。 ","date":"2022-07-07 15:19:49","objectID":"/iam_basic_func/:4:0","tags":["iam"],"title":"iam_basic_func","uri":"/iam_basic_func/"},{"categories":["iam"],"content":"开发阶段 开发阶段是开发者的主战场，完全由开发者来主导，它又可分为代码开发和代码提交两个子阶段。我们先来看下代码开发阶段。 代码开发 拿到需求之后，首先需要开发代码。这时，我们就需要选择一个适合团队和项目的 Git 工作流。因为 Git Flow 工作流比较适合大型的非开源项目，所以这里我们选择 Git Flow 工作流。代码开发的具体步骤如下： 第一步，基于 develop 分支，新建一个功能分支 feature/helloworld。 $ git checkout -b feature/helloworld develop 这里需要注意：新建的 branch 名要符合 Git Flow 工作流中的分支命名规则。否则，在 git commit 阶段，会因为 branch 不规范导致 commit 失败。IAM 项目的分支命令规则具体如下图所示： IAM 项目通过 pre-commit githooks 来确保分支名是符合规范的。在 IAM 项目根目录下执行 git commit 命令，git 会自动执行pre-commit脚本，该脚本会检查当前 branch 的名字是否符合规范。 这里还有一个地方需要你注意：git 不会提交 .git/hooks 目录下的 githooks 脚本，所以我们需要通过以下手段，确保开发者 clone 仓库之后，仍然能安装我们指定的 githooks 脚本到 .git/hooks 目录： # Copy githook scripts when execute makefile COPY_GITHOOK:=$(shell cp -f githooks/* .git/hooks/) 上述代码放在scripts/make-rules/common.mk文件中，每次执行 make 命令时都会执行，可以确保 githooks 都安装到 .git/hooks 目录下。 第二步，在 feature/helloworld 分支中，完成 helloworld 命令的添加。首先，通过 iamctl new helloworld 命令创建 helloworld 命令模板： $ iamctl new helloworld -d internal/iamctl/cmd/helloworld Command file generated: internal/iamctl/cmd/helloworld/helloworld.go 接着，编辑internal/iamctl/cmd/cmd.go文件，在源码文件中添加helloworld.NewCmdHelloworld(f, ioStreams),，加载 helloworld 命令。这里将 helloworld 命令设置为Troubleshooting and Debugging Commands命令分组： import ( \"github.com/marmotedu/iam/internal/iamctl/cmd/helloworld\" ) ... { Message: \"Troubleshooting and Debugging Commands:\", Commands: []*cobra.Command{ validate.NewCmdValidate(f, ioStreams), helloworld.NewCmdHelloworld(f, ioStreams), }, }, 这些操作中包含了 low code 的思想。在第 10 讲 中我就强调过，要尽可能使用代码自动生成这一技术。这样做有两个好处：一方面能够提高我们的代码开发效率；另一方面也能够保证规范，减少手动操作可能带来的错误。所以这里，我将 iamctl 的命令也模板化，并通过 iamctl new 自动生成。 第三步，生成代码。 $ make gen 如果改动不涉及代码生成，可以不执行make gen操作。 make gen 执行的其实是 gen.run 伪目标： gen.run: gen.clean gen.errcode gen.docgo.doc 可以看到，当执行 make gen.run 时，其实会先清理之前生成的文件，再分别自动生成 error code 和 doc.go 文件。这里需要注意，通过make gen 生成的存量代码要具有幂等性。只有这样，才能确保每次生成的代码是一样的，避免不一致带来的问题。 我们可以将更多的与自动生成代码相关的功能放在 gen.mk Makefile 中。例如：gen.docgo.doc，代表自动生成 doc.go 文件。gen.ca.%，代表自动生成 iamctl、iam-apiserver、iam-authz-server 证书文件。 第四步，版权检查。如果有新文件添加，我们还需要执行 make verify-copyright ，来检查新文件有没有添加版权头信息。 $ make verify-copyright 如果版权检查失败，可以执行make add-copyright自动添加版权头。添加版权信息只针对开源软件，如果你的软件不需要添加，就可以略过这一步。这里还有个 Makefile 编写技巧：如果 Makefile 的 command 需要某个命令，就可以使该目标依赖类似 tools.verify.addlicense 这种目标，tools.verify.addlicense 会检查该工具是否已安装，如果没有就先安装。 .PHONY: copyright.verify copyright.verify: tools.verify.addlicense ... tools.verify.%: @if ! which $* \u0026\u003e/dev/null; then $(MAKE) tools.install.$*; fi .PHONY: install.addlicense install.addlicense: @$(GO) get -u github.com/marmotedu/addlicense 通过这种方式，可以使 make copyright.verify 尽可能自动化，减少手动介入的概率。第五步，代码格式化。 $ make format 执行make format会依次执行以下格式化操作：调用 gofmt 格式化你的代码。调用 goimports 工具，自动增删依赖的包，并将依赖包按字母序排序并分类。调用 golines 工具，把超过 120 行的代码按 golines 规则，格式化成 \u003c120 行的代码。调用 go mod edit -fmt 格式化 go.mod 文件。 第六步，静态代码检查。 $ make lint 关于静态代码检查，在这里你可以先了解代码开发阶段有这个步骤，至于如何操作，我会在下一讲给你详细介绍。 第七步，单元测试。 $ make test 这里要注意，并不是所有的包都需要执行单元测试。你可以通过如下命令，排除掉不需要单元测试的包： go test `go list ./...|egrep -v $(subst $(SPACE),'|',$(sort $(EXCLUDE_TESTS)))` 在 go.test 的 command 中，我们还运行了以下命令： sed -i '/mock_.*.go/d' $(OUTPUT_DIR)/coverage.out 运行该命令的目的，是把 mock_.* .go 文件中的函数单元测试信息从 coverage.out 中删除。mock_.*.go 文件中的函数是不需要单元测试的，如果不删除，就会影响后面的单元测试覆盖率的计算。 如果想检查单元测试覆盖率，请执行： $ make cover 默认测试覆盖率至少为 60%，也可以在命令行指定覆盖率阈值为其他值，例如： $ make cover COVERAGE=90 如果测试覆盖率不满足要求，就会返回以下错误信息： test coverage is 62.1% test coverage does not meet expectations: 90%, please add test cases! make[1]: *** [go.test.cover] Error 1 make: *** [cover] Error 2 这里 make 命令的退出码为1。如果单元测试覆盖率达不到设置的阈值，就需要补充测试用例，否则禁止合并到 develop 和 master 分支。IAM 项目配置了 GitHub Actions CI 自动化流水线，CI 流水线会自动运行，检查单元测试覆盖率是否达到要求。 第八步，构建。最后，我们执行make build命令，构建出cmd/目录下所有的二进制安装文件。 $ make build make build 会自动构建 cmd/ 目录下的所有组件，如果只想构建其中的一个或多个组件，可以传入 BINS选项，组件之间用空格隔开，并用双引号引起来： $ make build BINS=\"iam-apiserver iamctl\" 到这里，我们就完成了代码开发阶段的全部操作。如果你觉得手动执行的 make 命令比较多，可以直接执行 make 命令： $ make ===========\u003e Generating iam error code go source files ===========\u003e Generating error code markdown documentation ===========\u003e Generating missing doc","date":"2022-07-07 15:19:49","objectID":"/iam_basic_func/:4:1","tags":["iam"],"title":"iam_basic_func","uri":"/iam_basic_func/"},{"categories":["iam"],"content":"测试阶段 在测试阶段，开发人员主要负责提供测试包和修复测试期间发现的 bug，这个过程中也可能会发现一些新的需求或变动点，所以需要合理评估这些新的需求或变动点是否要放在当前迭代修改。 测试阶段的操作流程如下。第一步，基于 develop 分支，创建 release 分支，测试代码。 $ git checkout -b release/1.0.0 develop $ make 第二步，提交测试。将 release/1.0.0 分支的代码提交给测试同学进行测试。这里假设一个测试失败的场景：我们要求打印“hello world”，但打印的是“Hello World”，需要修复。那具体应该怎么操作呢？ 你可以直接在 release/1.0.0 分支修改代码，修改完成后，本地构建并提交代码： $ make $ git add internal/iamctl/cmd/helloworld/ $ git commit -m \"fix: fix helloworld print bug\" $ git push origin release/1.0.0 push 到 release/1.0.0 后，GitHub Actions 会执行 CI 流水线。如果流水线执行成功，就将代码提供给测试；如果测试不成功，再重新修改，直到流水线执行成功。 测试同学会对 release/1.0.0 分支的代码进行充分的测试，例如功能测试、性能测试、集成测试、系统测试等。 第三步，测试通过后，将功能分支合并到 master 分支和 develop 分支。 $ git checkout develop $ git merge --no-ff release/1.0.0 $ git checkout master $ git merge --no-ff release/1.0.0 $ git tag -a v1.0.0 -m \"add print hello world\" # master分支打tag 到这里，测试阶段的操作就基本完成了。测试阶段的产物是 master/develop 分支的代码。 第四步，删除 feature/helloworld 分支，也可以选择性删除 release/1.0.0 分支。我们的代码都合并入 master/develop 分支后，feature 开发者可以选择是否要保留 feature。不过，如果没有特别的原因，我建议删掉，因为 feature 分支太多的话，不仅看起来很乱，还会影响性能，删除操作如下： $ git branch -d feature/helloworld ","date":"2022-07-07 15:19:49","objectID":"/iam_basic_func/:4:2","tags":["iam"],"title":"iam_basic_func","uri":"/iam_basic_func/"},{"categories":["iam"],"content":"IAM 项目的 Makefile 项目管理技巧 在上面的内容中，我们以研发流程为主线，亲身体验了 IAM 项目的 Makefile 项目管理功能。这些是你最应该掌握的核心功能，但 IAM 项目的 Makefile 还有很多功能和设计技巧。接下来，我会给你分享一些很有价值的 Makefile 项目管理技巧。 help 自动解析 因为随着项目的扩展，Makefile 大概率会不断加入新的管理功能，这些管理功能也需要加入到 make help 输出中。但如果每添加一个目标，都要修改 make help 命令，就比较麻烦，还容易出错。所以这里，我通过自动解析的方式，来生成make help输出： ## help: Show this help info. .PHONY: help help: Makefile @echo -e \"\\nUsage: make \u003cTARGETS\u003e \u003cOPTIONS\u003e ...\\n\\nTargets:\" @sed -n 's/^##//p' $\u003c | column -t -s ':' | sed -e 's/^/ /' @echo \"$$USAGE_OPTIONS\" 目标 help 的命令中，通过 sed -n ‘s/^##//p’ $\u003c | column -t -s ‘:’ | sed -e ‘s/^/ /’ 命令，自动解析 Makefile 中 ## 开头的注释行，从而自动生成 make help 输出。 Options 中指定变量值 通过以下赋值方式，变量可以在 Makefile options 中被指定： ifeq ($(origin COVERAGE),undefined) COVERAGE := 60 endif 例如，如果我们执行make ，则 COVERAGE 设置为默认值 60；如果我们执行make COVERAGE=90 ，则 COVERAGE 值为 90。通过这种方式，我们可以更灵活地控制 Makefile 的行为。 自动生成 CHANGELOG 一个项目最好有 CHANGELOG 用来展示每个版本之间的变更内容，作为 Release Note 的一部分。但是，如果每次都要手动编写 CHANGELOG，会很麻烦，也不容易坚持，所以这里我们可以借助git-chglog工具来自动生成。 IAM 项目的 git-chglog 工具的配置文件放在.chglog目录下，在学习 git-chglog 工具时，你可以参考下。 自动生成版本号 一个项目也需要有一个版本号，当前用得比较多的是语义化版本号规范。但如果靠开发者手动打版本号，工作效率低不说，经常还会出现漏打、打的版本号不规范等问题。所以最好的办法是，版本号也通过工具自动生成。在 IAM 项目中，采用了gsemver工具来自动生成版本号。 整个 IAM 项目的版本号，都是通过scripts/ensure_tag.sh脚本来生成的： version=v`gsemver bump` if [ -z \"`git tag -l $version`\" ];then git tag -a -m \"release version $version\" $version fi 在 scripts/ensure_tag.sh 脚本中，通过 gsemver bump 命令来自动化生成语义化的版本号，并执行 git tag -a 给仓库打上版本号标签，gsemver 命令会根据 Commit Message 自动生成版本号。 之后，Makefile 和 Shell 脚本用到的所有版本号均统一使用scripts/make-rules/common.mk文件中的 VERSION 变量： VERSION := $(shell git describe --tags --always --match='v*') 上述的 Shell 命令通过 git describe 来获取离当前提交最近的 tag（版本号）。 在执行 git describe 时，如果符合条件的 tag 指向最新提交，则只显示 tag 的名字，否则会有相关的后缀，来描述该 tag 之后有多少次提交，以及最新的提交 commit id。例如： $ git describe --tags --always --match='v*' v1.0.0-3-g1909e47 这里解释下版本号中各字符的含义：3：表示自打 tag v1.0.0 以来有 3 次提交。g1909e47：g 为 git 的缩写，在多种管理工具并存的环境中很有用处。1909e47：7 位字符表示为最新提交的 commit id 前 7 位。 最后解释下参数：–tags，使用所有的标签，而不是只使用带注释的标签（annotated tag）。git tag 生成一个 unannotated tag，git tag -a -m '’ 生成一个 annotated tag。–always，如果仓库没有可用的标签，那么使用 commit 缩写来替代标签。–match，只考虑与给定模式相匹配的标签。 保持行为一致 上面我们介绍了一些管理功能，例如检查 Commit Message 是否符合规范、自动生成 CHANGELOG、自动生成版本号。这些可以通过 Makefile 来操作，我们也可以手动执行。例如，通过以下命令，检查 IAM 的所有 Commit 是否符合 Angular Commit Message 规范： $ go-gitlint b62db1f: subject does not match regex [^(revert: )?(feat|fix|perf|style|refactor|test|ci|docs|chore)(\\(.+\\))?: [^A-Z].*[^.]$] 也可以通过以下命令，手动来生成 CHANGELOG： $ git-chglog v1.0.0 CHANGELOG/CHANGELOG-1.0.0.md 还可以执行 gsemver 来生成版本号： $ gsemver bump 1.0.1 这里要强调的是，我们要保证不管使用手动操作，还是通过 Makefile 操作，都要确保 git commit message 规范检查结果、生成的 CHANGELOG、生成的版本号是一致的。这需要我们采用同一种操作方式。 ","date":"2022-07-07 15:19:49","objectID":"/iam_basic_func/:4:3","tags":["iam"],"title":"iam_basic_func","uri":"/iam_basic_func/"},{"categories":["iam"],"content":"总结 在整个研发流程中，需要开发人员深度参与的阶段有两个，分别是开发阶段和测试阶段。在开发阶段，开发者完成代码开发之后，通常需要执行生成代码、版权检查、代码格式化、静态代码检查、单元测试、构建等操作。我们可以将这些操作集成在 Makefile 中，来提高效率，并借此统一操作。 另外，IAM 项目在编写 Makefile 时也采用了一些技巧，例如make help 命令中，help 信息是通过解析 Makefile 文件的注释来完成的；可以通过 git-chglog 自动生成 CHANGELOG；通过 gsemver 自动生成语义化的版本号等。 ","date":"2022-07-07 15:19:49","objectID":"/iam_basic_func/:4:4","tags":["iam"],"title":"iam_basic_func","uri":"/iam_basic_func/"},{"categories":["iam"],"content":"16 | 代码检查：如何进行静态代码检查？ 在做 Go 项目开发的过程中，我们肯定需要对 Go 代码做静态代码检查。虽然 Go 命令提供了 go vet 和 go tool vet，但是它们检查的内容还不够全面，我们需要一种更加强大的静态代码检查工具。 其实，Go 生态中有很多这样的工具，也不乏一些比较优秀的。今天我想给你介绍的 golangci-lint，是目前使用最多，也最受欢迎的静态代码检查工具，我们的 IAM 实战项目也用到了它。 接下来，我就从 golangci-lint 的优点、golangci-lint 提供的命令和选项、golangci-lint 的配置这三个方面来向你介绍下它。在你了解这些基础知识后，我会带着你使用 golangci-lint 进行静态代码检查，让你熟悉操作，在这个基础上，再把我使用 golangci-lint 时总结的一些经验技巧分享给你。 ","date":"2022-07-07 15:19:49","objectID":"/iam_basic_func/:5:0","tags":["iam"],"title":"iam_basic_func","uri":"/iam_basic_func/"},{"categories":["iam"],"content":"为什么选择 golangci-lint 做静态代码检查？ 选择 golangci-lint，是因为它具有其他静态代码检查工具不具备的一些优点。在我看来，它的核心优点至少有这些： 速度非常快：golangci-lint 是基于 gometalinter 开发的，但是平均速度要比 gometalinter 快 5 倍。golangci-lint 速度快的原因有三个：可以并行检查代码；可以复用 go build 缓存；会缓存分析结果。 可配置：支持 YAML 格式的配置文件，让检查更灵活，更可控。 IDE 集成：可以集成进多个主流的 IDE，例如 VS Code、GNU Emacs、Sublime Text、Goland 等。 linter 聚合器：1.41.1 版本的 golangci-lint 集成了 76 个 linter，不需要再单独安装这 76 个 linter。并且 golangci-lint 还支持自定义 linter。 最小的误报数：golangci-lint 调整了所集成 linter 的默认设置，大幅度减少了误报。 良好的输出：输出的结果带有颜色、代码行号和 linter 标识，易于查看和定位。 下图是一个 golangci-lint 的检查结果： 你可以看到，输出的检查结果中包括如下信息：检查出问题的源码文件、行号和错误行内容。出问题的原因，也就是打印出不符合检查规则的原因。报错的 linter。 通过查看 golangci-lint 的输出结果，可以准确地定位到报错的位置，快速弄明白报错的原因，方便开发者修复。 除了上述优点之外，在我看来 golangci-lint 还有一个非常大的优点：当前更新迭代速度很快，不断有新的 linter 被集成到 golangci-lint 中。有这么全的 linter 为你的代码保驾护航，你在交付代码时肯定会更有自信。 目前，有很多公司 / 项目使用了 golangci-lint 工具作为静态代码检查工具，例如 Google、Facebook、Istio、Red Hat OpenShift 等。 ","date":"2022-07-07 15:19:49","objectID":"/iam_basic_func/:5:1","tags":["iam"],"title":"iam_basic_func","uri":"/iam_basic_func/"},{"categories":["iam"],"content":"golangci-lint 提供了哪些命令和选项？ 在使用之前，首先需要安装 golangci-lint。golangci-lint 的安装方法也很简单，你只需要执行以下命令，就可以安装了。 $ go get github.com/golangci/golangci-lint/cmd/golangci-lint@v1.41.1 $ golangci-lint version # 输出 golangci-lint 版本号，说明安装成功 golangci-lint has version v1.39.0 built from (unknown, mod sum: \"h1:aAUjdBxARwkGLd5PU0vKuym281f2rFOyqh3GB4nXcq8=\") on (unknown) 这里注意，为了避免安装失败，强烈建议你安装 golangci-lint releases page 中的指定版本，例如 v1.41.1。另外，还建议你定期更新 golangci-lint 的版本，因为该项目正在被积极开发并不断改进。安装之后，就可以使用了。我们可以通过执行 golangci-lint -h 查看其用法，golangci-lint 支持的子命令见下表： 此外，golangci-lint 还支持一些全局选项。全局选项是指适用于所有子命令的选项，golangci-lint 支持的全局选项如下： 接下来，我就详细介绍下 golangci-lint 支持的核心子命令：run、cache、completion、config、linters。 run 命令 run 命令执行 golangci-lint，对代码进行检查，是 golangci-lint 最为核心的一个命令。run 没有子命令，但有很多选项。run 命令的具体使用方法，我会在讲解如何执行静态代码检查的时候详细介绍。 cache 命令 cache 命令用来进行缓存控制，并打印缓存的信息。它包含两个子命令：clean 用来清除 cache，当我们觉得 cache 的内容异常，或者 cache 占用空间过大时，可以通过golangci-lint cache clean清除 cache。status 用来打印 cache 的状态，比如 cache 的存放目录和 cache 的大小，例如： $ golangci-lint cache status Dir: /home/colin/.cache/golangci-lint Size: 773.4KiB completion 命令 completion 命令包含 4 个子命令 bash、fish、powershell 和 zsh，分别用来输出 bash、fish、powershell 和 zsh 的自动补全脚本。 下面是一个配置 bash 自动补全的示例： $ golangci-lint completion bash \u003e ~/.golangci-lint.bash $ echo \"source '$HOME/.golangci-lint.bash'\" \u003e\u003e ~/.bashrc $ source ~/.bashrc 执行完上面的命令，键入如下命令，即可自动补全子命令： $ golangci-lint comp\u003cTAB\u003e 上面的命令行会自动补全为golangci-lint completion 。 config 命令 config 命令可以打印 golangci-lint 当前使用的配置文件路径，例如： $ golangci-lint config path .golangci.yaml linters 命令 linters 命令可以打印出 golangci-lint 所支持的 linter，并将这些 linter 分成两类，分别是配置为启用的 linter 和配置为禁用的 linter，例如： $ golangci-lint linters Enabled by your configuration linters: ... deadcode: Finds unused code [fast: true, auto-fix: false] ... Disabled by your configuration linters: exportloopref: checks for pointers to enclosing loop variables [fast: true, auto-fix: false] ... 上面我介绍了 golangci-lint 提供的命令，接下来，我们再来看下 golangci-lint 的配置。 golangci-lint 配置 和其他 linter 相比，golangci-lint 一个非常大的优点是使用起来非常灵活，这要得益于它对自定义配置的支持。 golangci-lint 支持两种配置方式，分别是命令行选项和配置文件。如果 bool/string/int 的选项同时在命令行选项和配置文件中被指定，命令行的选项就会覆盖配置文件中的选项。如果是 slice 类型的选项，则命令行和配置中的配置会进行合并。golangci-lint run 支持很多命令行选项，可通过golangci-lint run -h查看，这里选择一些比较重要的选项进行介绍，见下表： 此外，我们还可以通过 golangci-lint配置文件进行配置，默认的配置文件名为.golangci.yaml、.golangci.toml、.golangci.json，可以通过-c选项指定配置文件名。通过配置文件，可以实现下面几类功能： golangci-lint 本身的一些选项，比如超时、并发，是否检查*_test.go文件等。 配置需要忽略的文件和文件夹。 配置启用哪些 linter，禁用哪些 linter。 配置输出格式。 golangci-lint 支持很多 linter，其中有些 linter 支持一些配置项，这些配置项可以在配置文件中配置。 配置符合指定正则规则的文件可以忽略的 linter。 设置错误严重级别，像日志一样，检查错误也是有严重级别的。 更详细的配置内容，你可以参考Configuration。另外，你也可以参考 IAM 项目的 golangci-lint 配置.golangci.yaml。.golangci.yaml 里面的一些配置，我建议你一定要设置，具体如下： run:skip-dirs:# 设置要忽略的目录- util- .*~- api/swagger/docsskip-files:# 设置不需要检查的go源码文件，支持正则匹配，这里**建议**包括：_test.go- \".*\\\\.my\\\\.go$\"- _test.golinters-settings:errcheck:check-type-assertions:true# 这里**建议**设置为true，如果确实不需要检查，可以写成`num, _ := strconv.Atoi(numStr)`check-blank:falsegci:# 将以`github.com/marmotedu/iam`开头的包放在第三方包后面local-prefixes:github.com/marmotedu/iamgodox:keywords:# **建议**设置为BUG、FIXME、OPTIMIZE、HACK- BUG- FIXME- OPTIMIZE- HACKgoimports:# 设置哪些包放在第三方包后面，可以设置多个包，逗号隔开local-prefixes:github.com/marmotedu/iamgomoddirectives:# 设置允许在go.mod中replace的包replace-local:truereplace-allow-list:- github.com/coreos/etcd- google.golang.org/grpc- github.com/marmotedu/api- github.com/marmotedu/component-base- github.com/marmotedu/marmotedu-sdk-gogomodguard:# 下面是根据需要选择可以使用的包和版本，**建议**设置allowed:modules:- gorm.io/gorm- gorm.io/driver/mysql- k8s.io/klogdomains:# List of allowed module domains- google.golang.org- gopkg.in- golang.org- github.com- go.uber.orgblocked:modules:- github.com/pkg/errors:recommendations:- github.com/marmotedu/errorsreason:\"`github.com/marmotedu/errors` is the log package used by marmotedu projects.\"versions:- github.com/MakeNowJust/heredoc:version:\"\u003e 2.0.9\"reason:\"use the latest version\"local_replace_directives:falselll:line-length:240# 这里可以设置为240，240一般是够用的importas:# 设置包的alias，根据需要设置jwt:github.com/app","date":"2022-07-07 15:19:49","objectID":"/iam_basic_func/:5:2","tags":["iam"],"title":"iam_basic_func","uri":"/iam_basic_func/"},{"categories":["iam"],"content":"如何使用 golangci-lint 进行静态代码检查？ 要对代码进行静态检查，只需要执行 golangci-lint run 命令即可。接下来，我会先给你介绍 5 种常见的 golangci-lint 使用方法。 对当前目录及子目录下的所有 Go 文件进行静态代码检查： $ golangci-lint run 命令等效于golangci-lint run ./…。 对指定的 Go 文件或者指定目录下的 Go 文件进行静态代码检查： $ golangci-lint run dir1 dir2/... dir3/file1.go 这里需要你注意：上述命令不会检查 dir1 下子目录的 Go 文件，如果想递归地检查一个目录，需要在目录后面追加/…，例如：dir2/…。 根据指定配置文件，进行静态代码检查： $ golangci-lint run -c .golangci.yaml ./... 运行指定的 linter： golangci-lint 可以在不指定任何配置文件的情况下运行，这会运行默认启用的 linter，你可以通过golangci-lint help linters查看它。 你可以传入参数-E/–enable来使某个 linter 可用，也可以使用-D/–disable参数来使某个 linter 不可用。下面的示例仅仅启用了 errcheck linter： $ golangci-lint run --no-config --disable-all -E errcheck ./... 这里你需要注意，默认情况下，golangci-lint 会从当前目录一层层往上寻找配置文件名.golangci.yaml、.golangci.toml、.golangci.json直到根（/）目录。如果找到，就以找到的配置文件作为本次运行的配置文件，所以为了防止读取到未知的配置文件，可以用 –no-config 参数使 golangci-lint 不读取任何配置文件。 禁止运行指定的 liner：如果我们想禁用某些 linter，可以使用-D选项。 $ golangci-lint run --no-config -D godot,errcheck 在使用 golangci-lint 进行代码检查时，可能会有很多误报。所谓的误报，其实是我们希望 golangci-lint 的一些 linter 能够容忍某些 issue。那么如何尽可能减少误报呢？golangci-lint 也提供了一些途径，我建议你使用下面这三种： 在命令行中添加-e参数，或者在配置文件的issues.exclude部分设置要排除的检查错误。你也可以使用issues.exclude-rules来配置哪些文件忽略哪些 linter。通过run.skip-dirs、run.skip-files或者issues.exclude-rules配置项，来忽略指定目录下的所有 Go 文件，或者指定的 Go 文件。通过在 Go 源码文件中添加//nolint注释，来忽略指定的代码行。 因为 golangci-lint 设置了很多 linters，对于一个大型项目，启用所有的 linter 会检查出很多问题，并且每个项目对 linter 检查的粒度要求也不一样，所以 glangci-lint使用 nolint 标记来开关某些检查项，不同位置的 nolint 标记效果也会不一样。接下来我想向你介绍 nolint 的几种用法。 忽略某一行所有 linter 的检查 var bad_name int //nolint 忽略某一行指定 linter 的检查，可以指定多个 linter，用逗号 , 隔开。 var bad_name int //nolint:golint,unused 忽略某个代码块的检查。 //nolint func allIssuesInThisFunctionAreExcluded() *string { // ... } //nolint:govet var ( a int b int ) 忽略某个文件的指定 linter 检查。在 package xx 上面一行添加//nolint注释。 //nolint:unparam package pkg ... 在使用 nolint 的过程中，有 3 个地方需要你注意。首先，如果启用了 nolintlint，你就需要在//nolint后面添加 nolint 的原因// xxxx。其次，你使用的应该是//nolint而不是// nolint。因为根据 Go 的规范，需要程序读取的注释 // 后面不应该有空格。最后，如果要忽略所有 linter，可以用//nolint；如果要忽略某个指定的 linter，可以用//nolint:,。 golangci-lint 使用技巧 我在使用 golangci-lint 时，总结了一些经验技巧，放在这里供你参考，希望能够帮助你更好地使用 golangci-lint。 技巧 1：第一次修改，可以按目录修改。如果你第一次使用 golangci-lint 检查你的代码，一定会有很多错误。为了减轻修改的压力，可以按目录检查代码并修改。这样可以有效减少失败条数，减轻修改压力。当然，如果错误太多，一时半会儿改不完，想以后慢慢修改或者干脆不修复存量的 issues，那么你可以使用 golangci-lint 的 –new-from-rev 选项，只检查新增的 code，例如： $ golangci-lint run --new-from-rev=HEAD~1 技巧 2：按文件修改，减少文件切换次数，提高修改效率。如果有很多检查错误，涉及很多文件，建议先修改一个文件，这样就不用来回切换文件。可以通过 grep 过滤出某个文件的检查失败项，例如： $ golangci-lint run ./...|grep pkg/storage/redis_cluster.go pkg/storage/redis_cluster.go:16:2: \"github.com/go-redis/redis/v7\" imported but not used (typecheck) pkg/storage/redis_cluster.go:82:28: undeclared name: `redis` (typecheck) pkg/storage/redis_cluster.go:86:14: undeclared name: `redis` (typecheck) ... 技巧 3：把 linters-setting.lll.line-length 设置得大一些。在 Go 项目开发中，为了易于阅读代码，通常会将变量名 / 函数 / 常量等命名得有意义，这样很可能导致每行的代码长度过长，很容易超过lll linter 设置的默认最大长度 80。这里建议将linters-setting.lll.line-length设置为 120/240。 技巧 4：尽可能多地使用 golangci-lint 提供的 linter。golangci-lint 集成了很多 linters，可以通过如下命令查看： $ golangci-lint linters Enabled by your configuration linters: deadcode: Finds unused code [fast: true, auto-fix: false] ... varcheck: Finds unused global variables and constants [fast: true, auto-fix: false] Disabled by your configuration linters: asciicheck: Simple linter to check that your code does not contain non-ASCII identifiers [fast: true, auto-fix: false] ... wsl: Whitespace Linter - Forces you to use empty lines! [fast: true, auto-fix: false] 这些 linter 分为两类，一类是默认启用的，另一类是默认禁用的。每个 linter 都有两个属性： fast：true/false，如果为 true，说明该 linter 可以缓存类型信息，支持快速检查。因为第一次缓存了这些信息，所以后续的运行会非常快。 auto-fix：true/false，如果为 true 说明该 linter 支持自动修复发现的错误；如果为 false 说明不支持自动修复。 如果配置了 golangci-lint 配置文件，则可以通过命令golangci-lint help linters查看在当前配置下启用和禁用了哪些 linter。golangci-lint 也支持自定义 linter 插件，具体你可以参考：New linters。 在使用 golangci-lint 的时候，我们要尽可能多的使用 linter。使用的 linter 越多，说明检查越严格，意味着代码越规范，质量越高。如果时间和精力允许，建议打开 golangci-lint 提供的所有 linter。 技巧 5：每次修改代码后，都要执行 golangci-lint。每次修改完代码后都要执行 golangci-lint，一方面可以及时修改不规范的地方，另一方面可以减少错误堆积，","date":"2022-07-07 15:19:49","objectID":"/iam_basic_func/:5:3","tags":["iam"],"title":"iam_basic_func","uri":"/iam_basic_func/"},{"categories":["iam"],"content":"总结 Go 项目开发中，对代码进行静态代码检查是必要的操作。当前有很多优秀的静态代码检查工具，但 golangci-lint 因为具有检查速度快、可配置、少误报、内置了大量 linter 等优点，成为了目前最受欢迎的静态代码检查工具。 golangci-lint 功能非常强大，支持诸如 run、cache、completion、linters 等命令。其中最常用的是 run 命令，run 命令可以通过以下方式来进行静态代码检查： $ golangci-lint run # 对当前目录及子目录下的所有Go文件进行静态代码检查 $ golangci-lint run dir1 dir2/... dir3/file1.go # 对指定的Go文件或者指定目录下的Go文件进行静态代码检查 $ golangci-lint run -c .golangci.yaml ./... # 根据指定配置文件，进行静态代码检查 $ golangci-lint run --no-config --disable-all -E errcheck ./... # 运行指定的 errcheck linter $ golangci-lint run --no-config -D godot,errcheck # 禁止运行指定的godot,errcheck liner 此外，golangci-lint 还支持 //nolint 、//nolint:golint,unused 等方式来减少误报。最后，我分享了一些自己使用 golangci-lint 时总结的经验。例如：第一次修改，可以按目录修改；按文件修改，减少文件切换次数，提高修改效率；尽可能多地使用 golangci-lint 提供的 linter。希望这些建议对你使用 golangci-lint 有一定帮助。 ","date":"2022-07-07 15:19:49","objectID":"/iam_basic_func/:5:4","tags":["iam"],"title":"iam_basic_func","uri":"/iam_basic_func/"},{"categories":["iam"],"content":"17 | API 文档：如何生成 Swagger API 文档 ？ 作为一名开发者，我们通常讨厌编写文档，因为这是一件重复和缺乏乐趣的事情。但是在开发过程中，又有一些文档是我们必须要编写的，比如 API 文档。 一个企业级的 Go 后端项目，通常也会有个配套的前端。为了加快研发进度，通常是后端和前端并行开发，这就需要后端开发者在开发后端代码之前，先设计好 API 接口，提供给前端。所以在设计阶段，我们就需要生成 API 接口文档。 一个好的 API 文档，可以减少用户上手的复杂度，也意味着更容易留住用户。好的 API 文档也可以减少沟通成本，帮助开发者更好地理解 API 的调用方式，从而节省时间，提高开发效率。这时候，我们一定希望有一个工具能够帮我们自动生成 API 文档，解放我们的双手。Swagger 就是这么一个工具，可以帮助我们生成易于共享且具有足够描述性的 API 文档。 接下来，我们就来看下，如何使用 Swagger 生成 API 文档。 ","date":"2022-07-07 15:19:49","objectID":"/iam_basic_func/:6:0","tags":["iam"],"title":"iam_basic_func","uri":"/iam_basic_func/"},{"categories":["iam"],"content":"Swagger 介绍 Swagger 是一套围绕 OpenAPI 规范构建的开源工具，可以设计、构建、编写和使用 REST API。Swagger 包含很多工具，其中主要的 Swagger 工具包括： Swagger 编辑器：基于浏览器的编辑器，可以在其中编写 OpenAPI 规范，并实时预览 API 文档。https://editor.swagger.io 就是一个 Swagger 编辑器，你可以尝试在其中编辑和预览 API 文档。 Swagger UI：将 OpenAPI 规范呈现为交互式 API 文档，并可以在浏览器中尝试 API 调用。 Swagger Codegen：根据 OpenAPI 规范，生成服务器存根和客户端代码库，目前已涵盖了 40 多种语言。 ","date":"2022-07-07 15:19:49","objectID":"/iam_basic_func/:6:1","tags":["iam"],"title":"iam_basic_func","uri":"/iam_basic_func/"},{"categories":["iam"],"content":"Swagger 和 OpenAPI 的区别 我们在谈到 Swagger 时，也经常会谈到 OpenAPI。那么二者有什么区别呢？ OpenAPI 是一个 API 规范，它的前身叫 Swagger 规范，通过定义一种用来描述 API 格式或 API 定义的语言，来规范 RESTful 服务开发过程，目前最新的 OpenAPI 规范是OpenAPI 3.0（也就是 Swagger 2.0 规范）。 OpenAPI 规范规定了一个 API 必须包含的基本信息，这些信息包括：对 API 的描述，介绍 API 可以实现的功能。每个 API 上可用的路径（/users）和操作（GET /users，POST /users）。每个 API 的输入 / 返回的参数。验证方法。联系信息、许可证、使用条款和其他信息。 所以，你可以简单地这么理解：OpenAPI 是一个 API 规范，Swagger 则是实现规范的工具。另外，要编写 Swagger 文档，首先要会使用 Swagger 文档编写语法，因为语法比较多，这里就不多介绍了，你可以参考 Swagger 官方提供的OpenAPI Specification来学习。 ","date":"2022-07-07 15:19:49","objectID":"/iam_basic_func/:6:2","tags":["iam"],"title":"iam_basic_func","uri":"/iam_basic_func/"},{"categories":["iam"],"content":"用 go-swagger 来生成 Swagger API 文档 在 Go 项目开发中，我们可以通过下面两种方法来生成 Swagger API 文档： 第一，如果你熟悉 Swagger 语法的话，可以直接编写 JSON/YAML 格式的 Swagger 文档。建议选择 YAML 格式，因为它比 JSON 格式更简洁直观。第二，通过工具生成 Swagger 文档，目前可以通过swag和go-swagger两个工具来生成。 对比这两种方法，直接编写 Swagger 文档，不比编写 Markdown 格式的 API 文档工作量小，我觉得不符合程序员“偷懒”的习惯。所以，本专栏我们就使用 go-swagger 工具，基于代码注释来自动生成 Swagger 文档。为什么选 go-swagger 呢？有这么几个原因： go-swagger 比 swag 功能更强大：go-swagger 提供了更灵活、更多的功能来描述我们的 API。使我们的代码更易读：如果使用 swag，我们每一个 API 都需要有一个冗长的注释，有时候代码注释比代码还要长，但是通过 go-swagger 我们可以将代码和注释分开编写，一方面可以使我们的代码保持简洁，清晰易读，另一方面我们可以在另外一个包中，统一管理这些 Swagger API 文档定义。更好的社区支持：go-swagger 目前有非常多的 Github star 数，出现 Bug 的概率很小，并且处在一个频繁更新的活跃状态。 你已经知道了，go-swagger 是一个功能强大的、高性能的、可以根据代码注释生成 Swagger API 文档的工具。除此之外，go-swagger 还有很多其他特性： 根据 Swagger 定义文件生成服务端代码。根据 Swagger 定义文件生成客户端代码。校验 Swagger 定义文件是否正确。启动一个 HTTP 服务器，使我们可以通过浏览器访问 API 文档。根据 Swagger 文档定义的参数生成 Go model 结构体定义。 可以看到，使用 go-swagger 生成 Swagger 文档，可以帮助我们减少编写文档的时间，提高开发效率，并能保证文档的及时性和准确性。这里需要注意，如果我们要对外提供 API 的 Go SDK，可以考虑使用 go-swagger 来生成客户端代码。但是我觉得 go-swagger 生成的服务端代码不够优雅，所以建议你自行编写服务端代码。 目前，有很多知名公司和组织的项目都使用了 go-swagger，例如 Moby、CoreOS、Kubernetes、Cilium 等。 安装 Swagger 工具 go-swagger 通过 swagger 命令行工具来完成其功能，swagger 安装方法如下： $ go get -u github.com/go-swagger/go-swagger/cmd/swagger $ swagger version dev swagger 命令行工具介绍 swagger 命令格式为swagger [OPTIONS] 。可以通过swagger -h查看 swagger 使用帮助。swagger 提供的子命令及功能见下表： 如何使用 swagger 命令生成 Swagger 文档？ go-swagger 通过解析源码中的注释来生成 Swagger 文档，go-swagger 的详细注释语法可参考官方文档。常用的有如下几类注释语法： 解析注释生成 Swagger 文档 swagger generate 命令会找到 main 函数，然后遍历所有源码文件，解析源码中与 Swagger 相关的注释，然后自动生成 swagger.json/swagger.yaml 文件。这一过程的示例代码为gopractise-demo/swagger。目录下有一个 main.go 文件，定义了如下 API 接口： package main import ( \"fmt\" \"log\" \"net/http\" \"github.com/gin-gonic/gin\" \"github.com/marmotedu/gopractise-demo/swagger/api\" // This line is necessary for go-swagger to find your docs! _ \"github.com/marmotedu/gopractise-demo/swagger/docs\" ) var users []*api.User func main() { r := gin.Default() r.POST(\"/users\", Create) r.GET(\"/users/:name\", Get) log.Fatal(r.Run(\":5555\")) } // Create create a user in memory. func Create(c *gin.Context) { var user api.User if err := c.ShouldBindJSON(\u0026user); err != nil { c.JSON(http.StatusBadRequest, gin.H{\"message\": err.Error(), \"code\": 10001}) return } for _, u := range users { if u.Name == user.Name { c.JSON(http.StatusBadRequest, gin.H{\"message\": fmt.Sprintf(\"user %s already exist\", user.Name), \"code\": 10001}) return } } users = append(users, \u0026user) c.JSON(http.StatusOK, user) } // Get return the detail information for a user. func Get(c *gin.Context) { username := c.Param(\"name\") for _, u := range users { if u.Name == username { c.JSON(http.StatusOK, u) return } } c.JSON(http.StatusBadRequest, gin.H{\"message\": fmt.Sprintf(\"user %s not exist\", username), \"code\": 10002}) } main 包中引入的 User struct 位于 gopractise-demo/swagger/api 目录下的user.go文件： // Package api defines the user model. package api // User represents body of User request and response. type User struct { // User's name. // Required: true Name string `json:\"name\"` // User's nickname. // Required: true Nickname string `json:\"nickname\"` // User's address. Address string `json:\"address\"` // User's email. Email string `json:\"email\"` } // Required: true说明字段是必须的，生成 Swagger 文档时，也会在文档中声明该字段是必须字段。为了使代码保持简洁，我们在另外一个 Go 包中编写带 go-swagger 注释的 API 文档。假设该 Go 包名字为 docs，在开始编写 Go API 注释之前，需要在 main.go 文件中导入 docs 包： _ \"github.com/marmotedu/gopractise-demo/swagger/docs\" 通过导入 docs 包，可以使 go-swagger 在递归解析 main 包的依赖包时，找到 docs 包，并解析包中的注释。在 gopractise-demo/swagger 目录下，创建 docs 文件夹： $ mkdir docs $ cd docs 在 docs 目录下，创建一个 doc.go 文件，在该文件中提供 API 接口的基本信息： // Package docs awesome. // // Documentation of our awesome API. // // Schemes: http, https // BasePath: / // Version: 0.1.0 // Host: some-url.com // // Consumes: // - application/json // // Produces: // - application/json // // Security: // - basic // // SecurityDefinitions: // basic: // type: basic // // swagger:meta package docs Package docs 后面的字符串 awesome 代表我们的 HTTP 服务名。Documentation of our awesome API是我们 API 的描述。其他都","date":"2022-07-07 15:19:49","objectID":"/iam_basic_func/:6:3","tags":["iam"],"title":"iam_basic_func","uri":"/iam_basic_func/"},{"categories":["iam"],"content":"IAM Swagger 文档 IAM 的 Swagger 文档定义在iam/api/swagger/docs目录下，遵循 go-swagger 规范进行定义。 iam/api/swagger/docs/doc.go文件定义了更多 Swagger 文档的基本信息，比如开源协议、联系方式、安全认证等。 更详细的定义，你可以直接查看 iam/api/swagger/docs 目录下的 Go 源码文件。为了便于生成文档和启动 HTTP 服务查看 Swagger 文档，该操作被放在 Makefile 中执行（位于iam/scripts/make-rules/swagger.mk文件中）： .PHONY: swagger.run swagger.run: tools.verify.swagger @echo \"===========\u003e Generating swagger API docs\" @swagger generate spec --scan-models -w $(ROOT_DIR)/cmd/genswaggertypedocs -o $(ROOT_DIR)/api/swagger/swagger.yaml .PHONY: swagger.serve swagger.serve: tools.verify.swagger @swagger serve -F=redoc --no-open --port 36666 $(ROOT_DIR)/api/swagger/swagger.yaml Makefile 文件说明： tools.verify.swagger：检查 Linux 系统是否安装了 go-swagger 的命令行工具 swagger，如果没有安装则运行 go get 安装。 swagger.run：运行 swagger generate spec 命令生成 Swagger 文档 swagger.yaml，运行前会检查 swagger 是否安装。 –scan-models 指定生成的文档中包含带有 swagger:model 注释的 Go Models。-w 指定 swagger 命令运行的目录。 swagger.serve：运行 swagger serve 命令打开 Swagger 文档 swagger.yaml，运行前会检查 swagger 是否安装。 在 iam 源码根目录下执行如下命令，即可生成并启动 HTTP 服务查看 Swagger 文档： $ make swagger $ make serve-swagger 2020/10/21 06:45:03 serving docs at http://localhost:36666/docs 打开浏览器，打开http://x.x.x.x:36666/docs查看 Swagger 文档，x.x.x.x 是服务器的 IP 地址，如下图所示： IAM 的 Swagger 文档，还可以通过在 iam 源码根目录下执行go generate ./…命令生成，为此，我们需要在 iam/cmd/genswaggertypedocs/swagger_type_docs.go 文件中，添加//go:generate注释。如下图所示： ","date":"2022-07-07 15:19:49","objectID":"/iam_basic_func/:6:4","tags":["iam"],"title":"iam_basic_func","uri":"/iam_basic_func/"},{"categories":["iam"],"content":"总结 在做 Go 服务开发时，我们要向前端或用户提供 API 文档，手动编写 API 文档工作量大，也难以维护。所以，现在很多项目都是自动生成 Swagger 格式的 API 文档。提到 Swagger，很多开发者不清楚其和 OpenAPI 的区别，所以我也给你总结了：OpenAPI 是一个 API 规范，Swagger 则是实现规范的工具。 在 Go 中，用得最多的是利用 go-swagger 来生成 Swagger 格式的 API 文档。go-swagger 包含了很多语法，我们可以访问Swagger 2.0进行学习。学习完 Swagger 2.0 的语法之后，就可以编写 swagger 注释了，之后可以通过 $ swagger generate spec -o swagger.yaml 来生成 swagger 文档 swagger.yaml。通过 $ swagger serve --no-open -F=swagger --port 36666 swagger.yaml 来提供一个前端界面，供我们访问 swagger 文档。为了方便管理，我们可以将 swagger generate spec 和 swagger serve 命令加入到 Makefile 文件中，通过 Makefile 来生成 Swagger 文档，并提供给前端界面。 ","date":"2022-07-07 15:19:49","objectID":"/iam_basic_func/:6:5","tags":["iam"],"title":"iam_basic_func","uri":"/iam_basic_func/"},{"categories":["iam"],"content":"18 | 错误处理（上）：如何设计一套科学的错误码？ 现代的软件架构，很多都是对外暴露 RESTful API 接口，内部系统通信采用 RPC 协议。因为 RESTful API 接口有一些天生的优势，比如规范、调试友好、易懂，所以通常作为直接面向用户的通信规范。 既然是直接面向用户，那么首先就要求消息返回格式是规范的；其次，如果接口报错，还要能给用户提供一些有用的报错信息，通常需要包含 Code 码（用来唯一定位一次错误）和 Message（用来展示出错的信息）。这就需要我们设计一套规范的、科学的错误码。 这一讲，我就来详细介绍下，如何设计一套规范的、科学的错误码。下一讲，我还会介绍如何提供一个 errors 包来支持我们设计的错误码。 ","date":"2022-07-07 15:19:49","objectID":"/iam_basic_func/:7:0","tags":["iam"],"title":"iam_basic_func","uri":"/iam_basic_func/"},{"categories":["iam"],"content":"期望错误码实现的功能 要想设计一套错误码，首先就得弄清我们的需求。 RESTful API 是基于 HTTP 协议的一系列 API 开发规范，HTTP 请求结束后，无论 API 请求成功或失败，都需要让客户端感知到，以便客户端决定下一步该如何处理。为了让用户拥有最好的体验，需要有一个比较好的错误码实现方式。这里我介绍下在设计错误码时，期望能够实现的功能。 第一个功能是有业务 Code 码标识。因为 HTTP Code 码有限，并且都是跟 HTTP Transport 层相关的 Code 码，所以我们希望能有自己的错误 Code 码。一方面，可以根据需要自行扩展，另一方面也能够精准地定位到具体是哪个错误。同时，因为 Code 码通常是对计算机友好的 10 进制整数，基于 Code 码，计算机也可以很方便地进行一些分支处理。当然了，业务码也要有一定规则，可以通过业务码迅速定位出是哪类错误。 第二个功能，考虑到安全，希望能够对外对内分别展示不同的错误信息。当开发一个对外的系统，业务出错时，需要一些机制告诉用户出了什么错误，如果能够提供一些帮助文档会更好。但是，我们不可能把所有的错误都暴露给外部用户，这不仅没必要，也不安全。所以也需要能让我们获取到更详细的内部错误信息的机制，这些内部错误信息可能包含一些敏感的数据，不宜对外展示，但可以协助我们进行问题定位。 所以，我们需要设计的错误码应该是规范的，能方便客户端感知到 HTTP 是否请求成功，并带有业务码和出错信息。 ","date":"2022-07-07 15:19:49","objectID":"/iam_basic_func/:7:1","tags":["iam"],"title":"iam_basic_func","uri":"/iam_basic_func/"},{"categories":["iam"],"content":"常见的错误码设计方式 在业务中，大致有三种错误码实现方式。我用一次因为用户账号没有找到而请求失败的例子，分别给你解释一下： 第一种方式，不论请求成功或失败，始终返回200 http status code，在 HTTP Body 中包含用户账号没有找到的错误信息。例如 Facebook API 的错误 Code 设计，始终返回 200 http status code： { \"error\": { \"message\": \"Syntax error \\\"Field picture specified more than once. This is only possible before version 2.1\\\" at character 23: id,name,picture,picture\", \"type\": \"OAuthException\", \"code\": 2500, \"fbtrace_id\": \"xxxxxxxxxxx\" } } 采用固定返回200 http status code的方式，有其合理性。比如，HTTP Code 通常代表 HTTP Transport 层的状态信息。当我们收到 HTTP 请求，并返回时，HTTP Transport 层是成功的，所以从这个层面上来看，HTTP Status 固定为 200 也是合理的。 但是这个方式的缺点也很明显：对于每一次请求，我们都要去解析 HTTP Body，从中解析出错误码和错误信息。实际上，大部分情况下，我们对于成功的请求，要么直接转发，要么直接解析到某个结构体中；对于失败的请求，我们也希望能够更直接地感知到请求失败。这种方式对性能会有一定的影响，对客户端不友好。所以我不建议你使用这种方式。 第二种方式，返回http 404 Not Found错误码，并在 Body 中返回简单的错误信息。例如：Twitter API 的错误设计，会根据错误类型，返回合适的 HTTP Code，并在 Body 中返回错误信息和自定义业务 Code。 HTTP/1.1 400 Bad Request x-connection-hash: xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx set-cookie: guest_id=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx Date: Thu, 01 Jun 2017 03:04:23 GMT Content-Length: 62 x-response-time: 5 strict-transport-security: max-age=631138519 Connection: keep-alive Content-Type: application/json; charset=utf-8 Server: tsa_b {\"errors\":[{\"code\":215,\"message\":\"Bad Authentication data.\"}]} 这种方式比第一种要好一些，通过http status code可以使客户端非常直接地感知到请求失败，并且提供给客户端一些错误信息供参考。但是仅仅靠这些信息，还不能准确地定位和解决问题。 第三种方式，返回http 404 Not Found错误码，并在 Body 中返回详细的错误信息。例如：微软 Bing API 的错误设计，会根据错误类型，返回合适的 HTTP Code，并在 Body 中返回详尽的错误信息。 HTTP/1.1 400 Date: Thu, 01 Jun 2017 03:40:55 GMT Content-Length: 276 Connection: keep-alive Content-Type: application/json; charset=utf-8 Server: Microsoft-IIS/10.0 X-Content-Type-Options: nosniff {\"SearchResponse\":{\"Version\":\"2.2\",\"Query\":{\"SearchTerms\":\"api error codes\"},\"Errors\":[{\"Code\":1001,\"Message\":\"Required parameter is missing.\",\"Parameter\":\"SearchRequest.AppId\",\"HelpUrl\":\"http\\u003a\\u002f\\u002fmsdn.microsoft.com\\u002fen-us\\u002flibrary\\u002fdd251042.aspx\"}]}} 这是我比较推荐的一种方式，既能通过http status code使客户端方便地知道请求出错，又可以使用户根据返回的信息知道哪里出错，以及如何解决问题。同时，返回了机器友好的业务 Code 码，可以在有需要时让程序进一步判断处理。 ","date":"2022-07-07 15:19:49","objectID":"/iam_basic_func/:7:2","tags":["iam"],"title":"iam_basic_func","uri":"/iam_basic_func/"},{"categories":["iam"],"content":"错误码设计建议 综合刚才讲到的，我们可以总结出一套优秀的错误码设计思路： 有区别于http status code的业务码，业务码需要有一定规则，可以通过业务码判断出是哪类错误。 请求出错时，可以通过http status code直接感知到请求出错。 需要在请求出错时，返回详细的信息，通常包括 3 类信息：业务 Code 码、错误信息和参考文档（可选）。 返回的错误信息，需要是可以直接展示给用户的安全信息，也就是说不能包含敏感信息；同时也要有内部更详细的错误信息，方便 debug。 返回的数据格式应该是固定的、规范的。 错误信息要保持简洁，并且提供有用的信息。 这里其实还有两个功能点需要我们实现：业务 Code 码设计，以及请求出错时，如何设置http status code。接下来，我会详细介绍下如何实现这两个功能点。 ","date":"2022-07-07 15:19:49","objectID":"/iam_basic_func/:7:3","tags":["iam"],"title":"iam_basic_func","uri":"/iam_basic_func/"},{"categories":["iam"],"content":"业务 Code 码设计 要解决业务 Code 码如何设计这个问题，我们先来看下为什么要引入业务 Code 码。 在实际开发中，引入业务 Code 码有下面几个好处： 可以非常方便地定位问题和定位代码行（看到错误码知道什么意思、grep 错误码可以定位到错误码所在行、某个错误类型的唯一标识）。 错误码包含一定的信息，通过错误码可以判断出错误级别、错误模块和具体错误信息。 Go 中的 HTTP 服务器开发都是引用 net/http 包，该包中只有 60 个错误码，基本都是跟 HTTP 请求相关的错误码，在一个大型系统中，这些错误码完全不够用，而且这些错误码跟业务没有任何关联，满足不了业务的需求。引入业务的 Code 码，则可以解决这些问题。 业务开发过程中，可能需要判断错误是哪种类型，以便做相应的逻辑处理，通过定制的错误可以很容易做到这点，例如： if err == code.ErrBind { ... } 这里要注意，业务 Code 码可以是一个整数，也可以是一个整型字符串，还可以是一个字符型字符串，它是错误的唯一标识。 通过研究腾讯云、阿里云、新浪的开放 API，我发现新浪的 API Code 码设计更合理些。所以，我参考新浪的 Code 码设计，总结出了我推荐的 Code 码设计规范：纯数字表示，不同部位代表不同的服务，不同的模块。 错误代码说明：10010110: 服务。01: 某个服务下的某个模块。01: 模块下的错误码序号，每个模块可以注册 100 个错误。 通过100101可以知道这个错误是服务 A，数据库模块下的记录没有找到错误。 你可能会问：按这种设计，每个模块下最多能注册 100 个错误，是不是有点少？其实在我看来，如果每个模块的错误码超过 100 个，要么说明这个模块太大了，建议拆分；要么说明错误码设计得不合理，共享性差，需要重新设计。 ","date":"2022-07-07 15:19:49","objectID":"/iam_basic_func/:7:4","tags":["iam"],"title":"iam_basic_func","uri":"/iam_basic_func/"},{"categories":["iam"],"content":"如何设置 HTTP Status Code Go net/http 包提供了 60 个错误码，大致分为如下 5 类： 1XX - （指示信息）表示请求已接收，继续处理。 2XX - （请求成功）表示成功处理了请求的状态代码。 3XX - （请求被重定向）表示要完成请求，需要进一步操作。通常，这些状态代码用来重定向。 4XX - （请求错误）这些状态代码表示请求可能出错，妨碍了服务器的处理，通常是客户端出错，需要客户端做进一步的处理。 5XX - （服务器错误）这些状态代码表示服务器在尝试处理请求时发生内部错误。这些错误可能是服务器本身的错误，而不是客户端的问题。 可以看到 HTTP Code 有很多种，如果每个 Code 都做错误映射，会面临很多问题。比如，研发同学不太好判断错误属于哪种http status code，到最后很可能会导致错误或者http status code不匹配，变成一种形式。而且，客户端也难以应对这么多的 HTTP 错误码。 所以，这里建议http status code不要太多，基本上只需要这 3 个 HTTP Code:200 - 表示请求成功执行。400 - 表示客户端出问题。500 - 表示服务端出问题。 如果觉得这 3 个错误码不够用，最多可以加如下 3 个错误码：401 - 表示认证失败。403 - 表示授权失败。404 - 表示资源找不到，这里的资源可以是 URL 或者 RESTful 资源。 将错误码控制在适当的数目内，客户端比较容易处理和判断，开发也比较容易进行错误码映射。 ","date":"2022-07-07 15:19:49","objectID":"/iam_basic_func/:7:5","tags":["iam"],"title":"iam_basic_func","uri":"/iam_basic_func/"},{"categories":["iam"],"content":"IAM 项目错误码设计规范 接下来，我们来看下 IAM 项目的错误码是如何设计的。 Code 设计规范 先来看下 IAM 项目业务的 Code 码设计规范，具体实现可参考internal/pkg/code 目录。IAM 项目的错误码设计规范符合上面介绍的错误码设计思路和规范，具体规范见下。 Code 代码从 100001 开始，1000 以下为 github.com/marmotedu/errors 保留 code。 错误代码说明：100001 服务和模块说明 通用：说明所有服务都适用的错误，提高复用性，避免重复造轮子。 错误信息规范说明 对外暴露的错误，统一大写开头，结尾不要加.。对外暴露的错误要简洁，并能准确说明问题。对外暴露的错误说明，应该是 该怎么做 而不是 哪里错了。 这里你需要注意，错误信息是直接暴露给用户的，不能包含敏感信息。 IAM API 接口返回值说明 如果返回结果中存在 code 字段，则表示调用 API 接口失败。例如： { \"code\": 100101, \"message\": \"Database error\", \"reference\": \"https://github.com/marmotedu/iam/tree/master/docs/guide/zh-CN/faq/iam-apiserver\" } 上述返回中 code 表示错误码，message 表示该错误的具体信息。每个错误同时也对应一个 HTTP 状态码。比如上述错误码对应了 HTTP 状态码 500(Internal Server Error)。另外，在出错时，也返回了reference字段，该字段包含了可以解决这个错误的文档链接地址。关于 IAM 系统支持的错误码，我给你列了一个表格，你可以看看： ","date":"2022-07-07 15:19:49","objectID":"/iam_basic_func/:7:6","tags":["iam"],"title":"iam_basic_func","uri":"/iam_basic_func/"},{"categories":["iam"],"content":"总结 对外暴露的 API 接口需要有一套规范的、科学的错误码。目前业界的错误码大概有 3 种设计方式，我用一次因为用户账号没有找到而请求失败的例子，给你做了解释： 不论请求成功失败，始终返回200 http status code，在 HTTP Body 中包含用户账号没有找到的错误信息。返回http 404 Not Found错误码，并在 Body 中返回简单的错误信息。返回http 404 Not Found错误码，并在 Body 中返回详细的错误信息。 这一讲，我参考这 3 个错误码设计，给出了自己的错误码设计建议：错误码包含 HTTP Code 和业务 Code，并且业务 Code 会映射为一个 HTTP Code。错误码也会对外暴露两种错误信息，一种是直接暴露给用户的，不包含敏感信息的信息；另一种是供内部开发查看，定位问题的错误信息。该错误码还支持返回参考文档，用于在出错时展示给用户，供用户查看解决问题。 ****建议你重点关注我总结的 Code 码设计规范：纯数字表示，不同部位代表不同的服务，不同的模块。 比如错误代码100101，其中 10 代表服务；中间的 01 代表某个服务下的某个模块；最后的 01 代表模块下的错误码序号，每个模块可以注册 100 个错误。 ","date":"2022-07-07 15:19:49","objectID":"/iam_basic_func/:7:7","tags":["iam"],"title":"iam_basic_func","uri":"/iam_basic_func/"},{"categories":["iam"],"content":"19 | 错误处理（下）：如何设计错误包？ 在 Go 项目开发中，错误是我们必须要处理的一个事项。除了我们上一讲学习过的错误码，处理错误也离不开错误包。业界有很多优秀的、开源的错误包可供选择，例如 Go 标准库自带的errors包、github.com/pkg/errors包。但是这些包目前还不支持业务错误码，很难满足生产级应用的需求。所以，在实际开发中，我们有必要开发出适合自己错误码设计的错误包。当然，我们也没必要自己从 0 开发，可以基于一些优秀的包来进行二次封装。这一讲里，我们就来一起看看，如何设计一个错误包来适配上一讲我们设计的错误码，以及一个错误码的具体实现。 ","date":"2022-07-07 15:19:49","objectID":"/iam_basic_func/:8:0","tags":["iam"],"title":"iam_basic_func","uri":"/iam_basic_func/"},{"categories":["iam"],"content":"错误包需要具有哪些功能？ 要想设计一个优秀的错误包，我们首先得知道一个优秀的错误包需要具备哪些功能。在我看来，至少需要有下面这六个功能： 首先，应该能支持错误堆栈。我们来看下面一段代码，假设保存在bad.go文件中： package main import ( \"fmt\" \"log\" ) func main() { if err := funcA(); err != nil { log.Fatalf(\"call func got failed: %v\", err) return } log.Println(\"call func success\") } func funcA() error { if err := funcB(); err != nil { return err } return fmt.Errorf(\"func called error\") } func funcB() error { return fmt.Errorf(\"func called error\") } 执行上面的代码： $ go run bad.go 2021/07/02 08:06:55 call func got failed: func called error exit status 1 这时我们想定位问题，但不知道具体是哪行代码报的错误，只能靠猜，还不一定能猜到。为了解决这个问题，我们可以加一些 Debug 信息，来协助我们定位问题。这样做在测试环境是没问题的，但是在线上环境，一方面修改、发布都比较麻烦，另一方面问题可能比较难重现。这时候我们会想，要是能打印错误的堆栈就好了。例如： 2021/07/02 14:17:03 call func got failed: func called error main.funcB /home/colin/workspace/golang/src/github.com/marmotedu/gopractise-demo/errors/good.go:27 main.funcA /home/colin/workspace/golang/src/github.com/marmotedu/gopractise-demo/errors/good.go:19 main.main /home/colin/workspace/golang/src/github.com/marmotedu/gopractise-demo/errors/good.go:10 runtime.main /home/colin/go/go1.16.2/src/runtime/proc.go:225 runtime.goexit /home/colin/go/go1.16.2/src/runtime/asm_amd64.s:1371 exit status 1 通过上面的错误输出，我们可以很容易地知道是哪行代码报的错，从而极大提高问题定位的效率，降低定位的难度。所以，在我看来，一个优秀的 errors 包，首先需要支持错误堆栈。 其次，能够支持不同的打印格式。例如%+v、%v、%s等格式，可以根据需要打印不同丰富度的错误信息。 再次，能支持 Wrap/Unwrap 功能，也就是在已有的错误上，追加一些新的信息。例如errors.Wrap(err, “open file failed”) 。Wrap 通常用在调用函数中，调用函数可以基于被调函数报错时的错误 Wrap 一些自己的信息，丰富报错信息，方便后期的错误定位，例如： func funcA() error { if err := funcB(); err != nil { return errors.Wrap(err, \"call funcB failed\") } return errors.New(\"func called error\") } func funcB() error { return errors.New(\"func called error\") } 这里要注意，不同的错误类型，Wrap 函数的逻辑也可以不同。另外，在调用 Wrap 时，也会生成一个错误堆栈节点。我们既然能够嵌套 error，那有时候还可能需要获取被嵌套的 error，这时就需要错误包提供Unwrap函数。 还有，错误包应该有Is方法。在实际开发中，我们经常需要判断某个 error 是否是指定的 error。在 Go 1.13 之前，也就是没有 wrapping error 的时候，我们要判断 error 是不是同一个，可以使用如下方法： if err == os.ErrNotExist { // normal code } 但是现在，因为有了 wrapping error，这样判断就会有问题。因为你根本不知道返回的 err 是不是一个嵌套的 error，嵌套了几层。这种情况下，我们的错误包就需要提供Is函数： func Is(err, target error) bool 当 err 和 target 是同一个，或者 err 是一个 wrapping error 的时候，如果 target 也包含在这个嵌套 error 链中，返回 true，否则返回 fasle。 **另外，错误包应该支持 As 函数。**在 Go 1.13 之前，没有 wrapping error 的时候，我们要把 error 转为另外一个 error，一般都是使用 type assertion 或者 type switch，也就是类型断言。例如： if perr, ok := err.(*os.PathError); ok { fmt.Println(perr.Path) } 但是现在，返回的 err 可能是嵌套的 error，甚至好几层嵌套，这种方式就不能用了。所以，我们可以通过实现 As 函数来完成这种功能。现在我们把上面的例子，用 As 函数实现一下： var perr *os.PathError if errors.As(err, \u0026perr) { fmt.Println(perr.Path) } 这样就可以完全实现类型断言的功能，而且还更强大，因为它可以处理 wrapping error。最后，能够支持两种错误创建方式：非格式化创建和格式化创建。例如： errors.New(\"file not found\") errors.Errorf(\"file %s not found\", \"iam-apiserver\") 上面，我们介绍了一个优秀的错误包应该具备的功能。一个好消息是，Github 上有不少实现了这些功能的错误包，其中github.com/pkg/errors包最受欢迎。所以，我基于github.com/pkg/errors包进行了二次封装，用来支持上一讲所介绍的错误码。 ","date":"2022-07-07 15:19:49","objectID":"/iam_basic_func/:8:1","tags":["iam"],"title":"iam_basic_func","uri":"/iam_basic_func/"},{"categories":["iam"],"content":"错误包实现 明确优秀的错误包应该具备的功能后，我们来看下错误包的实现。实现的源码存放在github.com/marmotedu/errors。 我通过在文件github.com/pkg/errors/errors.go中增加新的withCode结构体，来引入一种新的错误类型，该错误类型可以记录错误码、stack、cause 和具体的错误信息。 type withCode struct { err error // error 错误 code int // 业务错误码 cause error // cause error *stack // 错误堆栈 } 下面，我们通过一个示例，来了解下github.com/marmotedu/errors所提供的功能。假设下述代码保存在errors.go文件中： package main import ( \"fmt\" \"github.com/marmotedu/errors\" code \"github.com/marmotedu/sample-code\" ) func main() { if err := bindUser(); err != nil { // %s: Returns the user-safe error string mapped to the error code or the error message if none is specified. fmt.Println(\"====================\u003e %s \u003c====================\") fmt.Printf(\"%s\\n\\n\", err) // %v: Alias for %s. fmt.Println(\"====================\u003e %v \u003c====================\") fmt.Printf(\"%v\\n\\n\", err) // %-v: Output caller details, useful for troubleshooting. fmt.Println(\"====================\u003e %-v \u003c====================\") fmt.Printf(\"%-v\\n\\n\", err) // %+v: Output full error stack details, useful for debugging. fmt.Println(\"====================\u003e %+v \u003c====================\") fmt.Printf(\"%+v\\n\\n\", err) // %#-v: Output caller details, useful for troubleshooting with JSON formatted output. fmt.Println(\"====================\u003e %#-v \u003c====================\") fmt.Printf(\"%#-v\\n\\n\", err) // %#+v: Output full error stack details, useful for debugging with JSON formatted output. fmt.Println(\"====================\u003e %#+v \u003c====================\") fmt.Printf(\"%#+v\\n\\n\", err) // do some business process based on the error type if errors.IsCode(err, code.ErrEncodingFailed) { fmt.Println(\"this is a ErrEncodingFailed error\") } if errors.IsCode(err, code.ErrDatabase) { fmt.Println(\"this is a ErrDatabase error\") } // we can also find the cause error fmt.Println(errors.Cause(err)) } } func bindUser() error { if err := getUser(); err != nil { // Step3: Wrap the error with a new error message and a new error code if needed. return errors.WrapC(err, code.ErrEncodingFailed, \"encoding user 'Lingfei Kong' failed.\") } return nil } func getUser() error { if err := queryDatabase(); err != nil { // Step2: Wrap the error with a new error message. return errors.Wrap(err, \"get user failed.\") } return nil } func queryDatabase() error { // Step1. Create error with specified error code. return errors.WithCode(code.ErrDatabase, \"user 'Lingfei Kong' not found.\") } 上述代码中，通过WithCode函数来创建新的 withCode 类型的错误；通过WrapC来将一个 error 封装成一个 withCode 类型的错误；通过IsCode来判断一个 error 链中是否包含指定的 code。 withCode 错误实现了一个func (w *withCode) Format(state fmt.State, verb rune)方法，该方法用来打印不同格式的错误信息，见下表： 例如，%+v会打印以下错误信息： get user failed. - #1 [/home/colin/workspace/golang/src/github.com/marmotedu/gopractise-demo/errors/errortrack_errors.go:19 (main.getUser)] (100101) Database error; user 'Lingfei Kong' not found. - #0 [/home/colin/workspace/golang/src/github.com/marmotedu/gopractise-demo/errors/errortrack_errors.go:26 (main.queryDatabase)] (100101) Database error 那么你可能会问，这些错误信息中的100101错误码，还有Database error这种对外展示的报错信息等等，是从哪里获取的？这里我简单解释一下。 首先， withCode 中包含了 int 类型的错误码，例如100101。其次，当使用github.com/marmotedu/errors包的时候，需要调用Register或者MustRegister，将一个 Coder 注册到github.com/marmotedu/errors开辟的内存中，数据结构为： var codes = map[int]Coder{} Coder 是一个接口，定义为： type Coder interface { // HTTP status that should be used for the associated error code. HTTPStatus() int // External (user) facing error text. String() string // Reference returns the detail documents for user. Reference() string // Code returns the code of the coder Code() int } 这样 withCode 的Format方法，就能够通过 withCode 中的 code 字段获取到对应的 Coder，并通过 Coder 提供的 HTTPStatus、String、Reference、Code 函数，来获取 withCode 中 code 的详细信息，最后格式化打印。 这里要注意，我们实现了两个注册函数：Register和MustRegister，二者唯一区别是：当重复定义同一个错误 Code 时，MustRegister会 panic，这样可以防止后面注册的错误覆盖掉之前注册的错误。在实际开发中，建议使用MustRegister。 XXX()和MustXXX()的函数命名方式，是一种 Go 代码设计技巧，在 Go 代码中经常使用，例如 Go 标准库中regexp包提供的Compile和MustCompile函数。和XXX相比，MustXXX 会在某种情况不满足时 panic。因此使用MustXXX的开发者看到函数名就会有一个心理预期：使用不当，会造成程序 panic。 最后，我还有一个建议：在实际的生产环境中，我们可以使用 JSON 格式打印日志，JSON 格式的日志可以非常方便的供日志","date":"2022-07-07 15:19:49","objectID":"/iam_basic_func/:8:2","tags":["iam"],"title":"iam_basic_func","uri":"/iam_basic_func/"},{"categories":["iam"],"content":"如何记录错误？ 上面，我们一起看了怎么设计一个优秀的错误包，那如何用我们设计的错误包来记录错误呢？根据我的开发经验，我推荐两种记录错误的方式，可以帮你快速定位问题。 方式一：通过github.com/marmotedu/errors包提供的错误堆栈能力，来跟踪错误。具体你可以看看下面的代码示例。以下代码保存在errortrack_errors.go中。 package main import ( \"fmt\" \"github.com/marmotedu/errors\" code \"github.com/marmotedu/sample-code\" ) func main() { if err := getUser(); err != nil { fmt.Printf(\"%+v\\n\", err) } } func getUser() error { if err := queryDatabase(); err != nil { return errors.Wrap(err, \"get user failed.\") } return nil } func queryDatabase() error { return errors.WithCode(code.ErrDatabase, \"user 'Lingfei Kong' not found.\") } 执行上述的代码： $ go run errortrack_errors.go get user failed. - #1 [/home/colin/workspace/golang/src/github.com/marmotedu/gopractise-demo/errors/errortrack_errors.go:19 (main.getUser)] (100101) Database error; user 'Lingfei Kong' not found. - #0 [/home/colin/workspace/golang/src/github.com/marmotedu/gopractise-demo/errors/errortrack_errors.go:26 (main.queryDatabase)] (100101) Database error 可以看到，打印的日志中打印出了详细的错误堆栈，包括错误发生的函数、文件名、行号和错误信息，通过这些错误堆栈，我们可以很方便地定位问题。 你使用这种方法时，我推荐的用法是，在错误最开始处使用 errors.WithCode() 创建一个 withCode 类型的错误。上层在处理底层返回的错误时，可以根据需要，使用 Wrap 函数基于该错误封装新的错误信息。如果要包装的 error 不是用github.com/marmotedu/errors包创建的，建议用 errors.WithCode() 新建一个 error。 方式二：在错误产生的最原始位置调用日志包记录函数，打印错误信息，其他位置直接返回（当然，也可以选择性的追加一些错误信息，方便故障定位）。示例代码（保存在errortrack_log.go）如下： package main import ( \"fmt\" \"github.com/marmotedu/errors\" \"github.com/marmotedu/log\" code \"github.com/marmotedu/sample-code\" ) func main() { if err := getUser(); err != nil { fmt.Printf(\"%v\\n\", err) } } func getUser() error { if err := queryDatabase(); err != nil { return err } return nil } func queryDatabase() error { opts := \u0026log.Options{ Level: \"info\", Format: \"console\", EnableColor: true, EnableCaller: true, OutputPaths: []string{\"test.log\", \"stdout\"}, ErrorOutputPaths: []string{}, } log.Init(opts) defer log.Flush() err := errors.WithCode(code.ErrDatabase, \"user 'Lingfei Kong' not found.\") if err != nil { log.Errorf(\"%v\", err) } return err } 执行以上代码： $ go run errortrack_log.go 2021-07-03 14:37:31.597 ERROR errors/errortrack_log.go:41 Database error Database error 当错误发生时，调用 log 包打印错误。通过 log 包的 caller 功能，可以定位到 log 语句的位置，也就是定位到错误发生的位置。你使用这种方式来打印日志时，我有两个建议。 只在错误产生的最初位置打印日志，其他地方直接返回错误，一般不需要再对错误进行封装。当代码调用第三方包的函数时，第三方包函数出错时打印错误信息。比如： if err := os.Chdir(\"/root\"); err != nil { log.Errorf(\"change dir failed: %v\", err) } ","date":"2022-07-07 15:19:49","objectID":"/iam_basic_func/:8:3","tags":["iam"],"title":"iam_basic_func","uri":"/iam_basic_func/"},{"categories":["iam"],"content":"一个错误码的具体实现 接下来，我们看一个依据上一讲介绍的错误码规范的具体错误码实现github.com/marmotedu/sample-code。 sample-code实现了两类错误码，分别是通用错误码（sample-code/base.go）和业务模块相关的错误码（sample-code/apiserver.go）。首先，我们来看通用错误码的定义： // 通用: 基本错误 // Code must start with 1xxxxx const ( // ErrSuccess - 200: OK. ErrSuccess int = iota + 100001 // ErrUnknown - 500: Internal server error. ErrUnknown // ErrBind - 400: Error occurred while binding the request body to the struct. ErrBind // ErrValidation - 400: Validation failed. ErrValidation // ErrTokenInvalid - 401: Token invalid. ErrTokenInvalid ) 在代码中，我们通常使用整型常量（ErrSuccess）来代替整型错误码（100001），因为使用 ErrSuccess 时，一看就知道它代表的错误类型，可以方便开发者使用。 错误码用来指代一个错误类型，该错误类型需要包含一些有用的信息，例如对应的 HTTP Status Code、对外展示的 Message，以及跟该错误匹配的帮助文档。所以，我们还需要实现一个 Coder 来承载这些信息。这里，我们定义了一个实现了github.com/marmotedu/errors.Coder接口的ErrCode结构体： // ErrCode implements `github.com/marmotedu/errors`.Coder interface. type ErrCode struct { // C refers to the code of the ErrCode. C int // HTTP status that should be used for the associated error code. HTTP int // External (user) facing error text. Ext string // Ref specify the reference document. Ref string } 可以看到ErrCode结构体包含了以下信息：int 类型的业务码。对应的 HTTP Status Code。暴露给外部用户的消息。错误的参考文档。 下面是一个具体的 Coder 示例： coder := \u0026ErrCode{ C: 100001, HTTP: 200, Ext: \"OK\", Ref: \"https://github.com/marmotedu/sample-code/blob/master/README.md\", } 接下来，我们就可以调用github.com/marmotedu/errors包提供的Register或者MustRegister函数，将 Coder 注册到github.com/marmotedu/errors包维护的内存中。一个项目有很多个错误码，如果每个错误码都手动调用MustRegister函数会很麻烦，这里我们通过代码自动生成的方法，来生成 register 函数调用： //go:generate codegen -type=int //go:generate codegen -type=int -doc -output ./error_code_generated.md //go:generate codegen -type=int 会调用codegen工具，生成sample_code_generated.go源码文件： func init() { register(ErrSuccess, 200, \"OK\") register(ErrUnknown, 500, \"Internal server error\") register(ErrBind, 400, \"Error occurred while binding the request body to the struct\") register(ErrValidation, 400, \"Validation failed\") // other register function call } 这些register调用放在 init 函数中，在加载程序的时候被初始化。这里要注意，在注册的时候，我们会检查 HTTP Status Code，只允许定义 200、400、401、403、404、500 这 6 个 HTTP 错误码。这里通过程序保证了错误码是符合 HTTP Status Code 使用要求的。 //go:generate codegen -type=int -doc -output ./error_code_generated.md会生成错误码描述文档 error_code_generated.md。当我们提供 API 文档时，也需要记着提供一份错误码描述文档，这样客户端才可以根据错误码，知道请求是否成功，以及具体发生哪类错误，好针对性地做一些逻辑处理。 codegen工具会根据错误码注释生成sample_code_generated.go和error_code_generated.md文件： // ErrSuccess - 200: OK. ErrSuccess int = iota + 100001 codegen 工具之所以能够生成sample_code_generated.go和error_code_generated.md，是因为我们的错误码注释是有规定格式的：// \u003c错误码整型常量\u003e - \u003c对应的HTTP Status Code\u003e: .。codegen 工具可以在 IAM 项目根目录下，执行以下命令来安装： $ make tools.install.codegen 安装完 codegen 工具后，可以在 github.com/marmotedu/sample-code 包根目录下执行 go generate 命令，来生成sample_code_generated.go和error_code_generated.md。这里有个技巧需要你注意：生成的文件建议统一用 xxxx_generated.xx 来命名，这样通过 generated ，我们就知道这个文件是代码自动生成的，有助于我们理解和使用。 在实际的开发中，我们可以将错误码独立成一个包，放在 internal/pkg/code/目录下，这样可以方便整个应用调用。例如 IAM 的错误码就放在 IAM 项目根目录下的internal/pkg/code/目录下。我们的错误码是分服务和模块的，所以这里建议你把相同的服务放在同一个 Go 源文件中，例如 IAM 的错误码存放文件： $ ls base.go apiserver.go authzserver.go apiserver.go authzserver.go base.go 一个应用中会有多个服务，例如 IAM 应用中，就包含了 iam-apiserver、iam-authz-server、iam-pump 三个服务。这些服务有一些通用的错误码，为了便于维护，可以将这些通用的错误码统一放在 base.go 源码文件中。其他的错误码，我们可以按服务分别放在不同的文件中：iam-apiserver 服务的错误码统一放在 apiserver.go 文件中；iam-authz-server 的错误码统一存放在 authzserver.go 文件中。其他服务以此类推。 另外，同一个服务中不同模块的错误码，可以按以下格式来组织：相同模块的错误码放在同一个 const 代码块中，不同模块的错误码放在不同的 const 代码块中。每个 const 代码块的开头注释就是该模块的错误码定义。例如： // iam-apiserver: user errors. const ( // ErrUserNotFound - 404: User not found. ErrUserNotFound int = iota + 110001 // ErrUserAlreadyExist - 400: User already exist. ErrUserAlreadyExist ) // iam-apiserver: secret errors. const ( // ErrEncrypt - 400: Secret reach the max count. ErrReachMaxCount int = iota + 110101 // ErrSecretNotFound - 404: Secret not found. ErrSecretNotFound ) 最后，我们还需要将错误码定义记录在项目的文件中，供开发者查阅、遵守和使用，例如 IAM 项目的错误码定义记录文档为code_specification.md。这个文档中记录了错误码说明、错误描述规范和错误记录规范等。 ","date":"2022-07-07 15:19:49","objectID":"/iam_basic_func/:8:4","tags":["iam"],"title":"iam_basic_func","uri":"/iam_basic_func/"},{"categories":["iam"],"content":"错误码实际使用方法示例 上面，我讲解了错误包和错误码的实现方式，那你一定想知道在实际开发中我们是如何使用的。这里，我就举一个在 gin web 框架中使用该错误码的例子： // Response defines project response format which in marmotedu organization. type Response struct { Code errors.Code `json:\"code,omitempty\"` Message string `json:\"message,omitempty\"` Reference string `json:\"reference,omitempty\"` Data interface{} `json:\"data,omitempty\"` } // WriteResponse used to write an error and JSON data into response. func WriteResponse(c *gin.Context, err error, data interface{}) { if err != nil { coder := errors.ParseCoder(err) c.JSON(coder.HTTPStatus(), Response{ Code: coder.Code(), Message: coder.String(), Reference: coder.Reference(), Data: data, }) } c.JSON(http.StatusOK, Response{Data: data}) } func GetUser(c *gin.Context) { log.Info(\"get user function called.\", \"X-Request-Id\", requestid.Get(c)) // Get the user by the `username` from the database. user, err := store.Client().Users().Get(c.Param(\"username\"), metav1.GetOptions{}) if err != nil { core.WriteResponse(c, errors.WithCode(code.ErrUserNotFound, err.Error()), nil) return } core.WriteResponse(c, nil, user) } 上述代码中，通过WriteResponse统一处理错误。在 WriteResponse 函数中，如果err != nil，则从 error 中解析出 Coder，并调用 Coder 提供的方法，获取错误相关的 Http Status Code、int 类型的业务码、暴露给用户的信息、错误的参考文档链接，并返回 JSON 格式的信息。如果 err == nil 则返回 200 和数据。 ","date":"2022-07-07 15:19:49","objectID":"/iam_basic_func/:8:5","tags":["iam"],"title":"iam_basic_func","uri":"/iam_basic_func/"},{"categories":["iam"],"content":"总结 记录错误是应用程序必须要做的一件事情，在实际开发中，我们通常会封装自己的错误包。一个优秀的错误包，应该能够支持错误堆栈、不同的打印格式、Wrap/Unwrap/Is/As 等函数，并能够支持格式化创建 error。 根据这些错误包设计要点，我基于 github.com/pkg/errors 包设计了 IAM 项目的错误包 github.com/marmotedu/errors ，该包符合我们上一讲设计的错误码规范。另外，本讲也给出了一个具体的错误码实现 sample-code ， sample-code 支持业务 Code 码、HTTP Status Code、错误参考文档、可以对内对外展示不同的错误信息。 最后，因为错误码注释是有固定格式的，所以我们可以通过 codegen 工具解析错误码的注释，生成 register 函数调用和错误码文档。这种做法也体现了我一直强调的 low code 思想，可以提高开发效率，减少人为失误。 ","date":"2022-07-07 15:19:49","objectID":"/iam_basic_func/:8:6","tags":["iam"],"title":"iam_basic_func","uri":"/iam_basic_func/"},{"categories":["iam"],"content":"20 | 日志处理（上）：如何设计日志包并记录日志？ 在做 Go 项目开发时，除了处理错误之外，我们必须要做的另外一件事是记录日志。通过记录日志，可以完成一些基本功能，比如开发、测试期间的 Debug，故障排除，数据分析，监控告警，以及记录发生的事件等。 要实现这些功能，首先我们需要一个优秀的日志包。另外，我还发现不少 Go 项目开发者记录日志很随意，输出的日志并不能有效定位到问题。所以，我们还需要知道怎么更好地记录日志，这就需要一个日志记录规范。 有了优秀的日志包和日志记录规范，我们就能很快地定位到问题，获取足够的信息，并完成后期的数据分析和监控告警，也可以很方便地进行调试了。这一讲，我就来详细介绍下，如何设计日志包和日志记录规范。首先，我们来看下如何设计日志包。 ","date":"2022-07-07 15:19:49","objectID":"/iam_basic_func/:9:0","tags":["iam"],"title":"iam_basic_func","uri":"/iam_basic_func/"},{"categories":["iam"],"content":"如何设计日志包 目前，虽然有很多优秀的开源日志包可供我们选择，但在一个大型系统中，这些开源日志包很可能无法满足我们定制化的需求，这时候我们就需要自己开发日志包。 这些日志包可能是基于某个，或某几个开源的日志包改造而来，也可能是全新开发的日志包。那么在开发日志包时，我们需要实现哪些功能，又如何实现呢？接下来，我们就来详细聊聊。先来看下日志包需要具备哪些功能。根据功能的重要性，我将日志包需要实现的功能分为基础功能、高级功能和可选功能。基础功能是一个日志包必须要具备的功能；高级功能、可选功能都是在特定场景下可增加的功能。我们先来说基础功能。 ","date":"2022-07-07 15:19:49","objectID":"/iam_basic_func/:9:1","tags":["iam"],"title":"iam_basic_func","uri":"/iam_basic_func/"},{"categories":["iam"],"content":"基础功能 基础功能，是优秀日志包必备的功能，能够满足绝大部分的使用场景，适合一些中小型的项目。一个日志包应该具备以下 4 个基础功能。 支持基本的日志信息 日志包需要支持基本的日志信息，包括时间戳、文件名、行号、日志级别和日志信息。时间戳可以记录日志发生的时间。在定位问题时，我们往往需要根据时间戳，来复原请求过程，核对相同时间戳下的上下文，从而定位出问题。 文件名和行号，可以使我们更快速定位到打印日志的位置，找到问题代码。一个日志库如果不支持文件名和行号，排查故障就会变得非常困难，基本只能靠 grep 和记忆来定位代码。对于企业级的服务，需要保证服务在故障后能够快速恢复，恢复的时间越久，造成的损失就越大，影响就越大。这就要求研发人员能够快速定位并解决问题。通过文件名和行号，我们可以精准定位到问题代码，尽快地修复问题并恢复服务。 通过日志级别，可以知道日志的错误类型，最通常的用法是：直接过滤出 Error 级别的日志，这样就可以直接定位出问题出错点，然后再结合其他日志定位出出错的原因。如果不支持日志级别，在定位问题时，可能要查看一大堆无用的日志。在大型系统中，一次请求的日志量很多，这会大大延长我们定位问题的时间。而通过日志信息，我们可以知道错误发生的具体原因。 支持不同的日志级别 不同的日志级别代表不同的日志类型，例如：Error 级别的日志，说明日志是错误类型，在排障时，会首先查看错误级别的日志。Warn 级别日志说明出现异常，但还不至于影响程序运行，如果程序执行的结果不符合预期，则可以参考 Warn 级别的日志，定位出异常所在。Info 级别的日志，可以协助我们 Debug，并记录一些有用的信息，供后期进行分析。 通常一个日志包至少要实现 6 个级别，我给你提供了一张表格，按优先级从低到高排列如下： 有些日志包，例如 logrus，还支持 Trace 日志级别。Trace 级别比 Debug 级别还低，能够打印更细粒度的日志信息。在我看来，Trace 级别不是必须的，你可以根据需要自行选择。 打印日志时，一个日志调用其实具有两个属性： 输出级别：打印日志时，我们期望日志的输出级别。例如，我们调用 glog.Info(“This is info message”) 打印一条日志，则输出日志级别为 Info。 开关级别：启动应用程序时，期望哪些输出级别的日志被打印。例如，使用 glog 时 -v=4 ，说明了只有日志级别高于 4 的日志才会被打印。 如果开关级别设置为 L ，只有输出级别 \u003e=L 时，日志才会被打印。例如，开关级别为 Warn，则只会记录 Warn、Error 、Panic 和 Fatal 级别的日志。具体的输出关系如下图所示： 支持自定义配置 不同的运行环境，需要不同的日志输出配置，例如：开发测试环境为了能够方便地 Debug，需要设置日志级别为 Debug 级别；现网环境为了提高应用程序的性能，则需要设置日志级别为 Info 级别。又比如，现网环境为了方便日志采集，通常会输出 JSON 格式的日志；开发测试环境为了方便查看日志，会输出 TEXT 格式的日志。 所以，我们的日志包需要能够被配置，还要不同环境采用不同的配置。通过配置，可以在不重新编译代码的情况下，改变记录日志的行为。 支持输出到标准输出和文件 日志总是要被读的，要么输出到标准输出，供开发者实时读取，要么保存到文件，供开发者日后查看。输出到标准输出和保存到文件是一个日志包最基本的功能。 ","date":"2022-07-07 15:19:49","objectID":"/iam_basic_func/:9:2","tags":["iam"],"title":"iam_basic_func","uri":"/iam_basic_func/"},{"categories":["iam"],"content":"高级功能 除了上面提到的这些基本功能外，在一些大型系统中，通常还会要求日志包具备一些高级功能。这些高级功能可以帮我们更好地记录日志，并实现更丰富的功能，例如日志告警。那么一个日志包可以具备哪些高级功能呢？ 支持多种日志格式 日志格式也是我们要考虑的一个点，一个好的日志格式，不仅方便查看日志，还能方便一些日志采集组件采集日志，并对接类似 Elasticsearch 这样的日志搜索引擎。 一个日志包至少需要提供以下两种格式： TEXT 格式：TEXT 格式的日志具有良好的可读性，可以方便我们在开发联调阶段查看日志，例如： 2020-12-02T01:16:18+08:00 INFO example.go:11 std log 2020-12-02T01:16:18+08:00 DEBUG example.go:13 change std log to debug level JSON 格式：JSON 格式的日志可以记录更详细的信息，日志中包含一些通用的或自定义的字段，可供日后的查询、分析使用，而且可以很方便地供 filebeat、logstash 这类日志采集工具采集并上报。下面是 JSON 格式的日志： {\"level\":\"DEBUG\",\"time\":\"2020-12-02T01:16:18+08:00\",\"file\":\"example.go:15\",\"func\":\"main.main\",\"message\":\"log in json format\"} {\"level\":\"INFO\",\"time\":\"2020-12-02T01:16:18+08:00\",\"file\":\"example.go:16\",\"func\":\"main.main\",\"message\":\"another log in json format\"} 我建议在开发联调阶段使用 TEXT 格式的日志，在现网环境使用 JSON 格式的日志。一个优秀的日志库，例如 logrus，除了提供基本的输出格式外，还应该允许开发者自定义日志输出格式。 能够按级别分类输出 为了能够快速定位到需要的日志，一个比较好的做法是将日志按级别分类输出，至少错误级别的日志可以输出到独立的文件中。这样，出现问题时，可以直接查找错误文件定位问题。例如，glog 就支持分类输出，如下图所示： 支持结构化日志 结构化日志（Structured Logging），就是使用 JSON 或者其他编码方式使日志结构化，这样可以方便后续使用 Filebeat、Logstash Shipper 等各种工具，对日志进行采集、过滤、分析和查找。就像下面这个案例，使用 zap 进行日志打印： package main import ( \"time\" \"go.uber.org/zap\" ) func main() { logger, _ := zap.NewProduction() defer logger.Sync() // flushes buffer, if any url := \"http://marmotedu.com\" // 结构化日志打印 logger.Sugar().Infow(\"failed to fetch URL\", \"url\", url, \"attempt\", 3, \"backoff\", time.Second) // 非结构化日志打印 logger.Sugar().Infof(\"failed to fetch URL: %s\", url) } 上述代码输出为： {\"level\":\"info\",\"ts\":1607303966.9903321,\"caller\":\"zap/structured_log.go:14\",\"msg\":\"failed to fetch URL\",\"url\":\"http://marmotedu.com\",\"attempt\":3,\"backoff\":1} {\"level\":\"info\",\"ts\":1607303966.9904354,\"caller\":\"zap/structured_log.go:17\",\"msg\":\"failed to fetch URL: http://marmotedu.com\"} 支持日志轮转 在一个大型项目中，一天可能会产生几十个 G 的日志。为了防止日志把磁盘空间占满，导致服务器或者程序异常，就需要确保日志大小达到一定量级时，对日志进行切割、压缩，并转存。 如何切割呢？你可以按照日志大小进行切割，也可以按日期切割。日志的切割、压缩和转存功能可以基于 GitHub 上一些优秀的开源包来封装，例如：lumberjack可以支持按大小和日期归档日志，file-rotatelogs支持按小时数进行日志切割。 对于日志轮转功能，其实我不建议在日志包中添加，因为这会增加日志包的复杂度，我更建议的做法是借助其他的工具来实现日志轮转。例如，在 Linux 系统中可以使用 Logrotate 来轮转日志。Logrotate 功能强大，是一个专业的日志轮转工具。 具备 Hook 能力 Hook 能力可以使我们对日志内容进行自定义处理。例如，当某个级别的日志产生时，发送邮件或者调用告警接口进行告警。很多优秀的开源日志包提供了 Hook 能力，例如 logrus 和 zap。 在一个大型系统中，日志告警是非常重要的功能，但更好的实现方式是将告警能力做成旁路功能。通过旁路功能，可以保证日志包功能聚焦、简洁。例如：可以将日志收集到 Elasticsearch，并通过 ElastAlert 进行日志告警。 ","date":"2022-07-07 15:19:49","objectID":"/iam_basic_func/:9:3","tags":["iam"],"title":"iam_basic_func","uri":"/iam_basic_func/"},{"categories":["iam"],"content":"可选功能 除了基础功能和高级功能外，还有一些功能。这些功能不会影响到日志包的核心功能，但是如果具有这些功能，会使日志包更加易用。比如下面的这三个功能。 支持颜色输出 在开发、测试时开启颜色输出，不同级别的日志会被不同颜色标识，这样我们可以很轻松地发现一些 Error、Warn 级别的日志，方便开发调试。发布到生产环境时，可以关闭颜色输出，以提高性能。 兼容标准库 log 包 一些早期的 Go 项目大量使用了标准库 log 包，如果我们的日志库能够兼容标准库 log 包，我们就可以很容易地替换掉标准库 log 包。例如，logrus 就兼容标准库 log 包。这里，我们来看一个使用了标准库 log 包的代码： package main import ( \"log\" ) func main() { log.Print(\"call Print: line1\") log.Println(\"call Println: line2\") } 只需要使用log “github.com/sirupsen/logrus\"替换\"log\"就可以完成标准库 log 包的切换： package main import ( log \"github.com/sirupsen/logrus\" ) func main() { log.Print(\"call Print: line1\") log.Println(\"call Println: line2\") } 支持输出到不同的位置 在分布式系统中，一个服务会被部署在多台机器上，这时候如果我们要查看日志，就需要分别登录不同的机器查看，非常麻烦。我们更希望将日志统一投递到 Elasticsearch 上，在 Elasticsearch 上查看日志。 我们还可能需要从日志中分析某个接口的调用次数、某个用户的请求次数等信息，这就需要我们能够对日志进行处理。一般的做法是将日志投递到 Kafka，数据处理服务消费 Kafka 中保存的日志，从而分析出调用次数等信息。 以上两种场景，分别需要把日志投递到 Elasticsearch、Kafka 等组件，如果我们的日志包支持将日志投递到不同的目的端，那会是一项非常让人期待的功能： 如果日志不支持投递到不同的下游组件，例如 Elasticsearch、Kafka、Fluentd、Logstash 等位置，也可以通过 Filebeat 采集磁盘上的日志文件，进而投递到下游组件。 ","date":"2022-07-07 15:19:49","objectID":"/iam_basic_func/:9:4","tags":["iam"],"title":"iam_basic_func","uri":"/iam_basic_func/"},{"categories":["iam"],"content":"设计日志包时需要关注的点 上面，我们介绍了日志包具备的功能，这些功能可以指导我们完成日志包设计。这里，我们再来看下设计日志包时，我们还需要关注的几个层面： 高性能：因为我们要在代码中频繁调用日志包，记录日志，所以日志包的性能是首先要考虑的点，一个性能很差的日志包必然会导致整个应用性能很差。 并发安全：Go 应用程序会大量使用 Go 语言的并发特性，也就意味着需要并发地记录日志，这就需要日志包是并发安全的。 插件化能力：日志包应该能提供一些插件化的能力，比如允许开发者自定义输出格式，自定义存储位置，自定义错误发生时的行为（例如 告警、发邮件等）。插件化的能力不是必需的，因为日志自身的特性就能满足绝大部分的使用需求，例如：输出格式支持 JSON 和 TEXT，存储位置支持标准输出和文件，日志监控可以通过一些旁路系统来实现。 日志参数控制：日志包应该能够灵活地进行配置，初始化时配置或者程序运行时配置。例如：初始化配置可以通过 Init 函数完成，运行时配置可以通过 SetOptions / SetLevel 等函数来完成。 ","date":"2022-07-07 15:19:49","objectID":"/iam_basic_func/:9:5","tags":["iam"],"title":"iam_basic_func","uri":"/iam_basic_func/"},{"categories":["iam"],"content":"如何记录日志？ 前面我们介绍了在设计日志包时，要包含的一些功能、实现方法和注意事项。但在这个过程中，还有一项重要工作需要注意，那就是日志记录问题。 日志并不是越多越好，在实际开发中，经常会遇到一大堆无用的日志，却没有我们需要的日志；或者有效的日志被大量无用的日志淹没，查找起来非常困难。一个优秀的日志包可以协助我们更好地记录、查看和分析日志，但是如何记录日志决定了我们能否获取到有用的信息。日志包是工具，日志记录才是灵魂。这里，我就来详细讲讲如何记录日志。 想要更好地记录日志，我们需要解决以下几个问题：在何处打印日志？在哪个日志级别打印日志？如何记录日志内容？ 在何处打印日志？ 日志主要是用来定位问题的，所以整体来说，我们要在有需要的地方打印日志。那么具体是哪些地方呢？我给你几个建议。 在分支语句处打印日志。在分支语句处打印日志，可以判断出代码走了哪个分支，有助于判断请求的下一跳，继而继续排查问题。 写操作必须打印日志。写操作最可能会引起比较严重的业务故障，写操作打印日志，可以在出问题时找到关键信息。 在循环中打印日志要慎重。如果循环次数过多，会导致打印大量的日志，严重拖累代码的性能，建议的办法是在循环中记录要点，在循环外面总结打印出来。 在错误产生的最原始位置打印日志。对于嵌套的 Error，可在 Error 产生的最初位置打印 Error 日志，上层如果不需要添加必要的信息，可以直接返回下层的 Error。我给你举个例子： package main import ( \"flag\" \"fmt\" \"github.com/golang/glog\" ) func main() { flag.Parse() defer glog.Flush() if err := loadConfig(); err != nil { glog.Error(err) } } func loadConfig() error { return decodeConfig() // 直接返回 } func decodeConfig() error { if err := readConfig(); err != nil { return fmt.Errorf(\"could not decode configuration data for user %s: %v\", \"colin\", err) // 添加必要的信息，用户名称 } return nil } func readConfig() error { glog.Errorf(\"read: end of input.\") return fmt.Errorf(\"read: end of input\") } 通过在最初产生错误的位置打印日志，我们可以很方便地追踪到日志的根源，进而在上层追加一些必要的信息。这可以让我们了解到该错误产生的影响，有助于排障。另外，直接返回下层日志，还可以减少重复的日志打印。当代码调用第三方包的函数，且第三方包函数出错时，会打印错误信息。比如： if err := os.Chdir(\"/root\"); err != nil { log.Errorf(\"change dir failed: %v\", err) } 在哪个日志级别打印日志？ 不同级别的日志，具有不同的意义，能实现不同的功能，在开发中，我们应该根据目的，在合适的级别记录日志，这里我同样给你一些建议。 Debug 级别 为了获取足够的信息进行 Debug，通常会在 Debug 级别打印很多日志。例如，可以打印整个 HTTP 请求的请求 Body 或者响应 Body。 Debug 级别需要打印大量的日志，这会严重拖累程序的性能。并且，Debug 级别的日志，主要是为了能在开发测试阶段更好地 Debug，多是一些不影响现网业务的日志信息。所以，对于 Debug 级别的日志，在服务上线时我们一定要禁止掉。否则，就可能会因为大量的日志导致硬盘空间快速用完，从而造成服务宕机，也可能会影响服务的性能和产品体验。 Debug 这个级别的日志可以随意输出，任何你觉得有助于开发、测试阶段调试的日志，都可以在这个级别打印。 Info 级别 Info 级别的日志可以记录一些有用的信息，供以后的运营分析，所以 Info 级别的日志不是越多越好，也不是越少越好，应以满足需求为主要目标。一些关键日志，可以在 Info 级别记录，但如果日志量大、输出频度过高，则要考虑在 Debug 级别记录。 现网的日志级别一般是 Info 级别，为了不使日志文件占满整个磁盘空间，在记录日志时，要注意避免产生过多的 Info 级别的日志。例如，在 for 循环中，就要慎用 Info 级别的日志。 Warn 级别 一些警告类的日志可以记录在 Warn 级别，Warn 级别的日志往往说明程序运行异常，不符合预期，但又不影响程序的继续运行，或者是暂时影响，但后续会恢复。像这些日志，就需要你关注起来。Warn 更多的是业务级别的警告日志。 Error 级别 Error 级别的日志告诉我们程序执行出错，这些错误肯定会影响到程序的执行结果，例如请求失败、创建资源失败等。要记录每一个发生错误的日志，避免日后排障过程中这些错误被忽略掉。大部分的错误可以归在 Error 级别。 Panic 级别 Panic 级别的日志在实际开发中很少用，通常只在需要错误堆栈，或者不想因为发生严重错误导致程序退出，而采用 defer 处理错误时使用。 Fatal 级别 Fatal 是最高级别的日志，这个级别的日志说明问题已经相当严重，严重到程序无法继续运行，通常是系统级的错误。在开发中也很少使用，除非我们觉得某个错误发生时，整个程序无法继续运行。 这里用一张图来总结下，如何选择 Debug、Info、Warn、Error、Panic、Fatal 这几种日志级别。 如何记录日志内容？ 关于如何记录日志内容，我有几条建议： 在记录日志时，不要输出一些敏感信息，例如密码、密钥等。 为了方便调试，通常会在 Debug 级别记录一些临时日志，这些日志内容可以用一些特殊的字符开头，例如 log.Debugf(“XXXXXXXXXXXX-1:Input key was: %s”, setKeyName) 。这样，在完成调试后，可以通过查找 XXXXXXXXXXXX 字符串，找到这些临时日志，在 commit 前删除。 日志内容应该小写字母开头，以英文点号 . 结尾，例如 log.Info(“update user function called.\") 。 为了提高性能，尽可能使用明确的类型，例如使用 log.Warnf(“init datastore: %s”, err.Error()) 而非 log.Warnf(“init datastore: %v”, err) 。 根据需要，日志最好包含两个信息。一个是请求 ID（RequestID），是每次请求的唯一 ID，便于从海量日志中过滤出某次请求的日志，可以将请求 ID 放在请求的通用日志字段中。另一个是用户和行为，用于标识谁做了什么。 不要将日志记录在错误的日志级别上。例如，我在项目开发中，经常会发现有同事将正常的日志信息打印在 Error 级别，将错误的日志信息打印在 Info 级别。 ","date":"2022-07-07 15:19:49","objectID":"/iam_basic_func/:9:6","tags":["iam"],"title":"iam_basic_func","uri":"/iam_basic_func/"},{"categories":["iam"],"content":"记录日志的“最佳”实践总结 关于日志记录问题，我从以上三个层面给你讲解了。综合来说，对于日志记录的最佳实践，你在平时都可以注意或进行尝试，我把这些重点放在这里，方便你后续查阅。 开发调试、现网故障排障时，不要遗忘一件事情：根据排障的过程优化日志打印。好的日志，可能不是一次就可以写好的，可以在实际开发测试，还有现网定位问题时，不断优化。但这需要你重视日志，而不是把日志仅仅当成记录信息的一种方式，甚至不知道为什么打印一条 Info 日志。 打印日志要“不多不少”，避免打印没有作用的日志，也不要遗漏关键的日志信息。最好的信息是，仅凭借这些关键的日志就能定位到问题。 支持动态日志输出，方便线上问题定位。 总是将日志记录在本地文件：通过将日志记录在本地文件，可以和日志中心化平台进行解耦，这样当网络不可用，或者日志中心化平台故障时，仍然能够正常的记录日志。 集中化日志存储处理：因为应用可能包含多个服务，一个服务包含多个实例，为了查看日志方便，最好将这些日志统一存储在同一个日志平台上，例如 Elasticsearch，方便集中管理和查看日志。 结构化日志记录：添加一些默认通用的字段到每行日志，方便日志查询和分析。 支持 RequestID：使用 RequestID 串联一次请求的所有日志，这些日志可能分布在不同的组件，不同的机器上。支持 RequestID 可以大大提高排障的效率，降低排障难度。在一些大型分布式系统中，没有 RequestID 排障简直就是灾难。 支持动态开关 Debug 日志：对于定位一些隐藏得比较深的问题，可能需要更多的信息，这时候可能需要打印 Debug 日志。但现网的日志级别会设置为 Info 级别，为了获取 Debug 日志，我们可能会修改日志级别为 Debug 级别并重启服务，定位完问题后，再修改日志级别为 Info 级别，然后再重启服务，这种方式不仅麻烦而且还可能会对现网业务造成影响，最好的办法是能够在请求中通过 debug=true 这类参数动态控制某次请求是否开启 Debug 日志。 ","date":"2022-07-07 15:19:49","objectID":"/iam_basic_func/:9:7","tags":["iam"],"title":"iam_basic_func","uri":"/iam_basic_func/"},{"categories":["iam"],"content":"拓展内容：分布式日志解决方案（EFK/ELK） 前面我们介绍了设计日志包和记录日志的规范，除此之外，还有一个问题你应该了解，那就是：我们记录的日志如何收集、处理和展示。 在实际 Go 项目开发中，为了实现高可用，同一个服务至少需要部署两个实例，通过轮询的负载均衡策略转发请求。另外，一个应用又可能包含多个服务。假设我们的应用包含两个服务，每个服务部署两个实例，如果应用出故障，我们可能需要登陆 4（2 x 2）台服务器查看本地的日志文件，定位问题，非常麻烦，会增加故障恢复时间。所以在真实的企业场景中，我们会将这些日志统一收集并展示。 在业界，日志的收集、处理和展示，早已经有了一套十分流行的日志解决方案：EFK（Elasticsearch + Filebeat + Kibana）或者 ELK（Elasticsearch + Logstash + Kibana），EFK 可以理解为 ELK 的演进版，把日志收集组件从 Logstash 替换成了 Filebeat。用 Filebeat 替换 Logstash，主要原因是 Filebeat 更轻量级，占用的资源更少。关于日志处理架构，你可以参考这张图。 通过 log 包将日志记录在本地文件中（*.log 文件），再通过 Shipper 收集到 Kafka 中。Shipper 可以根据需要灵活选择，常见的 Shipper 有 Logstash Shipper、Flume、Fluentd、Filebeat。其中 Filebeat 和 Logstash Shipper 用得最多。Shipper 没有直接将日志投递到 Logstash indexer，或者 Elasticsearch，是因为 Kafka 能够支持更大的吞吐量，起到削峰填谷的作用。 Kafka 中的日志消息会被 Logstash indexer 消费，处理后投递到 Elasticsearch 中存储起来。Elasticsearch 是实时全文搜索和分析引擎，提供搜集、分析、存储数据三大功能。Elasticsearch 中存储的日志，可以通过 Kibana 提供的图形界面来展示。Kibana 是一个基于 Web 的图形界面，用于搜索、分析和可视化存储在 Elasticsearch 中的日志数据。 Logstash 负责采集、转换和过滤日志。它支持几乎任何类型的日志，包括系统日志、错误日志和自定义应用程序日志。Logstash 又分为 Logstash Shipper 和 Logstash indexer。其中，Logstash Shipper 监控并收集日志，并将日志内容发送到 Logstash indexer，然后 Logstash indexer 过滤日志，并将日志提交给 Elasticsearch。 ","date":"2022-07-07 15:19:49","objectID":"/iam_basic_func/:9:8","tags":["iam"],"title":"iam_basic_func","uri":"/iam_basic_func/"},{"categories":["iam"],"content":"总结 记录日志，是应用程序必备的功能。记录日志最大的作用是排障，如果想更好地排障，我们需要一个优秀的工具，日志包。那么如何设计日志包呢？首先我们需要知道日志包的功能，在我看来日志包需要具备以下功能： 基础功能：支持基本的日志信息、支持不同的日志级别、支持自定义配置、支持输出到标准输出和文件。 高级功能：支持多种日志格式、能够按级别分类输出、支持结构化日志、支持日志轮转、具备 Hook 能力。 可选功能：支持颜色输出、兼容标准库 log 包、支持输出到不同的位置。 另外，一个日志包还需要支持不同级别的日志，按日志级别优先级从低到高分别是：Trace \u003c Debug \u003c Info \u003c Warn/Warning \u003c Error \u003c Panic \u003c Fatal。其中 Debug、Info、Warn、Error、Fatal 是比较基础的级别，建议在开发一个日志包时包含这些级别。Trace、Panic 是可选的级别。 在我们掌握了日志包的功能之后，就可以设计、开发日志包了。但我们在开发过程中，还需要确保我们的日志包具有比较高的性能、并发安全、支持插件化的能力，并支持日志参数控制。有了日志包，我们还需要知道如何更好地使用日志包，也就是如何记录日志。在文中，我给出了一些记录建议，内容比较多，你可以返回文中查看。 最后，我还给出了分布式日志解决方案：EFK/ELK。EFK 是 ELK 的升级版，在实际项目开发中，我们可以直接选择 EFK。在 EFK 方案中，通过 Filebeat 将日志上传到 Kafka，Logstash indexer 消费 Kafka 中的日志，并投递到 Elasticsearch 中存储起来，最后通过 Kibana 图形界面来查看日志。 ","date":"2022-07-07 15:19:49","objectID":"/iam_basic_func/:9:9","tags":["iam"],"title":"iam_basic_func","uri":"/iam_basic_func/"},{"categories":["iam"],"content":"21 | 日志处理（下）：手把手教你从 0 编写一个日志包 在实际开发中，我们可以选择一些优秀的开源日志包，不加修改直接拿来使用。但更多时候，是基于一个或某几个优秀的开源日志包进行二次开发。想要开发或者二次开发一个日志包，就要掌握日志包的实现方式。那么这一讲中，我来带你从 0 到 1，实现一个具备基本功能的日志包，让你从中一窥日志包的实现原理和实现方法。在开始实战之前，我们先来看下目前业界有哪些优秀的开源日志包。 ","date":"2022-07-07 15:19:49","objectID":"/iam_basic_func/:10:0","tags":["iam"],"title":"iam_basic_func","uri":"/iam_basic_func/"},{"categories":["iam"],"content":"有哪些优秀的开源日志包？ 在 Go 项目开发中，我们可以通过修改一些优秀的开源日志包，来实现项目的日志包。Go 生态中有很多优秀的开源日志包，例如标准库 log 包、glog、logrus、zap、seelog、zerolog、log15、apex/log、go-logging 等。其中，用得比较多的是标准库 log 包、glog、logrus 和 zap。 为了使你了解开源日志包的现状，接下来我会简单介绍下这几个常用的日志包。至于它们的具体使用方法，你可以参考我整理的一篇文章：优秀开源日志包使用教程。 标准库 log 包 标准库 log 包的功能非常简单，只提供了 Print、Panic 和 Fatal 三类函数用于日志输出。因为是标准库自带的，所以不需要我们下载安装，使用起来非常方便。标准库 log 包只有不到 400 行的代码量，如果你想研究如何实现一个日志包，阅读标准库 log 包是一个不错的开始。Go 的标准库大量使用了 log 包，例如net/http 、 net/rpc 等。 glog glog是 Google 推出的日志包，跟标准库 log 包一样，它是一个轻量级的日志包，使用起来简单方便。但 glog 比标准库 log 包提供了更多的功能，它具有如下特性： 支持 4 种日志级别：Info、Warning、Error、Fatal。 支持命令行选项，例如-alsologtostderr、-log_backtrace_at、-log_dir、-logtostderr、-v等，每个参数实现某种功能。 支持根据文件大小切割日志文件。 支持日志按级别分类输出。 支持 V level。V level 特性可以使开发者自定义日志级别。 支持 vmodule。vmodule 可以使开发者对不同的文件使用不同的日志级别。 支持 traceLocation。traceLocation 可以打印出指定位置的栈信息。 Kubernetes 项目就使用了基于 glog 封装的 klog，作为其日志库。 logrus logrus是目前 GitHub 上 star 数量最多的日志包，它的优点是功能强大、性能高效、高度灵活，还提供了自定义插件的功能。很多优秀的开源项目，例如 Docker、Prometheus 等，都使用了 logrus。除了具有日志的基本功能外，logrus 还具有如下特性： 支持常用的日志级别。logrus 支持 Debug、Info、Warn、Error、Fatal 和 Panic 这些日志级别。 可扩展。logrus 的 Hook 机制允许使用者通过 Hook 的方式，将日志分发到任意地方，例如本地文件、标准输出、Elasticsearch、Logstash、Kafka 等。 支持自定义日志格式。logrus 内置了 JSONFormatter 和 TextFormatter 两种格式。除此之外，logrus 还允许使用者通过实现 Formatter 接口，来自定义日志格式。 结构化日志记录。logrus 的 Field 机制允许使用者自定义日志字段，而不是通过冗长的消息来记录日志。 预设日志字段。logrus 的 Default Fields 机制，可以给一部分或者全部日志统一添加共同的日志字段，例如给某次 HTTP 请求的所有日志添加 X-Request-ID 字段。 Fatal handlers。logrus 允许注册一个或多个 handler，当产生 Fatal 级别的日志时调用。当我们的程序需要优雅关闭时，这个特性会非常有用。 zap zap是 uber 开源的日志包，以高性能著称，很多公司的日志包都是基于 zap 改造而来。除了具有日志基本的功能之外，zap 还具有很多强大的特性： 支持常用的日志级别，例如：Debug、Info、Warn、Error、DPanic、Panic、Fatal。 性能非常高。zap 具有非常高的性能，适合对性能要求比较高的场景。 支持针对特定的日志级别，输出调用堆栈。 像 logrus 一样，zap 也支持结构化的目录日志、预设日志字段，也因为支持 Hook 而具有可扩展性。 ","date":"2022-07-07 15:19:49","objectID":"/iam_basic_func/:10:1","tags":["iam"],"title":"iam_basic_func","uri":"/iam_basic_func/"},{"categories":["iam"],"content":"开源日志包选择 上面我介绍了很多日志包，每种日志包使用的场景不同，你可以根据自己的需求，结合日志包的特性进行选择： 标准库 log 包： 标准库 log 包不支持日志级别、日志分割、日志格式等功能，所以在大型项目中很少直接使用，通常用于一些短小的程序，比如用于生成 JWT Token 的 main.go 文件中。标准库日志包也很适合一些简短的代码，用于快速调试和验证。 glog： glog 实现了日志包的基本功能，非常适合一些对日志功能要求不多的小型项目。 logrus： logrus 功能强大，不仅实现了日志包的基本功能，还有很多高级特性，适合一些大型项目，尤其是需要结构化日志记录的项目。 zap： zap 提供了很强大的日志功能，性能高，内存分配次数少，适合对日志性能要求很高的项目。另外，zap 包中的子包 zapcore，提供了很多底层的日志接口，适合用来做二次封装。 举个我自己选择日志包来进行二次开发的例子：我在做容器云平台开发时，发现 Kubernetes 源码中大量使用了 glog，这时就需要日志包能够兼容 glog。于是，我基于 zap 和 zapcore 封装了github.com/marmotedu/iam/pkg/log日志包，这个日志包可以很好地兼容 glog。 在实际项目开发中，你可以根据项目需要，从上面几个日志包中进行选择，直接使用，但更多时候，你还需要基于这些包来进行定制开发。为了使你更深入地掌握日志包的设计和开发，接下来，我会从 0 到 1 带你开发一个日志包。 ","date":"2022-07-07 15:19:49","objectID":"/iam_basic_func/:10:2","tags":["iam"],"title":"iam_basic_func","uri":"/iam_basic_func/"},{"categories":["iam"],"content":"从 0 编写一个日志包 接下来，我会向你展示如何快速编写一个具备基本功能的日志包，让你通过这个简短的日志包实现掌握日志包的核心设计思路。该日志包主要实现以下几个功能： 支持自定义配置。 支持文件名和行号。 支持日志级别 Debug、Info、Warn、Error、Panic、Fatal。 支持输出到本地文件和标准输出。 支持 JSON 和 TEXT 格式的日志输出，支持自定义日志格式。 支持选项模式。 日志包名称为cuslog，示例项目完整代码存放在 cuslog。具体实现分为以下四个步骤： 定义：定义日志级别和日志选项。 创建：创建 Logger 及各级别日志打印方法。 写入：将日志输出到支持的输出中。 自定义：自定义日志输出格式。 定义日志级别和日志选项 一个基本的日志包，首先需要定义好日志级别和日志选项。本示例将定义代码保存在options.go文件中。 type Level uint8 const ( DebugLevel Level = iota InfoLevel WarnLevel ErrorLevel PanicLevel FatalLevel ) var LevelNameMapping = map[Level]string{ DebugLevel: \"DEBUG\", InfoLevel: \"INFO\", WarnLevel: \"WARN\", ErrorLevel: \"ERROR\", PanicLevel: \"PANIC\", FatalLevel: \"FATAL\", } 在日志输出时，要通过对比开关级别和输出级别的大小，来决定是否输出，所以日志级别 Level 要定义成方便比较的数值类型。几乎所有的日志包都是用常量计数器 iota 来定义日志级别。另外，因为要在日志输出中，输出可读的日志级别（例如输出 INFO 而不是 1），所以需要有 Level 到 Level Name 的映射 LevelNameMapping，LevelNameMapping 会在格式化时用到。 接下来看定义日志选项。日志需要是可配置的，方便开发者根据不同的环境设置不同的日志行为，比较常见的配置选项为：日志级别。输出位置，例如标准输出或者文件。输出格式，例如 JSON 或者 Text。是否开启文件名和行号。 本示例的日志选项定义如下： type options struct { output io.Writer level Level stdLevel Level formatter Formatter disableCaller bool } 为了灵活地设置日志的选项，你可以通过选项模式，来对日志选项进行设置： type Option func(*options) func initOptions(opts ...Option) (o *options) { o = \u0026options{} for _, opt := range opts { opt(o) } if o.output == nil { o.output = os.Stderr } if o.formatter == nil { o.formatter = \u0026TextFormatter{} } return } func WithLevel(level Level) Option { return func(o *options) { o.level = level } } ... func SetOptions(opts ...Option) { std.SetOptions(opts...) } func (l *logger) SetOptions(opts ...Option) { l.mu.Lock() defer l.mu.Unlock() for _, opt := range opts { opt(l.opt) } } 具有选项模式的日志包，可通过以下方式，来动态地修改日志的选项： cuslog.SetOptions(cuslog.WithLevel(cuslog.DebugLevel)) 你可以根据需要，对每一个日志选项创建设置函数 WithXXXX 。这个示例日志包支持如下选项设置函数： WithOutput（output io.Writer）：设置输出位置。 WithLevel（level Level）：设置输出级别。 WithFormatter（formatter Formatter）：设置输出格式。 WithDisableCaller（caller bool）：设置是否打印文件名和行号。 创建 Logger 及各级别日志打印方法 为了打印日志，我们需要根据日志配置，创建一个 Logger，然后通过调用 Logger 的日志打印方法，完成各级别日志的输出。本示例将创建代码保存在logger.go文件中。 可以通过如下方式创建 Logger： var std = New() type logger struct { opt *options mu sync.Mutex entryPool *sync.Pool } func New(opts ...Option) *logger { logger := \u0026logger{opt: initOptions(opts...)} logger.entryPool = \u0026sync.Pool{New: func() interface{} { return entry(logger) }} return logger } 上述代码中，定义了一个 Logger，并实现了创建 Logger 的 New 函数。日志包都会有一个默认的全局 Logger，本示例通过 var std = New() 创建了一个全局的默认 Logger。cuslog.Debug、cuslog.Info 和 cuslog.Warnf 等函数，则是通过调用 std Logger 所提供的方法来打印日志的。 定义了一个 Logger 之后，还需要给该 Logger 添加最核心的日志打印方法，要提供所有支持级别的日志打印方法。如果日志级别是 Xyz，则通常需要提供两类方法，分别是非格式化方法Xyz(args …interface{})和格式化方法Xyzf(format string, args …interface{})，例如： func (l *logger) Debug(args ...interface{}) { l.entry().write(DebugLevel, FmtEmptySeparate, args...) } func (l *logger) Debugf(format string, args ...interface{}) { l.entry().write(DebugLevel, format, args...) } 本示例实现了如下方法：Debug、Debugf、Info、Infof、Warn、Warnf、Error、Errorf、Panic、Panicf、Fatal、Fatalf。更详细的实现，你可以参考 cuslog/logger.go。这里要注意，Panic、Panicf 要调用 panic() 函数，Fatal、Fatalf 函数要调用 os.Exit(1) 函数。 将日志输出到支持的输出中 调用日志打印函数之后，还需要将这些日志输出到支持的输出中，所以需要实现 write 函数，它的写入逻辑保存在entry.go文件中。实现方式如下： type Entry struct { logger *logger Buffer *bytes.Buffer Map map[string]interface{} Level Level Time time.Time File string Line int Func string Format string Args []interface{} } func (e *Entry) write(level Level, format string, args ...interface{}) { if e.logger.opt.level \u003e level { return } e.Time = time.Now() e.Level = level e.Format = format e.Args = args if !e.logger.opt.disableCaller { if pc, file, line, ok := runtime.Caller(2); !ok { e.File = \"???\" e.Func = \"???\" } else { e.File, e.Line, e.Func = file, line, runtime.FuncForPC(pc).Name() e.Func = e.Func[strings.LastIndex(e.Func, \"/\")+1:] } } e.format() e.writer() e.release() } func (e *Entry) format() { _ = e.logger.opt.formatter.Format(e) } func (e *Entry) writer() { e.logger.mu.Lock() _, _ = e.logger.opt.output.Write(e.Buffer.Bytes()) e.logger.mu.Unlock() } func (e *Entry) release() { e.Args, e.Li","date":"2022-07-07 15:19:49","objectID":"/iam_basic_func/:10:3","tags":["iam"],"title":"iam_basic_func","uri":"/iam_basic_func/"},{"categories":["iam"],"content":"IAM 项目日志包设计 这一讲的最后，我们再来看下我们的 IAM 项目中，日志包是怎么设计的。 先来看一下 IAM 项目 log 包的存放位置：pkg/log。放在这个位置，主要有两个原因：第一个，log 包属于 IAM 项目，有定制开发的内容；第二个，log 包功能完备、成熟，外部项目也可以使用。 该 log 包是基于 go.uber.org/zap 包封装而来的，根据需要添加了更丰富的功能。接下来，我们通过 log 包的Options，来看下 log 包所实现的功能： type Options struct { OutputPaths []string `json:\"output-paths\" mapstructure:\"output-paths\"` ErrorOutputPaths []string `json:\"error-output-paths\" mapstructure:\"error-output-paths\"` Level string `json:\"level\" mapstructure:\"level\"` Format string `json:\"format\" mapstructure:\"format\"` DisableCaller bool `json:\"disable-caller\" mapstructure:\"disable-caller\"` DisableStacktrace bool `json:\"disable-stacktrace\" mapstructure:\"disable-stacktrace\"` EnableColor bool `json:\"enable-color\" mapstructure:\"enable-color\"` Development bool `json:\"development\" mapstructure:\"development\"` Name string `json:\"name\" mapstructure:\"name\"` } Options 各配置项含义如下： development：是否是开发模式。如果是开发模式，会对 DPanicLevel 进行堆栈跟踪。 name：Logger 的名字。 disable-caller：是否开启 caller，如果开启会在日志中显示调用日志所在的文件、函数和行号。 disable-stacktrace：是否在 Panic 及以上级别禁止打印堆栈信息。 enable-color：是否开启颜色输出，true，是；false，否。 level：日志级别，优先级从低到高依次为：Debug, Info, Warn, Error, Dpanic, Panic, Fatal。 format：支持的日志输出格式，目前支持 Console 和 JSON 两种。Console 其实就是 Text 格式。 output-paths：支持输出到多个输出，用逗号分开。支持输出到标准输出（stdout）和文件。 error-output-paths：zap 内部 (非业务) 错误日志输出路径，多个输出，用逗号分开。 log 包的 Options 结构体支持以下 3 个方法： Build 方法。Build 方法可以根据 Options 构建一个全局的 Logger。AddFlags 方法。 AddFlags 方法可以将 Options 的各个字段追加到传入的 pflag.FlagSet 变量中。 String 方法。String 方法可以将 Options 的值以 JSON 格式字符串返回。 log 包实现了以下 3 种日志记录方法： log.Info(\"This is a info message\", log.Int32(\"int_key\", 10)) log.Infof(\"This is a formatted %s message\", \"info\") log.Infow(\"Message printed with Infow\", \"X-Request-ID\", \"fbf54504-64da-4088-9b86-67824a7fb508\") Info 使用指定的 key/value 记录日志。Infof 格式化记录日志。 Infow 也是使用指定的 key/value 记录日志，跟 Info 的区别是：使用 Info 需要指定值的类型，通过指定值的日志类型，日志库底层不需要进行反射操作，所以使用 Info 记录日志性能最高。 log 包支持非常丰富的类型，具体你可以参考 types.go。上述日志输出为： 2021-07-06 14:02:07.070 INFO This is a info message {\"int_key\": 10} 2021-07-06 14:02:07.071 INFO This is a formatted info message 2021-07-06 14:02:07.071 INFO Message printed with Infow {\"X-Request-ID\": \"fbf54504-64da-4088-9b86-67824a7fb508\"} log 包为每种级别的日志都提供了 3 种日志记录方式，举个例子：假设日志格式为 Xyz ，则分别提供了 Xyz(msg string, fields …Field) ，Xyzf(format string, v …interface{}) ，Xyzw(msg string, keysAndValues …interface{}) 3 种日志记录方法。 另外，log 包相较于一般的日志包，还提供了众多记录日志的方法。 第一个方法， log 包支持 V Level，可以通过整型数值来灵活指定日志级别，数值越大，优先级越低。例如： // V level使用 log.V(1).Info(\"This is a V level message\") log.V(1).Infof(\"This is a %s V level message\", \"formatted\") log.V(1).Infow(\"This is a V level message with fields\", \"X-Request-ID\", \"7a7b9f24-4cae-4b2a-9464-69088b45b904\") 这里要注意，Log.V 只支持 Info 、Infof 、Infow三种日志记录方法。 第二个方法， log 包支持 WithValues 函数，例如： // WithValues使用 lv := log.WithValues(\"X-Request-ID\", \"7a7b9f24-4cae-4b2a-9464-69088b45b904\") lv.Infow(\"Info message printed with [WithValues] logger\") lv.Infow(\"Debug message printed with [WithValues] logger\") 上述日志输出如下： 2021-07-06 14:15:28.555 INFO Info message printed with [WithValues] logger {\"X-Request-ID\": \"7a7b9f24-4cae-4b2a-9464-69088b45b904\"} 2021-07-06 14:15:28.556 INFO Debug message printed with [WithValues] logger {\"X-Request-ID\": \"7a7b9f24-4cae-4b2a-9464-69088b45b904\"} WithValues 可以返回一个携带指定 key-value 的 Logger，供后面使用。 第三个方法， log 包提供 WithContext 和 FromContext 用来将指定的 Logger 添加到某个 Context 中，以及从某个 Context 中获取 Logger，例如： // Context使用 ctx := lv.WithContext(context.Background()) lc := log.FromContext(ctx) lc.Info(\"Message printed with [WithContext] logger\") WithContext和FromContext非常适合用在以context.Context传递的函数中，例如： func main() { ... // WithValues使用 lv := log.WithValues(\"X-Request-ID\", \"7a7b9f24-4cae-4b2a-9464-69088b45b904\") // Context使用 lv.Infof(\"Start to call pirntString\") ctx := lv.WithContext(context.Background()) pirntString(ctx, \"World\") } func pirntString(ctx context.Context, str string) { lc := log.FromContext(ctx) lc.Infof(\"Hello %s\", str) } 上述代码输出如下： 2021-07-06 14:38:02.050 INFO Start to call pirntString {\"X-Request-ID\": \"7a7b9f24-","date":"2022-07-07 15:19:49","objectID":"/iam_basic_func/:10:4","tags":["iam"],"title":"iam_basic_func","uri":"/iam_basic_func/"},{"categories":["iam"],"content":"总结 开发一个日志包，我们很多时候需要基于一些业界优秀的开源日志包进行二次开发。当前很多项目的日志包都是基于 zap 日志包来封装的，如果你有封装的需要，我建议你优先选择 zap 日志包。 这一讲中，我先给你介绍了标准库 log 包、glog、logrus 和 zap 这四种常用的日志包，然后向你展现了开发一个日志包的四个步骤，步骤如下：定义日志级别和日志选项。创建 Logger 及各级别日志打印方法。将日志输出到支持的输出中。自定义日志输出格式。 最后，我介绍了 IAM 项目封装的 log 包的设计和使用方式。log 包基于 go.uber.org/zap封装，并提供了以下强大特性： log 包支持 V Level，可以灵活的通过整型数值来指定日志级别。 log 包支持 WithValues 函数， WithValues 可以返回一个携带指定 key-value 对的 Logger，供后面使用。 log 包提供 WithContext 和 FromContext 用来将指定的 Logger 添加到某个 Context 中和从某个 Context 中获取 Logger。 log 包提供了 Log.L() 函数，可以很方便的从 Context 中提取出指定的 key-value 对，作为上下文添加到日志输出中。 ","date":"2022-07-07 15:19:49","objectID":"/iam_basic_func/:10:5","tags":["iam"],"title":"iam_basic_func","uri":"/iam_basic_func/"},{"categories":["iam"],"content":"22 | 应用构建三剑客：Pflag、Viper、Cobra 核心功能介绍 因为 IAM 项目使用了 Pflag、Viper 和 Cobra 包来构建 IAM 的应用框架，为了让你后面学习更加容易，这里简单介绍下这 3 个包的核心功能和使用方式。其实如果单独讲每个包的话，还是有很多功能可讲的，但我们这一讲的目的是减小你后面学习 IAM 源码的难度，所以我会主要介绍跟 IAM 相关的功能。在正式介绍这三个包之前，我们先来看下如何构建应用的框架。 ","date":"2022-07-07 15:19:49","objectID":"/iam_basic_func/:11:0","tags":["iam"],"title":"iam_basic_func","uri":"/iam_basic_func/"},{"categories":["iam"],"content":"如何构建应用框架 想知道如何构建应用框架，首先你要明白，一个应用框架包含哪些部分。在我看来，一个应用框架需要包含以下 3 个部分： 命令行参数解析：主要用来解析命令行参数，这些命令行参数可以影响命令的运行效果。 配置文件解析：一个大型应用，通常具有很多参数，为了便于管理和配置这些参数，通常会将这些参数放在一个配置文件中，供程序读取并解析。 应用的命令行框架：应用最终是通过命令来启动的。这里有 3 个需求点，一是命令需要具备 Help 功能，这样才能告诉使用者如何去使用；二是命令需要能够解析命令行参数和配置文件；三是命令需要能够初始化业务代码，并最终启动业务进程。也就是说，我们的命令需要具备框架的能力，来纳管这 3 个部分。 这 3 个部分的功能，你可以自己开发，也可以借助业界已有的成熟实现。跟之前的想法一样，我不建议你自己开发，更建议你采用业界已有的成熟实现。命令行参数可以通过Pflag来解析，配置文件可以通过Viper来解析，应用的命令行框架则可以通过Cobra来实现。这 3 个包目前也是最受欢迎的包，并且这 3 个包不是割裂的，而是有联系的，我们可以有机地组合这 3 个包，从而实现一个非常强大、优秀的应用命令行框架。 接下来，我们就来详细看下，这 3 个包在 Go 项目开发中是如何使用的。 ","date":"2022-07-07 15:19:49","objectID":"/iam_basic_func/:11:1","tags":["iam"],"title":"iam_basic_func","uri":"/iam_basic_func/"},{"categories":["iam"],"content":"命令行参数解析工具：Pflag 使用介绍 Go 服务开发中，经常需要给开发的组件加上各种启动参数来配置服务进程，影响服务的行为。像 kube-apiserver 就有多达 200 多个启动参数，而且这些参数的类型各不相同（例如：string、int、ip 类型等），使用方式也不相同（例如：需要支持–长选项，-短选项等），所以我们需要一个强大的命令行参数解析工具。 虽然 Go 源码中提供了一个标准库 Flag 包，用来对命令行参数进行解析，但在大型项目中应用更广泛的是另外一个包：Pflag。Pflag 提供了很多强大的特性，非常适合用来构建大型项目，一些耳熟能详的开源项目都是用 Pflag 来进行命令行参数解析的，例如：Kubernetes、Istio、Helm、Docker、Etcd 等。 接下来，我们就来介绍下如何使用 Pflag。Pflag 主要是通过创建 Flag 和 FlagSet 来使用的。我们先来看下 Flag。 Pflag 包 Flag 定义 Pflag 可以对命令行参数进行处理，一个命令行参数在 Pflag 包中会解析为一个 Flag 类型的变量。Flag 是一个结构体，定义如下： type Flag struct { Name string // flag长选项的名称 Shorthand string // flag短选项的名称，一个缩写的字符 Usage string // flag的使用文本 Value Value // flag的值 DefValue string // flag的默认值 Changed bool // 记录flag的值是否有被设置过 NoOptDefVal string // 当flag出现在命令行，但是没有指定选项值时的默认值 Deprecated string // 记录该flag是否被放弃 Hidden bool // 如果值为true，则从help/usage输出信息中隐藏该flag ShorthandDeprecated string // 如果flag的短选项被废弃，当使用flag的短选项时打印该信息 Annotations map[string][]string // 给flag设置注解 } Flag 的值是一个 Value 类型的接口，Value 定义如下： type Value interface { String() string // 将flag类型的值转换为string类型的值，并返回string的内容 Set(string) error // 将string类型的值转换为flag类型的值，转换失败报错 Type() string // 返回flag的类型，例如：string、int、ip等 } 通过将 Flag 的值抽象成一个 interface 接口，我们就可以自定义 Flag 的类型了。只要实现了 Value 接口的结构体，就是一个新类型。 Pflag 包 FlagSet 定义 Pflag 除了支持单个的 Flag 之外，还支持 FlagSet。FlagSet 是一些预先定义好的 Flag 的集合，几乎所有的 Pflag 操作，都需要借助 FlagSet 提供的方法来完成。在实际开发中，我们可以使用两种方法来获取并使用 FlagSet： 方法一，调用 NewFlagSet 创建一个 FlagSet。方法二，使用 Pflag 包定义的全局 FlagSet：CommandLine。实际上 CommandLine 也是由 NewFlagSet 函数创建的。 先来看下第一种方法，自定义 FlagSet。下面是一个自定义 FlagSet 的示例： var version bool flagSet := pflag.NewFlagSet(\"test\", pflag.ContinueOnError) flagSet.BoolVar(\u0026version, \"version\", true, \"Print version information and quit.\") 我们可以通过定义一个新的 FlagSet 来定义命令及其子命令的 Flag。再来看下第二种方法，使用全局 FlagSet。下面是一个使用全局 FlagSet 的示例： import ( \"github.com/spf13/pflag\" ) pflag.BoolVarP(\u0026version, \"version\", \"v\", true, \"Print version information and quit.\") 这其中，pflag.BoolVarP 函数定义如下： func BoolVarP(p *bool, name, shorthand string, value bool, usage string) { flag := CommandLine.VarPF(newBoolValue(value, p), name, shorthand, usage) flag.NoOptDefVal = \"true\" } 可以看到 pflag.BoolVarP 最终调用了 CommandLine，CommandLine 是一个包级别的变量，定义为： // CommandLine is the default set of command-line flags, parsed from os.Args. var CommandLine = NewFlagSet(os.Args[0], ExitOnError) 在一些不需要定义子命令的命令行工具中，我们可以直接使用全局的 FlagSet，更加简单方便。 Pflag 使用方法 上面，我们介绍了使用 Pflag 包的两个核心结构体。接下来，我来详细介绍下 Pflag 的常见使用方法。Pflag 有很多强大的功能，我这里介绍 7 个常见的使用方法。 支持多种命令行参数定义方式。 Pflag 支持以下 4 种命令行参数定义方式： 支持长选项、默认值和使用文本，并将标志的值存储在指针中。 var name = pflag.String(\"name\", \"colin\", \"Input Your Name\") 支持长选项、短选项、默认值和使用文本，并将标志的值存储在指针中。 var name = pflag.StringP(\"name\", \"n\", \"colin\", \"Input Your Name\") 支持长选项、默认值和使用文本，并将标志的值绑定到变量。 var name string pflag.StringVar(\u0026name, \"name\", \"colin\", \"Input Your Name\") 支持长选项、短选项、默认值和使用文本，并将标志的值绑定到变量。 var name string pflag.StringVarP(\u0026name, \"name\", \"n\",\"colin\", \"Input Your Name\") 上面的函数命名是有规则的：函数名带Var说明是将标志的值绑定到变量，否则是将标志的值存储在指针中。函数名带P说明支持短选项，否则不支持短选项。 使用Get获取参数的值。 可以使用Get来获取标志的值，代表 Pflag 所支持的类型。例如：有一个 pflag.FlagSet，带有一个名为 flagname 的 int 类型的标志，可以使用GetInt()来获取 int 值。需要注意 flagname 必须存在且必须是 int，例如： i, err := flagset.GetInt(\"flagname\") 获取非选项参数。 代码示例如下： package main import ( \"fmt\" \"github.com/spf13/pflag\" ) var ( flagvar = pflag.Int(\"flagname\", 1234, \"help message for flagname\") ) func main() { pflag.Parse() fmt.Printf(\"argument number is: %v\\n\", pflag.NArg()) fmt.Printf(\"argument list is: %v\\n\", pflag.Args()) fmt.Printf(\"the first argument is: %v\\n\", pflag.Arg(0)) } 执行上述代码，输出如下： $ go run example1.go arg1 arg2 argument number is: 2 argument list is: [arg1 arg2] the first argument is: arg1 在定义完标志之后，可以调用pflag.Parse()来解析定义的标志。解析后，可通过pflag.Args()返回所有的非选项参数，通过pflag.Arg(i)返回第 i 个非选项参数。参数下标 0 到 pflag.NArg() - 1。 指定了选项但是没有指定选项值时的默认值。 创建一个 Flag 后，可以为这个 Flag 设置pflag.NoOptDefVal。如果一个 Flag 具有 NoOptDefVal，并且该 Flag 在命令行上没有设置这个 Flag 的值，则该标志将设置为 NoOptDefVal 指定的值。例如： var ip = pflag.IntP(\"flagname\", \"f\", 1234, \"help message\") pflag.Lookup(\"flagname\").NoOptDefVal = \"4321\" 上面的代码会产生结果，具体你可以参照下表： 弃用标志或者","date":"2022-07-07 15:19:49","objectID":"/iam_basic_func/:11:2","tags":["iam"],"title":"iam_basic_func","uri":"/iam_basic_func/"},{"categories":["iam"],"content":"配置解析神器：Viper 使用介绍 几乎所有的后端服务，都需要一些配置项来配置我们的服务，一些小型的项目，配置不是很多，可以选择只通过命令行参数来传递配置。但是大型项目配置很多，通过命令行参数传递就变得很麻烦，不好维护。标准的解决方案是将这些配置信息保存在配置文件中，由程序启动时加载和解析。Go 生态中有很多包可以加载并解析配置文件，目前最受欢迎的是 Viper 包。 Viper 是 Go 应用程序现代化的、完整的解决方案，能够处理不同格式的配置文件，让我们在构建现代应用程序时，不必担心配置文件格式。Viper 也能够满足我们对应用配置的各种需求。 Viper 可以从不同的位置读取配置，不同位置的配置具有不同的优先级，高优先级的配置会覆盖低优先级相同的配置，按优先级从高到低排列如下： 通过 viper.Set 函数显示设置的配置 命令行参数 环境变量 配置文件 Key/Value 存储 默认值 这里需要注意，Viper 配置键不区分大小写。Viper 有很多功能，最重要的两类功能是读入配置和读取配置，Viper 提供不同的方式来实现这两类功能。接下来，我们就来详细介绍下 Viper 如何读入配置和读取配置。 读入配置 读入配置，就是将配置读入到 Viper 中，有如下读入方式： 设置默认的配置文件名。 读取配置文件。 监听和重新读取配置文件。 从 io.Reader 读取配置。 从环境变量读取。 从命令行标志读取。 从远程 Key/Value 存储读取。 这几个方法的具体读入方式，你可以看下面的展示。 设置默认值。一个好的配置系统应该支持默认值。Viper 支持对 key 设置默认值，当没有通过配置文件、环境变量、远程配置或命令行标志设置 key 时，设置默认值通常是很有用的，可以让程序在没有明确指定配置时也能够正常运行。例如： viper.SetDefault(\"ContentDir\", \"content\") viper.SetDefault(\"LayoutDir\", \"layouts\") viper.SetDefault(\"Taxonomies\", map[string]string{\"tag\": \"tags\", \"category\": \"categories\"}) 读取配置文件。Viper 可以读取配置文件来解析配置，支持 JSON、TOML、YAML、YML、Properties、Props、Prop、HCL、Dotenv、Env 格式的配置文件。Viper 支持搜索多个路径，并且默认不配置任何搜索路径，将默认决策留给应用程序。 以下是如何使用 Viper 搜索和读取配置文件的示例： package main import ( \"fmt\" \"github.com/spf13/pflag\" \"github.com/spf13/viper\" ) var ( cfg = pflag.StringP(\"config\", \"c\", \"\", \"Configuration file.\") help = pflag.BoolP(\"help\", \"h\", false, \"Show this help message.\") ) func main() { pflag.Parse() if *help { pflag.Usage() return } // 从配置文件中读取配置 if *cfg != \"\" { viper.SetConfigFile(*cfg) // 指定配置文件名 viper.SetConfigType(\"yaml\") // 如果配置文件名中没有文件扩展名，则需要指定配置文件的格式，告诉viper以何种格式解析文件 } else { viper.AddConfigPath(\".\") // 把当前目录加入到配置文件的搜索路径中 viper.AddConfigPath(\"$HOME/.iam\") // 配置文件搜索路径，可以设置多个配置文件搜索路径 viper.SetConfigName(\"config\") // 配置文件名称（没有文件扩展名） } if err := viper.ReadInConfig(); err != nil { // 读取配置文件。如果指定了配置文件名，则使用指定的配置文件，否则在注册的搜索路径中搜索 panic(fmt.Errorf(\"Fatal error config file: %s \\n\", err)) } fmt.Printf(\"Used configuration file is: %s\\n\", viper.ConfigFileUsed()) } Viper 支持设置多个配置文件搜索路径，需要注意添加搜索路径的顺序，Viper 会根据添加的路径顺序搜索配置文件，如果找到则停止搜索。如果调用 SetConfigFile 直接指定了配置文件名，并且配置文件名没有文件扩展名时，需要显式指定配置文件的格式，以使 Viper 能够正确解析配置文件。 如果通过搜索的方式查找配置文件，则需要注意，SetConfigName 设置的配置文件名是不带扩展名的，在搜索时 Viper 会在文件名之后追加文件扩展名，并尝试搜索所有支持的扩展类型。 监听和重新读取配置文件。Viper 支持在运行时让应用程序实时读取配置文件，也就是热加载配置。可以通过 WatchConfig 函数热加载配置。在调用 WatchConfig 函数之前，需要确保已经添加了配置文件的搜索路径。另外，还可以为 Viper 提供一个回调函数，以便在每次发生更改时运行。这里我也给你个示例： viper.WatchConfig() viper.OnConfigChange(func(e fsnotify.Event) { // 配置文件发生变更之后会调用的回调函数 fmt.Println(\"Config file changed:\", e.Name) }) 我不建议在实际开发中使用热加载功能，因为即使配置热加载了，程序中的代码也不一定会热加载。例如：修改了服务监听端口，但是服务没有重启，这时候服务还是监听在老的端口上，会造成不一致。 设置配置值。我们可以通过 viper.Set() 函数来显式设置配置： viper.Set(\"user.username\", \"colin\") 使用环境变量。Viper 还支持环境变量，通过如下 5 个函数来支持环境变量： AutomaticEnv() BindEnv(input …string) error SetEnvPrefix(in string) SetEnvKeyReplacer(r *strings.Replacer) AllowEmptyEnv(allowEmptyEnv bool) 这里要注意：Viper 读取环境变量是区分大小写的。Viper 提供了一种机制来确保 Env 变量是唯一的。通过使用 SetEnvPrefix，可以告诉 Viper 在读取环境变量时使用前缀。BindEnv 和 AutomaticEnv 都将使用此前缀。比如，我们设置了 viper.SetEnvPrefix(“VIPER”)，当使用 viper.Get(“apiversion”) 时，实际读取的环境变量是VIPER_APIVERSION。 BindEnv 需要一个或两个参数。第一个参数是键名，第二个是环境变量的名称，环境变量的名称区分大小写。如果未提供 Env 变量名，则 Viper 将假定 Env 变量名为：环境变量前缀_键名全大写。例如：前缀为 VIPER，key 为 username，则 Env 变量名为VIPER_USERNAME。当显示提供 Env 变量名（第二个参数）时，它不会自动添加前缀。例如，如果第二个参数是 ID，Viper 将查找环境变量 ID。 在使用 Env 变量时，需要注意的一件重要事情是：每次访问该值时都将读取它。Viper 在调用 BindEnv 时不固定该值。 还有一个魔法函数 SetEnvKeyReplacer，SetEnvKeyReplacer 允许你使用 strings.Replacer 对象来重写 Env 键。如果你想在 Get() 调用中使用-或者.，但希望你的环境变量使用_分隔符，可以通过 SetEnvKeyReplacer 来实现。比如，我们设置了环境变量USER_SECRET_KEY=bVix2WBv0VPfrDrvlLWrhEdzjLpPCNYb，但我们想用viper.Get(“user.secret-key”)，那我们就调用函数： viper.SetEnvKeyReplacer(strings.NewReplacer(\".\", \"_\", \"-\", \"_\")) 上面的代码，在调用 viper.Get() 函数时，会用 _ 替换.和-。默认情况下，空环境变量被认为是未设置的，并将返回到下一个配置源。若要将空环境变量视为已设置，可以使用 AllowEmptyEnv 方法。使用环境变量示例如下： // 使用环境变量 os.Setenv(\"VIPER_USER_SECRET_ID\", \"QLdywI2MrmDVjSSv6e95weNRvmteRjfKAuNV\") os.Setenv(\"VIPER_USER_SECRET_KEY\", \"bVix2WBv0VPfrDrvlLWrhEdzjLpPCNYb\") viper.AutomaticEnv() // 读取环境变量 viper.SetEnvPrefix(\"VIPER\") // 设置环境变量前缀：VIPER_，如果是viper，将自动转变为大写。 vip","date":"2022-07-07 15:19:49","objectID":"/iam_basic_func/:11:3","tags":["iam"],"title":"iam_basic_func","uri":"/iam_basic_func/"},{"categories":["iam"],"content":"现代化的命令行框架：Cobra 全解 Cobra 既是一个可以创建强大的现代 CLI 应用程序的库，也是一个可以生成应用和命令文件的程序。有许多大型项目都是用 Cobra 来构建应用程序的，例如 Kubernetes、Docker、etcd、Rkt、Hugo 等。 Cobra 建立在 commands、arguments 和 flags 结构之上。commands 代表命令，arguments 代表非选项参数，flags 代表选项参数（也叫标志）。一个好的应用程序应该是易懂的，用户可以清晰地知道如何去使用这个应用程序。应用程序通常遵循如下模式：APPNAME VERB NOUN –ADJECTIVE或者APPNAME COMMAND ARG –FLAG，例如： git clone URL --bare # clone 是一个命令，URL是一个非选项参数，bare是一个选项参数 这里，VERB 代表动词，NOUN 代表名词，ADJECTIVE 代表形容词。 Cobra 提供了两种方式来创建命令：Cobra 命令和 Cobra 库。Cobra 命令可以生成一个 Cobra 命令模板，而命令模板也是通过引用 Cobra 库来构建命令的。所以，这里我直接介绍如何使用 Cobra 库来创建命令。 使用 Cobra 库创建命令 如果要用 Cobra 库编码实现一个应用程序，需要首先创建一个空的 main.go 文件和一个 rootCmd 文件，之后可以根据需要添加其他命令。具体步骤如下： 创建 rootCmd。 $ mkdir -p newApp2 \u0026\u0026 cd newApp2 通常情况下，我们会将 rootCmd 放在文件 cmd/root.go 中。 var rootCmd = \u0026cobra.Command{ Use: \"hugo\", Short: \"Hugo is a very fast static site generator\", Long: `A Fast and Flexible Static Site Generator built with love by spf13 and friends in Go. Complete documentation is available at http://hugo.spf13.com`, Run: func(cmd *cobra.Command, args []string) { // Do Stuff Here }, } func Execute() { if err := rootCmd.Execute(); err != nil { fmt.Println(err) os.Exit(1) } } 还可以在 init() 函数中定义标志和处理配置，例如 cmd/root.go。 import ( \"fmt\" \"os\" homedir \"github.com/mitchellh/go-homedir\" \"github.com/spf13/cobra\" \"github.com/spf13/viper\" ) var ( cfgFile string projectBase string userLicense string ) func init() { cobra.OnInitialize(initConfig) rootCmd.PersistentFlags().StringVar(\u0026cfgFile, \"config\", \"\", \"config file (default is $HOME/.cobra.yaml)\") rootCmd.PersistentFlags().StringVarP(\u0026projectBase, \"projectbase\", \"b\", \"\", \"base project directory eg. github.com/spf13/\") rootCmd.PersistentFlags().StringP(\"author\", \"a\", \"YOUR NAME\", \"Author name for copyright attribution\") rootCmd.PersistentFlags().StringVarP(\u0026userLicense, \"license\", \"l\", \"\", \"Name of license for the project (can provide `licensetext` in config)\") rootCmd.PersistentFlags().Bool(\"viper\", true, \"Use Viper for configuration\") viper.BindPFlag(\"author\", rootCmd.PersistentFlags().Lookup(\"author\")) viper.BindPFlag(\"projectbase\", rootCmd.PersistentFlags().Lookup(\"projectbase\")) viper.BindPFlag(\"useViper\", rootCmd.PersistentFlags().Lookup(\"viper\")) viper.SetDefault(\"author\", \"NAME HERE \u003cEMAIL ADDRESS\u003e\") viper.SetDefault(\"license\", \"apache\") } func initConfig() { // Don't forget to read config either from cfgFile or from home directory! if cfgFile != \"\" { // Use config file from the flag. viper.SetConfigFile(cfgFile) } else { // Find home directory. home, err := homedir.Dir() if err != nil { fmt.Println(err) os.Exit(1) } // Search config in home directory with name \".cobra\" (without extension). viper.AddConfigPath(home) viper.SetConfigName(\".cobra\") } if err := viper.ReadInConfig(); err != nil { fmt.Println(\"Can't read config:\", err) os.Exit(1) } } 创建 main.go。我们还需要一个 main 函数来调用 rootCmd，通常我们会创建一个 main.go 文件，在 main.go 中调用 rootCmd.Execute() 来执行命令： package main import ( \"{pathToYourApp}/cmd\" ) func main() { cmd.Execute() } 需要注意，main.go 中不建议放很多代码，通常只需要调用 cmd.Execute() 即可。 添加命令。除了 rootCmd，我们还可以调用 AddCommand 添加其他命令，通常情况下，我们会把其他命令的源码文件放在 cmd/ 目录下，例如，我们添加一个 version 命令，可以创建 cmd/version.go 文件，内容为： package cmd import ( \"fmt\" \"github.com/spf13/cobra\" ) func init() { rootCmd.AddCommand(versionCmd) } var versionCmd = \u0026cobra.Command{ Use: \"version\", Short: \"Print the version number of Hugo\", Long: `All software has versions. This is Hugo's`, Run: func(cmd *cobra.Command, args []string) { fmt.Println(\"Hugo Static Site Generator v0.9 -- HEAD\") }, } 本示例中，我们通过调用rootCmd.AddCommand(versionCmd)给 rootCmd 命令添加了一个 versionCmd 命令。 编译并运行。将 main.go 中{pathToYourApp}替换为对应的路径，例如本示例中 pathToYourApp 为github.com/marmotedu/gopractise-demo/cobra/newApp2。 $ go mod init github.com/marmotedu/gopractise-demo/cobra/newApp2 $ go build -v . $ ./newApp2 -h A Fast and Flexible Static Site Generator built with love by spf13 and friends in Go. Complete documentation is available at http://hugo.spf13.com Usage: hugo [flags] hugo [command] Available Commands: help Help about any command","date":"2022-07-07 15:19:49","objectID":"/iam_basic_func/:11:4","tags":["iam"],"title":"iam_basic_func","uri":"/iam_basic_func/"},{"categories":["iam"],"content":"总结 在开发 Go 项目时，我们可以通过 Pflag 来解析命令行参数，通过 Viper 来解析配置文件，用 Cobra 来实现命令行框架。你可以通过 pflag.String()、 pflag.StringP()、pflag.StringVar()、pflag.StringVarP() 方法来设置命令行参数，并使用 Get来获取参数的值。 同时，你也可以使用 Viper 从命令行参数、环境变量、配置文件等位置读取配置项。最常用的是从配置文件中读取，可以通过 viper.AddConfigPath 来设置配置文件搜索路径，通过 viper.SetConfigFile 和 viper.SetConfigType 来设置配置文件名，通过 viper.ReadInConfig 来读取配置文件。读取完配置文件，然后在程序中使用 Get/Get来读取配置项的值。最后，你可以使用 Cobra 来构建一个命令行框架，Cobra 可以很好地集成 Pflag 和 Viper。 ","date":"2022-07-07 15:19:49","objectID":"/iam_basic_func/:11:5","tags":["iam"],"title":"iam_basic_func","uri":"/iam_basic_func/"},{"categories":["iam"],"content":"23 | 应用构建实战：如何构建一个优秀的企业应用框架？ 应用开发是软件开发工程师最核心的工作。在我这 7 年的 Go 开发生涯中，我构建了大大小小不下 50 个后端应用，深谙其中的痛点，比如：重复造轮子。同样的功能却每次都要重新开发，浪费非常多的时间和精力不说，每次实现的代码质量更是参差不齐。理解成本高。相同的功能，有 N 个服务对应着 N 种不同的实现方式，如果功能升级，或者有新成员加入，都可能得重新理解 N 次。功能升级的开发工作量大。一个应用由 N 个服务组成，如果要升级其中的某个功能，你需要同时更新 N 个服务的代码。 想要解决上面这些问题，一个比较好的思路是：找出相同的功能，然后用一种优雅的方式去实现它，并通过 Go 包的形式，供所有的服务使用。 我会带你找出服务的通用功能，并给出优雅的构建方式，帮助你一劳永逸地解决这些问题。在提高开发效率的同时，也能提高你的代码质量。 接下来，我们先来分析并找出 Go 服务通用的功能。 ","date":"2022-07-07 15:19:49","objectID":"/iam_basic_func/:12:0","tags":["iam"],"title":"iam_basic_func","uri":"/iam_basic_func/"},{"categories":["iam"],"content":"构建应用的基础：应用的三大基本功能 我们目前见到的 Go 后端服务，基本上可以分为 API 服务和非 API 服务两类。 API 服务：通过对外提供 HTTP/RPC 接口来完成指定的功能。比如订单服务，通过调用创建订单的 API 接口，来创建商品订单。 非 API 服务：通过监听、定时运行等方式，而不是通过 API 调用来完成某些任务。比如数据处理服务，定时从 Redis 中获取数据，处理后存入后端存储中。再比如消息处理服务，监听消息队列（如 NSQ/Kafka/RabbitMQ），收到消息后进行处理。 对于 API 服务和非 API 服务来说，它们的启动流程基本一致，都可以分为三步：应用框架的构建，这是最基础的一步。应用初始化。服务启动。 如下图所示： 图中，命令行程序、命令行参数解析和配置文件解析，是所有服务都需要具备的功能，这些功能有机结合到一起，共同构成了应用框架。所以，我们要构建的任何一个应用程序，至少要具备命令行程序、命令行参数解析和配置文件解析这 3 种功能。 命令行程序：用来启动一个应用。命令行程序需要实现诸如应用描述、help、参数校验等功能。根据需要，还可以实现命令自动补全、打印命令行参数等高级功能。命令行参数解析：用来在启动时指定应用程序的命令行参数，以控制应用的行为。配置文件解析：用来解析不同格式的配置文件。 另外，上述 3 类功能跟业务关系不大，可以抽象成一个统一的框架。应用初始化、创建 API/ 非 API 服务、启动服务，跟业务联系比较紧密，难以抽象成一个统一的框架。 ","date":"2022-07-07 15:19:49","objectID":"/iam_basic_func/:12:1","tags":["iam"],"title":"iam_basic_func","uri":"/iam_basic_func/"},{"categories":["iam"],"content":"iam-apiserver 是如何构建应用框架的？ 这里，我通过讲解 iam-apiserver 的应用构建方式，来给你讲解下如何构建应用。iam-apiserver 程序的 main 函数位于 apiserver.go 文件中，其构建代码可以简化为： import ( ... \"github.com/marmotedu/iam/internal/apiserver\" \"github.com/marmotedu/iam/pkg/app\" ) func main() { ... apiserver.NewApp(\"iam-apiserver\").Run() } const commandDesc = `The IAM API server validates and configures data ...` // NewApp creates a App object with default parameters. func NewApp(basename string) *app.App { opts := options.NewOptions() application := app.NewApp(\"IAM API Server\", basename, app.WithOptions(opts), app.WithDescription(commandDesc), app.WithDefaultValidArgs(), app.WithRunFunc(run(opts)), ) return application } func run(opts *options.Options) app.RunFunc { return func(basename string) error { log.Init(opts.Log) defer log.Flush() cfg, err := config.CreateConfigFromOptions(opts) if err != nil { return err } return Run(cfg) } } 可以看到，我们是通过调用包 github.com/marmotedu/iam/pkg/app 来构建应用的。也就是说，我们将构建应用的功能抽象成了一个 Go 包，通过 Go 包可以提高代码的封装性和复用性。iam-authz-server 和 iam-pump 组件也都是通过 github.com/marmotedu/iam/pkg/app 来构建应用的。 构建应用的流程也很简单，只需要创建一个 application 实例即可： opts := options.NewOptions() application := app.NewApp(\"IAM API Server\", basename, app.WithOptions(opts), app.WithDescription(commandDesc), app.WithDefaultValidArgs(), app.WithRunFunc(run(opts)), ) 在创建应用实例时，我传入了下面这些参数。IAM API Server：应用的简短描述。basename：应用的二进制文件名。opts：应用的命令行选项。commandDesc：应用的详细描述。run(opts)：应用的启动函数，初始化应用，并最终启动 HTTP 和 GRPC Web 服务。 创建应用时，你还可以根据需要来配置应用实例，比如 iam-apiserver 组件在创建应用时，指定了 WithDefaultValidArgs 来校验命令行非选项参数的默认校验逻辑。 可以看到，iam-apiserver 通过简单的几行代码，就创建出了一个应用。**之所以这么方便，是因为应用框架的构建代码都封装在了 github.com/marmotedu/iam/pkg/app 包中。**接下来，我们来重点看下 github.com/marmotedu/iam/pkg/app 包是如何实现的。为了方便描述，我在下文中统称为 App 包。 App 包设计和实现 我们先来看下 App 包目录下的文件： [colin@dev iam]$ ls pkg/app/ app.go cmd.go config.go doc.go flag.go help.go options.go pkg/app 目录下的 5 个主要文件是 app.go、cmd.go、config.go、flag.go、options.go，分别实现了应用程序框架中的应用、命令行程序、命令行参数解析、配置文件解析和命令行选项 5 个部分，具体关系如下图所示： 我再来解释下这张图。应用由命令行程序、命令行参数解析、配置文件解析三部分组成，命令行参数解析功能通过命令行选项来构建，二者通过接口解耦合： type CliOptions interface { // AddFlags adds flags to the specified FlagSet object. // AddFlags(fs *pflag.FlagSet) Flags() (fss cliflag.NamedFlagSets) Validate() []error } 通过接口，应用可以定制自己独有的命令行参数。接下来，我们再来看下如何具体构建应用的每一部分。 第 1 步：构建应用 APP 包提供了 NewApp 函数来创建一个应用： func NewApp(name string, basename string, opts ...Option) *App { a := \u0026App{ name: name, basename: basename, } for _, o := range opts { o(a) } a.buildCommand() return a } NewApp 中使用了设计模式中的选项模式，来动态地配置 APP，支持 WithRunFunc、WithDescription、WithValidArgs 等选项。 第 2 步：命令行程序构建 这一步，我们会使用 Cobra 包来构建应用的命令行程序。 NewApp 最终会调用 buildCommand 方法来创建 Cobra Command 类型的命令，命令的功能通过指定 Cobra Command 类型的各个字段来实现。通常可以指定：Use、Short、Long、SilenceUsage、SilenceErrors、RunE、Args 等字段。 在 buildCommand 函数中，也会根据应用的设置添加不同的命令行参数，例如： if !a.noConfig { addConfigFlag(a.basename, namedFlagSets.FlagSet(\"global\")) } 上述代码的意思是：如果我们设置了 noConfig=false，那么就会在命令行参数 global 分组中添加以下命令行选项： -c, --config FILE Read configuration from specified FILE, support JSON, TOML, YAML, HCL, or Java properties formats. 为了更加易用和人性化，命令还具有如下 3 个功能。 帮助信息：执行 -h/–help 时，输出的帮助信息。通过 cmd.SetHelpFunc 函数可以指定帮助信息。 使用信息（可选）：当用户提供无效的标志或命令时，向用户显示“使用信息”。通过 cmd.SetUsageFunc 函数，可以指定使用信息。如果不想每次输错命令打印一大堆 usage 信息，你可以通过设置 SilenceUsage: true 来关闭掉 usage。 版本信息：打印应用的版本。知道应用的版本号，对故障排查非常有帮助。通过 verflag.AddFlags 可以指定版本信息。例如，App 包通过 github.com/marmotedu/component-base/pkg/version 指定了以下版本信息： $ ./iam-apiserver --version gitVersion: v0.3.0 gitCommit: ccc31e292f66e6bad94efb1406b5ced84e64675c gitTreeState: dirty buildDate: 2020-12-17T12:24:37Z goVersion: go1.15.1 compiler: gc platform: linux/amd64 $ ./iam-apiserver --version=raw version.Info{GitVersion:\"v0.3.0\", GitCommit:\"ccc31e292f66e6bad94efb1406b5ced84e64675c\", GitTreeState:\"dirty\", BuildDate:\"2020-12-17T12:24:37Z\", GoVersion:\"go1.15.1\", Compiler:\"gc\", Platform:\"linux/amd64\"} 接下来，再来看下应用需要实现的另外一个重要功能，也就是命令行参数解析。 第 3 步：命令行参数解析 App 包在构建应用和执行应用两个阶段来实现命令行参数解析。 我们先看构建应用这个阶段。App 包在 buildCommand 方法中通过以下代码段，给应用添加了命令行参数： var namedFlagSets clifla","date":"2022-07-07 15:19:49","objectID":"/iam_basic_func/:12:2","tags":["iam"],"title":"iam_basic_func","uri":"/iam_basic_func/"},{"categories":["iam"],"content":"这样构建的应用程序，有哪些优秀特性？ 借助 Cobra 自带的能力，构建出的应用天然具备帮助信息、使用信息、子命令、子命令自动补全、非选项参数校验、命令别名、PreRun、PostRun 等功能，这些功能对于一个应用来说是非常有用的。 Cobra 可以集成 Pflag，通过将创建的 Pflag FlagSet 绑定到 Cobra 命令的 FlagSet 中，使得 Pflag 支持的标志能直接集成到 Cobra 命令中。集成到命令中有很多好处，例如：cobra -h 可以打印出所有设置的 flag，Cobra Command 命令提供的 GenBashCompletion 方法，可以实现命令行选项的自动补全。 通过 viper.BindPFlags 和 viper.ReadInConfig 函数，可以统一配置文件、命令行参数的配置项，使得应用的配置项更加清晰好记。面对不同场景可以选择不同的配置方式，使配置更加灵活。例如：配置 HTTPS 的绑定端口，可以通过 –secure.bind-port 配置，也可以通过配置文件配置（命令行参数优先于配置文件）： secure: bind-port: 8080 可以通过 viper.GetString(“secure.bind-port”) 这类方式获取应用的配置，获取方式更加灵活，而且全局可用。将应用框架的构建方法实现成了一个 Go 包，通过 Go 包可以提高应用构建代码的封装性和复用性。 ","date":"2022-07-07 15:19:49","objectID":"/iam_basic_func/:12:3","tags":["iam"],"title":"iam_basic_func","uri":"/iam_basic_func/"},{"categories":["iam"],"content":"如果你想自己构建应用，需要注意些什么？ 当然，你也可以使用其他方式构建你的应用程序。比如，我就见过很多开发者使用如下方式来构建应用：直接在 main.go 文件中通过 gopkg.in/yaml.v3 包解析配置，通过 Go 标准库的 flag 包简单地添加一些命令行参数，例如–help、–config、–version。 但是，在你自己独立构建应用程序时，很可能会踩这么 3 个坑： 构建的应用功能简单，扩展性差，导致后期扩展复杂。 构建的应用没有帮助信息和使用信息，或者信息格式杂乱，增加应用的使用难度。 命令行选项和配置文件支持的配置项相互独立，导致配合应用程序的时候，不知道该使用哪种方式来配置。 在我看来，对于小的应用，自己根据需要构建没什么问题，但是对于一个大型项目的话，还是在应用开发之初，就采用一些功能多、扩展性强的优秀包。这样，以后随着应用的迭代，可以零成本地进行功能添加和扩展，同时也能体现我们的专业性和技术深度，提高代码质量。 如果你有特殊需求，一定要自己构建应用框架，那么我有以下几个建议： 应用框架应该清晰易读、扩展性强。 应用程序应该至少支持如下命令行选项：-h 打印帮助信息；-v 打印应用程序的版本；-c 支持指定配置文件的路径。 如果你的应用有很多命令行选项，那么建议支持 –secure.bind-port 这样的长选项，通过选项名字，就可以知道选项的作用。 配置文件使用 yaml 格式，yaml 格式的配置文件，能支持复杂的配置，还清晰易读。 如果你有多个服务，那么要保持所有服务的应用构建方式是一致的。 ","date":"2022-07-07 15:19:49","objectID":"/iam_basic_func/:12:4","tags":["iam"],"title":"iam_basic_func","uri":"/iam_basic_func/"},{"categories":["iam"],"content":"总结 一个应用框架由命令、命令行参数解析、配置文件解析 3 部分功能组成，我们可以通过 Cobra 来构建命令，通过 Pflag 来解析命令行参数，通过 Viper 来解析配置文件。一个项目，可能包含多个应用，这些应用都需要通过 Cobra、Viper、Pflag 来构建。为了不重复造轮子，简化应用的构建，我们可以将这些功能实现为一个 Go 包，方便直接调用构建应用。 IAM 项目的应用都是通过 github.com/marmotedu/iam/pkg/app 包来构建的，在构建时，调用 App 包提供的 NewApp 函数，来构建一个应用： func NewApp(basename string) *app.App { opts := options.NewOptions() application := app.NewApp(\"IAM API Server\", basename, app.WithOptions(opts), app.WithDescription(commandDesc), app.WithDefaultValidArgs(), app.WithRunFunc(run(opts)), ) return application } 在构建应用时，只需要提供应用简短 / 详细描述、应用二进制文件名称和命令行选项即可。App 包会根据 Options 提供的 Flags() 方法，来给应用添加命令行选项。命令行选项中提供了 -c, –config 选项来指定配置文件，App 包也会加载并解析这个配置文件，并将配置文件和命令行选项相同配置项进行 Merge，最终将配置项的值保存在传入的 Options 变量中，供业务代码使用。 最后，如果你想自己构建应用，我给出了一些我的建议：设计一个清晰易读、易扩展的应用框架；支持一些常见的选项，例如 -h， -v， -c 等；如果应用的命令行选项比较多，建议使用 –secure.bind-port 这样的长选项。 ","date":"2022-07-07 15:19:49","objectID":"/iam_basic_func/:12:5","tags":["iam"],"title":"iam_basic_func","uri":"/iam_basic_func/"},{"categories":["iam"],"content":"4 | 规范设计（上）：项目开发杂乱无章，如何规范？（开源规范、文档规范和版本规范） 规范问题： 代码风格不一：代码仓库中有多种代码风格，读 / 改他人的代码都是一件痛苦的事情，整个代码库也会看起来很乱。目录杂乱无章：相同的功能被放在不同的目录，或者一个目录你根本不知道它要完成什么功能，新开发的代码你也不知道放在哪个目录或文件。这些都会严重降低代码的可维护性。接口不统一：对外提供的 API 接口不统一，例如修改用户接口为/v1/users/colin，但是修改密钥接口为/v1/secret?name=secret0，难以理解和记忆。错误码不规范：错误码会直接暴露给用户，主要用于展示错误类型，以定位错误问题。错误码不规范会导致难以辨别错误类型，或者同类错误拥有不同错误码，增加理解难度。 在设计阶段、编码之前，我们需要一个好的规范来约束开发者。 ","date":"2022-07-07 15:18:55","objectID":"/iam_standard_design/:1:0","tags":["iam"],"title":"iam_standard_design","uri":"/iam_standard_design/"},{"categories":["iam"],"content":"需要制定规范的地方 非编码类规范，主要包括开源规范、文档规范、版本规范、Commit 规范和发布规范。编码类规范，则主要包括目录规范、代码规范、接口规范、日志规范和错误码规范。 ","date":"2022-07-07 15:18:55","objectID":"/iam_standard_design/:1:1","tags":["iam"],"title":"iam_standard_design","uri":"/iam_standard_design/"},{"categories":["iam"],"content":"开源规范 为什么一定要知道开源规范呢？原因主要有两方面：一是，开源项目在代码质量、代码规范、文档等方面，要比非开源项目要求更高，在项目开发中按照开源项目的要求来规范自己的项目，可以更好地驱动项目质量的提高；二是，一些大公司为了不重复造轮子，会要求公司团队能够将自己的项目开源，所以提前按开源标准来驱动 Go 项目开发，也会为我们日后代码开源省去不少麻烦 开源规范详细列表 项目结构：一个开源项目应该有一个合理、专业的、符合语言特色的项目结构。 严格遵循代码规范：开源的代码，面向的人群是所有的开发者，一个不规范的代码，可读性差，不利于其他开发者阅读和贡献代码。 代码质量：开源的代码，一定要保证代码的质量，一个低质量的代码，不仅隐藏了很多性能和功能缺陷，而且还会影响开源项目的品牌，进而影响开源效果。 单元测试覆盖率：一个开源的 Go 项目，要保证整个项目的单元测试覆盖率，这样一方面可以保证代码的质量，另一方面可以使开源项目更专业，也能让你更加安心的发布版本。 版本发布规范：开源项目要遵循既定的版本规范，整个项目的更新迭代，要有版本号，目前用的比较多的是语义化的版本规范。 向下兼容：代码要做到向下兼容，这样可以尽可能减少发布变更的影响，遵循语义化的版本规范，可以在一定程度上保证代码的向下兼容能力。 详细的文档说明：要保证代码能够被其他开发者很容易的阅读和贡献代码，所以不仅要保证文档的质量和数量，还要确保有某些需要的文档： LICENSE（如果是开源项目，LICENSE 是必选的）：软件协议，声明该开源项目遵循什么软件协议。 README.md：README 文件，放在项目的根目录下，包含项目的描述、依赖项、安装方法、使用方法、贡献方法、作者和遵循的软件协议等。 CHANGELOG：目录，用来存放版本的变更历史，方便其他开发者了解新版本或旧版本的变更内容。 Makefile：对于一个复杂的项目，通常也会包含一个 Makefile 文件，用来对项目进行构建、测试、安装等操作。 CONTRIBUTING.md：用来说明如何给本项目贡献代码，包含贡献流程和流程中每个环节的详细操作。 docs：目录，用来存放本项目所有文档，例如：安装文档、使用文档、开发文档等。一些重要的文档，可以链接到项目根目录的 README.md 文档中。这些文档要确保开发者能够轻松的理解、部署和使用该项目。 examples：存放一些示例代码。 安全：开源的代码，要保证整个代码库和提交记录中，不能出现类似内部 IP、内部域名、密码、密钥这类信息。 完善的 examples：完善的 examples，可以帮助用户快速学习和使用开源项目。 好的 Commit Message 记录：开源项目在 commit 时，要遵循一定的规范，这样其他开发者才能够快速浏览和理解变更历史，减小学习成本，本项目遵循 Angular commit message 规范。 发布可用的版本：要确保每一次发布都经过充分的测试，每一个发布的版本都是可用的。 持续的更新：一个好的开源项目，应该能够持续的更新功能，修复 Bug。对于一些已经结项、不维护的开源项目，需要及时的对项目进行归档，并在项目描述中加以说明。 及时的处理 pull request、issue、评论等：当项目被别的开发者提交 pull request、issue、评论时，要及时的处理，一方面可以确保项目不断被更新，另一方面也可以激发其他开发者贡献代码的积极性。 建立讨论小组：如果条件允许，最好和贡献者建立讨论小组，每周或每月组织讨论，共同维护。 做好推广：如果有条件，可以宣传运营开源项目，让更多的人知道，更多的人用，更多的人贡献代码。例如：在掘金、简书等平台发表文章，创建 QQ、微信交流群等。 Git 工作流：选择合适的 Git 工作流，并遵循 GIt 工作流使用规范，例如 Gitflow 工作流。 开源协议 经常使用的 6 种开源协议 GPL： General Public License，开源项目最常用的许可证，衍生代码的分发需开源并且也要遵守此协议。该协议也有很多变种，不同变种要求会略微不同。 MPL： MPL 协议允许免费重发布、免费修改，但要求修改后的代码版权归软件的发起者，这种授权维护了商业软件的利益，它要求基于这种软件的修改无偿贡献版权给该软件。 LGPL： Lesser General Public Licence，是 GPL 的一个为主要为类库使用设计的开源协议。LGPL 允许商业软件通过类库引用的方式使用 LGPL 类库而不需要开源商业软件的代码。但是如果修改 LGPL 协议的代码或者衍生，则所有修改的代码，涉及修改部分的额外代码和衍生的代码都必须采用 LGPL 协议。 Apache： Apache 协议是 Apache 软件基金会发布的一个自由软件许可证，Apache 2.0 协议除了为用户提供版权许可之外，还有专利许可，非常适合涉及专利内容的项目。 BSD： BSD（Berkeley Software Distribution，伯克利软件发行版）。BSD 协议在软件分发方面，除需要包含一份版权提示和免责声明之外，没有任何限制，该协议还禁止用开源代码的作者/机构名字和原来产品的名字做市场推广。 MIT： 协议的主要内容为：该软件及其相关文档对所有人免费，可以任意处置，包括使用，复制，修改，合并，发表，分发，再授权，或者销售。唯一的限制是，软件中必须包含上述版权和许可提示。MIT 协议是所有开源许可中最宽松的一个，除了必须包含许可声明外，再无任何限制。在上图中，右边的协议比左边的协议宽松，在选择时，你可以根据菱形框中的选择项从上到下进行选择。为了使你能够毫无负担地使用 IAM 项目提供的源码，我选择了最宽松的 MIT 协议。另外，因为 Apache 是对商业应用友好的协议，使用者也可以在需要的时候修改代码来满足需要，并作为开源或商业产品发布 / 销售，所以大型公司的开源项目通常会采用 Apache 2.0 开源协议。 ","date":"2022-07-07 15:18:55","objectID":"/iam_standard_design/:1:2","tags":["iam"],"title":"iam_standard_design","uri":"/iam_standard_design/"},{"categories":["iam"],"content":"文档规范 工作中我发现，很多开发者非常注重代码产出，但不注重文档产出。他们觉得，即使没有软件文档也没太大关系，不影响软件交付。我要说的是，这种看法是错误的！因为文档属于软件交付的一个重要组成部分，没有文档的项目很难理解、部署和使用。因此，编写文档是一个必不可少的开发工作。那么一个项目需要编写哪些文档，又该如何编写呢？我认为项目中最需要的 3 类文档是 README 文档、项目文档和 API 接口文档。下面，我们一一来说它们的编写规范。 README 规范 README 文档是项目的门面，它是开发者学习项目时第一个阅读的文档，会放在项目的根目录下。因为它主要是用来介绍项目的功能、安装、部署和使用的，所以它是可以规范化的。下面，我们直接通过一个 README 模板，来看一下 README 规范中的内容： # 项目名称 \u003c!-- 写一段简短的话描述项目 --\u003e ## 功能特性 \u003c!-- 描述该项目的核心功能点 --\u003e ## 软件架构(可选) \u003c!-- 可以描述下项目的架构 --\u003e ## 快速开始 ### 依赖检查 \u003c!-- 描述该项目的依赖，比如依赖的包、工具或者其他任何依赖项 --\u003e ### 构建 \u003c!-- 描述如何构建该项目 --\u003e ### 运行 \u003c!-- 描述如何运行该项目 --\u003e ## 使用指南 \u003c!-- 描述如何使用该项目 --\u003e ## 如何贡献 \u003c!-- 告诉其他开发者如果给该项目贡献源码 --\u003e ## 社区(可选) \u003c!-- 如果有需要可以介绍一些社区相关的内容 --\u003e ## 关于作者 \u003c!-- 这里写上项目作者 --\u003e ## 谁在用(可选) \u003c!-- 可以列出使用本项目的其他有影响力的项目，算是给项目打个广告吧 --\u003e ## 许可证 \u003c!-- 这里链接上该项目的开源许可证 --\u003e 例子 项目文档规范 项目文档包括一切需要文档化的内容，它们通常集中放在 /docs 目录下。当我们在创建团队的项目文档时，通常会预先规划并创建好一些目录，用来存放不同的文档。因此，在开始 Go 项目开发之前，我们也要制定一个软件文档规范。好的文档规范有 2 个优点：易读和可以快速定位文档。不同项目有不同的文档需求，在制定文档规范时，你可以考虑包含两类文档。开发文档：用来说明项目的开发流程，比如如何搭建开发环境、构建二进制文件、测试、部署等。用户文档：软件的使用文档，对象一般是软件的使用者，内容可根据需要添加。比如，可以包括 API 文档、SDK 文档、安装文档、功能介绍文档、最佳实践、操作指南、常见问题等。为了方便全球开发者和用户使用，开发文档和用户文档，可以预先规划好英文和中文 2 个版本。为了加深你的理解，这里我们来看下实战项目的文档目录结构： docs ├── devel # 开发文档，可以提前规划好，英文版文档和中文版文档 │ ├── en-US/ # 英文版文档，可以根据需要组织文件结构 │ └── zh-CN # 中文版文档，可以根据需要组织文件结构 │ └── development.md # 开发手册，可以说明如何编译、构建、运行项目 ├── guide # 用户文档 │ ├── en-US/ # 英文版文档，可以根据需要组织文件结构 │ └── zh-CN # 中文版文档，可以根据需要组织文件结构 │ ├── api/ # API文档 │ ├── best-practice # 最佳实践，存放一些比较重要的实践文章 │ │ └── authorization.md │ ├── faq # 常见问题 │ │ ├── iam-apiserver │ │ └── installation │ ├── installation # 安装文档 │ │ └── installation.md │ ├── introduction/ # 产品介绍文档 │ ├── operation-guide # 操作指南，里面可以根据RESTful资源再划分为更细的子目录，用来存放系统核心/全部功能的操作手册 │ │ ├── policy.md │ │ ├── secret.md │ │ └── user.md │ ├── quickstart # 快速入门 │ │ └── quickstart.md │ ├── README.md # 用户文档入口文件 │ └── sdk # SDK文档 │ └── golang.md └── images # 图片存放目录 └── 部署架构v1.png API 接口文档规范 接口文档又称为 API 文档，一般由后台开发人员编写，用来描述组件提供的 API 接口，以及如何调用这些 API 接口。在项目初期，接口文档可以解耦前后端，让前后端并行开发：前端只需要按照接口文档实现调用逻辑，后端只需要按照接口文档提供功能。当前后端都开发完成之后，我们就可以直接进行联调，提高开发效率。在项目后期，接口文档可以提供给使用者，不仅可以降低组件的使用门槛，还能够减少沟通成本。显然，一个有固定格式、结构清晰、内容完善的接口文档，就非常重要了。那么我们该如何编写接口文档，它又有什么规范呢？接口文档有四种编写方式，包括编写 Word 格式文档、借助工具编写、通过注释生成和编写 Markdown 格式文档。具体的实现方式见下表： 其中，通过注释生成和编写 Markdown 格式文档这 2 种方式用得最多。在这个专栏，我采用编写 Markdown 格式文档的方式，原因如下：相比通过注释生成的方式，编写 Markdown 格式的接口文档，能表达更丰富的内容和格式，不需要在代码中添加大量注释。相比 Word 格式的文档，Markdown 格式文档占用的空间更小，能够跟随代码仓库一起发布，方便 API 文档的分发和查找。相比在线 API 文档编写工具，Markdown 格式的文档免去了第三方平台依赖和网络的限制。 API 接口文档又要遵循哪些规范呢？其实，一个规范的 API 接口文档，通常需要包含一个完整的 API 接口介绍文档、API 接口变更历史文档、通用说明、数据结构说明、错误码描述和 API 接口使用文档。API 接口使用文档中需要包含接口描述、请求方法、请求参数、输出参数和请求示例。当然，根据不同的项目需求，API 接口文档会有不同的格式和内容。我以这门课的实战项目采用的 API 接口文档规范为例，和你解释下。 接口文档拆分为以下几个 Markdown 文件，并存放在目录 docs/guide/zh-CN/api 中： README.md ：API 接口介绍文档，会分类介绍 IAM 支持的 API 接口，并会存放相关 API 接口文档的链接，方便开发者查看。 CHANGELOG.md ：API 接口文档变更历史，方便进行历史回溯，也可以使调用者决定是否进行功能更新和版本更新。 generic.md ：用来说明通用的请求参数、返回参数、认证方法和请求方法等。 struct.md ：用来列出接口文档中使用的数据结构。这些数据结构可能被多个 API 接口使用，会在 user.md、secret.md、policy.md 文件中被引用。 user.md 、 secret.md 、 policy.md ：API 接口文档，相同 REST 资源的接口会存放在一个文件中，以 REST 资源名命名文档名。 error_code.md ：错误码描述，通过程序自动生成。 这里我拿 user.md 接口文档为例，和你解释下接口文档是如何写的。user.md 文件记录了用户相关的接口，每个接口按顺序排列，包含如下 5 部分。 接口描述：描述接口实现了什么功能。请求方法：接口的请求方法，格式为 HTTP 方法 请求路径，例如 POST /v1/users。在 通用说明中的请求方法部分，会说明接口的请求协议和请求地址。输入参数：接口的输入字段，它又分为 Header 参数、Query 参数、Body 参数、Path 参数。每个字段通过：参数名称、必选、类型 和 描述 4 个属性来描述。如果参数有限制或者默认值，可以在描述部分注明。输出参数：接口的返回字段，每个字段通过 参数名称、类型 和 描述 3 个属性来描述。请求示例：一个真实的 API 接口请求和返回示例。 更详细的API接口文档 ","date":"2022-07-07 15:18:55","objectID":"/iam_standard_design/:1:3","tags":["iam"],"title":"iam_standard_design","uri":"/iam_standard_design/"},{"categories":["iam"],"content":"版本规范 在做 Go 项目开发时，我建议你把所有组件都加入版本机制。原因主要有两个：一是通过版本号，我们可以很明确地知道组件是哪个版本，从而定位到该组件的功能和代码，方便我们定位问题。二是发布组件时携带版本号，可以让使用者知道目前的项目进度，以及使用版本和上一个版本的功能差别等。目前业界主流的版本规范是语义化版本规范，也是 IAM 系统采用的版本规范。那什么是语义化版本规范呢？ 语义化版本规范（SemVer，Semantic Versioning）是 GitHub 起草的一个具有指导意义的、统一的版本号表示规范。它规定了版本号的表示、增加和比较方式，以及不同版本号代表的含义。在这套规范下，版本号及其更新方式包含了相邻版本间的底层代码和修改内容的信息。语义化版本格式为：主版本号.次版本号.修订号（X.Y.Z），其中 X、Y 和 Z 为非负的整数，且禁止在数字前方补零。 版本号可按以下规则递增：主版本号（MAJOR）：当做了不兼容的 API 修改。次版本号（MINOR）：当做了向下兼容的功能性新增及修改。这里有个不成文的约定需要你注意，偶数为稳定版本，奇数为开发版本。修订号（PATCH）：当做了向下兼容的问题修正。 你可能还看过这么一种版本号：v1.2.3-alpha.1+001。这其实是把先行版本号（Pre-release）和版本编译元数据，作为延伸加到了主版本号.次版本号.修订号的后面，格式为 X.Y.Z[-先行版本号][+版本编译元数据] 先行版本号意味着，该版本不稳定，可能存在兼容性问题，格式为：X.Y.Z-[一连串以句点分隔的标识符] ，比如下面这几个例子： 1.0.0-alpha 1.0.0-alpha.1 1.0.0-0.3.7 1.0.0-x.7.z.92 编译版本号，一般是编译器在编译过程中自动生成的，我们只定义其格式，并不进行人为控制。下面是一些编译版本号的示例： 1.0.0-alpha+001 1.0.0+20130313144700 1.0.0-beta+exp.sha.5114f85 先行版本号和编译版本号只能是字母、数字，且不可以有空格。 语义化版本控制规范 语义化版本控制规范比较多，这里我给你介绍几个比较重要的。如果你需要了解更详细的规范，可以参考 这个链接 的内容。 标记版本号的软件发行后，禁止改变该版本软件的内容，任何修改都必须以新版本发行。主版本号为零（0.y.z）的软件处于开发初始阶段，一切都可能随时被改变，这样的公共 API 不应该被视为稳定版。1.0.0 的版本号被界定为第一个稳定版本，之后的所有版本号更新都基于该版本进行修改。修订号 Z（x.y.Z | x \u003e 0）必须在只做了向下兼容的修正时才递增，这里的修正其实就是 Bug 修复。次版本号 Y（x.Y.z | x \u003e 0）必须在有向下兼容的新功能出现时递增，在任何公共 API 的功能被标记为弃用时也必须递增，当有改进时也可以递增。其中可以包括修订级别的改变。每当次版本号递增时，修订号必须归零。主版本号 X（X.y.z | X \u003e 0）必须在有任何不兼容的修改被加入公共 API 时递增。其中可以包括次版本号及修订级别的改变。每当主版本号递增时，次版本号和修订号必须归零。 如何确定版本号？说了这么多，我们到底该如何确定版本号呢？这里我给你总结了这么几个经验：第一，在实际开发的时候，我建议你使用 0.1.0 作为第一个开发版本号，并在后续的每次发行时递增次版本号。第二，当我们的版本是一个稳定的版本，并且第一次对外发布时，版本号可以定为 1.0.0。 第三，当我们严格按照 Angular commit message 规范提交代码时，版本号可以这么来确定：fix 类型的 commit 可以将修订号 +1。feat 类型的 commit 可以将次版本号 +1。带有 BREAKING CHANGE 的 commit 可以将主版本号 +1。 ","date":"2022-07-07 15:18:55","objectID":"/iam_standard_design/:1:4","tags":["iam"],"title":"iam_standard_design","uri":"/iam_standard_design/"},{"categories":["iam"],"content":"总结 一个项目的规范设计主要包括编码类和非编码类这两类规范。今天我们一起学习了开源规范、文档规范和版本规范，现在我们回顾一下重点内容吧。新开发的项目最好按照开源标准来规范，以驱动其成为一个高质量的项目。开发之前，最好提前规范好文档目录，并选择一种合适的方式来编写 API 文档。在这门课的实战项目中，我采用的是 Markdown 格式，也推荐你使用这种方式。项目要遵循版本规范，目前业界主流的版本规范是语义化版本规范，也是我推荐的版本规范。 ","date":"2022-07-07 15:18:55","objectID":"/iam_standard_design/:1:5","tags":["iam"],"title":"iam_standard_design","uri":"/iam_standard_design/"},{"categories":["iam"],"content":"5 | 规范设计（下）：commit 信息风格迥异、难以阅读，如何规范？ 在 Go 项目开发时，一个好的 Commit Message 至关重要： 可以使自己或者其他开发人员能够清晰地知道每个 commit 的变更内容，方便快速浏览变更历史，比如可以直接略过文档类型或者格式化类型的代码变更。 可以基于这些 Commit Message 进行过滤查找，比如只查找某个版本新增的功能：git log –oneline –grep “^feat|^fix|^perf”。 可以基于规范化的 Commit Message 生成 Change Log。 可以依据某些类型的 Commit Message 触发构建或者发布流程，比如当 type 类型为 feat、fix 时我们才触发 CI 流程。 确定语义化版本的版本号。比如 fix 类型可以映射为 PATCH 版本，feat 类型可以映射为 MINOR 版本。带有 BREAKING CHANGE 的 commit，可以映射为 MAJOR 版本。在这门课里，我就是通过这种方式来自动生成版本号。 接下来，我们来看下如何规范 Commit Message。另外，除了 Commit Message 之外，我还会介绍跟 Commit 相关的 3 个重点，以及如何通过自动化流程来保证 Commit Message 的规范化。 ","date":"2022-07-07 15:18:55","objectID":"/iam_standard_design/:2:0","tags":["iam"],"title":"iam_standard_design","uri":"/iam_standard_design/"},{"categories":["iam"],"content":"Commit Message 的规范有哪些？ 毫无疑问，我们可以根据需要自己来制定 Commit Message 规范，但是我更建议你采用开源社区中比较成熟的规范。一方面，可以避免重复造轮子，提高工作效率。另一方面，这些规范是经过大量开发者验证的，是科学、合理的。目前，社区有多种 Commit Message 的规范，例如 jQuery、Angular 等。我将这些规范及其格式绘制成下面一张图片，供你参考： 在这些规范中，Angular 规范在功能上能够满足开发者 commit 需求，在格式上清晰易读，目前也是用得最多的。Angular 规范其实是一种语义化的提交规范（Semantic Commit Messages），所谓语义化的提交规范包含以下内容： Commit Message 是语义化的：Commit Message 都会被归为一个有意义的类型，用来说明本次 commit 的类型。 Commit Message 是规范化的：Commit Message 遵循预先定义好的规范，比如 Commit Message 格式固定、都属于某个类型，这些规范不仅可被开发者识别也可以被工具识别。 为了方便你理解 Angular 规范，我们直接看一个遵循 Angular 规范的 commit 历史记录，见下图： 再来看一个完整的符合 Angular 规范的 Commit Message，如下图所示： 通过上面 2 张图，我们可以看到符合 Angular Commit Message 规范的 commit 都是有一定格式，有一定语义的。 那我们该怎么写出符合 Angular 规范的 Commit Message 呢？在 Angular 规范中，Commit Message 包含三个部分，分别是 Header、Body 和 Footer，格式如下： \u003ctype\u003e[optional scope]: \u003cdescription\u003e // 空行 [optional body] // 空行 [optional footer(s)] 其中，Header 是必需的，Body 和 Footer 可以省略。在以上规范中，必须用括号 () 括起来， [] 后必须紧跟冒号 ，冒号后必须紧跟空格，2 个空行也是必需的。 在实际开发中，为了使 Commit Message 在 GitHub 或者其他 Git 工具上更加易读，我们往往会限制每行 message 的长度。根据需要，可以限制为 50/72/100 个字符，这里我将长度限制在 72 个字符以内（也有一些开发者会将长度限制为 100，你可根据需要自行选择）。以下是一个符合 Angular 规范的 Commit Message： fix($compile): couple of unit tests for IE9 # Please enter the Commit Message for your changes. Lines starting # with '#' will be ignored, and an empty message aborts the commit. # On branch master # Changes to be committed: # ... Older IEs serialize html uppercased, but IE9 does not... Would be better to expect case insensitive, unfortunately jasmine does not allow to user regexps for throw expectations. Closes #392 Breaks foo.bar api, foo.baz should be used instead 接下来，我们详细看看 Angular 规范中 Commit Message 的三个部分。 Header Header 部分只有一行，包括三个字段：type（必选）、scope（可选）和 subject（必选）。我们先来说 type，它用来说明 commit 的类型。为了方便记忆，我把这些类型做了归纳，它们主要可以归为 Development 和 Production 共两类。它们的含义是： Development：这类修改一般是项目管理类的变更，不会影响最终用户和生产环境的代码，比如 CI 流程、构建方式等的修改。遇到这类修改，通常也意味着可以免测发布。 Production：这类修改会影响最终的用户和生产环境的代码。所以对于这种改动，我们一定要慎重，并在提交前做好充分的测试。 我在这里列出了 Angular 规范中的常见 type 和它们所属的类别，你在提交 Commit Message 的时候，一定要注意区分它的类别。举个例子，我们在做 Code Review 时，如果遇到 Production 类型的代码，一定要认真 Review，因为这种类型，会影响到现网用户的使用和现网应用的功能。 有这么多 type，我们该如何确定一个 commit 所属的 type 呢？这里我们可以通过下面这张图来确定。 如果我们变更了应用代码，比如某个 Go 函数代码，那这次修改属于代码类。在代码类中，有 4 种具有明确变更意图的类型：feat、fix、perf 和 style；如果我们的代码变更不属于这 4 类，那就全都归为 refactor 类，也就是优化代码。如果我们变更了非应用代码，例如更改了文档，那它属于非代码类。在非代码类中，有 3 种具有明确变更意图的类型：test、ci、docs；如果我们的非代码变更不属于这 3 类，那就全部归入到 chore 类。 Angular 的 Commit Message 规范提供了大部分的 type，在实际开发中，我们可以使用部分 type，或者扩展添加我们自己的 type。但无论选择哪种方式，我们一定要保证一个项目中的 type 类型一致。 接下来，我们说说 Header 的第二个字段 scope。 scope 是用来说明 commit 的影响范围的，它必须是名词。显然，不同项目会有不同的 scope。在项目初期，我们可以设置一些粒度比较大的 scope，比如可以按组件名或者功能来设置 scope；后续，如果项目有变动或者有新功能，我们可以再用追加的方式添加新的 scope。我们这门课采用的 scope，主要是根据组件名和功能来设置的。例如，支持 apiserver、authzserver、user 这些 scope。 这里想强调的是，scope 不适合设置太具体的值。太具体的话，一方面会导致项目有太多的 scope，难以维护。另一方面，开发者也难以确定 commit 属于哪个具体的 scope，导致错放 scope，反而会使 scope 失去了分类的意义。当然了，在指定 scope 时，也需要遵循我们预先规划的 scope，所以我们要将 scope 文档化，放在类似 devel 这类文档中。这一点你可以参考下 IAM 项目的 scope 文档： IAM commit message scope 。 最后，我们再说说 subject。subject 是 commit 的简短描述，必须以动词开头、使用现在时。比如，我们可以用 change，却不能用 changed 或 changes，而且这个动词的第一个字母必须是小写。通过这个动词，我们可以明确地知道 commit 所执行的操作。此外我们还要注意，subject 的结尾不能加英文句号。 Body Header 对 commit 做了高度概括，可以方便我们查看 Commit Message。那我们如何知道具体做了哪些变更呢？答案就是，可以通过 Body 部分，它是对本次 commit 的更详细描述，是可选的。Body 部分可以分成多行，而且格式也比较自由。不过，和 Header 里的一样，它也要以动词开头，使用现在时。此外，它还必须要包括修改的动机，以及和跟上一版本相比的改动点。我在下面给出了一个范例，你可以看看： The body is mandatory for all commits except for those of scope \"docs\". When the body is required it must be at least 20 characters long. Footer Footer 部分不是必选的，可以根据需要来选择，主要用来说明本次 commit 导致的后果。在实际应用中，Footer 通常用来说明不兼容的改动和关闭的 Issue 列表，格式如下： BREAKING CHANGE: \u003cbreaking change summary\u003e // 空行 \u003cbreaking change description + migration instructions\u003e // 空行 // 空行 Fixes #\u003cissue number\u003e 接下来，我给你详细说明下这两种情况： 不兼容的改动：如果当前代码跟上一个版本不兼容，需要在 Footer 部分，以 BREAKING CHANG: 开头，后面跟上不兼容改动的摘要。Footer 的其他部分需要说明变动的描述、变动的理由和迁移方法，例如： BREAKING CHANGE: isolate scope bindings definition has changed and the inject option for the directive c","date":"2022-07-07 15:18:55","objectID":"/iam_standard_design/:2:1","tags":["iam"],"title":"iam_standard_design","uri":"/iam_standard_design/"},{"categories":["iam"],"content":"总结 今天我向你介绍了 Commit Message 规范，主要讲了业界使用最多的 Angular 规范。Angular 规范中，Commit Message 包含三个部分：Header、Body 和 Footer。Header 对 commit 做了高度概括，Body 部分是对本次 commit 的更详细描述，Footer 部分主要用来说明本次 commit 导致的后果。格式如下： \u003ctype\u003e[optional scope]: \u003cdescription\u003e // 空行 [optional body] // 空行 [optional footer(s)] 另外，我们也需要控制 commit 的提交频率，比如可以在开发完一个功能、修复完一个 bug、下班前提交 commit。最后，我们也需要掌握一些常见的提交操作，例如通过 git rebase -i 来合并提交 commit，通过 git commit –amend 或 git rebase -i 来修改 commit message。 ","date":"2022-07-07 15:18:55","objectID":"/iam_standard_design/:2:2","tags":["iam"],"title":"iam_standard_design","uri":"/iam_standard_design/"},{"categories":["iam"],"content":"6 | 目录结构设计：如何组织一个可维护、可扩展的代码目录？ 那具体怎么组织一个好的代码目录呢？在今天这一讲，我会从 2 个维度来解答这个问题。首先，我会介绍组织目录的一些基本原则，这些原则可以指导你去组织一个好的代码目录。然后，我会向你介绍一些具体的、优秀的目录结构。你可以通过学习它们，提炼总结出你自己的目录结构设计方法，或者你也可以直接用它们作为你的目录结构规范，也就是说结构即规范。 一个好的目录结构至少要满足以下几个要求。 命名清晰：目录命名要清晰、简洁，不要太长，也不要太短，目录名要能清晰地表达出该目录实现的功能，并且目录名最好用单数。一方面是因为单数足以说明这个目录的功能，另一方面可以统一规范，避免单复混用的情况。 功能明确：一个目录所要实现的功能应该是明确的、并且在整个项目目录中具有很高的辨识度。也就是说，当需要新增一个功能时，我们能够非常清楚地知道把这个功能放在哪个目录下。 全面性：目录结构应该尽可能全面地包含研发过程中需要的功能，例如文档、脚本、源码管理、API 实现、工具、第三方包、测试、编译产物等。 可预测性：项目规模一定是从小到大的，所以一个好的目录结构应该能够在项目变大时，仍然保持之前的目录结构。 可扩展性：每个目录下存放了同类的功能，在项目变大时，这些目录应该可以存放更多同类功能。举个例子，有如下目录结构： $ ls internal/ app pkg README.md internal 目录用来实现内部代码，app 和 pkg 目录下的所有文件都属于内部代码。如果 internal 目录不管项目大小，永远只有 2 个文件 app 和 pkg，那么就说明 internal 目录是不可扩展的。相反，如果 internal 目录下直接存放每个组件的源码目录（一个项目可以由一个或多个组件组成），当项目变大、组件增多时，可以将新增加的组件代码存放到 internal 目录，这时 internal 目录就是可扩展的。例如： $ ls internal/ apiserver authzserver iamctl pkg pump watcher 刚才我讲了目录结构的总体规范，现在来看 2 个具体的、可以作为目录规范的目录结构。通常，根据功能，我们可以将目录结构分为结构化目录结构和平铺式目录结构两种。结构化目录结构主要用在 Go 应用中，相对来说比较复杂；而平铺式目录结构主要用在 Go 包中，相对来说比较简单。 ","date":"2022-07-07 15:18:55","objectID":"/iam_standard_design/:3:0","tags":["iam"],"title":"iam_standard_design","uri":"/iam_standard_design/"},{"categories":["iam"],"content":"平铺式目录结构 一个 Go 项目可以是一个应用，也可以是一个代码框架 / 库，当项目是代码框架 / 库时，比较适合采用平铺式目录结构。平铺方式就是在项目的根目录下存放项目的代码，整个目录结构看起来更像是一层的，这种方式在很多框架 / 库中存在，使用这种方式的好处是引用路径长度明显减少，比如 github.com/marmotedu/log/pkg/options，可缩短为 github.com/marmotedu/log/options。例如 log 包 github.com/golang/glog 就是平铺式的，目录如下： $ ls glog/ glog_file.go glog.go glog_test.go LICENSE README ","date":"2022-07-07 15:18:55","objectID":"/iam_standard_design/:3:1","tags":["iam"],"title":"iam_standard_design","uri":"/iam_standard_design/"},{"categories":["iam"],"content":"结构化目录结构 当前 Go 社区比较推荐的结构化目录结构是 project-layout 。虽然它并不是官方和社区的规范，但因为组织方式比较合理，被很多 Go 开发人员接受。所以，我们可以把它当作是一个事实上的规范。 首先，我们来看下在开发一个 Go 项目时，通常应该包含的功能。这些功能内容比较多，我放在了 GitHub 的 Go 项目通常包含的功能 里，我们设计的目录结构应该能够包含这些功能。我结合 project-layout，以及上面列出的 Go 项目常见功能，总结出了一套 Go 的代码结构组织方式，也就是 IAM 项目使用的目录结构。这种方式保留了 project-layout 优势的同时，还加入了一些我个人的理解，希望为你提供一个拿来即用的目录结构规范。接下来，我们一起看看这门课的实战项目所采用的 Go 目录结构。因为实战项目目录比较多，这里只列出了一些重要的目录和文件，你可以快速浏览以加深理解。 ├── api │ ├── openapi │ └── swagger ├── build │ ├── ci │ ├── docker │ │ ├── iam-apiserver │ │ ├── iam-authz-server │ │ └── iam-pump │ ├── package ├── CHANGELOG ├── cmd │ ├── iam-apiserver │ │ └── apiserver.go │ ├── iam-authz-server │ │ └── authzserver.go │ ├── iamctl │ │ └── iamctl.go │ └── iam-pump │ └── pump.go ├── configs ├── CONTRIBUTING.md ├── deployments ├── docs │ ├── devel │ │ ├── en-US │ │ └── zh-CN │ ├── guide │ │ ├── en-US │ │ └── zh-CN │ ├── images │ └── README.md ├── examples ├── githooks ├── go.mod ├── go.sum ├── init ├── internal │ ├── apiserver │ │ ├── api │ │ │ └── v1 │ │ │ └── user │ │ ├── apiserver.go │ │ ├── options │ │ ├── service │ │ ├── store │ │ │ ├── mysql │ │ │ ├── fake │ │ └── testing │ ├── authzserver │ │ ├── api │ │ │ └── v1 │ │ │ └── authorize │ │ ├── options │ │ ├── store │ │ └── testing │ ├── iamctl │ │ ├── cmd │ │ │ ├── completion │ │ │ ├── user │ │ └── util │ ├── pkg │ │ ├── code │ │ ├── options │ │ ├── server │ │ ├── util │ │ └── validation ├── LICENSE ├── Makefile ├── _output │ ├── platforms │ │ └── linux │ │ └── amd64 ├── pkg │ ├── util │ │ └── genutil ├── README.md ├── scripts │ ├── lib │ ├── make-rules ├── test │ ├── testdata ├── third_party │ └── forked └── tools 看到这一长串目录是不是有些晕？没关系，这里我们一起给这个大目录分下类，然后再具体看看每一类目录的作用，你就清楚了。在我看来，一个 Go 项目包含 3 大部分：Go 应用 、项目管理和文档。所以，我们的项目目录也可以分为这 3 大类。同时，Go 应用又贯穿开发阶段、测试阶段和部署阶段，相应的应用类的目录，又可以按开发流程分为更小的子类。当然了，这些是我建议的目录，Go 项目目录中还有一些不建议的目录。所以整体来看，我们的目录结构可以按下图所示的方式来分类： 接下来你就先专心跟着我走一遍每个目录、每个文件的作用，等你下次组织代码目录的时候，可以再回过头来看看，那时你一定会理解得更深刻。 ","date":"2022-07-07 15:18:55","objectID":"/iam_standard_design/:3:2","tags":["iam"],"title":"iam_standard_design","uri":"/iam_standard_design/"},{"categories":["iam"],"content":"Go 应用 ：主要存放前后端代码 首先，我们来说说开发阶段所涉及到的目录。我们开发的代码包含前端代码和后端代码，可以分别存放在前端目录和后端目录中。 /web前端代码存放目录，主要用来存放 Web 静态资源，服务端模板和单页应用（SPAs）。/cmd一个项目有很多组件，可以把组件 main 函数所在的文件夹统一放在/cmd 目录下，例如： $ ls cmd/ gendocs geniamdocs genman genswaggertypedocs genyaml iam-apiserver iam-authz-server iamctl iam-pump $ ls cmd/iam-apiserver/ apiserver.go 每个组件的目录名应该跟你期望的可执行文件名是一致的。这里要保证 /cmd/\u003c组件名\u003e 目录下不要存放太多的代码，如果你认为代码可以导入并在其他项目中使用，那么它应该位于 /pkg 目录中。如果代码不是可重用的，或者你不希望其他人重用它，请将该代码放到 /internal 目录中。 /internal存放私有应用和库代码。如果一些代码，你不希望在其他应用和库中被导入，可以将这部分代码放在/internal 目录下。在引入其它项目 internal 下的包时，Go 语言会在编译时报错： An import of a path containing the element “internal” is disallowed if the importing code is outside the tree rooted at the parent of the \"internal\" directory. 可以通过 Go 语言本身的机制来约束其他项目 import 项目内部的包。/internal 目录建议包含如下目录： /internal/apiserver：该目录中存放真实的应用代码。这些应用的共享代码存放在/internal/pkg 目录下。 /internal/pkg：存放项目内可共享，项目外不共享的包。这些包提供了比较基础、通用的功能，例如工具、错误码、用户验证等功能。 我的建议是，一开始将所有的共享代码存放在 /internal/pkg 目录下，当该共享代码做好了对外开发的准备后，再转存到/pkg目录下。下面，我详细介绍下 IAM 项目的 internal目录 ，来加深你对 internal 的理解，目录结构如下： ├── apiserver │ ├── api │ │ └── v1 │ │ └── user │ ├── options │ ├── config │ ├── service │ │ └── user.go │ ├── store │ │ ├── mysql │ │ │ └── user.go │ │ ├── fake │ └── testing ├── authzserver │ ├── api │ │ └── v1 │ ├── options │ ├── store │ └── testing ├── iamctl │ ├── cmd │ │ ├── cmd.go │ │ ├── info └── pkg ├── code ├── middleware ├── options └── validation /internal 目录大概分为 3 类子目录： /internal/pkg：内部共享包存放的目录。 /internal/authzserver、/internal/apiserver、/internal/pump、/internal/iamctl：应用目录，里面包含应用程序的实现代码。 /internal/iamctl：对于一些大型项目，可能还会需要一个客户端工具。 在每个应用程序内部，也会有一些目录结构，这些目录结构主要根据功能来划分： /internal/apiserver/api/v1：HTTP API 接口的具体实现，主要用来做 HTTP 请求的解包、参数校验、业务逻辑处理、返回。注意这里的业务逻辑处理应该是轻量级的，如果业务逻辑比较复杂，代码量比较多，建议放到 /internal/apiserver/service 目录下。该源码文件主要用来串流程。 /internal/apiserver/options：应用的 command flag。 /internal/apiserver/config：根据命令行参数创建应用配置。 /internal/apiserver/service：存放应用复杂业务处理代码。 /internal/apiserver/store/mysql：一个应用可能要持久化的存储一些数据，这里主要存放跟数据库交互的代码，比如 Create、Update、Delete、Get、List 等。 /internal/pkg 目录存放项目内可共享的包，通常可以包含如下目录： /internal/pkg/code：项目业务 Code 码。 /internal/pkg/validation：一些通用的验证函数。 /internal/pkg/middleware：HTTP 处理链。 /pkg/pkg 目录是 Go 语言项目中非常常见的目录，我们几乎能够在所有知名的开源项目（非框架）中找到它的身影，例如 Kubernetes、Prometheus、Moby、Knative 等。该目录中存放可以被外部应用使用的代码库，其他项目可以直接通过 import 导入这里的代码。所以，我们在将代码库放入该目录时一定要慎重。 /vendor项目依赖，可通过 go mod vendor 创建。需要注意的是，如果是一个 Go 库，不要提交 vendor 依赖包。 /third_party外部帮助工具，分支代码或其他第三方应用（例如 Swagger UI）。比如我们 fork 了一个第三方 go 包，并做了一些小的改动，我们可以放在目录 /third_party/forked 下。一方面可以很清楚的知道该包是 fork 第三方的，另一方面又能够方便地和 upstream 同步。 ","date":"2022-07-07 15:18:55","objectID":"/iam_standard_design/:3:3","tags":["iam"],"title":"iam_standard_design","uri":"/iam_standard_design/"},{"categories":["iam"],"content":"Go 应用：主要存放测试相关的文件和代码 接着，我们再来看下测试阶段相关的目录，它可以存放测试相关的文件。 /test用于存放其他外部测试应用和测试数据。/test 目录的构建方式比较灵活：对于大的项目，有一个数据子目录是有意义的。例如，如果需要 Go 忽略该目录中的内容，可以使用 /test/data 或 /test/testdata 目录。需要注意的是，Go 也会忽略以“.”或 “_” 开头的目录或文件。这样在命名测试数据目录方面，可以具有更大的灵活性。 ","date":"2022-07-07 15:18:55","objectID":"/iam_standard_design/:3:4","tags":["iam"],"title":"iam_standard_design","uri":"/iam_standard_design/"},{"categories":["iam"],"content":"Go 应用：存放跟应用部署相关的文件 接着，我们再来看下与部署阶段相关的目录，这些目录可以存放部署相关的文件。 /configs这个目录用来配置文件模板或默认配置。例如，可以在这里存放 confd 或 consul-template 模板文件。这里有一点要注意，配置中不能携带敏感信息，这些敏感信息，我们可以用占位符来替代，例如： apiVersion: v1 user: username: ${CONFIG_USER_USERNAME} # iam 用户名 password: ${CONFIG_USER_PASSWORD} # iam 密码 /deployments用来存放 Iaas、PaaS 系统和容器编排部署配置和模板（Docker-Compose，Kubernetes/Helm，Mesos，Terraform，Bosh）。在一些项目，特别是用 Kubernetes 部署的项目中，这个目录可能命名为 deploy。为什么要将这类跟 Kubernetes 相关的目录放到目录结构中呢？主要是因为当前软件部署基本都在朝着容器化的部署方式去演进。 /init存放初始化系统（systemd，upstart，sysv）和进程管理配置文件（runit，supervisord）。比如 sysemd 的 unit 文件。这类文件，在非容器化部署的项目中会用到。 ","date":"2022-07-07 15:18:55","objectID":"/iam_standard_design/:3:5","tags":["iam"],"title":"iam_standard_design","uri":"/iam_standard_design/"},{"categories":["iam"],"content":"项目管理：存放用来管理 Go 项目的各类文件 在做项目开发时，还有些目录用来存放项目管理相关的文件，这里我们一起来看下。 /Makefile虽然 Makefile 是一个很老的项目管理工具，但它仍然是最优秀的项目管理工具。所以，一个 Go 项目在其根目录下应该有一个 Makefile 工具，用来对项目进行管理，Makefile 通常用来执行静态代码检查、单元测试、编译等功能。其他常见功能： 静态代码检查(lint)：推荐用 golangci-lint。 单元测试(test)：运行 go test ./…。 编译(build)：编译源码，支持不同的平台，不同的 CPU 架构。 镜像打包和发布(image/image.push)：现在的系统比较推荐用 Docker/Kubernetes 进行部署，所以一般也要有镜像构建功能。 清理（clean）:清理临时文件或者编译后的产物。 代码生成（gen）：比如要编译生成 protobuf pb.go 文件。 部署（deploy，可选）：一键部署功能，方便测试。 发布（release）：发布功能，比如：发布到 Docker Hub、github 等。 帮助（help）:告诉 Makefile 有哪些功能，如何执行这些功能。 版权声明（add-copyright）：如果是开源项目，可能需要在每个文件中添加版权头，这可以通过 Makefile 来添加。 API 文档（swagger）：如果使用 swagger 来生成 API 文档，这可以通过 Makefile 来生成。 我还有一条建议：直接执行 make 时，执行如下各项 format -\u003e lint -\u003e test -\u003e build，如果是有代码生成的操作，还可能需要首先生成代码 gen -\u003e format -\u003e lint -\u003e test -\u003e build。在实际开发中，我们可以将一些重复性的工作自动化，并添加到 Makefile 文件中统一管理。 /scripts该目录主要用来存放脚本文件，实现构建、安装、分析等不同功能。不同项目，里面可能存放不同的文件，但通常可以考虑包含以下 3 个目录： /scripts/make-rules：用来存放 makefile 文件，实现 /Makefile 文件中的各个功能。Makefile 有很多功能，为了保持它的简洁，我建议你将各个功能的具体实现放在/scripts/make-rules 文件夹下。 /scripts/lib：shell 库，用来存放 shell 脚本。一个大型项目中有很多自动化任务，比如发布、更新文档、生成代码等，所以要写很多 shell 脚本，这些 shell 脚本会有一些通用功能，可以抽象成库，存放在/scripts/lib 目录下，比如 logging.sh，util.sh 等。 /scripts/install：如果项目支持自动化部署，可以将自动化部署脚本放在此目录下。如果部署脚本简单，也可以直接放在 /scripts 目录下。 另外，shell 脚本中的函数名，建议采用语义化的命名方式，例如 iam::log::info 这种语义化的命名方式，可以使调用者轻松的辨别出函数的功能类别，便于函数的管理和引用。在 Kubernetes 的脚本中，就大量采用了这种命名方式。 /build这里存放安装包和持续集成相关的文件。这个目录下有 3 个大概率会使用到的目录，在设计目录结构时可以考虑进去。 /build/package：存放容器（Docker）、系统（deb, rpm, pkg）的包配置和脚本。 /build/ci：存放 CI（travis，circle，drone）的配置文件和脚本。 /build/docker：存放子项目各个组件的 Dockerfile 文件。 /tools存放这个项目的支持工具。这些工具可导入来自 /pkg 和 /internal 目录的代码。 /githooksGit 钩子。比如，我们可以将 commit-msg 存放在该目录。 /assets项目使用的其他资源 (图片、CSS、JavaScript 等)。 /website如果你不使用 GitHub 页面，那么可以在这里放置项目网站相关的数据。 ","date":"2022-07-07 15:18:55","objectID":"/iam_standard_design/:3:6","tags":["iam"],"title":"iam_standard_design","uri":"/iam_standard_design/"},{"categories":["iam"],"content":"文档：主要存放项目的各类文档 一个项目，也包含一些文档，这些文档有很多类别，也需要一些目录来存放这些文档，这里我们也一起来看下。 /README.md项目的 README 文件一般包含了项目的介绍、功能、快速安装和使用指引、详细的文档链接以及开发指引等。有时候 README 文档会比较长，为了能够快速定位到所需内容，需要添加 markdown toc 索引，可以借助工具 tocenize 来完成索引的添加。这里还有个建议，前面我们也介绍过 README 是可以规范化的，所以这个 README 文档，可以通过脚本或工具来自动生成。 /docs存放设计文档、开发文档和用户文档等（除了 godoc 生成的文档）。推荐存放以下几个子目录： /docs/devel/{en-US,zh-CN}：存放开发文档、hack 文档等。 /docs/guide/{en-US,zh-CN}: 存放用户手册，安装、quickstart、产品文档等，分为中文文档和英文文档。 /docs/images：存放图片文件。 /CONTRIBUTING.md如果是一个开源就绪的项目，最好还要有一个 CONTRIBUTING.md 文件，用来说明如何贡献代码，如何开源协同等等。CONTRIBUTING.md 不仅能够规范协同流程，还能降低第三方开发者贡献代码的难度。 /api /api 目录中存放的是当前项目对外提供的各种不同类型的 API 接口定义文件，其中可能包含类似 /api/protobuf-spec、/api/thrift-spec、/api/http-spec、openapi、swagger 的目录，这些目录包含了当前项目对外提供和依赖的所有 API 文件。例如，如下是 IAM 项目的 /api 目录： ├── openapi/ │ └── README.md └── swagger/ ├── docs/ ├── README.md └── swagger.yaml 二级目录的主要作用，就是在一个项目同时提供了多种不同的访问方式时，可以分类存放。用这种方式可以避免潜在的冲突，也能让项目结构更加清晰。 /LICENSE版权文件可以是私有的，也可以是开源的。常用的开源协议有：Apache 2.0、MIT、BSD、GPL、Mozilla、LGPL。有时候，公有云产品为了打造品牌影响力，会对外发布一个本产品的开源版本，所以在项目规划初期最好就能规划下未来产品的走向，选择合适的 LICENSE。为了声明版权，你可能会需要将 LICENSE 头添加到源码文件或者其他文件中，这部分工作可以通过工具实现自动化，推荐工具： addlicense 。当代码中引用了其它开源代码时，需要在 LICENSE 中说明对其它源码的引用，这就需要知道代码引用了哪些源码，以及这些源码的开源协议，可以借助工具来进行检查，推荐工具： glice 。至于如何说明对其它源码的引用，大家可以参考下 IAM 项目的 LICENSE 文件。 /CHANGELOG当项目有更新时，为了方便了解当前版本的更新内容或者历史更新内容，需要将更新记录存放到 CHANGELOG 目录。编写 CHANGELOG 是一个复杂、繁琐的工作，我们可以结合 Angular 规范 和 git-chglog 来自动生成 CHANGELOG。 /examples存放应用程序或者公共包的示例代码。这些示例代码可以降低使用者的上手门槛。 ","date":"2022-07-07 15:18:55","objectID":"/iam_standard_design/:3:7","tags":["iam"],"title":"iam_standard_design","uri":"/iam_standard_design/"},{"categories":["iam"],"content":"不建议的目录 除了上面这些我们建议的目录，在 Go 项目中，还有一些目录是不建议包含的，这些目录不符合 Go 的设计哲学。 /src/一些开发语言，例如 Java 项目中会有 src 目录。在 Java 项目中， src 目录是一种常见的模式，但在 Go 项目中，不建议使用 src 目录。其中一个重要的原因是：在默认情况下，Go 语言的项目都会被放置到$GOPATH/src 目录下。这个目录中存放着所有代码，如果我们在自己的项目中使用/src 目录，这个包的导入路径中就会出现两个 src，例如： $GOPATH/src/github.com/marmotedu/project/src/main.go 这样的目录结构看起来非常怪。 xxs/在 Go 项目中，要避免使用带复数的目录或者包。建议统一使用单数。 ","date":"2022-07-07 15:18:55","objectID":"/iam_standard_design/:3:8","tags":["iam"],"title":"iam_standard_design","uri":"/iam_standard_design/"},{"categories":["iam"],"content":"一些建议 上面介绍的目录结构包含很多目录，但一个小型项目用不到这么多目录。对于小型项目，可以考虑先包含 cmd、pkg、internal 3 个目录，其他目录后面按需创建，例如： $ tree --noreport -L 2 tms tms ├── cmd ├── internal ├── pkg └── README.md 另外，在设计目录结构时，一些空目录无法提交到 Git 仓库中，但我们又想将这个空目录上传到 Git 仓库中，以保留目录结构。这时候，可以在空目录下加一个 .keep 文件，例如： $ ls -A build/ci/ .keep ","date":"2022-07-07 15:18:55","objectID":"/iam_standard_design/:3:9","tags":["iam"],"title":"iam_standard_design","uri":"/iam_standard_design/"},{"categories":["iam"],"content":"7 | 工作流设计：如何设计合理的多人开发模式？ 一个企业级项目是由多人合作完成的，不同开发者在本地开发完代码之后，可能提交到同一个代码仓库，同一个开发者也可能同时开发几个功能特性。这种多人合作开发、多功能并行开发的特性如果处理不好，就会带来诸如丢失代码、合错代码、代码冲突等问题。所以，在编码之前，我们需要设计一个合理的开发模式。又因为目前开发者基本都是基于 Git 进行开发的，所以本节课，我会教你怎么基于 Git 设计出一个合理的开发模式。 那么如何设计工作流呢？你可以根据需要，自己设计工作流，也可以采用业界沉淀下来的、设计好的、受欢迎的工作流。一方面，这些工作流经过长时间的实践，被证明是合理的；另一方面，采用一种被大家熟知且业界通用的工作流，会减少团队内部磨合的时间。在这一讲中，我会为你介绍 4 种受欢迎的工作流，你可以选择其中一种作为你的工作流设计。 在使用 Git 开发时，有 4 种常用的工作流，也叫开发模式，按演进顺序分为集中式工作流、功能分支工作流、Git Flow 工作流和 Forking 工作流。接下来，我会按演进顺序分别介绍这 4 种工作流。 ","date":"2022-07-07 15:18:55","objectID":"/iam_standard_design/:4:0","tags":["iam"],"title":"iam_standard_design","uri":"/iam_standard_design/"},{"categories":["iam"],"content":"集中式工作流 我们先来看看集中式工作流，它是最简单的一种开发方式。集中式工作流的工作模式如下图所示：A、B、C 为 3 位开发者，每位开发者都在本地有一份远程仓库的拷贝：本地仓库。A、B、C 在本地的 master 分支开发完代码之后，将修改后的代码 commit 到远程仓库，如果有冲突就先解决本地的冲突再提交。在进行了一段时间的开发之后，远程仓库 master 分支的日志可能如下图所示： 集中式工作流是最简单的开发模式，但它的缺点也很明显：不同开发人员的提交日志混杂在一起，难以定位问题。如果同时开发多个功能，不同功能同时往 master 分支合并，代码之间也会相互影响，从而产生代码冲突。 和其他工作流相比，集中式工作流程的代码管理较混乱，容易出问题，因此适合用在团队人数少、开发不频繁、不需要同时维护多个版本的小项目中。当我们想要并行开发多个功能时，这种工作流就不适用了，这时候怎么办呢？我们接下来看功能分支工作流。 ","date":"2022-07-07 15:18:55","objectID":"/iam_standard_design/:4:1","tags":["iam"],"title":"iam_standard_design","uri":"/iam_standard_design/"},{"categories":["iam"],"content":"功能分支工作流 功能分支工作流基于集中式工作流演进而来。在开发新功能时，基于 master 分支新建一个功能分支，在功能分支上进行开发，而不是直接在本地的 master 分支开发，开发完成之后合并到 master 分支，如下图所示： 相较于集中式工作流，这种工作流让不同功能在不同的分支进行开发，只在最后一步合并到 master 分支，不仅可以避免不同功能之间的相互影响，还可以使提交历史看起来更加简洁。还有，在合并到 master 分支时，需要提交 PR（pull request），而不是直接将代码 merge 到 master 分支。PR 流程不仅可以把分支代码提供给团队其他开发人员进行 CR（Code Review），还可以在 PR 页面讨论代码。通过 CR ，我们可以确保合并到 master 的代码是健壮的；通过 PR 页面的讨论，可以使开发者充分参与到代码的讨论中，有助于提高代码的质量，并且提供了一个代码变更的历史回顾途径。 那么，功能分支工作流具体的开发流程是什么呢？我们一起来看下。 基于 master 分支新建一个功能分支，功能分支可以取一些有意义的名字，便于理解，例如 feature/rate-limiting。 $ git checkout -b feature/rate-limiting 在功能分支上进行代码开发，开发完成后 commit 到功能分支。 $ git add limit.go $ git commit -m \"add rate limiting\" 将本地功能分支代码 push 到远程仓库。 $ git push origin feature/rate-limiting 在远程仓库上创建 PR（例如：GitHub）。进入 GitHub 平台上的项目主页，点击 Compare \u0026 pull request 提交 PR，如下图所示。 点击 Compare \u0026 pull request 后会进入 PR 页面，在该页面中可以根据需要填写评论，最后点击 Create pull request 提交 PR。 代码管理员收到 PR 后，可以 CR 代码，CR 通过后，再点击 Merge pull request 将 PR 合并到 master，如下图所示。 图中的“Merge pull request” 提供了 3 种 merge 方法： Create a merge commit：GitHub 的底层操作是 git merge –no-ff。feature 分支上所有的 commit 都会加到 master 分支上，并且会生成一个 merge commit。这种方式可以让我们清晰地知道是谁做了提交，做了哪些提交，回溯历史的时候也会更加方便。 Squash and merge：GitHub 的底层操作是 git merge –squash。Squash and merge 会使该 pull request 上的所有 commit 都合并成一个 commit ，然后加到 master 分支上，但原来的 commit 历史会丢失。如果开发人员在 feature 分支上提交的 commit 非常随意，没有规范，那么我们可以选择这种方法来丢弃无意义的 commit。但是在大型项目中，每个开发人员都应该是遵循 commit 规范的，因此我不建议你在团队开发中使用 Squash and merge。 Rebase and merge：GitHub 的底层操作是 git rebase。这种方式会将 pull request 上的所有提交历史按照原有顺序依次添加到 master 分支的头部（HEAD）。因为 git rebase 有风险，在你不完全熟悉 Git 工作流时，我不建议 merge 时选择这个。 通过分析每个方法的优缺点，在实际的项目开发中，我比较推荐你使用 Create a merge commit 方式。 从刚才讲完的具体开发流程中，我们可以感受到，功能分支工作流上手比较简单，不仅能使你并行开发多个功能，还可以添加 code review，从而保障代码质量。当然它也有缺点，就是无法给分支分配明确的目的，不利于团队配合。它适合用在开发团队相对固定、规模较小的项目中。接下来我们要讲的 Git Flow 工作流以功能分支工作流为基础，较好地解决了上述问题。 ","date":"2022-07-07 15:18:55","objectID":"/iam_standard_design/:4:2","tags":["iam"],"title":"iam_standard_design","uri":"/iam_standard_design/"},{"categories":["iam"],"content":"Git Flow 工作流 Git Flow 工作流是一个非常成熟的方案，也是非开源项目中最常用到的工作流。它定义了一个围绕项目发布的严格分支模型，通过为代码开发、发布和维护分配独立的分支来让项目的迭代流程更加顺畅，比较适合大型的项目或者迭代速度快的项目。接下来，我会通过介绍 Git Flow 的 5 种分支和工作流程，来给你讲解 GIt Flow 是如何工作的。 Git Flow 的 5 种分支 Git Flow 中定义了 5 种分支，分别是 master、develop、feature、release 和 hotfix。其中，master 和 develop 为常驻分支，其他为非常驻分支，不同的研发阶段会用到不同的分支。这 5 种分支的详细介绍见下表： Git Flow 开发流程 这里我们用一个实际的例子来演示下 Git Flow 的开发流程。场景如下： a. 当前版本为：0.9.0。b. 需要新开发一个功能，使程序执行时向标准输出输出“hello world”字符串。c. 在开发阶段，线上代码有 Bug 需要紧急修复。 假设我们的 Git 项目名为 gitflow-demo，项目目录下有 2 个文件，分别是 README.md 和 main.go，内容如下。 package main import \"fmt\" func main() { fmt.Println(\"callmainfunction\") } 具体的开发流程有 12 步，你可以跟着以下步骤操作练习。创 # 建一个常驻的分支：develop。 $ git checkout -b develop master # 基于 develop 分支，新建一个功能分支：feature/print-hello-world。 $ git checkout -b feature/print-hello-world develop feature/print-hello-world 分支中，在 main.go 文件中添加一行代码fmt.Println(“Hello”)，添加后的代码如下。 package main import \"fmt\" func main() { fmt.Println(\"callmainfunction\") fmt.Println(\"Hello\") } 紧急修复 Bug。我们正处在新功能的开发中（只完成了 fmt.Println(“Hello”)而非 fmt.Println(“Hello World”)）突然线上代码发现了一个 Bug，我们要立即停止手上的工作，修复线上的 Bug，步骤如下。 $ git stash # 1. 开发工作只完成了一半，还不想提交，可以临时保存修改至堆栈区 $ git checkout -b hotfix/print-error master # 2. 从 master 建立 hotfix 分支 $ vi main.go # 3. 修复 bug，callmainfunction -\u003e call main function $ git commit -a -m 'fix print message error bug' # 4. 提交修复 $ git checkout develop # 5. 切换到 develop 分支 $ git merge --no-ff hotfix/print-error # 6. 把 hotfix 分支合并到 develop 分支 $ git checkout master # 7. 切换到 master 分支 $ git merge --no-ff hotfix/print-error # 8. 把 hotfix 分支合并到 master $ git tag -a v0.9.1 -m \"fix log bug\" # 9. master 分支打 tag $ go build -v . # 10. 编译代码，并将编译好的二进制更新到生产环境 $ git branch -d hotfix/print-error # 11. 修复好后，删除 hotfix/xxx 分支 $ git checkout feature/print-hello-world # 12. 切换到开发分支下 $ git merge --no-ff develop # 13. 因为 develop 有更新，这里最好同步更新下 $ git stash pop # 14. 恢复到修复前的工作状态 继续开发。在 main.go 中加入 fmt.Println(“Hello World”)。 提交代码到 feature/print-hello-world 分支。 $ git commit -a -m \"print 'hello world'\" 在 feature/print-hello-world 分支上做 code review。首先，我们需要将 feature/print-hello-world push 到代码托管平台，例如 GitHub 上。 $ git push origin feature/print-hello-world 然后，我们在 GitHub 上，基于 feature/print-hello-world 创建 pull request，如下图所示。 创建完 pull request 之后，我们就可以指定 Reviewers 进行 code review，如下图所示。 code review 通过后，由代码仓库 matainer 将功能分支合并到 develop 分支。 $ git checkout develop $ git merge --no-ff feature/print-hello-world 基于 develop 分支，创建 release 分支，测试代码。 $ git checkout -b release/1.0.0 develop $ go build -v . # 构建后，部署二进制文件，并测试 测试失败，因为我们要求打印“hello world”，但打印的是“Hello World”，修复的时候，我们直接在 release/1.0.0 分支修改代码，修改完成后，提交并编译部署。 $ git commit -a -m \"fix bug\" $ go build -v . 测试通过后，将功能分支合并到 master 分支和 develop 分支。 $ git checkout develop $ git merge --no-ff release/1.0.0 $ git checkout master $ git merge --no-ff release/1.0.0 $ git tag -a v1.0.0 -m \"add print hello world\" # master 分支打 tag 删除 feature/print-hello-world 分支，也可以选择性删除 release/1.0.0 分支。 $ git branch -d feature/print-hello-world 亲自操作一遍之后，你应该会更了解这种模式的优缺点。它的缺点，就是你刚才已经体会到的，它有一定的上手难度。不过 Git Flow 工作流还是有很多优点的：Git Flow 工作流的每个分支分工明确，这可以最大程度减少它们之间的相互影响。因为可以创建多个分支，所以也可以并行开发多个功能。另外，和功能分支工作流一样，它也可以添加 code review，保障代码质量。 因此，Git Flow 工作流比较适合开发团队相对固定，规模较大的项目。 ","date":"2022-07-07 15:18:55","objectID":"/iam_standard_design/:4:3","tags":["iam"],"title":"iam_standard_design","uri":"/iam_standard_design/"},{"categories":["iam"],"content":"Forking 工作流 上面讲的 Git Flow 是非开源项目中最常用的，而在开源项目中，最常用到的是 Forking 工作流，例如 Kubernetes、Docker 等项目用的就是这种工作流。这里，我们先来了解下 fork 操作。fork 操作是在个人远程仓库新建一份目标远程仓库的副本，比如在 GitHub 上操作时，在项目的主页点击 fork 按钮（页面右上角），即可拷贝该目标远程仓库。Forking 工作流的流程如下图所示。 假设开发者 A 拥有一个远程仓库，如果开发者 B 也想参与 A 项目的开发，B 可以 fork 一份 A 的远程仓库到自己的 GitHub 账号下。后续 B 可以在自己的项目进行开发，开发完成后，B 可以给 A 提交一个 PR。这时候 A 会收到通知，得知有新的 PR 被提交，A 会去查看 PR 并 code review。如果有问题，A 会直接在 PR 页面提交评论，B 看到评论后会做进一步的修改。最后 A 通过 B 的 PR 请求，将代码合并进了 A 的仓库。这样就完成了 A 代码仓库新特性的开发。如果有其他开发者想给 A 贡献代码，也会执行相同的操作。 GitHub 中的 Forking 工作流详细步骤共有 6 步（假设目标仓库为 gitflow-demo），你可以跟着以下步骤操作练习。 Fork 远程仓库到自己的账号下。访问https://github.com/marmotedu/gitflow-demo ，点击 fork 按钮。fork 后的仓库地址为：https://github.com/colin404fork/gitflow-demo 。 克隆 fork 的仓库到本地。 $ git clone https://github.com/colin404fork/gitflow-demo $ cd gitflow-demo $ git remote add upstream https://github.com/marmotedu/gitflow-demo $ git remote set-url --push upstream no_push # Never push to upstream master $ git remote -v # Confirm that your remotes make sense origin https://github.com/colin404fork/gitflow-demo (fetch) origin https://github.com/colin404fork/gitflow-demo (push) upstream https://github.com/marmotedu/gitflow-demo (fetch) upstream https://github.com/marmotedu/gitflow-demo (push) 创建功能分支。首先，要同步本地仓库的 master 分支为最新的状态（跟 upstream master 分支一致）。 $ git fetch upstream $ git checkout master $ git rebase upstream/master 然后，创建功能分支。 $ git checkout -b feature/add-function 提交 commit。在 feature/add-function 分支上开发代码，开发完代码后，提交 commit。 $ git fetch upstream # commit 前需要再次同步 feature 跟 upstream/master $ git rebase upstream/master $ git add \u003cfile\u003e $ git status $ git commit 分支开发完成后，可能会有一堆 commit，但是合并到主干时，我们往往希望只有一个（或最多两三个）commit，这可以使功能修改都放在一个或几个 commit 中，便于后面的阅读和维护。这个时候，我们可以用 git rebase 来合并和修改我们的 commit，操作如下： $ git rebase -i origin/master 第 5 讲已经介绍过了git rebase -i 的使用方法 ，如果你有疑问可以再去看看，这里不再说明。还有另外一种合并 commit 的简便方法，就是先撤销过去 5 个 commit，然后再建一个新的： $ git reset HEAD~5 $ git add . $ git commit -am \"Here's the bug fix that closes #28\" $ git push --force squash 和 fixup 命令，还可以当作命令行参数使用，自动合并 commit。 $ git commit --fixup $ git rebase -i --autosquash push 功能分支到个人远程仓库。在完成了开发，并 commit 后，需要将功能分支 push 到个人远程代码仓库，代码如下： $ git push -f origin feature/add-function 在个人远程仓库页面创建 pull request。提交到远程仓库以后，我们就可以创建 pull request，然后请求 reviewers 进行代码 review，确认后合并到 master。这里要注意，创建 pull request 时，base 通常选择目标远程仓库的 master 分支。 我们已经讲完了 Forking 工作流的具体步骤，你觉得它有什么优缺点呢？ 结合操作特点，我们来看看它的优点：Forking 工作流中，项目远程仓库和开发者远程仓库完全独立，开发者通过提交 Pull Request 的方式给远程仓库贡献代码，项目维护者选择性地接受任何开发者的提交，通过这种方式，可以避免授予开发者项目远程仓库的权限，从而提高项目远程仓库的安全性，这也使得任意开发者都可以参与项目的开发。但 Forking 工作流也有局限性，就是对于职能分工明确且不对外开源的项目优势不大。 Forking 工作流比较适用于以下三种场景：（1）开源项目中；（2）开发者有衍生出自己的衍生版的需求；（3）开发者不固定，可能是任意一个能访问到项目的开发者。 ","date":"2022-07-07 15:18:55","objectID":"/iam_standard_design/:4:4","tags":["iam"],"title":"iam_standard_design","uri":"/iam_standard_design/"},{"categories":["iam"],"content":"回顾 这一讲中，我基于 Git 向你介绍了 4 种开发模式，现在跟我回顾一下吧。 集中式工作流：开发者直接在本地 master 分支开发代码，开发完成后 push 到远端仓库 master 分支。功能分支工作流：开发者基于 master 分支创建一个新分支，在新分支进行开发，开发完成后合并到远端仓库 master 分支。Git Flow 工作流：Git Flow 工作流为不同的分支分配一个明确的角色，并定义分支之间什么时候、如何进行交互，比较适合大型项目的开发。Forking 工作流：开发者先 fork 项目到个人仓库，在个人仓库完成开发后，提交 pull request 到目标远程仓库，远程仓库 review 后，合并 pull request 到 master 分支。 集中式工作流是最早的 Git 工作流，功能分支工作流以集中式工作流为基础，Git Flow 工作流又是以功能分支工作流为基础，Forking 工作流在 Git Flow 工作流基础上，解耦了个人远端仓库和项目远端仓库。 每种开发模式各有优缺点，适用于不同的场景，我总结在下表中： 总的来说，在选择工作流时，我的推荐如下：非开源项目采用 Git Flow 工作流。开源项目采用 Forking 工作流。因为这门课的实战项目对于项目开发者来说是一个偏大型的非开源项目，所以采用了 Git Flow 工作流。 ","date":"2022-07-07 15:18:55","objectID":"/iam_standard_design/:4:5","tags":["iam"],"title":"iam_standard_design","uri":"/iam_standard_design/"},{"categories":["iam"],"content":"8 | 研发流程设计（上）：如何设计 Go 项目的开发流程？ 一个不合理的研发流程会带来很多问题，例如：代码管理混乱。合并代码时出现合错、合丢、代码冲突等问题。研发效率低。编译、测试、静态代码检查等全靠手动操作，效率低下。甚至，因为没有标准的流程，一些开发者会漏掉测试、静态代码检查等环节。发布效率低。发布周期长，以及发布不规范造成的现网问题频发。 项目研发流程会因为团队、项目、需求等的不同而不同，很难概括出一个方法论让你去设计研发流程。 在这一讲中，我会介绍一种业界已经设计好的、相对标准的研发流程，来告诉你怎么设计研发流程。通过学习它，你不仅能够了解到项目研发的通用流程，而且还可以基于这个流程来优化、定制，满足你自己的流程需求。 ","date":"2022-07-07 15:18:55","objectID":"/iam_standard_design/:5:0","tags":["iam"],"title":"iam_standard_design","uri":"/iam_standard_design/"},{"categories":["iam"],"content":"在设计研发流程时，需要关注哪些点？ 在看具体的研发流程之前，我们需要先思考一个问题：你觉得，一个好的流程应该是什么样子的？虽然我们刚才说了，不同团队、项目、需求的研发流程不会一成不变，但为了最大限度地提高研发效能，这些不同的流程都会遵循下面这几个原则。 发布效率高：研发流程应该能提高发布效率，减少发布时间和人工介入的工作量。 发布质量高：研发流程应该能够提高发布质量，确保发布出去的代码是经过充分测试的，并且完全避免人为因素造成的故障。 迭代速度快：整个研发流程要能支持快速迭代，产品迭代速度越快，意味着产品的竞争力越强，在互联网时代越能把握先机。 明确性：整个研发流程中角色的职责、使用的工具、方法和流程都应该是明确的，这可以增强流程的可执行性。 流程合理：研发流程最终是供产品、开发、测试、运维等人员使用的，所以整个流程设计不能是反人类的，要能够被各类参与人员接受并执行。 柔性扩展：研发流程应该是柔性且可扩展的，能够灵活变通，并适应各类场景。 输入输出：研发流程中的每个阶段都应该有明确的输入和输出，这些输入和输出标志着上一个阶段的完成，下一个阶段的开始。 明确了这些关注点，我们就有了设计、优化研发流程的抓手了。接下来，我们就可以一起去学习一套业界相对标准的研发流程了。在学习的过程中，你也能更好地理解我对各个流程的一些经验和建议了。 ","date":"2022-07-07 15:18:55","objectID":"/iam_standard_design/:5:1","tags":["iam"],"title":"iam_standard_design","uri":"/iam_standard_design/"},{"categories":["iam"],"content":"业界相对标准的研发流程，长啥样？ 一个项目从立项到结项，中间会经历很多阶段。业界相对标准的划分，是把研发流程分为六个阶段，分别是需求阶段、设计阶段、开发阶段、测试阶段、发布阶段、运营阶段。其中，开发人员需要参与的阶段有 4 个：设计阶段、开发阶段、测试阶段和发布阶段。下图就是业界相对比较标准的流程： 每个阶段结束时，都需要有一个最终的产出物，可以是文档、代码或者部署组件等。这个产出物既是当前阶段的结束里程碑，又是下一阶段的输入。所以说，各个阶段不是割裂的，而是密切联系的整体。每个阶段又细分为很多步骤，这些步骤是需要不同的参与者去完成的工作任务。在完成任务的过程中，可能需要经过多轮的讨论、修改，最终形成定稿。 这里有个点我们一定要注意：研发流程也是一种规范，很难靠开发者的自觉性去遵守。为了让项目参与人员尽可能地遵守规范，需要借助一些工具、系统来对他们进行强约束。所以，在我们设计完整个研发流程之后，需要认真思考下，有哪些地方可以实现自动化，有哪些地方可以靠工具、系统来保障规范的执行。这些自动化工具会在第 16 讲 中详细介绍。接下来，咱们就具体看看研发的各个阶段，以及每个阶段的具体内容。 ","date":"2022-07-07 15:18:55","objectID":"/iam_standard_design/:5:2","tags":["iam"],"title":"iam_standard_design","uri":"/iam_standard_design/"},{"categories":["iam"],"content":"需求阶段 需求阶段是将一个抽象的产品思路具化成一个可实施产品的阶段。在这个阶段，产品人员会讨论产品思路、调研市场需求，并对需求进行分析，整理出一个比较完善的需求文档。最后，产品人员会组织相关人员对需求进行评审，如果评审通过，就会进入设计阶段。 需求阶段，一般不需要研发人员参与。但这里，我还是建议你积极参与产品需求的讨论。虽然我们是研发，但我们的视野和对团队的贡献，可以不仅仅局限在研发领域。这里有个点需要提醒你，如果你们团队有测试人员，这个阶段也需要拉测试人员旁听下。因为了解产品设计，对测试阶段测试用例的编写和功能测试等都很有帮助。需求阶段的产出物是一个通过评审的详细的需求文档。 ","date":"2022-07-07 15:18:55","objectID":"/iam_standard_design/:5:3","tags":["iam"],"title":"iam_standard_design","uri":"/iam_standard_design/"},{"categories":["iam"],"content":"设计阶段 设计阶段，是整个产品研发过程中非常重要的阶段，包括的内容也比较多，你可以看一下这张表： 这里的每一个设计项都应该经过反复的讨论、打磨，最终在团队内达成共识。这样可以确保设计是合理的，并减少返工的概率。**这里想提醒你的是，技术方案和实现都要经过认真讨论，并获得一致通过，否则后面因为技术方案设计不当，需要返工，你要承担大部分责任。**对于后端开发人员，在设计技术方案之前，要做好充足的调研。一个技术方案，不仅要调研业界优秀的实现，还要了解友商相同技术的实现。只有这样，才可以确保我们的技术用最佳的方式实现。 除此之外，在这个阶段一些设计项可以并行，以缩短设计阶段的耗时。例如，产品设计和技术设计可以并行展开。另外，如果你们团队有测试人员，研发阶段最好也拉上测试人员旁听下，有利于后面的测试。该阶段的产出物是一系列的设计文档，这些文档会指导后面的整个研发流程。 ","date":"2022-07-07 15:18:55","objectID":"/iam_standard_design/:5:4","tags":["iam"],"title":"iam_standard_design","uri":"/iam_standard_design/"},{"categories":["iam"],"content":"开发阶段 开发阶段，从它的名字你就知道了，这是开发人员的主战场，同时它可能也是持续时间最长的阶段。在这一阶段，开发人员根据技术设计文档，编码实现产品需求。开发阶段是整个项目的核心阶段，包含很多工作内容，而且每一个 Go 项目具体的步骤是不同的。我把开发阶段的常见步骤总结在了下图中，帮助你对它进行整体把握。 让我们来详细看下这张图里呈现的步骤。开发阶段又可以分为“开发”和“构建”两部分，我们先来看开发。首先，我们需要制定一个所有研发人员共同遵循的 Git 工作流规范。最常使用的是 Git Flow 工作流或者 Forking 工作流。 为了提高开发效率，越来越多的开发者采用生成代码的方式来生成一部分代码，所以在真正编译之前可能还需要先生成代码，比如生成.pb.go 文件、API 文档、测试用例、错误码等。我的建议是，在项目开发中，你要思考怎么尽可能自动生成代码。这样不仅能提高研发效率，还能减少错误。 对于一个开源项目，我们可能还需要检查新增的文件是否有版权信息。此外，根据项目不同，开发阶段还可能有其它不同的步骤。在流程的最后，通常会进行静态代码检查、单元测试和编译。编译之后，我们就可以启动服务，并进行自测了。自测之后，我们可以遵循 Git Flow 工作流，将开发分支 push 到代码托管平台进行 code review。code review 通过之后，我们就可以将代码 merge 到 develop 分支上。 接下来进入构建阶段。这一阶段最好借助 CI/CD 平台实现自动化，提高构建效率。合并到 develop 分支的代码同样需要进行代码扫描、单元测试，并编译打包。最后，我们需要进行归档，也就是将编译后的二进制文件或 Docker 镜像上传到制品库或镜像仓库。 我刚刚带着你完整走了一遍开发阶段的常见步骤。可以看到，整个开发阶段步骤很多，而且都是高频的操作。那怎么提高效率呢？这里我推荐你两种方法： 将开发阶段的步骤通过 Makefile 实现集中管理； 将构建阶段的步骤通过 CI/CD 平台实现自动化。 你还需要特别注意**这一点：在最终合并代码到 master 之前，要确保代码是经过充分测试的。**这就要求我们一定要借助代码管理平台提供的 Webhook 能力，在代码提交时触发 CI/CD 作业，对代码进行扫描、测试，最终编译打包，并以整个作业的成功执行作为合并代码的先决条件。 开发阶段的产出物是满足需求的源代码、开发文档，以及编译后的归档文件。 ","date":"2022-07-07 15:18:55","objectID":"/iam_standard_design/:5:5","tags":["iam"],"title":"iam_standard_design","uri":"/iam_standard_design/"},{"categories":["iam"],"content":"测试阶段 测试阶段由测试工程师（也叫质量工程师）负责，这个阶段的主要流程是：测试工程师根据需求文档创建测试计划、编写测试用例，并拉研发同学一起评审测试计划和用例。评审通过后，测试工程师就会根据测试计划和测试用例对服务进行测试。 为了提高整个研发效率，测试计划的创建和测试用例的编写可以跟开发阶段并行。研发人员在交付给测试时，要提供自测报告、自测用例和安装部署文档。这里我要强调的是：在测试阶段，为了不阻塞测试，确保项目按时发布，研发人员应该优先解决测试同学的 Bug，至少是阻塞类的 Bug。为了减少不必要的沟通和排障，安装部署文档要尽可能详尽和准确。 另外，你也可以及时跟进测试，了解测试同学当前遇到的卡点。因为实际工作中，一些测试同学在遇到卡点时，不善于或者不会及时地跟你同步卡点，往往研发 1 分钟就可以解决的问题，可能要花测试同学几个小时或者更久的时间去解决。 当然，测试用例几乎不可能涵盖整个变更分支，所以对于一些难测，隐藏的测试，需要研发人员自己加强测试。最后，一个大特性测试完，请测试同学吃个饭吧，大家唠唠家常，联络联络感情，下次合作会更顺畅。测试阶段的产出物是满足产品需求、达到发布条件的源代码，以及编译后的归档文件。 ","date":"2022-07-07 15:18:55","objectID":"/iam_standard_design/:5:6","tags":["iam"],"title":"iam_standard_design","uri":"/iam_standard_design/"},{"categories":["iam"],"content":"发布阶段 发布阶段主要是将软件部署上线，为了保证发布的效率和质量，我们需要遵循一定的发布流程，如下图所示： 发布阶段按照时间线排序又分为代码发布、发布审批和服务发布 3 个子阶段。接下来，我详细给你介绍下这 3 个子阶段。我们先来看一下代码发布。 首先，开发人员首先需要将经过测试后的代码合并到主干，通常是 master 分支，并生成版本号，然后给最新的 commit 打上版本标签。之后，可以将代码 push 到代码托管平台，并触发 CI 流程，CI 流程一般会执行代码扫描、单元测试、编译，最后将构建产物发布到制品库。CI 流程中，我们可以根据需要添加任意功能。 接着，进入到发布审批阶段。首先需要申请资源，资源申请周期可能会比较久，所以申请得越早越好，甚至资源申请可以在测试阶段发起。在资源申请阶段，可以申请诸如服务器、MySQL、Redis、Kafka 之类资源。资源申请通常是开发人员向运维人员提需求，由运维人员根据需求，在指定的时间前准备好各类资源。如果是物理机通常申请周期会比较久，但当前越来越多的项目选择容器化部署，这可以极大地缩短资源的申请周期。如果在像腾讯云弹性容器这类 Serverless 容器平台上部署业务，甚至可以秒申请资源。所以这里，我也建议优先采用容器化部署。 发布之前需要创建发布计划，里面需要详细描述本次的变更详情，例如变更范围、发布方案、测试结果、验证和回滚方案等。这里需要你注意，在创建发布计划时，一定要全面梳理这次变更的影响点。例如，是否有不兼容的变更，是否需要变更配置，是否需要变更数据库等。任何一个遗漏，都可能造成现网故障，影响产品声誉和用户使用。 接下来，需要创建发布单，在发布单中可以附上发布计划，并根据团队需求填写其它发布内容，发布计划需要跟相关参与者对齐流程、明确职责。发布单最终提交给审批人（通常是技术 leader）对本次发布进行审批，审批通过后，才可以进行部署。 最后，就可以进入到服务发布阶段，将服务发布到现网。在正式部署的时候，应用需要先部署到预发环境。在预发环境，产品人员、测试人员和研发人员会分别对产品进行验证。其中，产品人员主要验证产品功能的体验是否流畅，开发和测试人员主要验证产品是否有 Bug。预发环境验证通过，产品才能正式发布到现网。 这里，我强烈建议，**编写一些自动化的测试用例，在服务发布到现网之后，对现网服务做一次比较充分的回归测试。**通过这个自动化测试，可以以最小的代价，最快速地验证现网功能，从而保障发布质量。 另外，我们还要注意，现网可能有多个地域，每个地域发布完成之后都要进行现网验证。发布阶段的产出物是正式上线的软件。 ","date":"2022-07-07 15:18:55","objectID":"/iam_standard_design/:5:7","tags":["iam"],"title":"iam_standard_design","uri":"/iam_standard_design/"},{"categories":["iam"],"content":"运营阶段 研发流程的最后一个阶段是运营阶段，该阶段主要分为产品运营和运维两个部分。 产品运营：通过一系列的运营活动，比如线下的技术沙龙、线上的免费公开课、提高关键词排名或者输出一些技术推广文章等方式，来推高整个产品的知名度，提高产品的用户数量，并提高月活和日活。 运维：由运维工程师负责，核心目标是确保系统稳定的运行，如果系统异常，能够及时发现并修复问题。长期目标是通过技术手段或者流程来完善整个系统架构，减少人力投入、提高运维效率，并提高系统的健壮性和恢复能力。 从上面可以看到，运维属于技术类，运营属于产品类，这二者不要搞混。为了加深你的理解和记忆，我将这些内容，总结在了下面一张图中。在运营阶段，研发人员的主要职责就是协助运维解决现网 Bug，优化部署架构。当然，研发人员可能也需要配合运营人员开发一些运营接口，供运营人员使用。 到这里，业界相对标准的这套研发流程，我们就学完了。在学习过程中，你肯定也发现了，整个研发流程会涉及很多角色，不同角色参与不同的阶段，负责不同的任务。这里我再给你额外扩展一个点，就是这些核心角色和分工是啥样的。这些扩展内容，我放在了一张图和一张表里。这些角色和分工比较好理解，也不需要你背下来，只要先有一个大概的印象就可以了。 具体分工如下表所示。 ","date":"2022-07-07 15:18:55","objectID":"/iam_standard_design/:5:8","tags":["iam"],"title":"iam_standard_design","uri":"/iam_standard_design/"},{"categories":["iam"],"content":"总结 在开发 Go 项目时，掌握项目的研发流程很重要。掌握研发流程，会让项目研发对我们更加白盒，并且有利于我们制定详细的工作任务。那么如何设计项目研发流程呢？你可以根据需要自行设计。自行设计时有些点是一定要关注的，例如我们的流程需要支持高的发布效率和发布质量，支持快速迭代，流程是合理、可扩展的，等等。如果你不想自己设计，也可以。在这一讲中，我介绍了一套相对通用、标准的研发流程，如果合适可以直接拿来作为自己设计的研发流程。这套研发流程包含 6 个阶段：需求阶段、设计阶段、开发阶段、测试阶段、发布阶段和运营阶段。这里我将这些流程和每个流程的核心点总结在下面一张图中。 ","date":"2022-07-07 15:18:55","objectID":"/iam_standard_design/:5:9","tags":["iam"],"title":"iam_standard_design","uri":"/iam_standard_design/"},{"categories":["iam"],"content":"9 | 研发流程设计（下）：如何管理应用的生命周期？ 应用的生命周期管理，怎么理解呢？其实，就是指采用一些好的工具或方法在应用的整个生命周期中对应用进行管理，以提高应用的研发效率和质量。 这一讲我们就一起学习下，业界在不同时期沉淀下来的优秀管理手段，以及我对这些管理手段的经验和建议，帮助你选到一个最合适的。 ","date":"2022-07-07 15:18:55","objectID":"/iam_standard_design/:6:0","tags":["iam"],"title":"iam_standard_design","uri":"/iam_standard_design/"},{"categories":["iam"],"content":"应用生命周期管理技术有哪些？ 那么，有哪些应用生命周期管理技术呢？在这里我先整体介绍一下，你先有个大致的印象，一会我们再一个个细讲。我们可以从两个维度来理解应用生命周期管理技术。 第一个维度是演进维度。应用生命周期，最开始主要是通过研发模式来管理的，按时间线先后出现了瀑布模式、迭代模式、敏捷模式。接着，为了解决研发模式中的一些痛点出现了另一种管理技术，也就是 CI/CD 技术。随着 CI/CD 技术的成熟，又催生了另一种更高级的管理技术 DevOps。 第二个维度是管理技术的类别。应用生命周期管理技术可以分为两类： 研发模式，用来确保整个研发流程是高效的。DevOps，主要通过协调各个部门之间的合作，来提高软件的发布效率和质量。 DevOps 中又包含了很多种技术，主要包括 CI/CD 和多种 Ops，例如 AIOps、ChatOps、GitOps、NoOps 等。其中，CI/CD 技术提高了软件的发布效率和质量，而 Ops 技术则提高了软件的运维和运营效率。 尽管这些应用生命周期管理技术有很多不同，但是它们彼此支持、相互联系。研发模式专注于开发过程，DevOps 技术里的 CI/CD 专注于流程，Ops 则专注于实战。为了帮助你理解，我总结出了下面这张图供你参考。 这两个维度涉及的管理技术虽然不少，但一共就是那几类。所以，为了能够逻辑清晰地给你讲解明白这些技术，我会从演进维度来展开，也就是按照这样的顺序：研发模式（瀑布模式 -\u003e 迭代模式 -\u003e 敏捷模式） -\u003e CI/CD -\u003e DevOps。 接下来，我们就详细说说这些应用生命周期的管理方法，先来看专注于开发过程的研发模式部分。 ","date":"2022-07-07 15:18:55","objectID":"/iam_standard_design/:6:1","tags":["iam"],"title":"iam_standard_design","uri":"/iam_standard_design/"},{"categories":["iam"],"content":"研发模式 研发模式主要有三种，演进顺序为瀑布模式 -\u003e 迭代模式 -\u003e 敏捷模式，现在我们逐一看下。 瀑布模式 在早期阶段，软件研发普遍采用的是瀑布模式，像我们熟知的 RHEL、Fedora 等系统就是采用瀑布模式。瀑布模式按照预先规划好的研发阶段来推进研发进度。比如，按照需求阶段、设计阶段、开发阶段、测试阶段、发布阶段、运营阶段的顺序串行执行开发任务。每个阶段完美完成之后，才会进入到下一阶段，阶段之间通过文档进行交付。整个过程如下图所示。 瀑布模式最大的优点是简单。它严格按照研发阶段来推进研发进度，流程清晰，适合按项目交付的应用。 但它的缺点也很明显，最突出的就是这两个：只有在项目研发的最后阶段才会交付给客户。交付后，如果客户发现问题，变更就会非常困难，代价很大。研发周期比较长，很难适应互联网时代对产品快速迭代的诉求。 为了解决这两个问题，迭代式研发模式诞生了。 迭代模式 迭代模式，是一种与瀑布式模式完全相反的开发过程：研发任务被切分为一系列轮次，每一个轮次都是一个迭代，每一次迭代都是一个从设计到实现的完整过程。它不要求每一个阶段的任务都做到最完美，而是先把主要功能搭建起来，然后再通过客户的反馈信息不断完善。迭代开发可以帮助产品改进和把控进度，它的灵活性极大地提升了适应需求变化的能力，克服了高风险、难变更、复用性低的特点。但是，迭代模式的问题在于比较专注于开发过程，很少从项目管理的视角去加速和优化项目开发过程。接下来要讲的敏捷模式，就弥补了这个缺点。 敏捷模式 敏捷模式把一个大的需求分成多个、可分阶段完成的小迭代，每个迭代交付的都是一个可使用的软件。在开发过程中，软件要一直处于可使用状态。敏捷模式中具有代表性的开发模式，是 Scrum 开发模型。Scrum 开发模型网上有很多介绍，你可以去看看。在敏捷模式中，我们会把一个大的需求拆分成很多小的迭代，这意味着开发过程中会有很多个开发、构建、测试、发布和部署的流程。这种高频度的操作会给研发、运维和测试人员带来很大的工作量，降低了工作效率。为了解决这个问题，CI/CD 技术诞生了。 ","date":"2022-07-07 15:18:55","objectID":"/iam_standard_design/:6:2","tags":["iam"],"title":"iam_standard_design","uri":"/iam_standard_design/"},{"categories":["iam"],"content":"CI/CD：自动化构建和部署应用 CI/CD 技术通过自动化的手段，来快速执行代码检查、测试、构建、部署等任务，从而提高研发效率，解决敏捷模式带来的弊端。 CI/CD 包含了 3 个核心概念。CI：Continuous Integration，持续集成。CD：Continuous Delivery，持续交付。CD：Continuous Deployment，持续部署。 CI 容易理解，但两个 CD 很多开发者区分不开。这里，我来详细说说这 3 个核心概念。 首先是持续集成。它的含义为：频繁地（一天多次）将开发者的代码合并到主干上。它的流程为：在开发人员完成代码开发，并 push 到 Git 仓库后，CI 工具可以立即对代码进行扫描、（单元）测试和构建，并将结果反馈给开发者。持续集成通过后，会将代码合并到主干。CI 流程可以使应用软件的问题在开发阶段就暴露出来，这会让开发人员交付代码时更有信心。因为 CI 流程内容比较多，而且执行比较频繁，所以 CI 流程需要有自动化工具来支撑。 其次是持续交付，它指的是一种能够使软件在较短的循环中可靠发布的软件方法。持续交付在持续集成的基础上，将构建后的产物自动部署在目标环境中。这里的目标环境，可以是测试环境、预发环境或者现网环境。通常来说，持续部署可以自动地将服务部署到测试环境或者预发环境。因为部署到现网环境存在一定的风险，所以如果部署到现网环境，需要手工操作。手工操作的好处是，可以使相关人员评估发布风险，确保发布的正确性。 最后是持续部署，持续部署在持续交付的基础上，将经过充分测试的代码自动部署到生产环境，整个流程不再需要相关人员的审核。持续部署强调的是自动化部署，是交付的最高阶段。我们可以借助下面这张图，来了解持续集成、持续交付、持续部署的关系。 持续集成、持续交付和持续部署强调的是持续性，也就是能够支持频繁的集成、交付和部署，这离不开自动化工具的支持，离开了这些工具，CI/CD 就不再具有可实施性。持续集成的核心点在代码，持续交付的核心点在可交付的产物，持续部署的核心点在自动部署。 ","date":"2022-07-07 15:18:55","objectID":"/iam_standard_design/:6:3","tags":["iam"],"title":"iam_standard_design","uri":"/iam_standard_design/"},{"categories":["iam"],"content":"DevOps：研发运维一体化 CI/CD 技术的成熟，加速了 DevOps 这种应用生命周期管理技术的成熟和落地。DevOps（Development 和 Operations 的组合）是一组过程、方法与系统的统称，用于促进开发（应用程序 / 软件工程）、技术运营和质量保障（QA）部门之间的沟通、协作与整合。这 3 个部门的相互协作，可以提高软件质量、快速发布软件。如下图所示： 要实现 DevOps，需要一些工具或者流程的支持，CI/CD 可以很好地支持 DevOps 这种软件开发模式，如果没有 CI/CD 自动化的工具和流程，DevOps 就是没有意义的，CI/CD 使得 DevOps 变得可行。 听到这里是不是有些晕？你可能想问，DevOps 跟 CI/CD 到底是啥区别呢？其实，这也是困扰很多开发者的问题。这里，我们可以这么理解：DevOps ！= CI/CD。DevOps 是一组过程、方法和系统的统称，而 CI/CD 只是一种软件构建和发布的技术。 DevOps 技术之前一直有，但是落地不好，因为没有一个好的工具来实现 DevOps 的理念。但是随着容器、CI/CD 技术的诞生和成熟，DevOps 变得更加容易落地。也就是说，这几年越来越多的人采用 DevOps 手段来提高研发效能。 随着技术的发展，目前已经诞生了很多 Ops 手段，来实现运维和运营的高度自动化。下面，我们就来看看 DevOps 中的四个 Ops 手段：AIOps、ChatOps、GitOps、NoOps。 AIOps：智能运维 在 2016 年，Gartner 提出利用 AI 技术的新一代 IT 运维，即 AIOps（智能运维）。通过 AI 手段，来智能化地运维 IT 系统。AIOps 通过搜集海量的运维数据，并利用机器学习算法，智能地定位并修复故障。 也就是说，AIOps 在自动化的基础上，增加了智能化，从而进一步推动了 IT 运维自动化，减少了人力成本。随着 IT 基础设施规模和复杂度的倍数增长，企业应用规模、数量的指数级增长，传统的人工 / 自动化运维，已经无法胜任愈加沉重的运维工作，而 AIOps 提供了一个解决方案。在腾讯、阿里等大厂很多团队已经在尝试和使用 AIOps，并享受到了 AIOps 带来的红利。例如，故障告警更加灵敏、准确，一些常见的故障，可以自动修复，无须运维人员介入等。 ChatOps：聊着天就把事情给办了 随着企业微信、钉钉等企业内通讯工具的兴起，最近几年出现了一个新的概念 ChatOps。简单来说，ChatOps 就是在一个聊天工具中，发送一条命令给 ChatBot 机器人，然后 ChatBot 会执行预定义的操作。这些操作可以是执行某个工具、调用某个接口等，并返回执行结果。 这种新型智能工作方式的优势是什么呢？它可以利用 ChatBot 机器人让团队成员和各项辅助工具连接在一起，以沟通驱动的方式完成工作。ChatOps 可以解决人与人、人与工具、工具与工具之间的信息孤岛，从而提高协作体验和工作效率。ChatOps 的工作流程如下图所示（网图）： 开发 / 运维 / 测试人员通过 @聊天窗口中的机器人 Bot 来触发任务，机器人后端会通过 API 接口调用等方式对接不同的系统，完成不同的任务，例如持续集成、测试、发布等工作。机器人可以是我们自己研发的，也可以是开源的。目前，业界有很多流行的机器人可供选择，常用的有 Hubot、Lita、Errbot、StackStorm 等。使用 ChatOps 可以带来以下几点好处。 友好、便捷：所有的操作均在同一个聊天界面中，通过 @机器人以聊天的方式发送命令，免去了打开不同系统，执行不同操作的繁琐操作，方式更加友好和便捷。 信息透明：在同一个聊天界面中的所有同事都能够看到其他同事发送的命令，以及命令执行的结果，可以消除沟通壁垒，工作历史有迹可循，团队合作更加顺畅。 移动友好：可以在移动端向机器人发送命令、执行任务，让移动办公变为可能。 DevOps 文化打造：通过与机器人对话，可以降低项目开发中，各参与人员的理解和使用成本，从而使 DevOps 更容易落地和推广。 GitOps： 一种实现云原生的持续交付模型 GitOps 是一种持续交付的方式。它的核心思想是将应用系统的声明性基础架构（YAML）和应用程序存放在 Git 版本库中。将 Git 作为交付流水线的核心，每个开发人员都可以提交拉取请求（Pull Request），并使用 Gi​​t 来加速和简化 Kubernetes 的应用程序部署和运维任务。通过 Git 这样的工具，开发人员可以将精力聚焦在功能开发，而不是软件运维上，以此提高软件的开发效率和迭代速度。 使用 GitOps 可以带来很多优点，其中最核心的是：当使用 Git 变更代码时，GitOps 可以自动将这些变更应用到程序的基础架构上。因为整个流程都是自动化的，所以部署时间更短；又因为 Git 代码是可追溯的，所以我们部署的应用也能够稳定且可重现地回滚。我们可以从概念和流程上来理解 GitOps，它有 3 个关键概念。 声明性容器编排：通过 Kubernetes YAML 格式的资源定义文件，来定义如何部署应用。 不可变基础设施：基础设施中的每个组件都可以自动的部署，组件在部署完成后，不能发生变更。如果需要变更，则需要重新部署一个新的组件。例如，Kubernetes 中的 Pod 就是一个不可变基础设施。 连续同步：不断地查看 Git 存储库，将任何状态更改反映到 Kubernetes 集群中。 GitOps 的工作流程如下：首先，开发人员开发完代码后推送到 Git 仓库，触发 CI 流程，CI 流程通过编译构建出 Docker 镜像，并将镜像 push 到 Docker 镜像仓库中。Push 动作会触发一个 push 事件，通过 webhook 的形式通知到 Config Updater 服务，Config Updater 服务会从 webhook 请求中获取最新 push 的镜像名，并更新 Git 仓库中的 Kubernetes YAML 文件。 然后，GitOps 的 Deploy Operator 服务，检测到 YAML 文件的变动，会重新从 Git 仓库中提取变更的文件，并将镜像部署到 Kubernetes 集群中。Config Updater 和 Deploy Operator 两个组件需要开发人员设计开发。 NoOps：无运维 NoOps 即无运维，完全自动化的运维。在 NoOps 中不再需要开发人员、运营运维人员的协同，把微服务、低代码、无服务全都结合了起来，开发者在软件生命周期中只需要聚焦业务开发即可，所有的维护都交由云厂商来完成。 毫无疑问，NoOps 是运维的终极形态，在我看来它像 DevOps 一样，更多的是一种理念，需要很多的技术和手段来支撑。当前整个运维技术的发展，也是朝着 NoOps 的方向去演进的，例如 GitOps、AIOps 可以使我们尽可能减少运维，Serverless 技术甚至可以使我们免运维。相信未来 NoOps 会像现在的 Serverless 一样，成为一种流行的、可落地的理念。 ","date":"2022-07-07 15:18:55","objectID":"/iam_standard_design/:6:4","tags":["iam"],"title":"iam_standard_design","uri":"/iam_standard_design/"},{"categories":["iam"],"content":"如何选择合适的应用生命周期管理技术？ 好了，到这里我们就把主要的应用生命周期管理技术，学得差不多了。那在实际开发中，如何选择适合自己的呢？在我看来，你可以从这么几个方面考虑。 首先，根据团队、项目选择一个合适的研发模式。如果项目比较大，需求变更频繁、要求快速迭代，建议选择敏捷开发模式。敏捷开发模式，也是很多大公司选择的研发模式，在互联网时代很受欢迎。接着，要建立自己的 CI/CD 流程。任何变更代码在合并到 master 分支时，一定要通过 CI/CD 的流程的验证。我建议，你在 CI/CD 流程中设置质量红线，确保合并代码的质量。接着，除了建立 CI/CD 系统，我还建议将 ChatOps 带入工作中，尽可能地将可以自动化的工作实现自动化，并通过 ChatOps 来触发自动化流程。随着企业微信、钉钉等企业聊天软件成熟和发展，ChatOps 变得流行和完善。最后，GitOps、AIOps 可以将部署和运维自动化做到极致，在团队有人力的情况下，值得探索。 到这里你可能会问了，大厂是如何管理应用生命周期的？大厂普遍采用敏捷开发的模式，来适应互联网对应用快速迭代的诉求。例如，腾讯的TAPD、Coding的 Scrum 敏捷管理就是一个敏捷开发平台。CI/CD 强制落地，ChatOps 已经广泛使用，AIOps 也有很多落地案例，GitOps 目前还在探索阶段，NoOps 还处在理论阶段。 ","date":"2022-07-07 15:18:55","objectID":"/iam_standard_design/:6:5","tags":["iam"],"title":"iam_standard_design","uri":"/iam_standard_design/"},{"categories":["iam"],"content":"总结 这一讲，我从技术演进的维度介绍了应用生命周期管理技术，这些技术可以提高应用的研发效率和质量。应用生命周期管理最开始是通过研发模式来管理的。在研发模式中，我按时间线分别介绍了瀑布模式、迭代模式和敏捷模式，其中的敏捷模式适应了互联网时代对应用快速迭代的诉求，所以用得越来越多。在敏捷模式中，我们需要频繁构建和发布我们的应用，这就给开发人员带来了额外的工作量，为了解决这个问题，出现了 CI/CD 技术。CI/CD 可以将代码的检查、测试、构建和部署等工作自动化，不仅提高了研发效率，还从一定程度上保障了代码的质量。另外，CI/CD 技术使得 DevOps 变得可行，当前越来越多的团队采用 DevOps 来管理应用的生命周期。 另外，这一讲中我也介绍了几个大家容易搞混的概念。 持续交付和持续部署。二者都是持续地部署应用，但是持续部署整个过程是自动化的，而持续交付中，应用在发布到现网前需要人工审批是否允许发布。 CI/CD 和 DevOps。DevOps 是一组过程、方法与系统的统称，其中也包含了 CI/CD 技术。而 CI/CD 是一种自动化的技术，DevOps 理念的落地需要 CI/CD 技术的支持。 最后，关于如何管理应用的生命周期，我给出了一些建议：研发模式建议选择敏捷模式，因为它更能胜任互联网时代快速迭代的诉求。DevOps 则要优先确保落地 CI/CD 技术，接着尝试落地 ChatOps 技术，如果有条件可以积极探索 AIOps 和 GitOps。 ","date":"2022-07-07 15:18:55","objectID":"/iam_standard_design/:6:6","tags":["iam"],"title":"iam_standard_design","uri":"/iam_standard_design/"},{"categories":["iam"],"content":"10 | 设计方法：怎么写出优雅的 Go 项目？ Go 语言简单易学，对于大部分开发者来说，编写可运行的代码并不是一件难事，但如果想真正成为 Go 编程高手，你需要花很多精力去研究 Go 的编程哲学。 在我的 Go 开发生涯中，我见过各种各样的代码问题，例如：代码不规范，难以阅读；函数共享性差，代码重复率高；不是面向接口编程，代码扩展性差，代码不可测；代码质量低下。究其原因，是因为这些代码的开发者很少花时间去认真研究如何开发一个优雅的 Go 项目，更多时间是埋头在需求开发中。 其实，我们之前所学的各种规范设计，也都是为了写出一个优雅的 Go 项目。在这一讲，我又补充了一些内容，从而形成了一套“写出优雅 Go 项目”的方法论。这一讲内容比较多，但很重要。 ","date":"2022-07-07 15:18:55","objectID":"/iam_standard_design/:7:0","tags":["iam"],"title":"iam_standard_design","uri":"/iam_standard_design/"},{"categories":["iam"],"content":"如何写出优雅的 Go 项目？ 那么，如何写出一个优雅的 Go 项目呢？在回答这个问题之前，我们先来看另外两个问题：为什么是 Go 项目，而不是 Go 应用？一个优雅的 Go 项目具有哪些特点？ 先来看第一个问题。Go 项目是一个偏工程化的概念，不仅包含了 Go 应用，还包含了项目管理和项目文档： 这就来到了第二个问题，一个优雅的 Go 项目，不仅要求我们的 Go 应用是优雅的，还要确保我们的项目管理和文档也是优雅的。这样，我们根据前面几讲学到的 Go 设计规范，很容易就能总结出一个优雅的 Go 应用需要具备的特点： 符合 Go 编码规范和最佳实践；易阅读、易理解，易维护；易测试、易扩展；代码质量高。 解决了这两个问题，让我们回到这一讲的核心问题：如何写出优雅的 Go 项目？写出一个优雅的 Go 项目，在我看来，就是用“最佳实践”的方式去实现 Go 项目中的 Go 应用、项目管理和项目文档。具体来说，就是编写高质量的 Go 应用、高效管理项目、编写高质量的项目文档。 为了协助你理解，我将这些逻辑绘制成了下面一张图。 接下来，我们就看看如何根据前面几讲学习的 Go 项目设计规范，实现一个优雅的 Go 项目。我们先从编写高质量的 Go 应用看起。 ","date":"2022-07-07 15:18:55","objectID":"/iam_standard_design/:7:1","tags":["iam"],"title":"iam_standard_design","uri":"/iam_standard_design/"},{"categories":["iam"],"content":"编写高质量的 Go 应用 基于我的研发经验，要编写一个高质量的 Go 应用，其实可以归纳为 5 个方面：代码结构、代码规范、代码质量、编程哲学和软件设计方法，见下图。 接下来，我们详细说说这些内容。 代码结构 为什么先说代码结构呢？因为组织合理的代码结构是一个项目的门面。我们可以通过两个手段来组织代码结构。 第一个手段是，组织一个好的目录结构。关于如何组合一个好的目录结构，你可以回顾 06 讲 的内容。第二个手段是，选择一个好的模块拆分方法。做好模块拆分，可以使项目内模块职责分明，做到低耦合高内聚。 那么 Go 项目开发中，如何拆分模块呢？目前业界有两种拆分方法，分别是按层拆分和按功能拆分。 首先，我们看下按层拆分，最典型的是 MVC 架构中的模块拆分方式。在 MVC 架构中，我们将服务中的不同组件按访问顺序，拆分成了 Model、View 和 Controller 三层。 每层完成不同的功能：View（视图）是提供给用户的操作界面，用来处理数据的显示。Controller（控制器），负责根据用户从 View 层输入的指令，选取 Model 层中的数据，然后对其进行相应的操作，产生最终结果。Model（模型），是应用程序中用于处理数据逻辑的部分。 我们看一个典型的按层拆分的目录结构： $ tree --noreport -L 2 layers layers ├── controllers │ ├── billing │ ├── order │ └── user ├── models │ ├── billing.go │ ├── order.go │ └── user.go └── views └── layouts 在 Go 项目中，按层拆分会带来很多问题。最大的问题是循环引用：相同功能可能在不同层被使用到，而这些功能又分散在不同的层中，很容易造成循环引用。所以，你只要大概知道按层拆分是什么意思就够了，在 Go 项目中我建议你使用的是按功能拆分的方法，这也是 Go 项目中最常见的拆分方法。 那什么是按功能拆分呢？我给你看一个例子你就明白了。比如，一个订单系统，我们可以根据不同功能将其拆分成用户（user）、订单（order）和计费（billing）3 个模块，每一个模块提供独立的功能，功能更单一： 下面是该订单系统的代码目录结构： $ tree pkg $ tree --noreport -L 2 pkg pkg ├── billing ├── order │ └── order.go └── user 相较于按层拆分，按功能拆分模块带来的好处也很好理解：不同模块，功能单一，可以实现高内聚低耦合的设计哲学。因为所有的功能只需要实现一次，引用逻辑清晰，会大大减少出现循环引用的概率。 所以，有很多优秀的 Go 项目采用的都是按功能拆分的模块拆分方式，例如 Kubernetes、Docker、Helm、Prometheus 等。除了组织合理的代码结构这种方式外，编写高质量 Go 应用的另外一个行之有效的方法，是遵循 Go 语言代码规范来编写代码。在我看来，这也是最容易出效果的方式。 代码规范 那我们要遵循哪些代码规范来编写 Go 应用呢？在我看来，其实就两类：编码规范和最佳实践。 首先，我们的代码要符合 Go 编码规范，这是最容易实现的途径。Go 社区有很多这类规范可供参考，其中，比较受欢迎的是Uber Go 语言编码规范。阅读这些规范确实有用，也确实花时间、花精力。所以，我在参考了已有的很多规范后，结合自己写 Go 代码的经验，特地为你整理了一篇 Go 编码规范作为加餐，也就是“特别放送 | 给你一份清晰、可直接套用的 Go 编码规范”。 有了可以参考的编码规范之后，我们需要扩展到团队、部门甚至公司层面。只有大家一起参与、遵守，规范才会变得有意义。其实，我们都清楚，要开发者靠自觉来遵守所有的编码规范，不是一件容易的事儿。这时候，我们可以使用静态代码检查工具，来约束开发者的行为。 有了静态代码检查工具后，不仅可以确保开发者写出的每一行代码都是符合 Go 编码规范的，还可以将静态代码检查集成到 CI/CD 流程中。这样，在代码提交后自动地检查代码，就保证了只有符合编码规范的代码，才会被合入主干。 Go 语言的静态代码检查工具有很多，目前用的最多的是golangci-lint，这也是我极力推荐你使用的一个工具。关于这个工具的使用，我会在第 15 讲和你详细介绍。 除了遵循编码规范，要想成为 Go 编程高手，你还得学习并遵循一些最佳实践。“最佳实践”是社区经过多年探索沉淀下来的、符合 Go 语言特色的经验和共识，它可以帮助你开发出一个高质量的代码。这里我给你推荐几篇介绍 Go 语言最佳实践的文章，供你参考： Effective Go：高效 Go 编程，由 Golang 官方编写，里面包含了编写 Go 代码的一些建议，也可以理解为最佳实践。Go Code Review Comments：Golang 官方编写的 Go 最佳实践，作为 Effective Go 的补充。Style guideline for Go packages：包含了如何组织 Go 包、如何命名 Go 包、如何写 Go 包文档的一些建议。 代码质量 有了组织合理的代码结构、符合 Go 语言代码规范的 Go 应用代码之后，我们还需要通过一些手段来确保我们开发出的是一个高质量的代码，这可以通过单元测试和 Code Review 来实现。 单元测试非常重要。我们开发完一段代码后，第一个执行的测试就是单元测试。它可以保证我们的代码是符合预期的，一些异常变动能够被及时感知到。进行单元测试，不仅需要编写单元测试用例，还需要我们确保代码是可测试的，以及具有一个高的单元测试覆盖率。 接下来，我就来介绍下如何编写一个可测试的代码。如果我们要对函数 A 进行测试，并且 A 中的所有代码均能够在单元测试环境下按预期被执行，那么函数 A 的代码块就是可测试的。我们来看下一般的单元测试环境有什么特点： 可能无法连接数据库。可能无法访问第三方服务。 如果函数 A 依赖数据库连接、第三方服务，那么在单元测试环境下执行单元测试就会失败，函数就没法测试，函数是不可测的。解决方法也很简单：将依赖的数据库、第三方服务等抽象成接口，在被测代码中调用接口的方法，在测试时传入 mock 类型，从而将数据库、第三方服务等依赖从具体的被测函数中解耦出去。如下图所示： 为了提高代码的可测性，降低单元测试的复杂度，对 function 和 mock 的要求是：要尽可能减少 function 中的依赖，让 function 只依赖必要的模块。编写一个功能单一、职责分明的函数，会有利于减少依赖。依赖模块应该是易 Mock 的。 为了协助你理解，我们先来看一段不可测试的代码： package post import \"google.golang.org/grpc\" type Post struct { Name string Address string } func ListPosts(client *grpc.ClientConn) ([]*Post, error) { return client.ListPosts() } 这段代码中的 ListPosts 函数是不可测试的。因为 ListPosts 函数中调用了client.ListPosts()方法，该方法依赖于一个 gRPC 连接。而我们在做单元测试时，可能因为没有配置 gRPC 服务的地址、网络隔离等原因，导致没法建立 gRPC 连接，从而导致 ListPosts 函数执行失败。下面，我们把这段代码改成可测试的，如下： package main type Post struct { Name string Address string } type Service interface { ListPosts() ([]*Post, error) } func ListPosts(svc Service) ([]*Post, error) { return svc.ListPosts() } 上面代码中，ListPosts 函数入参为 Service 接口类型，只要我们传入一个实现了 Service 接口类型的实例，ListPosts 函数即可成功运行。因此，我们可以在单元测试中可以实现一个不依赖任何第三方服务的 fake 实例，并传给 ListPosts。上述可测代码的单元测试代码如下： package main import \"testing\" type fakeService struct { } func NewFakeService() Service { return \u0026fakeService{} } func (s *fakeService) ListPosts() ([]*Post, error) { posts := make([]*Post, 0) posts = append(posts, \u0026Post{ Name: \"colin\", Address: \"Shenzhen\", }) posts = append(posts, \u0026Post{ Name: \"alex\", Address: \"Beijing\", }) return posts, nil } func TestListPosts(t *testing.T) { fake := NewFakeService() if _, err := ListPosts(fake); err != nil { t.Fatal(","date":"2022-07-07 15:18:55","objectID":"/iam_standard_design/:7:2","tags":["iam"],"title":"iam_standard_design","uri":"/iam_standard_design/"},{"categories":["iam"],"content":"软件设计方法 接下来，我们继续学习编写高质量 Go 代码的第二项内功，也就是让编写的代码遵循一些业界沉淀下来的，优秀的软件设计方法。 优秀的软件设计方法有很多，其中有两类方法对我们代码质量的提升特别有帮助，分别是设计模式（Design pattern）和 SOLID 原则。在我看来，设计模式可以理解为业界针对一些特定的场景总结出来的最佳实现方式。它的特点是解决的场景比较具体，实施起来会比较简单；而 SOLID 原则更侧重设计原则，需要我们彻底理解，并在编写代码时多思考和落地。 关于设计模式和 SOLID 原则，我是这么安排的：在第 11 讲，我会带你学习 Go 项目常用的设计模式；至于 SOLID 原则，网上已经有很多高质量的文章了，所以我会简单告诉你这个原则是啥，然后给你推荐一篇介绍文章。 我们先了解下有哪些设计模式。在软件领域，沉淀了一些比较优秀的设计模式，其中最受欢迎的是 GOF 设计模式。GOF 设计模式中包含了 3 大类（创建型模式、结构型模式、行为型模式），共 25 种经典的、可以解决常见软件设计问题的设计方案。这 25 种设计方案同样也适用于 Go 语言开发的项目。 这里，我将这 25 种设计模式总结成了一张图，你可以先看看，有个大概的印象，对于一些在 Go 项目开发中常用的设计模式，我会在第 11 讲详细介绍。 如果说设计模式解决的是具体的场景，那么 SOLID 原则就是我们设计应用代码时的指导方针。SOLID 原则，是由罗伯特·C·马丁在 21 世纪早期引入的，包括了面向对象编程和面向对象设计的五个基本原则： 遵循 SOLID 原则可以确保我们设计的代码是易维护、易扩展、易阅读的。SOLID 原则同样也适用于 Go 程序设计。如果你需要更详细地了解 SOLID 原则，可以参考下SOLID 原则介绍这篇文章。 到这里，我们就学完了“编写高质量的 Go 应用”这部分内容。接下来，我们再来学习下如何高效管理 Go 项目，以及如何编写高质量的项目文档。这里面的大部分内容，之前我们都有学习过，因为它们是“如何写出优雅的 Go 项目”的重要组成部分，所以，这里我仍然会简单介绍下它们。 ","date":"2022-07-07 15:18:55","objectID":"/iam_standard_design/:7:3","tags":["iam"],"title":"iam_standard_design","uri":"/iam_standard_design/"},{"categories":["iam"],"content":"高效管理项目 一个优雅的 Go 项目，还需要具备高效的项目管理特性。那么如何高效管理我们的项目呢？ 不同团队、不同项目会采用不同的方法来管理项目，在我看来比较重要的有 3 点，分别是制定一个高效的开发流程、使用 Makefile 管理项目和将项目管理自动化。我们可以通过自动生成代码、借助工具、对接 CI/CD 系统等方法来将项目管理自动化。具体见下图： 高效的开发流程 高效管理项目的第一步，就是要有一个高效的开发流程，这可以提高开发效率、减少软件维护成本。你可以回想一下设计开发流程的知识，如果印象比较模糊了，一定要回去复习下 08 讲的内容，因为这部分很重要 。 使用 Makefile 管理项目 为了更好地管理项目，除了一个高效的开发流程之外，使用 Makefile 也很重要。Makefile 可以将项目管理的工作通过 Makefile 依赖的方式实现自动化，除了可以提高管理效率之外，还能够减少人为操作带来的失误，并统一操作方式，使项目更加规范。 IAM 项目的所有操作均是通过 Makefile 来完成的，具体 Makefile 完成了如下操作： build Build source code for host platform. build.multiarch Build source code for multiple platforms. See option PLATFORMS. image Build docker images for host arch. image.multiarch Build docker images for multiple platforms. See option PLATFORMS. push Build docker images for host arch and push images to registry. push.multiarch Build docker images for multiple platforms and push images to registry. deploy Deploy updated components to development env. clean Remove all files that are created by building. lint Check syntax and styling of go sources. test Run unit test. cover Run unit test and get test coverage. release Release iam format Gofmt (reformat) package sources (exclude vendor dir if existed). verify-copyright Verify the boilerplate headers for all files. add-copyright Ensures source code files have copyright license headers. gen Generate all necessary files, such as error code files. ca Generate CA files for all iam components. install Install iam system with all its components. swagger Generate swagger document. serve-swagger Serve swagger spec and docs. dependencies Install necessary dependencies. tools install dependent tools. check-updates Check outdated dependencies of the go projects. help Show this help info. 自动生成代码 低代码的理念现在越来越流行。虽然低代码有很多缺点，但确实有很多优点，例如：自动化生成代码，减少工作量，提高工作效率。代码有既定的生成规则，相比人工编写代码，准确性更高、更规范。 目前来看，自动生成代码现在已经成为趋势，比如 Kubernetes 项目有很多代码都是自动生成的。我认为，想写出一个优雅的 Go 项目，你也应该认真思考哪些地方的代码可以自动生成。在这门课的 IAM 项目中，就有大量的代码是自动生成的，我放在这里供你参考： 错误码、错误码说明文档。自动生成缺失的 doc.go 文件。利用 gotests 工具，自动生成单元测试用例。使用 Swagger 工具，自动生成 Swagger 文档。使用 Mock 工具，自动生成接口的 Mock 实例。 善于借助工具 在开发 Go 项目的过程中，我们也要善于借助工具，来帮助我们完成一部分工作。利用工具可以带来很多好处： 解放双手，提高工作效率。利用工具的确定性，可以确保执行结果的一致性。例如，使用 golangci-lint 对代码进行检查，可以确保不同开发者开发的代码至少都遵循 golangci-lint 的代码检查规范。有利于实现自动化，可以将工具集成到 CI/CD 流程中，触发流水线自动执行。 那么，Go 项目中，有哪些工具可以为我们所用呢？这里，我给你整理了一些有用的工具： 所有这些工具都可以通过下面的方式安装。 $ cd $IAM_ROOT $ make tools.install IAM 项目使用了上面这些工具的绝大部分，用来尽可能提高整个项目的自动化程度，提高项目维护效率。 对接 CI/CD 代码在合并入主干时，应该有一套 CI/CD 流程来自动化地对代码进行检查、编译、单元测试等，只有通过后的代码才可以并入主干。通过 CI/CD 流程来保证代码的质量。当前比较流行的 CI/CD 工具有 Jenkins、GitLab、Argo、Github Actions、JenkinsX 等。在第 51 讲 和 第 52 讲中，我会详细介绍 CI/CD 的原理和实战。 ","date":"2022-07-07 15:18:55","objectID":"/iam_standard_design/:7:4","tags":["iam"],"title":"iam_standard_design","uri":"/iam_standard_design/"},{"categories":["iam"],"content":"编写高质量的项目文档 最后，一个优雅的项目，还应该有完善的文档。例如 README.md、安装文档、开发文档、使用文档、API 接口文档、设计文档等等。这些内容在第 04 讲的文档规范部分有详细介绍，你可以去复习下。 ","date":"2022-07-07 15:18:55","objectID":"/iam_standard_design/:7:5","tags":["iam"],"title":"iam_standard_design","uri":"/iam_standard_design/"},{"categories":["iam"],"content":"总结 使用 Go 语言做项目开发，核心目的其实就是开发一个优雅的 Go 项目。那么如何开发一个优雅的 Go 项目呢？Go 项目包含三大内容，即 Go 应用、项目管理、项目文档，因此开发一个优雅的 Go 项目，其实就是编写高质量的 Go 应用、高效管理项目和编写高质量的项目文档。针对每一项，我都给出了一些实现方式，这些方式详见下图： ","date":"2022-07-07 15:18:55","objectID":"/iam_standard_design/:7:6","tags":["iam"],"title":"iam_standard_design","uri":"/iam_standard_design/"},{"categories":["iam"],"content":"11 | 设计模式：Go常用设计模式概述 在软件开发中，经常会遇到各种各样的编码场景，这些场景往往重复发生，因此具有典型性。针对这些典型场景，我们可以自己编码解决，也可以采取更为省时省力的方式：直接采用设计模式。 设计模式是啥呢？简单来说，就是将软件开发中需要重复性解决的编码场景，按最佳实践的方式抽象成一个模型，模型描述的解决方法就是设计模式。使用设计模式，可以使代码更易于理解，保证代码的重用性和可靠性。 在软件领域，GoF（四人帮，全拼 Gang of Four）首次系统化提出了 3 大类、共 25 种可复用的经典设计方案，来解决常见的软件设计问题，为可复用软件设计奠定了一定的理论基础。从总体上说，这些设计模式可以分为创建型模式、结构型模式、行为型模式 3 大类，用来完成不同的场景。这一讲，我会介绍几个在 Go 项目开发中比较常用的设计模式，帮助你用更加简单快捷的方法应对不同的编码场景。其中，简单工厂模式、抽象工厂模式和工厂方法模式都属于工厂模式，我会把它们放在一起讲解。 ","date":"2022-07-07 15:18:55","objectID":"/iam_standard_design/:8:0","tags":["iam"],"title":"iam_standard_design","uri":"/iam_standard_design/"},{"categories":["iam"],"content":"创建型模式 首先来看创建型模式（Creational Patterns），它提供了一种在创建对象的同时隐藏创建逻辑的方式，而不是使用 new 运算符直接实例化对象。这种类型的设计模式里，单例模式和工厂模式（具体包括简单工厂模式、抽象工厂模式和工厂方法模式三种）在 Go 项目开发中比较常用。我们先来看单例模式。 单例模式 单例模式（Singleton Pattern），是最简单的一个模式。在 Go 中，单例模式指的是全局只有一个实例，并且它负责创建自己的对象。单例模式不仅有利于减少内存开支，还有减少系统性能开销、防止多个实例产生冲突等优点。因为单例模式保证了实例的全局唯一性，而且只被初始化一次，所以比较适合全局共享一个实例，且只需要被初始化一次的场景，例如数据库实例、全局配置、全局任务池等。单例模式又分为饿汉方式和懒汉方式。饿汉方式指全局的单例实例在包被加载时创建，而懒汉方式指全局的单例实例在第一次被使用时创建。你可以看到，这种命名方式非常形象地体现了它们不同的特点。 接下来，我就来分别介绍下这两种方式。先来看饿汉方式。下面是一个饿汉方式的单例模式代码： package singleton type singleton struct { } var ins *singleton = \u0026singleton{} func GetInsOr() *singleton { return ins } 你需要注意，因为实例是在包被导入时初始化的，所以如果初始化耗时，会导致程序加载时间比较长。懒汉方式是开源项目中使用最多的，但它的缺点是非并发安全，在实际使用时需要加锁。以下是懒汉方式不加锁的一个实现： package singleton type singleton struct { } var ins *singleton func GetInsOr() *singleton { if ins == nil { ins = \u0026singleton{} } return ins } 可以看到，在创建 ins 时，如果 ins==nil，就会再创建一个 ins 实例，这时候单例就会有多个实例。为了解决懒汉方式非并发安全的问题，需要对实例进行加锁，下面是带检查锁的一个实现： import \"sync\" type singleton struct { } var ins *singleton var mu sync.Mutex func GetIns() *singleton { if ins == nil { mu.Lock() if ins == nil { ins = \u0026singleton{} } mu.Unlock() } return ins } 上述代码只有在创建时才会加锁，既提高了代码效率，又保证了并发安全。除了饿汉方式和懒汉方式，在 Go 开发中，还有一种更优雅的实现方式，我建议你采用这种方式，代码如下： package singleton import ( \"sync\" ) type singleton struct { } var ins *singleton var once sync.Once func GetInsOr() *singleton { once.Do(func() { ins = \u0026singleton{} }) return ins } 使用once.Do可以确保 ins 实例全局只被创建一次，once.Do 函数还可以确保当同时有多个创建动作时，只有一个创建动作在被执行。另外，IAM 应用中大量使用了单例模式，如果你想了解更多单例模式的使用方式，可以直接查看 IAM 项目代码。IAM 中单例模式有 GetStoreInsOr、GetEtcdFactoryOr、GetMySQLFactoryOr、GetCacheInsOr等。 工厂模式 工厂模式（Factory Pattern）是面向对象编程中的常用模式。在 Go 项目开发中，你可以通过使用多种不同的工厂模式，来使代码更简洁明了。Go 中的结构体，可以理解为面向对象编程中的类，例如 Person 结构体（类）实现了 Greet 方法。 type Person struct { Name string Age int } func (p Person) Greet() { fmt.Printf(\"Hi! My name is %s\", p.Name) } 有了 Person“类”，就可以创建 Person 实例。我们可以通过简单工厂模式、抽象工厂模式、工厂方法模式这三种方式，来创建一个 Person 实例。这三种工厂模式中，简单工厂模式是最常用、最简单的。它就是一个接受一些参数，然后返回 Person 实例的函数： type Person struct { Name string Age int } func (p Person) Greet() { fmt.Printf(\"Hi! My name is %s\", p.Name) } func NewPerson(name string, age int) *Person { return \u0026Person{ Name: name, Age: age, } } 和p：=＆Person {}这种创建实例的方式相比，简单工厂模式可以确保我们创建的实例具有需要的参数，进而保证实例的方法可以按预期执行。例如，通过NewPerson创建 Person 实例时，可以确保实例的 name 和 age 属性被设置。再来看抽象工厂模式，它和简单工厂模式的唯一区别，就是它返回的是接口而不是结构体。通过返回接口，可以在你不公开内部实现的情况下，让调用者使用你提供的各种功能，例如： type Person interface { Greet() } type person struct { name string age int } func (p person) Greet() { fmt.Printf(\"Hi! My name is %s\", p.name) } // Here, NewPerson returns an interface, and not the person struct itself func NewPerson(name string, age int) Person { return person{ name: name, age: age, } } 上面这个代码，定义了一个不可导出的结构体person，在通过 NewPerson 创建实例的时候返回的是接口，而不是结构体。通过返回接口，我们还可以实现多个工厂函数，来返回不同的接口实现，例如： // We define a Doer interface, that has the method signature // of the `http.Client` structs `Do` method type Doer interface { Do(req *http.Request) (*http.Response, error) } // This gives us a regular HTTP client from the `net/http` package func NewHTTPClient() Doer { return \u0026http.Client{} } type mockHTTPClient struct{} func (*mockHTTPClient) Do(req *http.Request) (*http.Response, error) { // The `NewRecorder` method of the httptest package gives us // a new mock request generator res := httptest.NewRecorder() // calling the `Result` method gives us // the default empty *http.Response object return res.Result(), nil } // This gives us a mock HTTP client, which returns // an empty response for any request sent to it func NewMockHTTPClient() Doer { return \u0026mockHTTPClient{} } NewHTTPClient和NewMockHTTPClient都返回了同一个接口类型 Doer，这使得二者可以互换使用。当你想测试一段调用了 Doer 接口 Do 方法的代码时，这一点特别有用。因为你可以使用一个 Mock 的 HTTP 客户端，从而避免了调用真实外部接口可能带来的失败。来看个例子，假设我们想测试下面这段代码： func QueryUser(doer Doer) error { req, err := http.NewRequest(\"Get\", \"http://iam.api.marmotedu.com:8080/v1/secrets\", nil) if err != nil { return err } _, err := doer.Do(req) if err != nil { return err } return nil } 其测试用例为： func TestQueryUser(t *testing.T) { doer :=","date":"2022-07-07 15:18:55","objectID":"/iam_standard_design/:8:1","tags":["iam"],"title":"iam_standard_design","uri":"/iam_standard_design/"},{"categories":["iam"],"content":"结构型模式 我已经向你介绍了单例模式、工厂模式这两种创建型模式，接下来我们来看结构型模式（Structural Patterns），它的特点是关注类和对象的组合。这一类型里，我想详细讲讲策略模式和模板模式。 策略模式 策略模式（Strategy Pattern）定义一组算法，将每个算法都封装起来，并且使它们之间可以互换。 在什么时候，我们需要用到策略模式呢？在项目开发中，我们经常要根据不同的场景，采取不同的措施，也就是不同的策略。比如，假设我们需要对 a、b 这两个整数进行计算，根据条件的不同，需要执行不同的计算方式。我们可以把所有的操作都封装在同一个函数中，然后通过 if … else … 的形式来调用不同的计算方式，这种方式称之为硬编码。 在实际应用中，随着功能和体验的不断增长，我们需要经常添加 / 修改策略，这样就需要不断修改已有代码，不仅会让这个函数越来越难维护，还可能因为修改带来一些 bug。所以为了解耦，需要使用策略模式，定义一些独立的类来封装不同的算法，每一个类封装一个具体的算法（即策略）。下面是一个实现策略模式的代码： package strategy // 策略模式 // 定义一个策略类 type IStrategy interface { do(int, int) int } // 策略实现：加 type add struct{} func (*add) do(a, b int) int { return a + b } // 策略实现：减 type reduce struct{} func (*reduce) do(a, b int) int { return a - b } // 具体策略的执行者 type Operator struct { strategy IStrategy } // 设置策略 func (operator *Operator) setStrategy(strategy IStrategy) { operator.strategy = strategy } // 调用策略中的方法 func (operator *Operator) calculate(a, b int) int { return operator.strategy.do(a, b) } 在上述代码中，我们定义了策略接口 IStrategy，还定义了 add 和 reduce 两种策略。最后定义了一个策略执行者，可以设置不同的策略，并执行，例如： func TestStrategy(t *testing.T) { operator := Operator{} operator.setStrategy(\u0026add{}) result := operator.calculate(1, 2) fmt.Println(\"add:\", result) operator.setStrategy(\u0026reduce{}) result = operator.calculate(2, 1) fmt.Println(\"reduce:\", result) } 可以看到，我们可以随意更换策略，而不影响 Operator 的所有实现。 模版模式 模版模式 (Template Pattern) 定义一个操作中算法的骨架，而将一些步骤延迟到子类中。这种方法让子类在不改变一个算法结构的情况下，就能重新定义该算法的某些特定步骤。 简单来说，模板模式就是将一个类中能够公共使用的方法放置在抽象类中实现，将不能公共使用的方法作为抽象方法，强制子类去实现，这样就做到了将一个类作为一个模板，让开发者去填充需要填充的地方。以下是模板模式的一个实现： package template import \"fmt\" type Cooker interface { fire() cooke() outfire() } // 类似于一个抽象类 type CookMenu struct { } func (CookMenu) fire() { fmt.Println(\"开火\") } // 做菜，交给具体的子类实现 func (CookMenu) cooke() { } func (CookMenu) outfire() { fmt.Println(\"关火\") } // 封装具体步骤 func doCook(cook Cooker) { cook.fire() cook.cooke() cook.outfire() } type XiHongShi struct { CookMenu } func (*XiHongShi) cooke() { fmt.Println(\"做西红柿\") } type ChaoJiDan struct { CookMenu } func (ChaoJiDan) cooke() { fmt.Println(\"做炒鸡蛋\") } 这里来看下测试用例： func TestTemplate(t *testing.T) { // 做西红柿 xihongshi := \u0026XiHongShi{} doCook(xihongshi) fmt.Println(\"\\n=====\u003e 做另外一道菜\") // 做炒鸡蛋 chaojidan := \u0026ChaoJiDan{} doCook(chaojidan) } ","date":"2022-07-07 15:18:55","objectID":"/iam_standard_design/:8:2","tags":["iam"],"title":"iam_standard_design","uri":"/iam_standard_design/"},{"categories":["iam"],"content":"行为型模式 然后，让我们来看最后一个类别，行为型模式（Behavioral Patterns），它的特点是关注对象之间的通信。这一类别的设计模式中，我们会讲到代理模式和选项模式。 代理模式 代理模式 (Proxy Pattern)，可以为另一个对象提供一个替身或者占位符，以控制对这个对象的访问。以下代码是一个代理模式的实现： package proxy import \"fmt\" type Seller interface { sell(name string) } // 火车站 type Station struct { stock int //库存 } func (station *Station) sell(name string) { if station.stock \u003e 0 { station.stock-- fmt.Printf(\"代理点中：%s买了一张票,剩余：%d \\n\", name, station.stock) } else { fmt.Println(\"票已售空\") } } // 火车代理点 type StationProxy struct { station *Station // 持有一个火车站对象 } func (proxy *StationProxy) sell(name string) { if proxy.station.stock \u003e 0 { proxy.station.stock-- fmt.Printf(\"代理点中：%s买了一张票,剩余：%d \\n\", name, proxy.station.stock) } else { fmt.Println(\"票已售空\") } } 上述代码中，StationProxy 代理了 Station，代理类中持有被代理类对象，并且和被代理类对象实现了同一接口。 选项模式 选项模式（Options Pattern）也是 Go 项目开发中经常使用到的模式，例如，grpc/grpc-go 的NewServer函数，uber-go/zap 包的New函数都用到了选项模式。使用选项模式，我们可以创建一个带有默认值的 struct 变量，并选择性地修改其中一些参数的值。 在 Python 语言中，创建一个对象时，可以给参数设置默认值，这样在不传入任何参数时，可以返回携带默认值的对象，并在需要时修改对象的属性。这种特性可以大大简化开发者创建一个对象的成本，尤其是在对象拥有众多属性时。 而在 Go 语言中，因为不支持给参数设置默认值，为了既能够创建带默认值的实例，又能够创建自定义参数的实例，不少开发者会通过以下两种方法来实现： 第一种方法，我们要分别开发两个用来创建实例的函数，一个可以创建带默认值的实例，一个可以定制化创建实例。 package options import ( \"time\" ) const ( defaultTimeout = 10 defaultCaching = false ) type Connection struct { addr string cache bool timeout time.Duration } // NewConnect creates a connection. func NewConnect(addr string) (*Connection, error) { return \u0026Connection{ addr: addr, cache: defaultCaching, timeout: defaultTimeout, }, nil } // NewConnectWithOptions creates a connection with options. func NewConnectWithOptions(addr string, cache bool, timeout time.Duration) (*Connection, error) { return \u0026Connection{ addr: addr, cache: cache, timeout: timeout, }, nil } 使用这种方式，创建同一个 Connection 实例，却要实现两个不同的函数，实现方式很不优雅。另外一种方法相对优雅些。我们需要创建一个带默认值的选项，并用该选项创建实例： package options import ( \"time\" ) const ( defaultTimeout = 10 defaultCaching = false ) type Connection struct { addr string cache bool timeout time.Duration } type ConnectionOptions struct { Caching bool Timeout time.Duration } func NewDefaultOptions() *ConnectionOptions { return \u0026ConnectionOptions{ Caching: defaultCaching, Timeout: defaultTimeout, } } // NewConnect creates a connection with options. func NewConnect(addr string, opts *ConnectionOptions) (*Connection, error) { return \u0026Connection{ addr: addr, cache: opts.Caching, timeout: opts.Timeout, }, nil } 使用这种方式，虽然只需要实现一个函数来创建实例，但是也有缺点：为了创建 Connection 实例，每次我们都要创建 ConnectionOptions，操作起来比较麻烦。那么有没有更优雅的解决方法呢？答案当然是有的，就是使用选项模式来创建实例。以下代码通过选项模式实现上述功能： package options import ( \"time\" ) type Connection struct { addr string cache bool timeout time.Duration } const ( defaultTimeout = 10 defaultCaching = false ) type options struct { timeout time.Duration caching bool } // Option overrides behavior of Connect. type Option interface { apply(*options) } type optionFunc func(*options) func (f optionFunc) apply(o *options) { f(o) } func WithTimeout(t time.Duration) Option { return optionFunc(func(o *options) { o.timeout = t }) } func WithCaching(cache bool) Option { return optionFunc(func(o *options) { o.caching = cache }) } // Connect creates a connection. func NewConnect(addr string, opts ...Option) (*Connection, error) { options := options{ timeout: defaultTimeout, caching: defaultCaching, } for _, o := range opts { o.apply(\u0026options) } return \u0026Connection{ addr: addr, cache: options.caching, timeout: options.timeout, }, nil } 在上面的代码中，首先我们定义了options结构体，它携带了 timeout、caching 两个属性。接下来，我们通过NewConnect创建了一个连接，NewConnect函数中先创建了一个带有默认值的options结构体变量，并通过调用 for _, o := range opts { o.apply(\u0026options) } 来修改所创建的options结构体变量。 需要修改的属性，是在NewConnect时，通过 Option 类型的选项参数传递进来的。可以通过WithXXX函数来创建 Option 类型的选项参数：WithTimeout、WithCaching。Option 类型的选项参数需要实现apply(*options)函数，结合 WithTimeout、WithCaching 函数的返回值和 optionFunc 的 apply 方法实现，可以知道o.apply(\u0026options)其实就是把 WithTimeout、WithCaching 传入的参数赋值给 options 结构体变量，以此动态地设置 options 结构体变量的属性。 这里还有一个好处：我们可以在 apply 函数中自定义赋值逻辑，例如o.timeout = 100 * t。通过这种方式，我们会有更大的灵活性来设置结构体的属性。选项模式有很多优点，例如：支持传递多个参数，并且在参数","date":"2022-07-07 15:18:55","objectID":"/iam_standard_design/:8:3","tags":["iam"],"title":"iam_standard_design","uri":"/iam_standard_design/"},{"categories":["iam"],"content":"总结 设计模式，是业界沉淀下来的针对特定场景的最佳解决方案。在软件领域，GoF 首次系统化提出了 3 大类设计模式：创建型模式、结构型模式、行为型模式。这一讲，我介绍了 Go 项目开发中 6 种常用的设计模式。每种设计模式解决某一类场景，我给你总结成了一张表格，你可以根据自己的需要进行选择。 ","date":"2022-07-07 15:18:55","objectID":"/iam_standard_design/:8:4","tags":["iam"],"title":"iam_standard_design","uri":"/iam_standard_design/"},{"categories":["iam"],"content":"1 | IAM系统概述 ","date":"2022-07-07 15:18:47","objectID":"/iam_before_class/:1:0","tags":["iam"],"title":"iam_before_class","uri":"/iam_before_class/"},{"categories":["iam"],"content":"项目背景：为什么选择 IAM 系统作为实战项目？ 我们在做 Go 项目开发时，绕不开的一个话题是安全，如何保证 Go 应用的安全，是每个开发者都要解决的问题。虽然 Go 应用的安全包含很多方面，但大体可分为如下 2 类： 服务自身的安全：为了保证服务的安全，需要禁止非法用户访问服务。这可以通过服务器层面和软件层面来解决。服务器层面可以通过物理隔离、网络隔离、防火墙等技术从底层保证服务的安全性，软件层面可以通过 HTTPS、用户认证等手段来加强服务的安全性。服务器层面一般由运维团队来保障，软件层面则需要开发者来保障。 服务资源的安全：服务内有很多资源，为了避免非法访问，开发者要避免 UserA 访问到 UserB 的资源，也即需要对资源进行授权。通常，我们可以通过资源授权系统来对资源进行授权。 总的来说，为了保障 Go 应用的安全，我们需要对访问进行认证，对资源进行授权。那么，我们要如何实现访问认证和资源授权呢？认证功能不复杂，我们可以通过 JWT （JSON Web Token）认证来实现。授权功能比较复杂，授权功能的复杂性使得它可以囊括很多 Go 开发技能点。因此，在这个专栏中，我将认证和授权的功能实现升级为 IAM 系统，通过讲解它的构建过程，给你讲清楚 Go 项目开发的全部流程。 ","date":"2022-07-07 15:18:47","objectID":"/iam_before_class/:1:1","tags":["iam"],"title":"iam_before_class","uri":"/iam_before_class/"},{"categories":["iam"],"content":"IAM 系统是什么？ IAM（Identity and Access Management，身份识别与访问管理）系统是用 Go 语言编写的一个 Web 服务，用于给第三方用户提供访问控制服务。IAM 系统可以帮用户解决的问题是：在特定的条件下，谁能够 / 不能够对哪些资源做哪些操作（Who is able to do what on something given some context），也即完成资源授权功能。那么，IAM 系统是如何进行资源授权的呢？下面，我们通过 IAM 系统的资源授权的流程，来看下它是如何工作的，整个过程可以分为 4 步。用户需要提供昵称、密码、邮箱、电话等信息注册并登录到 IAM 系统，这里是以用户名和密码作为唯一的身份标识来访问 IAM 系统，并且完成认证。因为访问 IAM 的资源授权接口是通过密钥（secretID/secretKey）的方式进行认证的，所以用户需要在 IAM 中创建属于自己的密钥资源。因为 IAM 通过授权策略完成授权，所以用户需要在 IAM 中创建授权策略。请求 IAM 提供的授权接口，IAM 会根据用户的请求内容和授权策略来决定一个授权请求是否被允许。 我们可以看到，在上面的流程中，IAM 使用到了 3 种系统资源：用户（User）、密钥（Secret）和策略（Policy），它们映射到程序设计中就是 3 种 RESTful 资源：用户（User）：实现对用户的增、删、改、查、修改密码、批量修改等操作。密钥（Secret）：实现对密钥的增、删、改、查操作。策略（Policy）：实现对策略的增、删、改、查、批量删除操作。 ","date":"2022-07-07 15:18:47","objectID":"/iam_before_class/:1:2","tags":["iam"],"title":"iam_before_class","uri":"/iam_before_class/"},{"categories":["iam"],"content":"IAM 系统的架构长啥样？ 知道了 IAM 的功能之后，我们再来详细说说 IAM 系统的架构，架构图如下： 总的来说，IAM 架构中包括 9 大组件和 3 大数据库。我将这些组件和功能都总结在下面的表格中。这里面，我们主要记住 5 个核心组件，包括 iam-apiserver、iam-authz-server、iam-pump、marmotedu-sdk-go 和 iamctl 的功能，还有 3 个数据库 Redis、MySQL 和 MongoDB 的功能。 此外，IAM 系统为存储数据使用到的 3 种数据库的说明如下所示。 ","date":"2022-07-07 15:18:47","objectID":"/iam_before_class/:1:3","tags":["iam"],"title":"iam_before_class","uri":"/iam_before_class/"},{"categories":["iam"],"content":"IAM 使用流程 第 1 步，创建平台资源。用户通过 iam-webconsole（RESTful API）或 iamctl（sdk marmotedu-sdk-go）客户端请求 iam-apiserver 提供的 RESTful API 接口完成用户、密钥、授权策略的增删改查，iam-apiserver 会将这些资源数据持久化存储在 MySQL 数据库中。而且，为了确保通信安全，客户端访问服务端都是通过 HTTPS 协议来访问的。 第 2 步，请求 API 完成资源授权。用户可以通过请求 iam-authz-server 提供的 /v1/authz 接口进行资源授权，请求 /v1/authz 接口需要通过密钥认证，认证通过后 /v1/authz 接口会查询授权策略，从而决定资源请求是否被允许。为了提高 /v1/authz 接口的性能，iam-authz-server 将密钥和策略信息缓存在内存中，以便实现快速查询。那密钥和策略信息是如何实现缓存的呢？首先，iam-authz-server 通过调用 iam-apiserver 提供的 gRPC 接口，将密钥和授权策略信息缓存到内存中。同时，为了使内存中的缓存信息和 iam-apiserver 中的信息保持一致，当 iam-apiserver 中有密钥或策略被更新时，iam-apiserver 会往特定的 Redis Channel（iam-authz-server 也会订阅该 Channel）中发送 PolicyChanged 和 SecretChanged 消息。这样一来，当 iam-authz-server 监听到有新消息时就会获取并解析消息，根据消息内容判断是否需要重新调用 gRPC 接来获取密钥和授权策略信息，再更新到内存中。 第 3 步，授权日志数据分析。iam-authz-server 会将授权日志上报到 Redis 高速缓存中，然后 iam-pump 组件会异步消费这些授权日志，再把清理后的数据保存在 MongoDB 中，供运营系统 iam-operating-system 查询。这里还有一点你要注意：iam-authz-server 将授权日志保存在 Redis 高性能 key-value 数据库中，可以最大化减少写入延时。不保存在内存中是因为授权日志量我们没法预测，当授权日志量很大时，很可能会将内存耗尽，造成服务中断。 第 4 步，运营平台授权数据展示。iam-operating-system 是 IAM 的运营系统，它可以通过查询 MongoDB 获取并展示运营数据，比如某个用户的授权 / 失败次数、授权失败时的授权信息等。此外，我们也可以通过 iam-operating-system 调用 iam-apiserver 服务来做些运营管理工作。比如，以上帝视角查看某个用户的授权策略供排障使用，或者调整用户可创建密钥的最大个数，再或者通过白名单的方式，让某个用户不受密钥个数限制的影响等等。 ","date":"2022-07-07 15:18:47","objectID":"/iam_before_class/:1:4","tags":["iam"],"title":"iam_before_class","uri":"/iam_before_class/"},{"categories":["iam"],"content":"IAM 软件架构模式 在设计软件时，我们首先要做的就是选择一种软件架构模式，它对软件后续的开发方式、软件维护成本都有比较大的影响。因此，这里我也会和你简单聊聊 2 种最常用的软件架构模式，分别是前后端分离架构和 MVC 架构。 前后端分离架构 因为 IAM 系统采用的就是前后端分离的架构，所以我们就以 IAM 的运营系统 iam-operating-system 为例来详细说说这个架构。一般来说，运营系统的功能可多可少，对于一些具有复杂功能的运营系统，我们可以采用前后端分离的架构。其中，前端负责页面的展示以及数据的加载和渲染，后端只负责返回前端需要的数据。iam-operating-system 前后端分离架构如下图所示。 采用了前后端分离架构之后，当你通过浏览器请求前端 ops-webconsole 时，ops-webconsole 会先请求静态文件服务器加载静态文件，比如 HTML、CSS 和 JavaScript，然后它会执行 JavaScript，通过负载均衡请求后端数据，最后把后端返回的数据渲染到前端页面中。采用前后端分离的架构，让前后端通过 RESTful API 通信，会带来以下 5 点好处：可以让前、后端人员各自专注在自己业务的功能开发上，让专业的人做专业的事，来提高代码质量和开发效率前后端可并行开发和发布，这也能提高开发和发布效率，加快产品迭代速度前后端组件、代码分开，职责分明，可以增加代码的维护性和可读性，减少代码改动引起的 Bug 概率，同时也能快速定位 Bug前端 JavaScript 可以处理后台的数据，减少对后台服务器的压力可根据需要选择性水平扩容前端或者后端来节约成本 MVC 架构 但是，如果运营系统功能比较少，采用前后端分离框架的弊反而大于利，比如前后端分离要同时维护 2 个组件会导致部署更复杂，并且前后端分离将人员也分开了，这会增加一定程度的沟通成本。同时，因为代码中也需要实现前后端交互的逻辑，所以会引入一定的开发量。这个时候，我们可以尝试直接采用 MVC 软件架构，MVC 架构如下图所示。 MVC 的全名是 Model View Controller，它是一种架构模式，分为 Model、View、Controller 三层，每一层的功能如下：View（视图）：提供给用户的操作界面，用来处理数据的显示。Controller（控制器）：根据用户从 View 层输入的指令，选取 Model 层中的数据，然后对其进行相应的操作，产生最终结果。Model（模型）：应用程序中用于处理数据逻辑的部分。 MVC 架构的好处是通过控制器层将视图层和模型层分离之后，当更改视图层代码后时，我们就不需要重新编译控制器层和模型层的代码了。同样，如果业务流程发生改变也只需要变更模型层的代码就可以。在实际开发中为了更好的 UI 效果，视图层需要经常变更，但是通过 MVC 架构，在变更视图层时，我们根本不需要对业务逻辑层的代码做任何变化，这不仅减少了风险还能提高代码变更和发布的效率。 除此之外，还有一种跟 MVC 比较相似的软件开发架构叫三层架构，它包括 UI 层、BLL 层和 DAL 层。其中，UI 层表示用户界面，BLL 层表示业务逻辑，DAL 层表示数据访问。在实际开发中很多人将 MVC 当成三层架构在用，比如说，很多人喜欢把软件的业务逻辑放在 Controller 层里，将数据库访问操作的代码放在 Model 层里，软件最终的代码放在 View 层里，就这样硬生生将 MVC 架构变成了伪三层架构。这种代码不仅不伦不类，同时也失去了三层架构和 MVC 架构的核心优势，也就是：通过 Controller 层将 Model 层和 View 层解耦，从而使代码更容易维护和扩展。因此在实际开发中，我们也要注意遵循 MVC 架构的开发规范，发挥 MVC 的核心价值。 ","date":"2022-07-07 15:18:47","objectID":"/iam_before_class/:1:5","tags":["iam"],"title":"iam_before_class","uri":"/iam_before_class/"},{"categories":["iam"],"content":"2 | 配置go开发环境 ","date":"2022-07-07 15:18:47","objectID":"/iam_before_class/:2:0","tags":["iam"],"title":"iam_before_class","uri":"/iam_before_class/"},{"categories":["iam"],"content":"Linux 服务器申请和配置 毫无疑问，要安装一个 Go 开发环境，你首先需要有一个 Linux 服务器。Linux 服务器有很多操作系统可供选择，例如：CentOS、Ubuntu、RHEL、Debian 等，但目前生产环境用得最多的还是 CentOS 系统，为了跟生产环境保持一致，我们选择当前最新的 CentOS 版本：CentOS 8.2。因为本专栏的所有操作都是在 CentOS 8.2 系统上进行的，为了避免环境不一致导致的操作失败，我建议你也使用 CentOS 8.2。安装一个 Linux 服务器需要两步：服务器申请和配置。 Linux 服务器申请： 我们可以通过以下 3 种方式来安装一个 CentOS 8.2 系统。在物理机上安装一个 CentOS 8.2 系统。在 Windows/MacBook 上安装虚拟机管理软件，用虚拟机管理软件创建 CentOS 8.2 虚拟机。其中，Windows 建议用 VMWare Workstation 来创建虚拟机，MacBook 建议用 VirtualBox 来创建虚拟机。在诸如腾讯云、阿里云、华为云等平台上购买一个虚拟机，并预装 CentOS 8.2 系统。 Linux 服务器配置： 申请完 Linux 服务之后，我们需要通过 SecureCRT 或 Xshell 等工具登录 Linux 服务器，并对服务器做一些简单必要的配置，包括创建普通用户、添加 sudoers、配置 $HOME/.bashrc 文件。接下来，我们一一来说。 第一步，用 Root 用户登录 Linux 系统，并创建普通用户。一般来说，一个项目会由多个开发人员协作完成，为了节省企业成本，公司不会给每个开发人员都配备一台服务器，而是让所有开发人员共用一个开发机，通过普通用户登录开发机进行开发。因此，为了模拟真实的企业开发环境，我们也通过一个普通用户的身份来进行项目的开发，创建方法如下： # useradd going # 创建 going 用户，通过 going 用户登录开发机进行开发 # passwd going # 设置密码 Changing password for user going. New password: Retype new password: passwd: all authentication tokens updated successfully. 不仅如此，使用普通用户登录和操作开发机也可以保证系统的安全性，这是一个比较好的习惯，所以我们在日常开发中也要尽量避免使用 Root 用户。 第二步，添加 sudoers。我们知道很多时候，普通用户也要用到 Root 的一些权限，但 Root 用户的密码一般是由系统管理员维护并定期更改的，每次都向管理员询问密码又很麻烦。因此，我建议你将普通用户加入到 sudoers 中，这样普通用户就可以通过 sudo 命令来暂时获取 Root 的权限。具体来说，你可以执行如下命令添加： sed -i '/^root.*ALL=(ALL).*ALL/a\\going\\tALL=(ALL) \\tALL' /etc/sudoers 替换 CentOS 8.4 系统中自带的 Yum 源 由于 Red Hat 提前宣布 CentOS 8 于 2021 年 12 月 31 日停止维护，官方的 Yum 源已不可使用，所以需要切换官方的 Yum 源，这里选择阿里提供的 Yum 源。切换命令如下： mv /etc/yum.repos.d /etc/yum.repos.d.bak # 先备份原有的 Yum 源 mkdir /etc/yum.repos.d wget -O /etc/yum.repos.d/CentOS-Base.repo https://mirrors.aliyun.com/repo/Centos-vault-8.5.2111.repo yum clean all \u0026\u0026 yum makecache 第三步，用新的用户名（going）和密码登录 Linux 服务器。这一步也可以验证普通用户是否创建成功。第四步，配置 $HOME/.bashrc 文件。我们登录新服务器后的第一步就是配置 $HOME/.bashrc 文件，以使 Linux 登录 shell 更加易用，例如配置 LANG 解决中文乱码，配置 PS1 可以避免整行都是文件路径，并将 $HOME/bin 加入到 PATH 路径中。配置后的内容如下： mkdir workspace # .bashrc # User specific aliases and functions alias rm='rm -i' alias cp='cp -i' alias mv='mv -i' # Source global definitions if [ -f /etc/bashrc ]; then . /etc/bashrc fi # User specific environment # Basic envs export LANG=\"en_US.UTF-8\" # 设置系统语言为 en_US.UTF-8，避免终端出现中文乱码 export PS1='[\\u@dev \\W]\\$ ' # 默认的 PS1 设置会展示全部的路径，为了防止过长，这里只展示：\"用户名@dev 最后的目录名\" export WORKSPACE=\"$HOME/workspace\" # 设置工作目录 export PATH=$HOME/bin:$PATH # 将 $HOME/bin 目录加入到 PATH 变量中 # Default entry folder cd $WORKSPACE # 登录系统，默认进入 workspace 目录 有一点需要我们注意，在 export PATH 时，最好把 $PATH 放到最后，因为我们添加到目录中的命令是期望被优先搜索并使用的。配置完 $HOME/.bashrc 后，我们还需要创建工作目录 workspace。将工作文件统一放在 $HOME/workspace 目录中，有几点好处。可以使我们的$HOME目录保持整洁，便于以后的文件查找和分类。如果哪一天 /分区空间不足，可以将整个 workspace 目录 mv 到另一个分区中，并在 /分区中保留软连接，例如：/home/going/workspace -\u003e /data/workspace/。如果哪天想备份所有的工作文件，可以直接备份 workspace。 具体的操作指令是$ mkdir -p $HOME/workspace。配置好 $HOME/.bashrc 文件后，我们就可以执行 bash 命令将配置加载到当前 shell 中了。 ","date":"2022-07-07 15:18:47","objectID":"/iam_before_class/:2:1","tags":["iam"],"title":"iam_before_class","uri":"/iam_before_class/"},{"categories":["iam"],"content":"依赖安装和配置 在 Linux 系统上安装 IAM 系统会依赖一些 RPM 包和工具，有些是直接依赖，有些是间接依赖。为了避免后续的操作出现依赖错误，例如，因为包不存在而导致的编译、命令执行错误等，我们先统一依赖安装和配置。安装和配置步骤如下。 第一步，安装依赖。首先，我们在 CentOS 系统上通过 yum 命令来安装所需工具的依赖，安装命令如下： sudo yum -y install make autoconf automake cmake perl-CPAN libcurl-devel libtool gcc gcc-c++ glibc-headers zlib-devel git-lfs telnet ctags lrzsz jq expat-devel openssl-devel 第二步，安装 Git。因为安装 IAM 系统、执行 go get 命令、安装 protobuf 工具等都是通过 Git 来操作的，所以接下来我们还需要安装 Git。由于低版本的 Git 不支持–unshallow 参数，而 go get 在安装 Go 包时会用到 git fetch –unshallow 命令，因此我们要确保安装一个高版本的 Git，具体的安装方法如下： cd /tmp wget https://mirrors.edge.kernel.org/pub/software/scm/git/git-2.30.2.tar.gz tar -xvzf git-2.30.2.tar.gz cd git-2.30.2/ ./configure make sudo make install git --version # 输出 git 版本号，说明安装成功 把 Git 的二进制目录添加到 PATH 路径中 tee -a $HOME/.bashrc \u003c\u003c'EOF' # Configure for git export PATH=/usr/local/libexec/git-core:$PATH EOF 第三步，配置 Git。我们直接执行如下命令配置 Git： git config --global user.name \"QizhengZou\" # 用户名改成自己的 git config --global user.email \"2838264218@qq.com\" # 邮箱改成自己的 git config --global credential.helper store # 设置 Git，保存用户名和密码 git config --global core.longpaths true # 解决 Git 中 'Filename too long' 的错误 在 Git 中，我们会把非 ASCII 字符叫做 Unusual 字符。这类字符在 Git 输出到终端的时候默认是用 8 进制转义字符输出的（以防乱码），但现在的终端多数都支持直接显示非 ASCII 字符，所以我们可以关闭掉这个特性，具体的命令如下： git config --global core.quotepath off GitHub 限制最大只能克隆 100M 的单个文件，为了能够克隆大于 100M 的文件，我们还需要安装 Git Large File Storage，安装方式如下： git lfs install --skip-repo ","date":"2022-07-07 15:18:47","objectID":"/iam_before_class/:2:2","tags":["iam"],"title":"iam_before_class","uri":"/iam_before_class/"},{"categories":["iam"],"content":"Go 编译环境安装和配置 我们知道，Go 是一门编译型语言，所以在部署 IAM 系统之前，我们需要将代码编译成可执行的二进制文件。因此我们需要安装 Go 编译环境。除了 Go，我们也会用 gRPC 框架展示 RPC 通信协议的用法，所以我们也需要将 ProtoBuf 的.proto 文件编译成 Go 语言的接口。因此，我们也需要安装 ProtoBuf 的编译环境。 安装 Go 语言相对来说比较简单，我们只需要下载源码包、设置相应的环境变量即可。首先，我们从 Go 语言官方网站下载对应的 Go 安装包以及源码包，这里我下载的是 go1.17.2 版本： wget https://golang.google.cn/dl/go1.17.2.linux-amd64.tar.gz -O /tmp/go1.17.2.linux-amd64.tar.gz # 解压安装 mkdir -p $HOME/go tar -xvzf /tmp/go1.17.2.linux-amd64.tar.gz -C $HOME/go mv $HOME/go/go $HOME/go/go1.17.2 # 将下列环境变量追加到$HOME/.bashrc 文件中 tee -a $HOME/.bashrc \u003c\u003c'EOF' # Go envs export GOVERSION=go1.17.2 # Go 版本设置 export GO_INSTALL_DIR=$HOME/go # Go 安装目录 export GOROOT=$GO_INSTALL_DIR/$GOVERSION # GOROOT 设置 export GOPATH=$WORKSPACE/golang # GOPATH 设置 export PATH=$GOROOT/bin:$GOPATH/bin:$PATH # 将 Go 语言自带的和通过 go install 安装的二进制文件加入到 PATH 路径中 export GO111MODULE=\"on\" # 开启 Go moudles 特性 export GOPROXY=https://goproxy.cn,direct # 安装 Go 模块时，代理服务器设置 export GOPRIVATE= export GOSUMDB=off # 关闭校验 Go 依赖包的哈希值 EOF Go 语言是通过一系列的环境变量来控制 Go 编译器行为的。因此，我们一定要理解每一个环境变量的含义。 因为 Go 以后会用 Go modules 来管理依赖，所以我建议你将 GO111MODULE 设置为 on。在使用模块的时候，$GOPATH 是无意义的，不过它还是会把下载的依赖储存在 $GOPATH/pkg/mod 目录中，也会把 go install 的二进制文件存放在 $GOPATH/bin 目录中。另外，我们还要将$GOPATH/bin、$GOROOT/bin 加入到 Linux 可执行文件搜索路径中。这样一来，我们就可以直接在 bash shell 中执行 go 自带的命令，以及通过 go install 安装的命令。最后就是进行测试了，如果我们执行 go version 命令可以成功输出 Go 的版本，就说明 Go 编译环境安装成功。具体的命令如下： bash go version ProtoBuf 编译环境安装 接着，我们再来安装 protobuf 的编译器 protoc。protoc 需要 protoc-gen-go 来完成 Go 语言的代码转换，因此我们需要安装 protoc 和 protoc-gen-go 这 2 个工具。它们的安装方法比较简单，你直接看我下面给出的代码和操作注释就可以了。 # 第一步：安装 protobuf cd /tmp/ git clone --depth=1 https://github.com/protocolbuffers/protobuf cd protobuf ./autogen.sh ./configure make sudo make install protoc --version # 查看 protoc 版本，成功输出版本号，说明安装成功 libprotoc 3.15.6 # 第二步：安装 protoc-gen-go $ go get -u github.com/golang/protobuf/protoc-gen-go ","date":"2022-07-07 15:18:47","objectID":"/iam_before_class/:2:3","tags":["iam"],"title":"iam_before_class","uri":"/iam_before_class/"},{"categories":["iam"],"content":"Go 开发 IDE 安装和配置 编译环境准备完之后，我们还需要一个代码编辑器才能开始 Go 项目开发，并且为了提高开发效率，我们需要将这个编辑器配置成 Go IDE。目前，GoLand、VSCode 这些 IDE 都很优秀，我们使用的也很多，但它们都是 Windows 系统下的 IDE。因此，在 Linux 环境下我们可以选择将 Vim 配置成 Go IDE，熟悉 Vim IDE 的操作之后，它的开发效率不输 GoLand 和 VSCode。比如说，我们可以通过 SpaceVim 将 Vim 配置成一个 Go IDE。SpaceVim 是一个社区驱动的模块化的 Vim IDE，以模块的方式组织管理插件以及相关配置， 为不同的语言开发量身定制了相关的开发模块，该模块提供代码自动补全、 语法检查、格式化、调试、REPL 等特性。我们只需要载入相关语言的模块就能得到一个开箱即用的 Vim IDE 了。Vim 可以选择 NeoVim，NeoVim 是基于 Vim 的一个 fork 分支，它主要解决了 Vim8 之前版本中的异步执行、开发模式等问题，对 Vim 的兼容性很好。同时对 vim 的代码进行了大量地清理和重构，去掉了对老旧系统的支持，添加了新的特性。 虽然 Vim8 后来也新增了异步执行等特性，在使用层面两者差异不大，但是NeoVim 开发更激进，新特性更多，架构也相对更合理，所以我选择了 NeoVim，你也可以根据个人爱好来选择（都是很优秀的编辑器，这里不做优缺点比较）。Vim IDE 的安装和配置主要分五步： 第一步，安装 NeoVim。我们直接执行 pip3 和 yum 命令安装即可，安装方法如下： sudo pip3 install pynvim sudo yum install -y https://dl.fedoraproject.org/pub/epel/epel-release-latest-8.noarch.rpm sudo yum -y install neovim 第二步，配置 $HOME/.bashrc。先配置 nvim 的别名为 vi，这样当我们执行 vi 时，Linux 系统就会默认调用 nvim。同时，配置 EDITOR 环境变量可以使一些工具，例如 Git 默认使用 nvim。配置方法如下： tee -a $HOME/.bashrc \u003c\u003c'EOF' # Configure for nvim export EDITOR=nvim # 默认的编辑器（git 会用到） alias vi=\"nvim\" EOF 第三步，检查 nvim 是否安装成功。我们可以通过查看 NeoVim 版本来确认是否成功安装，如果成功输出版本号，说明 NeoVim 安装成功。 bash vi --version # 输出 NVIM v0.3.8 说明安装成功 NVIM v0.3.8 Build type: RelWithDebInfo ... 第四步，离线安装 SpaceVim。安装 SpaceVim 步骤稍微有点复杂，为了简化你的安装，同时消除网络的影响，我将安装和配置 SpaceVim 的步骤做成了一个离线安装包 marmotVim 。marmotVim 可以进行 SpaceVim 的安装、卸载、打包等操作，安装步骤如下： cd /tmp wget https://marmotedu-1254073058.cos.ap-beijing.myqcloud.com/tools/marmotVim.tar.gz tar -xvzf marmotVim.tar.gz cd marmotVim ./marmotVimCtl install SpaceVim 配置文件为：$HOME/.SpaceVim.d/init.toml 和$HOME/.SpaceVim.d/autoload/custom_init.vim，你可自行配置（配置文件中有配置说明）：init.toml：SpaceVim 的配置文件custom_init.vim：兼容 vimrc，用户自定义的配置文件SpaceVim Go IDE 常用操作的按键映射如下表所示： 第五步，Go 工具安装。SpaceVim 会用到一些 Go 工具，比如在函数跳转时会用到 guru、godef 工具，在格式化时会用到 goimports，所以我们也需要安装这些工具。安装方法有 2 种：Vim 底线命令安装：vi test.go，然后执行：:GoInstallBinaries 安装。拷贝工具：直接将整理好的工具文件拷贝到$GOPATH/bin 目录下。为了方便，你可以直接拷贝我已经打包好的 Go 工具到指定目录下： cd /tmp wget https://marmotedu-1254073058.cos.ap-beijing.myqcloud.com/tools/gotools-for-spacevim.tgz mkdir -p $GOPATH/bin tar -xvzf gotools-for-spacevim.tgz -C $GOPATH/bin ","date":"2022-07-07 15:18:47","objectID":"/iam_before_class/:2:4","tags":["iam"],"title":"iam_before_class","uri":"/iam_before_class/"},{"categories":["iam"],"content":"3 | 快速部署 总的来说，我把部署过程分成 2 大步。安装和配置数据库：我们需要安装和配置 MariaDB、Redis 和 MongoDB。安装和配置 IAM 服务：我们需要安装和配置 iam-apiserver、iam-authz-server、iam-pump、iamctl 和 man 文件。 一键部署（卸载） IAM 系统： $ mkdir -p $WORKSPACE/golang/src/github.com/marmotedu $ cd $WORKSPACE/golang/src/github.com/marmotedu $ git clone --depth=1 https://github.com/marmotedu/iam # 善用 alias，将常用操作配置成 alias，方便以后操作 $ tee -a $HOME/.bashrc \u003c\u003c 'EOF' # Alias for quick access export GOWORK=\"$WORKSPACE/golang/src\" export IAM_ROOT=\"$GOWORK/github.com/marmotedu/iam\" export LINUX_PASSWORD='iam59!z$' alias mm=\"cd $GOWORK/github.com/marmotedu\" alias i=\"cd $GOWORK/github.com/marmotedu/iam\" EOF $ bash ","date":"2022-07-07 15:18:47","objectID":"/iam_before_class/:3:0","tags":["iam"],"title":"iam_before_class","uri":"/iam_before_class/"},{"categories":["iam"],"content":"安装和配置数据库 因为 IAM 系统用到了 MariaDB、Redis、MongoDB 数据库来存储数据，而 IAM 服务在启动时会先尝试连接这些数据库，所以为了避免启动时连接数据库失败，这里我们先来安装需要的数据库。 安装和配置 MariaDBIAM： IAM 会把 REST 资源的定义信息存储在关系型数据库中，关系型数据库我选择了 MariaDB。为啥选择 MariaDB，而不是 MySQL 呢？。选择 MariaDB 一方面是因为它是发展最快的 MySQL 分支，相比 MySQL，它加入了很多新的特性，并且它能够完全兼容 MySQL，包括 API 和命令行。另一方面是因为 MariaDB 是开源的，而且迭代速度很快。首先，我们可以通过以下命令安装和配置 MariaDB，并将 Root 密码设置为 iam59!z$： $ cd $IAM_ROOT $ ./scripts/install/mariadb.sh iam::mariadb::install $ mysql -h127.0.0.1 -uroot -p'iam59!z$' MariaDB [(none)]\u003e 安装和配置 Redis： 在 IAM 系统中，由于 iam-authz-server 是从 iam-apiserver 拉取并缓存用户的密钥 / 策略信息的，因此同一份密钥 / 策略数据会分别存在 2 个服务中，这可能会出现数据不一致的情况。数据不一致会带来一些问题，例如当我们通过 iam-apiserver 创建了一对密钥，但是这对密钥还没有被 iam-authz-server 缓存，这时候通过这对密钥访问 iam-authz-server 就会访问失败。为了保证数据的一致性，我们可以使用 Redis 的发布订阅 (pub/sub) 功能进行消息通知。同时，iam-authz-server 也会将授权审计日志缓存到 Redis 中，所以也需要安装 Redis key-value 数据库。我们可以通过以下命令来安装和配置 Redis，并将 Redis 的初始密码设置为 iam59!z$ ： $ cd $IAM_ROOT $ ./scripts/install/redis.sh iam::redis::install $ redis-cli -h 127.0.0.1 -p 6379 -a 'iam59!z$' # 连接 Redis，-h 指定主机，-p 指定监听端口，-a 指定登录密码 安装和配置 MongoDB因为 iam-pump 会将 iam-authz-server 产生的数据处理后存储在 MongoDB 中，所以我们也需要安装 MongoDB 数据库。主要分两步安装：首先安装 MongoDB，然后再创建 MongoDB 账号。 第 1 步，安装 MongoDB 首先，我们可以通过以下 4 步来安装 MongoDB。配置 MongoDB yum 源，并安装 MongoDB。CentOS8.x 系统默认没有配置安装 MongoDB 需要的 yum 源，所以我们需要先配置好 yum 源再安装： $ sudo tee /etc/yum.repos.d/mongodb-org-4.4.repo\u003c\u003c'EOF' [mongodb-org-4.4] name=MongoDB Repository baseurl=https://repo.mongodb.org/yum/redhat/$releasever/mongodb-org/4.4/x86_64/ gpgcheck=1 enabled=1 gpgkey=https://www.mongodb.org/static/pgp/server-4.4.asc EOF $ sudo yum install -y mongodb-org 关闭 SELinux。在安装的过程中，SELinux 有可能会阻止 MongoDB 访问 /sys/fs/cgroup，所以我们还需要关闭 SELinux： $ sudo setenforce 0 $ sudo sed -i 's/^SELINUX=.*$/SELINUX=disabled/' /etc/selinux/config # 永久关闭 SELINUX 开启外网访问权限和登录验证。MongoDB 安装完之后，默认情况下是不会开启外网访问权限和登录验证，为了方便使用，我建议你先开启这些功能，执行如下命令开启： $ sudo sed -i '/bindIp/{s/127.0.0.1/0.0.0.0/}' /etc/mongod.conf $ sudo sed -i '/^#security/a\\security:\\n authorization: enabled' /etc/mongod.conf 配置完 MongoDB 之后，我们就可以启动它了，具体的命令如下： $ sudo systemctl start mongod $ sudo systemctl enable mongod # 设置开机启动 $ sudo systemctl status mongod # 查看 mongod 运行状态，如果输出中包含 active (running)字样说明 mongod 成功启动 # 登陆 $ mongo --quiet \"mongodb://127.0.0.1:27017\" \u003e 第 2 步，创建 MongoDB 账号安装完 MongoDB 之后，默认是没有用户账号的，为了方便 IAM 服务使用，我们需要先创建好管理员账号，通过管理员账户登录 MongoDB，我们可以执行创建普通用户、数据库等操作。创建管理员账户。首先，我们通过 use admin 指令切换到 admin 数据库，再通过 db.auth(“用户名”，“用户密码”) 验证用户登录权限。如果返回 1 表示验证成功；如果返回 0 表示验证失败。具体的命令如下： $ mongo --quiet \"mongodb://127.0.0.1:27017\" \u003e use admin switched to db admin \u003e db.createUser({user:\"root\",pwd:\"iam59!z$\",roles:[\"root\"]}) Successfully added user: { \"user\" : \"root\", \"roles\" : [ \"root\" ] } \u003e db.auth(\"root\", \"iam59!z$\") 1 此外，如果想删除用户，可以使用 db.dropUser(“用户名”) 命令。db.createUser 用到了以下 3 个参数。user: 用户名。pwd: 用户密码。roles: 用来设置用户的权限，比如读、读写、写等。 因为 admin 用户具有 MongoDB 的 Root 权限，权限过大安全性会降低。为了提高安全性，我们还需要创建一个 iam 普通用户来连接和操作 MongoDB。 $ mongo --quiet mongodb://root:'iam59!z$'@127.0.0.1:27017/iam_analytics?authSource=admin # 用管理员账户连接 MongoDB \u003e use iam_analytics switched to db iam_analytics \u003e db.createUser({user:\"iam\",pwd:\"iam59!z$\",roles:[\"dbOwner\"]}) Successfully added user: { \"user\" : \"iam\", \"roles\" : [ \"dbOwner\" ] } \u003e db.auth(\"iam\", \"iam59!z$\") 1 # 用iam用户登陆 $ mongo --quiet mongodb://iam:'iam59!z$'@127.0.0.1:27017/iam_analytics?authSource=iam_analytics ","date":"2022-07-07 15:18:47","objectID":"/iam_before_class/:3:1","tags":["iam"],"title":"iam_before_class","uri":"/iam_before_class/"},{"categories":["iam"],"content":"安装和配置 IAM 系统 要想完成 IAM 系统的安装，我们还需要安装和配置 iam-apiserver、iam-authz-server、iam-pump 和 iamctl。 准备工作 5步：初始化 MariaDB 数据库，创建 iam 数据库。配置 scripts/install/environment.sh。创建需要的目录。创建 CA 根证书和密钥。配置 hosts。 第 1 步，初始化 MariaDB 数据库，创建 iam 数据库。安装完 MariaDB 数据库之后，我们需要在 MariaDB 数据库中创建 IAM 系统需要的数据库、表和存储过程，以及创建 SQL 语句保存在 IAM 代码仓库中的 configs/iam.sql 文件中。具体的创建步骤如下。登录数据库并创建 iam 用户。 $ cd $IAM_ROOT $ mysql -h127.0.0.1 -P3306 -uroot -p'iam59!z$' # 连接 MariaDB，-h 指定主机，-P 指定监听端口，-u 指定登录用户，-p 指定登录密码 MariaDB [(none)]\u003e grant all on iam.* TO iam@127.0.0.1 identified by 'iam59!z$'; Query OK, 0 rows affected (0.000 sec) MariaDB [(none)]\u003e flush privileges; Query OK, 0 rows affected (0.000 sec) 用 iam 用户登录 MariaDB，执行 iam.sql 文件，创建 iam 数据库。 $ mysql -h127.0.0.1 -P3306 -uiam -p'iam59!z$' MariaDB [(none)]\u003e source configs/iam.sql; MariaDB [iam]\u003e show databases; +--------------------+ | Database | +--------------------+ | iam | | information_schema | +--------------------+ 2 rows in set (0.000 sec) 上面的命令会创建 iam 数据库，并创建以下数据库资源。表：user 是用户表，用来存放用户信息；secret 是密钥表，用来存放密钥信息；policy 是策略表，用来存放授权策略信息；policy_audit 是策略历史表，被删除的策略会被转存到该表。admin 用户：在 user 表中，我们需要创建一个管理员用户，用户名是 admin，密码是 Admin@2021。存储过程：删除用户时会自动删除该用户所属的密钥和策略信息。 第 2 步，配置 scripts/install/environment.sh。IAM 组件的安装配置都是通过环境变量文件 scripts/install/environment.sh 进行配置的，所以我们要先配置好 scripts/install/environment.sh 文件。这里，你可以直接使用默认值，提高你的安装效率。 第 3 步，创建需要的目录。在安装和运行 IAM 系统的时候，我们需要将配置、二进制文件和数据文件存放到指定的目录。所以我们需要先创建好这些目录，创建步骤如下。 $ cd $IAM_ROOT $ source scripts/install/environment.sh $ sudo mkdir -p ${IAM_DATA_DIR}/{iam-apiserver,iam-authz-server,iam-pump} # 创建 Systemd WorkingDirectory 目录 $ sudo mkdir -p ${IAM_INSTALL_DIR}/bin #创建 IAM 系统安装目录 $ sudo mkdir -p ${IAM_CONFIG_DIR}/cert # 创建 IAM 系统配置文件存放目录 $ sudo mkdir -p ${IAM_LOG_DIR} # 创建 IAM 日志文件存放目录 第 4 步， 创建 CA 根证书和密钥。为了确保安全，IAM 系统各组件需要使用 x509 证书对通信进行加密和认证。所以，这里我们需要先创建 CA 证书。CA 根证书是所有组件共享的，只需要创建一个 CA 证书，后续创建的所有证书都由它签名。我们可以使用 CloudFlare 的 PKI 工具集 cfssl 来创建所有的证书。安装 cfssl 工具集。我们可以直接安装 cfssl 已经编译好的二进制文件，cfssl 工具集中包含很多工具，这里我们需要安装 cfssl、cfssljson、cfssl-certinfo，功能如下。cfssl：证书签发工具。cfssljson：将 cfssl 生成的证书（json 格式）变为文件承载式证书。这两个工具的安装方法如下： $ cd $IAM_ROOT $ ./scripts/install/install.sh iam::install::install_cfssl 创建配置文件。CA 配置文件是用来配置根证书的使用场景 (profile) 和具体参数 (usage、过期时间、服务端认证、客户端认证、加密等)，可以在签名其它证书时用来指定特定场景： $ cd $IAM_ROOT $ tee ca-config.json \u003c\u003c EOF { \"signing\": { \"default\": { \"expiry\": \"87600h\" }, \"profiles\": { \"iam\": { \"usages\": [ \"signing\", \"key encipherment\", \"server auth\", \"client auth\" ], \"expiry\": \"876000h\" } } } } EOF 上面的 JSON 配置中，有一些字段解释如下。signing：表示该证书可用于签名其它证书（生成的 ca.pem 证书中 CA=TRUE）。server auth：表示 client 可以用该证书对 server 提供的证书进行验证。client auth：表示 server 可以用该证书对 client 提供的证书进行验证。expiry：876000h，证书有效期设置为 100 年。 创建证书签名请求文件。我们创建用来生成 CA 证书签名请求（CSR）的 JSON 配置文件： $ cd $IAM_ROOT $ tee ca-csr.json \u003c\u003c EOF { \"CN\": \"iam-ca\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"BeiJing\", \"L\": \"BeiJing\", \"O\": \"marmotedu\", \"OU\": \"iam\" } ], \"ca\": { \"expiry\": \"876000h\" } } EOF 上面的 JSON 配置中，有一些字段解释如下。C：Country，国家。ST：State，省份。L：Locality (L) or City，城市。CN：Common Name，iam-apiserver 从证书中提取该字段作为请求的用户名 (User Name) ，浏览器使用该字段验证网站是否合法。O：Organization，iam-apiserver 从证书中提取该字段作为请求用户所属的组 (Group)。OU：Company division (or Organization Unit – OU)，部门 / 单位。 除此之外，还有两点需要我们注意。不同证书 csr 文件的 CN、C、ST、L、O、OU 组合必须不同，否则可能出现 PEER’S CERTIFICATE HAS AN INVALID SIGNATURE 错误。后续创建证书的 csr 文件时，CN、OU 都不相同（C、ST、L、O 相同），以达到区分的目的。 创建 CA 证书和私钥 首先，我们通过 cfssl gencert 命令来创建： $ cd $IAM_ROOT $ source scripts/install/environment.sh $ cfssl gencert -initca ca-csr.json | cfssljson -bare ca $ ls ca* ca-config.json ca.csr ca-csr.json ca-key.pem ca.pem $ sudo mv ca* ${IAM_CONFIG_DIR}/cert # 需要将证书文件拷贝到指定文件夹下（分发证书），方便各组件引用 上述命令会创建运行 CA 所必需的文件 ca-key.pem（私钥）和 ca.pem（证书），还会生成 ca.csr（证书签名请求），用于交叉签名或重新签名。创建完之后，我们可以通过 cfssl certinfo 命名查看 cert 和 csr 信息： $ cfssl certinfo -cert ${IAM_CONFIG_DIR}/cert/ca.pem # 查看 cert(证书信息) $ cfssl certinfo -csr ${IAM_CONFIG_DIR}/cert/ca.csr # 查看 CSR(证书签名请求)信息 第 5 步，配置 hosts。iam 通过域名访问 API 接口，因为这些域名没有注册过，还不能在互联网上解析，所以需要配置 hosts，具体的操作如下： $ sudo ","date":"2022-07-07 15:18:47","objectID":"/iam_before_class/:3:2","tags":["iam"],"title":"iam_before_class","uri":"/iam_before_class/"},{"categories":["iam"],"content":"重要性 “云”是大势所趋，而 Go 是云时代的语言最近几年，我发现腾讯很多团队的开发语言都在转 Go。其实，不光腾讯，像阿里、华为和百度这类国内一线大厂也都在积极转 Go。甚至不少团队，所有项目都是用 Go 构建的。伴随而来的，就是各个公司对 Go 研发工程师的需求越来越旺盛。那么， Go 为什么会变得这么火热呢？我认为，原因主要有两个方面。一方面，Go 是一门非常优秀的语言，它具有很多核心优势，例如：语言简单、语言层面支持并发编程、跨平台编译和自带垃圾回收机制等，这些优势是这些团队选择 Go 最根本的原因。另一方面，也因为 Go 是云时代的语言。为什么这么说呢？下面，我来详细说说。随着云计算平台的逐渐成熟，应用上云已经成为一个不可逆转的趋势了，很多公司都选择将基础架构 / 业务架构云化，例如阿里、腾讯都在将公司内部业务全面云化。可以说，全面云化已经是公司层面的核心 KPI 了，我们甚至可以理解为以后所有的技术都会围绕着云来构建。而云目前是朝着云原生架构的方向演进的，云原生架构中具有统治力（影响力）的项目绝大部分又是用 Go 构建的。我们从下面这张云原生技术栈语言组成图中可以看到，有 63% 的具有统治力的云原生项目都是用 Go 来构建的。 完整的云原生技术栈可参考云原生技术图谱 因此，想要把基础架构 / 业务架构云化，离不开对这些云原生开源项目的学习、改造。而一个团队为了节省成本，技术栈最好统一。 ","date":"2022-07-07 15:18:12","objectID":"/iam_intro/:1:0","tags":["iam"],"title":"iam_intro","uri":"/iam_intro/"},{"categories":["iam"],"content":"问题 比如说，有个开发者写的代码依赖数据库连接，没法写单元测试。细问之后，我发现他参考的文章没有将数据库层跟业务层通过接口解耦。再比如说，还有一些开发者开发的项目很难维护，项目中出现了大量的 common、util、const 这类 Go 包。只看包名，我完全不知道包所实现的功能，问了之后才发现他是参考了一个带有 dao、model、controller、service 目录的、不符合 Go 设计哲学的项目。 我们在学习 Go 项目开发时会面临以下 4 大类问题。 知识盲区：Go 项目开发会涉及很多知识点，但自己对这些知识点却一无所知。想要学习，却发现网上很多文章结构混乱、讲解不透彻。想要搜索一遍优秀的文章，又要花费很多时间，劳神劳力。 学不到最佳实践，能力提升有限：网上有很多文章会介绍 Go 项目的构建方法，但很多都不是最佳实践，学完之后不能在能力和认知上带来最佳提升，还要自己花时间整理学习，事倍功半。 不知道如何完整地开发一个 Go 项目：学了很多 Go 开发相关的知识点、构建方法，但都不体系、不全面、不深入。学完之后，自己并不能把它们有机结合成一个 Go 项目研发体系，真正开发的时候还是一团乱，效率也很低。 缺乏一线项目练手，很难检验学习效果：为了避免闭门造车，我们肯定想学习一线大厂的大型项目构建和研发经验，来检验自己的学习成果，但自己平时又很难接触到，没有这样的学习途径。 ","date":"2022-07-07 15:18:12","objectID":"/iam_intro/:2:0","tags":["iam"],"title":"iam_intro","uri":"/iam_intro/"},{"categories":["iam"],"content":"概述 围绕一个可部署、可运行的企业应用源码，为你详细讲解实际开发流程中会涉及的技能点，让你彻底学会如何构建企业级 Go 应用，并解决 Go 项目开发所面临的各类问题。 除此之外，专栏中的每个技能点我都会尽可能朝着“最佳实践”的方向去设计。例如，我使用的 Go 包都是业界采纳度最高的包，而且设计时，我也会尽可能遵循 Go 设计模式、Go 开发规范、Go 最佳实践、go clean architecture 等。同时，我也会尽量把我自己做一线 Go 项目研发的经验，融合到讲解的过程中，给你最靠谱的建议，这些经验和建议可以让你在构建应用的过程中，少走很多弯路。 为了让你更好地学习这门课程，我把整个专栏划分为了 6 个模块。其中，第 1 个模块是实战环境准备，第 2 到第 6 个模块我会带着你按照研发的流程来实际构建一个应用。实战准备：我会先手把手带你准备一个实验环境，再带你部署我们的实战项目。加深你对实战项目的理解的同时，给你讲解一些部署的技能点，包括如何准备开发环境、制作 CA 证书，安装和配置用到的数据库、应用，以及 Shell 脚本编写技巧等。 实战第 1 站：规范设计：我会详细介绍开发中常见的 10 大规范，例如目录规范、日志规范、错误码规范、Commit 规范等。通过本模块，你能够学会如何设计常见的规范，为高效开发一个高质量、易阅读、易维护的 Go 应用打好基础。 实战第 2 站：基础功能设计或开发：我会教你设计或开发一些 Go 应用开发中的基础功能，这些功能会影响整个应用的构建方式，例如日志包、错误包、错误码等。 实战第 3 站：服务开发：我会带你一起解析一个企业级的 Go 项目代码，让你学会如何开发 Go 应用。在解析的过程中，我也会详细讲解 Go 开发阶段的各个技能点，例如怎么设计和开发 API 服务、Go SDK、客户端工具等。 实战第 4 站：服务测试：我会围绕实战项目来讲解进行单元测试、功能测试、性能分析和性能调优的方法，最终让你交付一个性能和稳定性都经过充分测试的、生产级可用的服务。 实战第 5 站：服务部署：本模块通过实战项目的部署，来告诉你如何部署一个高可用、安全、具备容灾能力，又可以轻松水平扩展的企业应用。这里，我会重点介绍 2 种部署方式：传统部署方式和容器化部署方式，每种方式在部署方法、复杂度和能力上都有所不同。 ","date":"2022-07-07 15:18:12","objectID":"/iam_intro/:3:0","tags":["iam"],"title":"iam_intro","uri":"/iam_intro/"},{"categories":["API Style"],"content":" 来自字节Kitex文档 ","date":"2022-06-10 09:28:10","objectID":"/kitex/:0:0","tags":["API style"],"title":"Kitex","uri":"/kitex/"},{"categories":["API Style"],"content":"准备 Golang 开发环境 如果您之前未搭建 Golang 开发环境， 可以参考 Golang 安装 推荐使用最新版本的 Golang，我们保证最新三个正式版本的兼容性(现在 \u003e= v1.16)。 确保打开 go mod 支持 (Golang \u003e= 1.15时，默认开启) kitex 暂时没有针对 Windows 做支持，如果本地开发环境是 Windows 建议使用 WSL2 ","date":"2022-06-10 09:28:10","objectID":"/kitex/:1:0","tags":["API style"],"title":"Kitex","uri":"/kitex/"},{"categories":["API Style"],"content":"快速上手 在完成环境准备后，本章节将帮助你快速上手 Kitex ","date":"2022-06-10 09:28:10","objectID":"/kitex/:2:0","tags":["API style"],"title":"Kitex","uri":"/kitex/"},{"categories":["API Style"],"content":"安装代码生成工具 首先，我们需要安装使用本示例所需要的命令行代码生成工具： 确保 GOPATH 环境变量已经被正确地定义（例如 export GOPATH=~/go）并且将$GOPATH/bin添加到 PATH 环境变量之中（例如 export PATH=$GOPATH/bin:$PATH）；请勿将 GOPATH 设置为当前用户没有读写权限的目录 安装 kitex：go install github.com/cloudwego/kitex/tool/cmd/kitex@latest 安装 thriftgo：go install github.com/cloudwego/thriftgo@latest 安装成功后，执行 kitex --version 和 thriftgo --version 应该能够看到具体版本号的输出（版本号有差异，以 x.x.x 示例）： $ kitex --version vx.x.x $ thriftgo --version thriftgo x.x.x 如果在安装阶段发生问题，可能主要是由于对 Golang 的不当使用造成，请依照报错信息进行检索 ","date":"2022-06-10 09:28:10","objectID":"/kitex/:2:1","tags":["API style"],"title":"Kitex","uri":"/kitex/"},{"categories":["API Style"],"content":"确定代码放置位置 若将代码放置于 $GOPATH/src 下，需在 $GOPATH/src 下创建额外目录，进入该目录后再获取代码： mkdir -p $(go env GOPATH)/src/github.com/cloudwego cd $(go env GOPATH)/src/github.com/cloudwego 若将代码放置于 GOPATH 之外，可直接获取 ","date":"2022-06-10 09:28:10","objectID":"/kitex/:2:2","tags":["API style"],"title":"Kitex","uri":"/kitex/"},{"categories":["API Style"],"content":"获取示例代码 你可以直接点击 此处 下载示例仓库 也可以克隆该示例仓库到本地 git clone https://github.com/cloudwego/kitex-examples.git ","date":"2022-06-10 09:28:10","objectID":"/kitex/:2:3","tags":["API style"],"title":"Kitex","uri":"/kitex/"},{"categories":["API Style"],"content":"运行示例代码 方式一：直接启动 进入示例仓库的 hello 目录 cd kitex-examples/hello 运行 server go run . 运行 client 另起一个终端后，go run ./client 方式二：使用 Docker 快速启动 进入示例仓库目录 cd kitex-examples 编译项目 docker build -t kitex-examples . 运行 server docker run --network host kitex-examples ./hello-server 运行 client 另起一个终端后，docker run --network host kitex-examples ./hello-client 恭喜你，你现在成功通过 Kitex 发起了 RPC 调用。 ","date":"2022-06-10 09:28:10","objectID":"/kitex/:2:4","tags":["API style"],"title":"Kitex","uri":"/kitex/"},{"categories":["API Style"],"content":"增加一个新的方法 打开 hello.thrift，你会看到如下内容： namespacegoapistructRequest{1:stringmessage}structResponse{1:stringmessage}serviceHello{Responseecho(1:Requestreq)} 现在让我们为新方法分别定义一个新的请求和响应，AddRequest 和 AddResponse，并在 service Hello 中增加 add 方法： namespacegoapistructRequest{1:stringmessage}structResponse{1:stringmessage}structAddRequest{1:i64first2:i64second}structAddResponse{1:i64sum}serviceHello{Responseecho(1:Requestreq)AddResponseadd(1:AddRequestreq)} 完成之后 hello.thrift 的内容应该和上面一样。 ","date":"2022-06-10 09:28:10","objectID":"/kitex/:2:5","tags":["API style"],"title":"Kitex","uri":"/kitex/"},{"categories":["API Style"],"content":"重新生成代码 运行如下命令后，kitex 工具将根据 hello.thrift 更新代码文件。 kitex -service a.b.c hello.thrift # 若当前目录不在 $GOPATH/src 下，需要加上 -module 参数，一般为 go.mod 下的名字 kitex -module \"your_module_name\" -service a.b.c hello.thrift 执行完上述命令后，kitex 工具将更新下述文件 更新 ./handler.go，在里面增加一个 Add 方法的基本实现 更新 ./kitex_gen，里面有框架运行所必须的代码文件 ","date":"2022-06-10 09:28:10","objectID":"/kitex/:2:6","tags":["API style"],"title":"Kitex","uri":"/kitex/"},{"categories":["API Style"],"content":"更新服务端处理逻辑 上述步骤完成后，./handler.go 中会自动补全一个 Add 方法的基本实现，类似如下代码： // Add implements the HelloImpl interface. func (s *HelloImpl) Add(ctx context.Context, req *api.AddRequest) (resp *api.AddResponse, err error) { // TODO: Your code here... return } 让我们在里面增加我们所需要的逻辑，类似如下代码： // Add implements the HelloImpl interface. func (s *HelloImpl) Add(ctx context.Context, req *api.AddRequest) (resp *api.AddResponse, err error) { // TODO: Your code here... resp = \u0026api.AddResponse{Sum: req.First + req.Second} return } ","date":"2022-06-10 09:28:10","objectID":"/kitex/:2:7","tags":["API style"],"title":"Kitex","uri":"/kitex/"},{"categories":["API Style"],"content":"增加客户端调用 服务端已经有了 Add 方法的处理，现在让我们在客户端增加对 Add 方法的调用。 在 ./client/main.go 中你会看到类似如下的 for 循环： for { req := \u0026api.Request{Message: \"my request\"} resp, err := client.Echo(context.Background(), req) if err != nil { log.Fatal(err) } log.Println(resp) time.Sleep(time.Second) } 现在让我们在里面增加 Add 方法的调用： for { req := \u0026api.Request{Message: \"my request\"} resp, err := client.Echo(context.Background(), req) if err != nil { log.Fatal(err) } log.Println(resp) time.Sleep(time.Second) addReq := \u0026api.AddRequest{First: 512, Second: 512} addResp, err := client.Add(context.Background(), addReq) if err != nil { log.Fatal(err) } log.Println(addResp) time.Sleep(time.Second) } ","date":"2022-06-10 09:28:10","objectID":"/kitex/:2:8","tags":["API style"],"title":"Kitex","uri":"/kitex/"},{"categories":["API Style"],"content":"重新运行示例代码 关闭之前运行的客户端和服务端之后 运行 server go run . 运行 client 另起一个终端后，go run ./client 现在，你应该能看到客户端在调用 Add 方法了。 ","date":"2022-06-10 09:28:10","objectID":"/kitex/:2:9","tags":["API style"],"title":"Kitex","uri":"/kitex/"},{"categories":["API Style"],"content":"基础教程 ","date":"2022-06-10 09:28:10","objectID":"/kitex/:3:0","tags":["API style"],"title":"Kitex","uri":"/kitex/"},{"categories":["API Style"],"content":"关于 Kitex Kitex 是一个 RPC 框架，既然是 RPC，底层就需要两大功能： Serialization 序列化 Transport 传输 Kitex 框架及命令行工具，默认支持 thrift 和 proto3 两种 IDL，对应的 Kitex 支持 thrift 和 protobuf 两种序列化协议。传输上 Kitex 使用扩展的 thrift 作为底层的传输协议（注：thrift 既是 IDL 格式，同时也是序列化协议和传输协议）。IDL 全称是 Interface Definition Language，接口定义语言。 ","date":"2022-06-10 09:28:10","objectID":"/kitex/:3:1","tags":["API style"],"title":"Kitex","uri":"/kitex/"},{"categories":["API Style"],"content":"为什么要使用 IDL 如果我们要进行 RPC，就需要知道对方的接口是什么，需要传什么参数，同时也需要知道返回值是什么样的，（即使我们的接口并没有真正实现调通，也可以通过IDL生成调用的代码，不会阻塞对方的开发）就好比两个人之间交流，需要保证在说的是同一个语言、同一件事。这时候，就需要通过 IDL 来约定双方的协议，就像在写代码的时候需要调用某个函数，我们需要知道函数签名一样。 Thrift IDL 语法可参考：Thrift interface description language。 proto3 语法可参考：Language Guide(proto3)。 ","date":"2022-06-10 09:28:10","objectID":"/kitex/:3:2","tags":["API style"],"title":"Kitex","uri":"/kitex/"},{"categories":["API Style"],"content":"创建项目目录 在开始后续的步骤之前，先让我们创建一个项目目录用于后续的教程。 $ mkdir example 然后让我们进入项目目录 $ cd example ","date":"2022-06-10 09:28:10","objectID":"/kitex/:3:3","tags":["API style"],"title":"Kitex","uri":"/kitex/"},{"categories":["API Style"],"content":"Kitex 命令行工具 Kitex 自带了一个同名的命令行工具 kitex，用来帮助大家很方便地生成代码，新项目的生成以及之后我们会学到的 server、client 代码的生成都是通过 kitex 工具进行。 安装 可以使用以下命令来安装或者更新 kitex： $ go install github.com/cloudwego/kitex/tool/cmd/kitex 完成后，可以通过执行 kitex 来检测是否安装成功。 $ kitex 如果出现如下输出，则安装成功。 $ kitex No IDL file found. 如果出现 command not found 错误，可能是因为没有把 $GOPATH/bin 加入到 $PATH 中，详见环境准备一章。 使用 kitex 的具体使用请参考代码生成工具 ","date":"2022-06-10 09:28:10","objectID":"/kitex/:3:4","tags":["API style"],"title":"Kitex","uri":"/kitex/"},{"categories":["API Style"],"content":"编写 IDL 首先我们需要编写一个 IDL，这里以 thrift IDL 为例。 首先创建一个名为 echo.thrift 的 thrift IDL 文件。 然后在里面定义我们的服务 namespacegoapistructRequest{1:stringmessage}structResponse{1:stringmessage}serviceEcho{Responseecho(1:Requestreq)} ","date":"2022-06-10 09:28:10","objectID":"/kitex/:3:5","tags":["API style"],"title":"Kitex","uri":"/kitex/"},{"categories":["API Style"],"content":"生成 echo 服务代码 有了 IDL 以后我们便可以通过 kitex 工具生成项目代码了，执行如下命令： $ kitex -module example -service example echo.thrift 上述命令中，-module 表示生成的该项目的 go module 名，-service 表明我们要生成一个服务端项目，后面紧跟的 example 为该服务的名字。最后一个参数则为该服务的 IDL 文件。 生成后的项目结构如下： . |-- build.sh |-- echo.thrift |-- handler.go |-- kitex_gen | `-- api | |-- echo | | |-- client.go | | |-- echo.go | | |-- invoker.go | | `-- server.go | |-- echo.go | `-- k-echo.go |-- main.go `-- script |-- bootstrap.sh `-- settings.py ","date":"2022-06-10 09:28:10","objectID":"/kitex/:3:6","tags":["API style"],"title":"Kitex","uri":"/kitex/"},{"categories":["API Style"],"content":"获取最新的 Kitex 框架 由于 kitex 要求使用 go mod 进行依赖管理，所以我们要升级 kitex 框架会很容易，只需要执行以下命令即可： $ go get github.com/cloudwego/kitex@latest $ go mod tidy 如果遇到类似如下报错： github.com/apache/thrift/lib/go/thrift: ambiguous import: found package github.com/apache/thrift/lib/go/thrift in multiple modules 先执行一遍下述命令，再继续操作： go mod edit -droprequire=github.com/apache/thrift/lib/go/thrift go mod edit -replace=github.com/apache/thrift=github.com/apache/thrift@v0.13.0 ","date":"2022-06-10 09:28:10","objectID":"/kitex/:3:7","tags":["API style"],"title":"Kitex","uri":"/kitex/"},{"categories":["API Style"],"content":"编写 echo 服务逻辑 我们需要编写的服务端逻辑都在 handler.go 这个文件中，现在这个文件应该如下所示： package main import ( \"context\" \"example/kitex_gen/api\" ) // EchoImpl implements the last service interface defined in the IDL. type EchoImpl struct{} // Echo implements the EchoImpl interface. func (s *EchoImpl) Echo(ctx context.Context, req *api.Request) (resp *api.Response, err error) { // TODO: Your code here... return } 这里的 Echo 函数就对应了我们之前在 IDL 中定义的 echo 方法。 现在让我们修改一下服务端逻辑，让 Echo 服务名副其实。 修改 Echo 函数为下述代码： func (s *EchoImpl) Echo(ctx context.Context, req *api.Request) (resp *api.Response, err error) { return \u0026api.Response{Message: req.Message}, nil } ","date":"2022-06-10 09:28:10","objectID":"/kitex/:3:8","tags":["API style"],"title":"Kitex","uri":"/kitex/"},{"categories":["API Style"],"content":"编译运行 kitex 工具已经帮我们生成好了编译和运行所需的脚本： 编译： $ sh build.sh 执行上述命令后，会生成一个 output 目录，里面含有我们的编译产物。 运行： $ sh output/bootstrap.sh 执行上述命令后，Echo 服务就开始运行啦！ ","date":"2022-06-10 09:28:10","objectID":"/kitex/:3:9","tags":["API style"],"title":"Kitex","uri":"/kitex/"},{"categories":["API Style"],"content":"编写客户端 有了服务端后，接下来就让我们编写一个客户端用于调用刚刚运行起来的服务端。 首先，同样的，先创建一个目录用于存放我们的客户端代码： $ mkdir client 进入目录： $ cd client 创建一个 main.go 文件，然后就开始编写客户端代码了。 创建 client 首先让我们创建一个调用所需的 client： import \"example/kitex_gen/api/echo\" import \"github.com/cloudwego/kitex/client\" ... c, err := echo.NewClient(\"example\", client.WithHostPorts(\"0.0.0.0:8888\")) if err != nil { log.Fatal(err) } 上述代码中，echo.NewClient 用于创建 client，其第一个参数为调用的 服务名，第二个参数为 options，用于传入参数，此处的 client.WithHostPorts 用于指定服务端的地址，更多参数可参考基本特性一节。 发起调用 接下来让我们编写用于发起调用的代码： import \"example/kitex_gen/api\" ... req := \u0026api.Request{Message: \"my request\"} resp, err := c.Echo(context.Background(), req, callopt.WithRPCTimeout(3*time.Second)) if err != nil { log.Fatal(err) } log.Println(resp) 上述代码中，我们首先创建了一个请求 req , 然后通过 c.Echo 发起了调用。 其第一个参数为 context.Context，通过通常用其传递信息或者控制本次调用的一些行为，你可以在后续章节中找到如何使用它。 其第二个参数为本次调用的请求。 其第三个参数为本次调用的 options ，Kitex 提供了一种 callopt 机制，顾名思义——调用参数 ，有别于创建 client 时传入的参数，这里传入的参数仅对此次生效。 此处的 callopt.WithRPCTimeout 用于指定此次调用的超时（通常不需要指定，此处仅作演示之用）同样的，你可以在基本特性一节中找到更多的参数。 ","date":"2022-06-10 09:28:10","objectID":"/kitex/:3:10","tags":["API style"],"title":"Kitex","uri":"/kitex/"},{"categories":["API Style"],"content":"发起调用 在编写完一个简单的客户端后，我们终于可以发起调用了。 你可以通过下述命令来完成这一步骤： $ go run main.go 如果不出意外，你可以看到类似如下输出： 2021/05/20 16:51:35 Response({Message:my request}) 恭喜你！至此你成功编写了一个 Kitex 的服务端和客户端，并完成了一次调用！ ","date":"2022-06-10 09:28:10","objectID":"/kitex/:3:11","tags":["API style"],"title":"Kitex","uri":"/kitex/"},{"categories":["Graduation project"],"content":" Hsiang-Fu Yu, Nikhil Rao, Inderjit S. Dhillon, 2016. Temporal regularized matrix factorization for high-dimensional time series prediction. 笔记中部分公式未渲染出来，文末截图可见 Temporal Regularized Matrix Factorization(TRMF) for High-dimensional Time Series Prediction ","date":"2022-05-13 09:33:30","objectID":"/nips16_trmf/:0:0","tags":["papers"],"title":"NIPS16_TRMF","uri":"/nips16_trmf/"},{"categories":["Graduation project"],"content":"摘要 现代应用程序需要具有高度可扩展性的方法，并且可以处理有噪声的或有缺失值的数据。 本文提出了一个支持数据驱动的时态学习和预测的时态正则化矩阵分解（TRMF）框架。我们在学习自回归模型中的依赖关系的背景下，与图正则化方法建立了有趣的联系框架。 实验结果表明：TRMF在维数为50000的问题上比其他方法快两个数量级，并能对现实世界的数据集（如沃尔玛电子商务数据集）生成更好的预测。 ","date":"2022-05-13 09:33:30","objectID":"/nips16_trmf/:1:0","tags":["papers"],"title":"NIPS16_TRMF","uri":"/nips16_trmf/"},{"categories":["Graduation project"],"content":"1. 介绍 现代时间序列应用程序给从业者带来了两个挑战：处理大n（数字）和T（时间帧）的可伸缩性，以及处理缺失值的灵活性。 AR和DLM侧重于低维时间序列数据，而没有处理上述两个问题。 对高维时间序列数据建模的一种自然方式是以矩阵的形式，行对应于每个一维时间序列，列对应于时间点。 鉴于n个时间序列通常高度相关，有人尝试应用低秩矩阵分解（MF）或矩阵完成（MC）技术来分析高维时间序列[2,14,16,23,26]。与上面的AR和DLM模型不同，最先进的MF方法以n为单位线性扩展，因此可以处理大型数据集。 在MF中，观测到的n维时间序列数据被组织在矩阵 $\\mathcal{Y} \\in \\mathbb{R}^{n \\times T}$ 中，矩阵 y 由维度特性矩阵 $W \\in \\mathbb{R}^{n \\times r}$ 与时间特性矩阵 $X \\in \\mathbb{R}^{r \\times T}$ 的组合进行低秩逼近，从而修补缺失数据。 $$ \\mathcal{Y} \\approx W X $$ 使用最小二乘法、梯度下降等方法求解下述最小化问题，从而对矩阵 W 与矩阵 X 进行逼近： $$ \\min {W X} \\sum{(i, t) \\in \\Omega}\\left(\\mathcal{Y}{i t}-w{i}^{T} x_{t}\\right)^{2}+\\lambda_{w} \\mathcal{R}_{w}(W)+\\lambda_{x} \\mathcal{R}_{x}(X) $$ 其中， $\\Omega$ 是原矩阵 $\\mathcal{Y} \\in \\mathbb{R}^{n \\times T}$ 中非零元所处位置的集合； $\\sum_{(i, t) \\in \\Omega}\\left(\\mathcal{Y}_{i t}-w_{i}^{T} x_{t}\\right)^{2}$ 为残差矩阵F范数的平方，用来描述 W X 矩阵 与原矩阵 Y 的差异；$R_w$ 与 $R_x$ 分别是 W 与 X 的正则项，用来防止过拟合。 $$ \\mathcal{R}_{w}(W)=|W|_{F}^{2}=\\sum_{i=1}^{n}\\left|\\boldsymbol{w}_{i}\\right|^{2}=\\sum_{i=1}^{n} w_{i}^{T} w_{i} $$ $$ \\mathcal{R}{x}(X)=|X|{F}^{2}=\\sum_{t=1}^{T}\\left|\\boldsymbol{x}_{t}\\right|^{2}=\\sum_{t=1}^{T} x_{t}^{T} x_{t} $$ 大多数现有的MF方法采用基于图（这个图指的是同一个特征的时序关系拼接成图，也就是X矩阵的一行）的方法来处理时间依赖性。具体来说，依赖关系由加权相似图描述，并通过拉普拉斯正则项进行约束。 图1：多重时间序列的矩阵分解模型。F捕捉矩阵Y中每个时间序列的特征，X捕捉潜在和时变变量 MF方法可以对缺失数据进行修复，但是对于预测问题则无能为力。此外，由于MF方法并没有考虑数据的时序特性，对上述的交通与天气数据的修复效果并不理想。 本文提出了一个新的时间正则化矩阵分解框架(TRMF)用于高维时间序列分析。 在TRMF中，我们考虑了一种原则性的方法来描述潜在时间嵌入之间的时间依赖性结构{$x_t$}，并设计了一个时间正则化器来将这种时间依赖性结构纳入标准MF公式。 与大多数现有的MF方法不同，我们的TRMF方法支持数据驱动的时间依赖性学习，并为矩阵分解方法带来预测未来值的能力。此外，TRMF方法继承了MF方法的属性，即使在存在许多缺失值的情况下，TRMF也可以轻松处理高维时间序列数据。 作为一个具体的例子，我们展示了一种新的自回归时间正则化器，它鼓励时间嵌入{$x_t$}之间的AR（autoregressive）结构。 我们还将提出的正则化框架与基于图的方法联系起来，其中甚至可以解释负相关。 这种连接不仅有助于更好地理解我们的框架所包含的依赖结构，而且还有助于使用现成的高效求解器(如GRALS)直接求解TRMF。 ","date":"2022-05-13 09:33:30","objectID":"/nips16_trmf/:2:0","tags":["papers"],"title":"NIPS16_TRMF","uri":"/nips16_trmf/"},{"categories":["Graduation project"],"content":"2. 具有时间依赖性的数据的现有矩阵分解方法 标准MF公式对列的排列保持不变（列不管怎么变，权重矩阵保持不变），这不适用于具有时间依赖性的数据。 因此，对于时间依赖性{$x_t$}，大多数现有的时间MF方法都转向基于图的正则化框架，并用图编码时间依赖性。 ","date":"2022-05-13 09:33:30","objectID":"/nips16_trmf/:3:0","tags":["papers"],"title":"NIPS16_TRMF","uri":"/nips16_trmf/"},{"categories":["Graduation project"],"content":"时间依赖性的图正则化 图2：时态依赖的基于图的正则化。 令G是一个时间依赖性{$x_t$}的图，$G_{ts}$是第t个点和第s个点之间的边权重。一种常见的正则化方式如下公式： $$ \\mathcal{R}_{x}(X)=\\mathcal{G}(X \\mid G, \\eta):=\\frac{1}{2} \\sum_{t \\sim s} G_{t s}\\left|\\boldsymbol{x}_{t}-\\boldsymbol{x}_{s}\\right|^{2}+\\frac{\\eta}{2} \\sum_{t}\\left|\\boldsymbol{x}_{t}\\right|^{2}(2) $$ 其中t~s代表了第t个点和第s个点之间的边；第二个正则化项是用来保证强凸性 一个很大的$G_{ts}$可以保证$x_t$和$x_s$在欧几里得距离上很接近 为了保证$\\mathcal{G}(X \\mid G, \\eta)$的凸性，我们让$G_{ts}$≥0 为了将基于图的正则化应用于时间依赖关系上，我们需要通过滞后集L和权值向量w重复地指定各个点之间的依赖模式，以便距离L的所有边t ~ s共享相同的权值 于是上面的公式可以改写成： $$ \\mathcal{G}(X \\mid G, \\eta)=\\frac{1}{2} \\sum_{l \\in \\mathcal{L}} \\sum_{t: t\u003el} w_{l}\\left(\\boldsymbol{x}_{t}-\\boldsymbol{x}_{t-l}\\right)^{2}+\\frac{\\eta}{2} \\sum_{t}\\left|\\boldsymbol{x}_{t}\\right|^{2}(3) $$ 这种直接使用基于图的方法虽然很直观，但有两个问题: 两个时间点之间可能存在负相关依赖关系; 显式的时态依赖结构通常不可用，必须使用者进行推断。 于是，很多现有的这种正则化的模型只能考虑很简单的时间依赖关系（比如滞后集L很小，L={1}），和统一的权重（比如不管两个点之间距离是多少，权重统一设置为1） 这导致现有MF方法对大规模时间序列的预测能力较差。 ","date":"2022-05-13 09:33:30","objectID":"/nips16_trmf/:3:1","tags":["papers"],"title":"NIPS16_TRMF","uri":"/nips16_trmf/"},{"categories":["Graduation project"],"content":"学习时间依赖性的挑战 也许有人会想：那我权重参数w让机器自己学不就好了吗？ 在这种假设下，我们有了以下的优化方程： $$ \\min {F, X, \\boldsymbol{w} \\geq \\mathbf{0}} \\sum{(i, t) \\in \\Omega}\\left(Y_{i t}-\\boldsymbol{f}_{i}^{\\top} \\boldsymbol{x}_{t}\\right)^{2}+\\lambda_{f} \\mathcal{R}_{f}(F)+\\frac{\\lambda_{x}}{2} \\sum_{l \\in \\mathcal{L}} \\sum_{t: t-l\u003e0} w_{l}\\left(\\boldsymbol{x}_{t}-\\boldsymbol{x}_{t-l}\\right)^{2}+\\frac{\\lambda_{x} \\eta}{2} \\sum_{t}\\left|\\boldsymbol{x}_{t}\\right|^{2} (4)$$ 我们不难发现，最终的优化结果，是所有的w都是0，意为没有空间依赖关系的时候，目标函数达到最小值。 为了避免让所有的w都是0，有人想到可以给w的和加上一个限制，比如$\\sum_{l \\in \\mathcal{L}} w_{l}=1$ 同样地，我们不难发现，最终的优化结果是$l^{}=\\arg \\min {l \\in \\mathcal{L}} \\sum{t: t\u003el}\\left|\\boldsymbol{x}{t}-\\boldsymbol{x}{t-l}\\right|^{2}$，对应的wl是1，其他的w是0 因此，通过简单地在MF公式中插入正则化器来自动学习权重并不是一个可行的选择。 ","date":"2022-05-13 09:33:30","objectID":"/nips16_trmf/:3:2","tags":["papers"],"title":"NIPS16_TRMF","uri":"/nips16_trmf/"},{"categories":["Graduation project"],"content":"3. TRMF 为了解决前面提到的限制，本文提出了时间正则化矩阵分解(TRMF)框架，这是一种将时间依赖性纳入矩阵分解模型的新方法。 与前面提到的基于图的方法不同，我们建议使用经过充分研究的时间序列模型来明确地描述{$x_t$}之间的时间依赖性。 $$\\boldsymbol{x}{t}=M{\\Theta}\\left(\\left{\\boldsymbol{x}{t-l}: l \\in \\mathcal{L}\\right}\\right)+\\boldsymbol{\\epsilon}{t} (5)$$ $\\boldsymbol{\\epsilon}_{t}$是一个高斯噪声向量 $M_{\\Theta}$是一个时间序列模型，参数是Θ和滞后集L L是一个包含滞后指标L的集合，表示t和t-l时间点之间的相关性 Θ捕捉时间相关性的权重信息(如AR模型中的转移矩阵) 基于此，我们提出了一个新的正则化项$\\mathcal{T}{\\mathrm{M}}(X \\mid \\Theta)$，这可以鼓励模型依照时间序列$M{\\Theta}$。 我们令： $$ \\mathcal{T}{\\mathrm{M}}(X \\mid \\Theta)=-\\log \\mathbb{P}\\left(\\boldsymbol{x}{1}, \\ldots, \\boldsymbol{x}_{T} \\mid \\Theta\\right) (6) $$ 当θ给定的时候，我们令$\\mathcal{T}_{\\mathrm{M}}(X \\mid \\Theta)$为矩阵分解的一个正则化项；当θ未知的时候，我们令θ为另外一部分参数，并且设计$R_θ$以作为另一个正则化项。 $$ \\min {F, X, \\Theta} \\sum{(i, t) \\in \\Omega}\\left(Y_{i t}-\\boldsymbol{f}_{i}^{\\top} \\boldsymbol{x}_{t}\\right)^{2}+\\lambda_{f} \\mathcal{R}_{f}(F)+\\lambda_{x} \\mathcal{T}_{\\mathrm{M}}(X \\mid \\Theta)+\\lambda_{\\theta} \\mathcal{R}_{\\theta}(\\Theta) (7) $$ 通过交替地优化更新F,X,Θ，可以解决上面的优化方程。 ","date":"2022-05-13 09:33:30","objectID":"/nips16_trmf/:4:0","tags":["papers"],"title":"NIPS16_TRMF","uri":"/nips16_trmf/"},{"categories":["Graduation project"],"content":"TRMF中数据驱动的时间依赖性学习 在TRMF中，当F和X是固定的时候，式（7）可以简化为： $$ \\min {\\Theta} \\lambda{x} \\mathcal{T}{M}(X \\mid \\Theta)+\\lambda{\\theta} \\mathcal{R}_{\\theta}(\\Theta) (8)$$ 其中第一项可以看成：$\\min _{\\Theta} -\\log{P}(X_1,…X_T \\mid \\Theta)$，即$\\max _{\\Theta}{P}(X_1,…X_T \\mid \\Theta)$ 也就是说，后一项可以看成最大后验概率 ","date":"2022-05-13 09:33:30","objectID":"/nips16_trmf/:4:1","tags":["papers"],"title":"NIPS16_TRMF","uri":"/nips16_trmf/"},{"categories":["Graduation project"],"content":"TRMF时间序列分析 我们可以看到，TRMF可以无缝地处理在分析具有时间依赖性的数据时经常遇到的各种任务: 时间序列预测 一旦我们有了潜在的嵌入{$x_t:1,…T$}的$M_{\\Theta}$，我们可以预测未来的嵌入{$x_t:t\u003eT$}，然后使用来预测结果 缺失值补全 我们可以使用$\\boldsymbol{f}{i}^{\\top} \\boldsymbol{x}{t}$来对这些缺失的条目进行插补，就像标准矩阵补全，在推荐系统和传感器网络中很有用。 ","date":"2022-05-13 09:33:30","objectID":"/nips16_trmf/:4:2","tags":["papers"],"title":"NIPS16_TRMF","uri":"/nips16_trmf/"},{"categories":["Graduation project"],"content":"4. 一种新的自回归时间正则化算法 在小节3中，我们大致介绍了TRMF的框架：正则项$\\mathcal{T}{M}(X \\mid \\Theta)$（有时间序列模型$M{\\Theta}$确定） 在这一小节中，我们将介绍一种TRMF框架：自回归模型，参数为滞后集L和权重 我们令xt是以下形式: $$ \\boldsymbol{x}{t}=\\sum{l \\in \\mathcal{L}} W^{(l)} \\boldsymbol{x}{t-l}+\\boldsymbol{\\epsilon}{t} $$ $\\boldsymbol{\\epsilon}_{t}$是一个高斯噪声向量。 为了简化，假设$\\boldsymbol{\\epsilon}{t} \\sim \\mathcal{N}\\left(0, \\sigma^{2} I{k}\\right)$ 于是，时间正则化项$\\mathcal{T}_{M}(X \\mid \\Theta)$可以写成： $$ \\mathcal{T}{\\mathrm{AR}}(X \\mid \\mathcal{L}, \\mathcal{W}, \\eta):=\\frac{1}{2} \\sum{t=m}^{T}\\left|\\boldsymbol{x}{t}-\\sum{l \\in \\mathcal{L}} W^{(l)} \\boldsymbol{x}{t-l}\\right|^{2}+\\frac{\\eta}{2} \\sum{t}\\left|\\boldsymbol{x}_{t}\\right|^{2} (9)$$ 其中：$m := 1 + L, L := max(L)$, and $\\eta \u003e 0$ 由于每个$W^{(l)} \\in R^{k*k}$所以我们有$|\\mathcal{L}| k^{2}$个参数要学习，这可能导致过拟合 为了避免过拟合，同时为了生成更可解释的结果，我们人为定义$W^{(l)} \\in R^{k*k}$为对角矩阵，这可以使得参数量减少至$|\\mathcal{L}| k$ 出于简化的考虑，我们使用W来表示这个k×L的矩阵，其中第l列表示$W^{(l)} \\in R^{k*k}$ 的对角线元素 简化后： $$ \\mathcal{T}{\\mathrm{AR}}(\\overline{\\boldsymbol{x}} \\mid \\mathcal{L}, \\overline{\\boldsymbol{w}}, \\eta)=\\frac{1}{2} \\sum{t=m}^{T}\\left(x_{t}-\\sum_{l \\in \\mathcal{L}} w_{l} x_{t-l}\\right)^{2}+\\frac{\\eta}{2}|\\overline{\\boldsymbol{x}}|^{2} (10)$$ $x_t$表示时刻t的向量。 将式10代入式7，得到式12： $$ \\min {F, X, \\mathcal{W}} \\sum{(i, t) \\in \\Omega}\\left(Y_{i t}-\\boldsymbol{f}_{i}^{\\top} \\boldsymbol{x}_{t}\\right)^{2}+\\lambda_{f} \\mathcal{R}_{f}(F)+\\sum_{r=1}^{k} \\lambda_{x} \\mathcal{T}_{\\mathrm{AR}}\\left(\\overline{\\boldsymbol{x}}_{r} \\mid \\mathcal{L}, \\overline{\\boldsymbol{w}}_{r}, \\eta\\right)+\\lambda_{w} \\mathcal{R}_{w}(\\mathcal{W}) (12)$$ 我们将式（12）命名为TRMF-AR. ","date":"2022-05-13 09:33:30","objectID":"/nips16_trmf/:5:0","tags":["papers"],"title":"NIPS16_TRMF","uri":"/nips16_trmf/"},{"categories":["Graduation project"],"content":"不同时间序列之间的关联性 尽管$W^{(l)} \\in R^{k*k}$是对角阵，但是TRMF还是可以建模不同时间序列（X矩阵不同行之间）的关联性。这个关联性在特征矩阵F中体现。 ","date":"2022-05-13 09:33:30","objectID":"/nips16_trmf/:5:1","tags":["papers"],"title":"NIPS16_TRMF","uri":"/nips16_trmf/"},{"categories":["Graduation project"],"content":"滞后集L的选择 TRMF中L的选择更加灵活。因此，TRMF可以提供重要的优势: 首先，因为不需要指定权重参数W，可以选择更大的L来考虑长期依赖性，这也可以产生更准确和稳健的预测。 其次，L中的时延不需要是连续的，这样就可以很容易地嵌入关于周期性或季节性的领域知识。例如，对于具有一年季节性的每周数据，可以考虑L ={1, 2, 3, 51, 52, 53}。 ","date":"2022-05-13 09:33:30","objectID":"/nips16_trmf/:5:2","tags":["papers"],"title":"NIPS16_TRMF","uri":"/nips16_trmf/"},{"categories":["Graduation project"],"content":"参数优化 式10和式12 ","date":"2022-05-13 09:33:30","objectID":"/nips16_trmf/:5:3","tags":["papers"],"title":"NIPS16_TRMF","uri":"/nips16_trmf/"},{"categories":["Graduation project"],"content":"5. 实验结果 ","date":"2022-05-13 09:33:30","objectID":"/nips16_trmf/:6:0","tags":["papers"],"title":"NIPS16_TRMF","uri":"/nips16_trmf/"},{"categories":["Graduation project"],"content":"数据集 对于synthetic数据集，我们先随机生成一个$F \\in R^{16*4}$，，生成{$x_t$}，它满足AR过程，且滞后集L={1,8}。然后Y通过$\\boldsymbol{y}{t}=F \\boldsymbol{x}{t}+\\boldsymbol{\\epsilon}{t}$ 且 $\\boldsymbol{\\epsilon}{t} \\sim \\mathcal{N}(\\mathbf{0}, 0.1 I)$生成 电力和交通数据集从UCI存储库获得，而Walmart -1和Walmart -2是来自Walmart电子商务的两个专有数据集，其中包含每周的销售信息。由于缺货等原因，missing rate分别为55.3%和49.3%。为了评价预测性能，我们考虑了归一化偏差(ND)和归一化均方根(NRMSE)。 ","date":"2022-05-13 09:33:30","objectID":"/nips16_trmf/:6:1","tags":["papers"],"title":"NIPS16_TRMF","uri":"/nips16_trmf/"},{"categories":["Graduation project"],"content":"实验结果 表3：缺失值插补结果：每种方法的ND/NRMSE。请注意，TRMF 在几乎所有情况下都优于所有竞争方法 ","date":"2022-05-13 09:33:30","objectID":"/nips16_trmf/:6:2","tags":["papers"],"title":"NIPS16_TRMF","uri":"/nips16_trmf/"},{"categories":["School courses"],"content":" 哈工大深圳苏敬勇本科生课程人工智能 参考教材：王万森《人工智能原理及其应用》，电子工业出版社， 2018 人工智能 30%+30%+40% ","date":"2022-02-24 14:06:32","objectID":"/ai_base_01/:0:0","tags":["AI"],"title":"AI_base_01","uri":"/ai_base_01/"},{"categories":["School courses"],"content":"第1章 人工智能概述 ","date":"2022-02-24 14:06:32","objectID":"/ai_base_01/:1:0","tags":["AI"],"title":"AI_base_01","uri":"/ai_base_01/"},{"categories":["School courses"],"content":"定义 自然智能 指人类和一些动物所具有的智力和行为能力 人类的自然智能（简称智能） 指人类在认识客观世界中，由思维过程和脑力活动所表现出的综合能力 定义智能的困难 从结构上，人脑有10^11-12 量级的神经元，是广泛分布、并行的、巨复杂系统 从功能上，人脑具有记忆、思维、观察、分析等能力 有待于人脑奥秘的揭示，进一步认识 智能的不同观点： 思维理论：智能来源于思维活动，智能的核心是思维，人的一切知识都是思维的产物。可望通过对思维规律和思维方法的研究，来揭示智能的本质。 知识阈值理论：智能取决于知识的数量及其可运用程度。一个系统所具有的可运用知识越多，其智能就会越高。 进化理论：是美国MIT的Brooks在对人造机器虫研究的基础上提出来的。智能取决于感知和行为，取决于对外界复杂环境的适应，智能不需要知识、不需要表示、不需要推理，智能可由逐步进化来实现。 人脑智能的层次结构 高层智能：以大脑皮层（抑制中枢）为主，主要完成记忆、思维等活动。 思维理论、知识阈值理论 中层智能：以丘脑（或称间脑，感觉中枢）为主，主要完成感知活动。 进化理论 低层智能：以小脑、脊髓为主，主要完成动作反应活动。 进化理论 智能包含的能力：感知能力、记忆和思维能力、学习和自适应能力、行为能力 人工智能的定义： 人工方法实现的智能 目前的“人工智能”一词是指用计算机模拟实现的智能，同时，人工智能又是一个学科名称。 人造的智能机器或系统 模仿、延伸以及拓展人的智能 典型的4种定义方法：（类人、理性（即以人为中心还是以理性为中心））、（思维、行为） 类人行为方法==图灵测试方法 理性行为方法与理性思维方法的关系： 首先，理性行为和理性思维强调的重点不同。理性思维方法强调的是正确思维，而理性行为方法强调的则是理性行动。 其次，理性行为可以依据理性思维进行。例如，对一些能够通过理性思维做出正确结论的事情，实现理性行为的途径往往是先通过逻辑推理得出该行为能达到的目标和结论，然后再付诸实施。 再其次，理性行为不一定要依据理性思维进行。例如，对有些事情，即使理性思维无法证明哪些行为是正确的，而其它行为是错误的，理性行为也必须有所行动。 人工智能的一般解释： **从能力的角度：人工智能是指用人工的方法在机器（计算机）上实现的智能。**是智能机器所执行的通常与人类智能有关的功能，如判断、推理、证明、识别、感知、理解、设计、思考、规划、学习和问题求解等思维活动。 **从学科的角度：人工智能是一门研究如何构造智能机器或智能系统，去模拟、延伸和扩展人类智能的学科。**是计算机科学中涉及研究、设计和应用智能机器的一个分支。 AI的研究意义： 研究人工智能是当前信息化社会的迫切要求。 智能化也是自动化发展的必然趋势。 探索人类自身智能的奥秘，发现自然智能的渊源。 目标： 远期目标：用自动机重现人类的思维过程和智能行为 近期目标：建造智能计算机代替人类的部分智力劳动 AI是一门新兴的学科，是自然科学与社会科学的交叉学科 ","date":"2022-02-24 14:06:32","objectID":"/ai_base_01/:1:1","tags":["AI"],"title":"AI_base_01","uri":"/ai_base_01/"},{"categories":["School courses"],"content":"发展简史 1956前 孕育期 1970前 形成期 1990前 知识应用期 2000前 学派融合期 兴起期 1956~2010：第一代人工智能：知识驱动（符号+知识） 2010-2020：第二代人工智能：数据驱动（大数据+深度学习） 2020-~：第三代人工智能：数据和知识驱动结合 AI四大要素：知识、数据、算法、算力 ","date":"2022-02-24 14:06:32","objectID":"/ai_base_01/:1:2","tags":["AI"],"title":"AI_base_01","uri":"/ai_base_01/"},{"categories":["School courses"],"content":"研究内容 如何获取知识? 如何将获取的知识以计算机内部代码形式加以合理表示? 如何运用知识进行推理，解决实际问题? ","date":"2022-02-24 14:06:32","objectID":"/ai_base_01/:1:3","tags":["AI"],"title":"AI_base_01","uri":"/ai_base_01/"},{"categories":["School courses"],"content":"研究方法与途径 运用计算机科学的方法（逻辑演绎） 符号主义 运用仿生学的方法（网络连接机制） 联结主义 运用进化论的思想（控制论和机器学习算法） 行为主义 ","date":"2022-02-24 14:06:32","objectID":"/ai_base_01/:1:4","tags":["AI"],"title":"AI_base_01","uri":"/ai_base_01/"},{"categories":["School courses"],"content":"研究领域 机器思维 推理 搜索 机器感知 计算机视觉 模式识别 自然语言处理 机器行为 智能控制/制造 机器学习 符号学习 神经学习 计算智能 神经计算 进化计算 模糊计算 分布智能 智能系统 ","date":"2022-02-24 14:06:32","objectID":"/ai_base_01/:1:5","tags":["AI"],"title":"AI_base_01","uri":"/ai_base_01/"},{"categories":["School courses"],"content":"应用 应用：智能机器人、智能网络、智能游戏…… ","date":"2022-02-24 14:06:32","objectID":"/ai_base_01/:1:6","tags":["AI"],"title":"AI_base_01","uri":"/ai_base_01/"},{"categories":["School courses"],"content":"对人类的影响 对经济、社会、文化都有不少积极影响，但也不能忽视其带来的威胁： 心理上的威胁 技术失控的危险 引发发法律问题 …… ","date":"2022-02-24 14:06:32","objectID":"/ai_base_01/:1:7","tags":["AI"],"title":"AI_base_01","uri":"/ai_base_01/"},{"categories":["School courses"],"content":"第2章 知识表示 ","date":"2022-02-24 14:06:32","objectID":"/ai_base_01/:2:0","tags":["AI"],"title":"AI_base_01","uri":"/ai_base_01/"},{"categories":["School courses"],"content":"知识与知识表示基本概念 知识与知识表示： 知识是人类智能的基础（符号主义学派）。 智能活动过程主要是一个获取知识并运用知识的过程。 人工智能问题的求解也是以知识为基础的，知识的获取、知识的表示和运用知识进行推理是人工智能学科研究的三个主要问题 知识点含义与结构： 费根鲍姆 知识是经过裁剪、塑造、解释和转换的信息。 Bernstein 知识是由特定领域的描述、关系和过程组成的。 Hayes-roth 知识=事实+信念+启发式 知识的层次结构：噪声-\u003e数据-\u003e信息-\u003e知识-\u003e元知识 知识、信息、数据： 数据 是记录信息的符号，是信息的载体和表示。 信息 是对数据的解释，是数据在具体的场合下具体的含义 一般把有关信息关联在一起所形成的信息结构称为知识。知识、信息、数据是3个层次的概念 知识的特性： 相对正确性 不确定性 可表示性 可利用性 知识表示： 面向计算机的知识描述或表达的形式和方法。 知识表示的过程就是把知识编码成某种数据结构的过程 知识表示的要求： 表示能力 可利用性 可组织性 可维护性 可实现性 自然性 可理解性 ","date":"2022-02-24 14:06:32","objectID":"/ai_base_01/:2:1","tags":["AI"],"title":"AI_base_01","uri":"/ai_base_01/"},{"categories":["School courses"],"content":"一阶谓词逻辑表示法 一阶谓词逻辑表示法是一种重要的知识表示方法，它以数理逻辑为基础，是到目前为止 能够表达人类思维活动规律的一种最精确的形式语言。 个体域： 个体变元的变化范围称为个体域/论述域 包揽一切事物的集合称为全总个体域 谓词逻辑中的n元谓词：$P(x_1,x_2…,x_n)$ P：谓词符号（大写字母） $x_n$：参量/项/个体 为了表达个体之间的对应关系，引入n元个体函数，简称函数：$f(x_1,x_2…,x_n)$ 函数符号（小写字母） 个体变元 量词： 全称量词 存在量词 谓词联结符号优先级：非\u003e与\u003e或\u003e蕴含（IF……THEN）\u003e等价 辖域： 紧接于量词之后被量词作用（即说明）的谓词公式称为该量词的辖域。 变元： 指导变元：量词后面的变元称为量词的指导变元； 约束变元：在一个量词的辖域中的与该量词的指导变元相同的变元称为约束变元； 自由变元：其它的变元称为自由变元； 改名规则： 一个变元在一个谓词公式中既可约束出现，又可自由出现，为了避免混淆，通常通过改名规则，使得一个谓词公式中一个变元仅以一种形式出现。 换名规则 在谓词公式中，将某量词辖域中出现的某个约束变元以及对应的指导变元更改为本辖域中没有出现过的个体变元符号，公式其它部分不变，谓词公式的等价性不变。 代替规则 在谓词公式中，将某量词辖域中出现的某个自由变元的所有出现用本辖域中未曾出现过的某个个体变元符号代替，谓词公式的等价性不变 谓词公式表示知识的步骤： 定义谓词及个体，确定每个谓词及个体的确切含义； 根据所要表达的事物或概念，为每个谓词中的变元赋以特定的值； 根据所要表达的知识的语义，用适当的联接符号将各个谓词联接起来，形成谓词公式 谓词逻辑表示法的特点： 优点 ⑴ 严密性。可以保证其演绎推理结果的正确性，可以较精确的表达知识。 ⑵ 自然性。谓词逻辑是一种接近于自然语言的形式语言。 ⑶ 通用性。拥有通用的逻辑演算方法和推理的规则。 ⑷ 易于实现。用它表示的知识易于模块化，便于知识的增删及修改，便于在计算机上实现 局限性 (1)知识表示能力差。不便于表达和加入非确定性、启发性知识等。 (2)组合爆炸。在其推理过程中，随着事实数目的增大及盲目的使用推例规则，有可能形成组合爆炸。 (3)效率低。由于推理是根据形式逻辑进行的，把推理演算与知识含义截然分开，抛弃了表达内容中所含有的语义信息，往往使推理过程太冗长，降低了系统的效率 ","date":"2022-02-24 14:06:32","objectID":"/ai_base_01/:2:2","tags":["AI"],"title":"AI_base_01","uri":"/ai_base_01/"},{"categories":["School courses"],"content":"产生式规则表示法 产生式表示法： 1943年 Post 首先在一种计算形式体系中提出。 形式上很简单，但在一定意义上模仿了人类思考的过程。 60年代开始，成为专家系统基本的知识表示方法 适合表示事实性知识和规则性知识 容易描述事实、规则以及它们的不确定性度 事实的概念 事实是断言一个语言变量的值或断言多个语言变量之间关系的陈述句。 语言变量的值: 例如，“雪是白的” 语言变量之间的关系: 例如，“王峰热爱祖国“ 事实的表示 确定性知识: (对象，属性，值) ， 例如， (snow, color, white) 或(雪，颜色，白)。其中，对象就是语言变量。(关系，对象1，对象2) ， 例如， (love, Wang Feng， country) 或 (热爱，王峰，祖国) 非确定性知识: (对象，属性，值，可信度因子)其中，“可信度因子\"是指该事实为真的相信程度。可用[0，1]之间的一个实数来表示 规则的表示： 规则的产生式表示形式常称为产生式规则，简称产生式或规则。 产生式的基本形式： P-\u003eQ或者IF P THEN Q。其中，P是产生式的前提，也称为前件，它给出了该产生式可否使用的先决条件。Q是一组结论或操作，也称为后件，它指出当P满足时，应该推出的结论或应该执行的动作 产生式系统的基本结构： 综合数据库 存放推理过程的各种当前信息。如：问题的初始状态、输入的事实、中间结论及最终结论 作为推理过程选择可用规则的依据。推理过程中某条规则是否可用，是通过该规则的前提与DB中的已知事实的匹配来确定的。可匹配的规则称为可用规则。利用可用规则进行推理，将会得到一个结论。该结论若不是目标，将作为新的事实放入DB，成为以后推理的已知事实 规则库RB(Rule Base)也称知识库KB(Knowledge Base) (1) 作用：用于存放推理所需要的所有规则，是整个产生式系统的知识集。是产生式系统能够进行推理的根本。 (2) 要求：知识的完整性、一致性、准确性、灵活性和可组织性 控制系统(Control system) 控制系统的主要作用 亦称推理机，用于控制整个产生式系统的运行，决定问题求解过程的推理线路。 控制系统的主要任务 选择匹配：按一定策略从规则库中选择规则与综合数据库中的已知事实进行匹配。匹配是指把所选规则的前提与综合数据库中的已知事实进行比较，若综合数据库中的事实与所选规则前提一致，则称匹配成功，该规则为可用；否则，称匹配失败，该规则不可用。 冲突消解：对匹配成功的规则，按照某种策略从中选出一条规则执行。 执行操作：对所执行的规则，若其后件为一个或多个结论，则把这些结论加入综合 数据库；若其后件为一个或多个操作时，执行这些操作。 终止推理：检查综合数据库中是否包含有目标，若有，则停止推理。 路径解释：在问题求解过程中，记住应用过的规则序列，以便最终能够给出问题的解的路径 规则库\u003c-\u003e控制系统\u003c-\u003e综合数据库 产生式系统的运行过程： 外部输入的初始事实放入综合数据库 从规则库中取一个条规则，将其前提同当前动态数据库中的事实/数据进行模式匹配 判断是否成功 不成功则回到2 成功则执行4 把该规则的结论放入当前动态数据库，或执行规则所规定的动作 产生式系统应用举例： 例1:设有以下两条规则 r 1: IF 动物有羽毛THEN 动物是鸟 r 2: IF 动物是鸟AND 动物善飞THEN 动物是信天翁 其中， r 1和r 2是上述两条规则在动物识别系统中的规则编号。 假设已知有以下事实: 动物有羽毛，动物善飞 求满足以上事实的动物是何种动物。 解: 由于已知事实\"动物有羽毛”，即r 1的前提条件满足，因此r 1可用，承认的r 1结论， 即推出新的事实\"动物是鸟”。此时， r 2的两个前提条件均满足，即 r 2的前提条件满 足，因此r 2可用，承认的r 2结论，即推出新的事实“动物是信天翁”。 求解： (1) 综合数据库： (M, B, Box, On, H) M: 猴子的位置 B： 香蕉的位置 Box: 箱子的位置 On=0: 猴子在地板上 On=1: 猴子在箱子上 H=0: 猴子没有抓到香蕉 H=1: 猴子抓到了香蕉 (2) 初始状态： (a, c, b, 0, 0) (3) 结束状态： （c, c, c, 1, 1）。 (4) 规则集： r1: IF (x, y, z, 0, 0) THEN (w, y, z, 0, 0)（猴子移动） r2: IF (x, y, x, 0, 0) THEN (z, y, z, 0, 0)（猴子推箱子） r3: IF (x, y, x, 0, 0) THEN (x, y, x, 1, 0)（猴子爬箱子） r4: IF (x, y, x, 1, 0) THEN (x, y, x, 0, 0)（猴子下箱子） r5: IF (x, x, x, 1, 0) THEN (x, x, x, 1, 1)（猴子抓香蕉） 其中， x, y, z, w 为变量。 解答： 根据具体问题可将规则具体为： r1: IF (a, c, b, 0, 0) THEN (b, c, b, 0, 0) r2: IF (b, c, b, 0, 0) THEN (c, c, c, 0, 0) r3: IF (b, c, b, 0, 0) THEN (b, c, b, 1, 0) r3: IF (c ,c, c, 0, 0) THEN (c, c, c, 1, 0) r4：IF (b, c, b, 1, 0) THEN (b, c, b, 0, 0) r5: IF (c, c, c, 1, 0) THEN (c, c, c, 1, 1) 在已知事实下，r1àr2àr3àr5,可得到香蕉 例2：传教士与野人问题。N个传教士，N个野人，一条船，可同时乘坐k个人，要求在任何时刻，在河的两岸，传教士和野人同时存在时，传教士的人数不能少于野人的人数。问：如何过河？(以N=3，k=2为例求解。) (1) 综合数据库： (m, c, b) 其中，0≤m≤3，0≤c≤3，b∈{ 0, 1 } (2) 初始状态： (3, 3, 1) (3) 目标状态： (0, 0, 0) (4) 规则集 r1: IF (m, c, 1) THEN (m-1, c, 0) r2: IF (m, c, 1) THEN (m, c-1, 0) r3: IF (m, c, 1) THEN (m-1, c-1, 0) r4: IF (m, c, 1) THEN (m-2, c, 0) r5: IF (m, c, 1) THEN (m, c-2, 0) r6: IF (m, c, 0) THEN (m+1, c, 1) r7: IF (m, c, 0) THEN (m, c+1, 1) r8: IF (m, c, 0) THEN (m+1, c+1, 1) r9: IF (m, c, 0) THEN (m+2, c, 1) r10: IF (m, c, 0) THEN (m, c+2, 1) 解答： 根据具体问题可将规则具体为： r5: IF (3, 3, 1) THEN (3, 1, 0) （船运2个c到右岸） r7: IF (3, 1, 0) THEN (3, 2, 1) （船运1个c到左岸） r5: IF (3 ,2, 1) THEN (3, 0, 0) （船运2个c到右岸） r7: IF (3, 0, 0) THEN (3, 1, 1) （船运1个c到左岸） r4: IF (3, 1, 1) THEN (1, 1, 0) （船运2个m到右岸） r8: IF (1, 1, 0) THEN (2, 2, 1) （船运1个m和1个c到左岸） r4: IF (2, 2, 1) THEN (0, 2, 0) （船运2个m到右岸） r7: IF (0, 2, 0) THEN (0, 3, 1) （船运1个c到左岸） r5: IF (0, 3, 1) THEN (0, 1, 0) （船运2个c到右岸） r7: IF (0, 1, 0) THEN (0, 2, 1) （船运1个c到左岸） r5: IF (0, 2, 1) THEN (0, 0, 0) （船运2个c到右岸） 在已知事实下，r5àr7àr5àr7àr4àr8àr4àr7àr5àr7àr5 ,可顺利过河 产生式系统的特点： 主要优点 自然性: 采用\"如果……，则……“的形式，人类的判断性知识基本一致。 模块性: 规则是规则库中最基本的知识单元，各规则之间只能通过综合数据库发生联系，而不能相互调用，从而增加了规则的模块性。 有效性: 产生式知识表示法既可以表示确定性知识，又可以表示不确定性知识。 主要缺点 效率较低: 各规则之间的联系必须以综合数据库为媒介。并且，其求解过程是一种反复进行的\"匹配一冲突消解一执行\"过程。这样的执行方式将导致执行的低效率。 不便于表示结构性知识: 由于产生式表示中的知识具有一致格式，且规则之间不能相互调用，因此那种具有结构关系或层次关系的知识则很难以自然的方式来表示 ","date":"2022-02-24 14:06:32","objectID":"/ai_base_01/:2:3","tags":["AI"],"title":"AI_base_01","uri":"/ai_base_01/"},{"categories":["School courses"],"content":"语义网络表示法 逻辑和产生式常用于表示有关领域中各个不同状态间的关系。 语义网络和产生式、一阶谓词逻辑有相对应的表示能力。 语义网络： 通过概念及语义关系来表示知识的一种网络图，它是一个带标注的有向图。 图中的各个节点表示各种概念、事物、对象、行为、状态等； 图中的有向弧表示节点间的联系或关系 一般由一些最基本的语义单元组成。这些最基本的语义单元被称为语义基元。可用如下三元组来表示：(节点1，弧，节点2)也可用有向图表示（有向弧） 把多个基本网元用相应的语义联系关联在一起时，就可得到一个语义网络。 语义网络中的节点还可以是一个语义子网络，所以，语义网络实质上是一种多层次的嵌套结构 基本语义关系： 实例关系ISA 体现的是“具体与抽象”的概念，含义为“是一个”，表示一个事物是另一个事物的一个实例。 分类关系AKO 亦称泛化关系，体现的是\"子类与超类\"的概念，含义为\"是一种”，表示一个事物是另一个事物的一种类型 成员关系: A-Member-of 体现的是\"个体与集体\"的关系，含义为\"是一员”，表示一个事物是另一个事物的一个成员。 属性具有继承性 聚类关系： 亦称包含关系。指具有组织或结构特征的\"部分与整体\"之间的关系。常用的包含关系是：Part-of: 含义为\"是一部分”，表示一个事物是另一个事物的一部分。 聚类关系与实例、分类、成员关系的主要区别聚类关系一般不具备属性的继承性。 属性关系 指事物和其属性之间的关系。常用的有:Have: 含义为\"有” ， 表示一个结点具有另一个结点所描述的属性Can: 含义为\"能”、“会”，表示一个结点能做另一个结点的事情 时间关系 指不同事件在其发生时间方面的先后次序关系。常用的时间关系有:Before: 含义为\"在前\"After: 含义为\"在后” 位置关系 指不同事物在位置方面的关系.常用的有: Located-on: 含义为\"在…上面” Located-under : 含义为\"在. . .下面” Located-at: 含义为\"在…” Located-inside : 含义为\"在. . .内” Located-outside : 含义为\"在…外” 相近关系 指不同事物在形状、内容等方面相似或接近。 常用的相近关系有: Similar-to: 含义为\"相似” Near-to: 含义为\"接近” 事物和概念的表示： 表示一元关系 一元关系 指可以用一元谓词P(x)表示的关系。谓词P说明实体的性质、属性等。描述的是一些最简单、最直观的事物或概念，常用:“是”、“有”、“会”、“能\"等语义关系来说明。如，“雪是白的”。 一元关系的描述 应该说，语义网络表示的是二元关系。如何用它来描述一元关系?结点1表示实体，结点2表示实体的性质或属性等， 弧表示语义关系 表示二元关系 二元关系 指可用二元谓词P(x，y)表示的关系。其中， x，y为实体， P为实体之间的关系。 二元关系的表示 单个二元关系可直接用一个基本网元来表示。复杂关系，可通过一些相对独立的二元或一元关系的组合来实现。 表示多元关系 多元关系 可用多元谓词P(x1 , x2, ……)表示的关系。其中， x1 , x2, ……为实体，谓词P说明这些实体之间的关系。 多元关系的表示 用语义网络表示多元关系时，可把它转化为一个或多个二元关系的组合，把这种多元关系表示出来。 情况和动作的表示： 情况的表示： 西蒙提出了增加情况和动作结点的描述方法。 例: 用语义网络表示:“小燕子这只燕子从春天到秋天占有一个巢” 事件和动作的表示 用这种方法表示事件或动作时，需要设立一个事件节点或动作结点。其中，事件节点由一些向外引出的弧来指出事件行为及发出者与接受者。动作结点由一些向外引出的孤来指出动作的主体与客体。 基于语义网络的推理： 语义网络的推理过程主要有两种，一种是继承，另一种是匹配。 继承的概念 是指把对事物的描述从抽象结点传递到实例结点。通过继承可以得到所需结点的一些属性值，它通常是沿着ISA、AKO等继承弧进行的。 继承的一般过程 (1) 建立一个结点表，用来存放待求解结点和所有以ISA 、AKO等继承弧与此结点相连的那些结点。初始情况下，表中只有待求解结点。 (2) 检查表中的第一个结点是否有继承弧。如果有，就把该弧所指的所有结点放入结点表的末尾，记录这些结点的所有属性，并从结点表中删除第一个结点。如果没有继承孤，仅从结点表中删除第一个结点。 (3) 重复(2) ，直到结点表为空。此时，记录下来的所有属性都是待求解结点继承来的属性。 匹配的概念 是指在知识库的语义网络中寻找与待求解问题相符的语义网络模式。 匹配的过程 (1) 根据待求解问题的要求构造一个网络片断，该网络片断中有些结点或孤的标识是空的，称为询问处，它反映的是待求解的问题。 (2) 根据该语义片断到知识库中去寻找所需要的信息。 (3) 当待求解问题的网络片断与知识库中的某语义网络片断相匹配时，则与询问处相匹配的事实就是问题的解 匹配推理示例： 设在语义网络系统的知识库中存在以下事实的语义网络：哈尔滨工业大学是一所学校，位于哈尔滨市，成立于1920年。假若要求解的问题是：哈尔滨工业大学位于哪个城市？如何利用语义网络进行推理求解？ 语义网络表示法的特点： 主要优点: 结构性: 采用把事物的属性以及事物间的各种语义联系显式地表示出来，是一种结构化的知识表示方法。 联想性: 本来是作为人类联想记忆模型提出来的，它着重强调事物间的语义联系，体现了人类的联想思维过程。 自索引性: 把各接点之间的联系以明确、简洁的方式表示出来，通过与某一结点连结的弧可以很容易的找出与该结点有关的信息，而不必查找整个知识库。这种自索引能力有效的避免搜索时所遇到的组合爆炸问题。 主要缺点 非严格性: 没有象谓词那样严格的形式表示体系，一个给定语义网络的含义完全依赖于处理程序对它所进行的解释，通过语义网络所实现的推理不能保证其正确性。 复杂性: 语义网络表示知识的手段是多种多样的，这虽然对其表示带来了灵活性，但同时也由于表示形式的不一致，使得它的处理增加了复杂性 ","date":"2022-02-24 14:06:32","objectID":"/ai_base_01/:2:4","tags":["AI"],"title":"AI_base_01","uri":"/ai_base_01/"},{"categories":["School courses"],"content":"框架表示法 框架表示法概述： 1975年，Minsky提出了框架理论。他根据人们在理解情景、故事时提出的心理学模型，认为人的知识以框架结构存在人脑中。 认为人们对现实世界中各种事物的认识都是以一种类似于框架的结构存储在记忆中的，当遇到一个新事物时，就从记忆中找出一个合适的框架，并根据新的情况对其细节加以修改、补充，从而形成对这个新事物的认识。例如，对饭店、教室等的认识。 框架: 是人们认识事物的一种通用的数据结构形式。即当新情况发生时，人们只要把新的数据加入到该通用数据结构中，便可形成一个具体的实体(类)，这样的通用数据结构就称为框架。 实例框架: 对于一个框架，当人们才把观察或认识到的具体细节填入后，就得到了该框架的一个具体实例，框架的这种具体实例被称为实例框架。 框架系统: 在框架理论中，框架是知识的基本单位，把一组有关的框架连结起来使可形成一个框架系统。 框架系统推理: 由框架之间的协调来完成。 框架组成： 一个“框架”由若干个“槽”组成，每个“槽”又划分为若干个“侧面”。一个“槽”描述对象的一个方面属性；一个“侧面”描述相应属性的一个方面。由框架名、槽名、侧面、值组成 例：一个直接描述硕士生有关情况的框架 Frame \u003cMASTER\u003e Name: Unit (Last-name, First-name) Sex: Area (male, female) Default: male Age: Unit (Years) Major: Unit (Major) Field: Unit (Field) Advisor: Unit (Last-name, First-name) Project: Area (National, Provincial, Other) Default: National Paper: Area (SCI, EI, Core, General) Default: Core Address: \u003c S-Address\u003e Telephone: Home Unit (Number) Mobile Unit (Number) 对那些结构比较复杂的知识，往往需要用多个相互联系的框架来表示。例如，对前面硕士生框架“MASTER\"可分为:“Student\"框架，描述所有学生的共性，上层框架。“Master\"框架，描述硕士生的个性，子框架，继承\"Student\"框架的属性 学生框架 Frame \u003cStudent\u003e Name: Unit (Last-name, First-name) Sex: Area (male, female) Default: male //缺省 Age: Unit (Years) If-Needed: Ask-Age //询问赋值 Address: \u003c S-Address\u003e Telephone: Home Unit (Number) Mobile Unit (Number) If-Needed: Ask-Telephone //询问赋值 硕士生框架 Frame \u003cMaster\u003e AKO: \u003cStudent\u003e //预定义槽名 Major: Unit (Major) //专业 If-Needed: Ask - Major //询问赋值 If-Added: Check-Major //后继处理 Field: Unit (Field) //方向 If-Needed : Ask - Field //询问赋值 Advisor: Unit (Last-name, First-name) //导师 If-Needed : Ask -Visor //询问赋值 Project: Area (National, Provincial, Other) //项目 Default: National //缺省 Paper: Area (SCI, EI, Core, General) //论文 Default: Core //缺省 这里，用到了一个系统预定义槽名AKO ，其含义为\"是一种”。 当AKO作为下层框架的糟名时，其槽值为上层框架的框架名，表示该下层框架所描述的事物比其上层框架更具体。并且，由AKO所联系的框架之间具有属性的继承关系 实例框架： 例如，有杨叶和柳青2个硕士生， 杨叶，女，计算机专业，参加了导师林海的网络智能研究方向的省部级项目； 柳青，22岁，计算机专业，导师是林海，论文被EI收录。 硕士生-1框架: Frame \u003cMaster-1\u003e ISA: \u003cMaster\u003e //是一个 Name: Yang, Ye Sex: female Major: Computer Field: Web-Intelligence //方向Web智能 Advisor: Lin Hai //导师林海 Project: Provincial //项目省部级 硕士生-2框架: Frame \u003cMaster-2\u003e ISA: \u003cMaster\u003e Name: Liu, Qing Age: 22 Major: Computer Advisor: Lin Hai Paper: EI //论文EI收录 其中用到了系统预定以槽名ISA，即Master-1和Master-2是2个具体的Master 框架系统： 基本结构 框架系统由框架之间的横向或纵向联系构成。 纵向联系 是指那种具有继承关系的上下层框架之间的联系。如:学生可按照接受教育的层次分为本、硕和博。每类学生又可按照所学专业的不同划分。纵向联系通过预定义槽名AKO和ISA等来实现。 横向联系 是指那种以另外一个框架名作为一个槽的槽值或侧面值所建立起来的框架之间的联系。如Student框架与S-Address框架之间就是一种横向联系。 特性继承 特性继承过程 通过ISA 、AKO链来实现。 当需要查询某一事物的某个属性，且描述该事物的框架未提供其属性值时，系统就沿ISA和AKO链追溯到具有相同槽的类或超类框架。 如果该槽提供有Default侧面值，就继承该默认值作为查询结果返回。 如果该槽提供有If-Needed侧面供继承，则执行If-Needed操作，去产生一个值作为查询结果。 如果对某个事物的某一属性进行了赋值或修改操作，则系统会自动沿ISA和AKO链追溯到具有相应的类或超类框架，去执行If-Added操作，作相应的后继处理。 If-Needed与If-Added过程的区别 它们的主要区别在于激活时机和操作目的不同。 If-Needed操作是在系统试图查询某个事物框架中未记载的属性值时激活，并根据查询需求，被动地即时产生所需要的属性值; If-Added操作是在系统对某个框架的属性作赋值或修改工作后激活，目的在于通过这些后继处理，主动做好配套操作，以消除可能存在的不一致。 特性继承的例子 如前面的学生框架 若要查询Master-l 的Sex , 则可直接回答; 但要查询Master-2的Sex , 则需要沿ISA链和AKO链到Student框架取其默认佳male. 若要查询Master-2的Field，需要沿ISA链到Master框架，执行Field槽If-Needed侧面的Ask-Field操作, 即时产生一个值,假设产生的值是Data-Mining, 则表示Master-2的研究方向为数据挖掘。 如果要修改Master-2 的Major，需要沿ISA链到Master框架, 执行Major槽If-Added侧面的Check-Major操作，对Field, Advisor进行修改, 以保持知识的一致性 匹配和填槽 框架的匹配实际上是通过对相应槽的槽名和槽值逐个进行比较，并利用继承关系来实现的。 例如，假设前面讨论的学生框架系统已建立在知识库中，若要求从知识库中找出一个满足如下条件的硕士生:male, Age\u003c25 , Major为Computer ， Project为National 把这些条件用框架表示出来，就可得到如下的初始问题框架 Frame \u003cMaster-x\u003e Name: Sex: male Age: Years \u003c25 Major: Computer Project: National 用此框架和知识库中的框架匹配，显然“Master -2”框架可以匹配。因为Age、Major槽都符合要求, Sex 槽和Project槽虽然没有给出，但由继承性可知它们分别取默认值male和National, 完全符合初始问题框架Master-x的要求，所以要找的学生有可能是Liu Qing。 例子：请用框架表示这一知识：范伟，男，30岁, 1996年10月到2012年8月间在计算机学院任讲师。 Frame：〈Teacher-1〉 Name: Fan，Wei Sex: Male Age: 30 Job： Lecturer Work-time: Start: 1996-10 End: 2012-08 Department： Computer Science 框架表示法的特征： 主要优点: 结构性:最突出特点是善于表示结构性知识，它能够把知识的内部结构关系以及知识问的特殊联系表示出来。 深层性:框架表示法不仅可以从多个方面、多重属性表示知识，而且还可以通过ISA 、AKO等槽以嵌套结构分层地对知识进行表示，因此能用来表达事物间复杂的深层联系。 继承性:在框架系统中，下层框架可以继承上层框架的槽值，也可以进行补充和修改，这样既减少知识冗余，又较好地保证了知识的一致性。 自然性:框架能把与某个实体或实体集相关特性都集中在一起，从而高度模拟了人脑对实体多方面、多层次的存储结构，直观，自然，易于理解。 主要缺点 缺乏框架的形式理论:","date":"2022-02-24 14:06:32","objectID":"/ai_base_01/:2:5","tags":["AI"],"title":"AI_base_01","uri":"/ai_base_01/"},{"categories":["School courses"],"content":"第3章 确定性推理 人工智能学科： 知识获取 知识表示 知识推理 确定性推理 不确定性推理 ","date":"2022-02-24 14:06:32","objectID":"/ai_base_01/:3:0","tags":["AI"],"title":"AI_base_01","uri":"/ai_base_01/"},{"categories":["School courses"],"content":"概述 推理就是按照某种策略从已有事实和知识推出结论的过程。 推理分类： 按推理的逻辑基础分类： 演绎推理 从已知的一般性知识出发，推理出适合于某种个别情况的结论过程。即从一般到个别的推理。常用形式：三段论法(大前提、小前提、结论) 大前提：是已知的一般性知识或推理过程得到的判断； 小前提：是关于某种具体情况或某个具体实例的判断； 结论：是由大前提推出的，并且适合于小前提的判断。 归纳推理 从大量特殊事例出发，归纳出一般性结论的推理过程。即从个别到一般的推理过程。 完全归纳推理 不完全归纳推理 枚举归纳推理 类比归纳推理 按所用知识的确定性分类 确定性推理 不确定性推理 推理时所用的知识和证据不都是确定的，推出的结论也不确定的 按推理中所用知识是否具有启发性分类 启发式推理 推理过程中应用与问题有关的启发性知识，即解决问题的的策略、技巧及经验，以加快推理过程，提高搜索效率。 非启发式推理 在推理过程中，不运用启发性知识，只按照一般的控制逻辑进行推理。这种方法缺乏对求解问题的针对性，所以推理效率较低，容易出现“组合爆炸”问题 推理过程不仅依赖于所用的推理方法，同时也依赖于推理的控制策略。 推理的控制策略是指如何使用领域知识使推理过程尽快达到目标的策略 控制策略的分类： 推理策略 推理方向控制策略 用于确定推理的控制方向，可分为正向推理、逆向推理、混合推理及双向推理。 求解策略 是指仅求一个解，还是求所有解或最优解等。 限制策略 是指对推理的深度、宽度、时间、空间等进行的限制。 冲突消解策略 是指当推理过程有多条知识可用时，如何从这多条可用知识中选出一条最佳知识用于推理的策略。 搜索策略 主要解决推理线路、推理效果、推理效率等问题。本章主要讨论推理策略，至于搜索策略见第四章。 ","date":"2022-02-24 14:06:32","objectID":"/ai_base_01/:3:1","tags":["AI"],"title":"AI_base_01","uri":"/ai_base_01/"},{"categories":["School courses"],"content":"产生式系统 基本结构 综合数据库： 存放推理过程的各种当前信息 作为推理过程选择可用规则的依据 存放推理所需所有规则的规则库RB(Rule Base)也称知识库KB(Knowledge Base) 控制系统： 控制系统的主要作用 亦称推理机，用于控制整个产生式系统的运行，决定问题求解过程的推理线路。 控制系统的主要任务 选择匹配：按一定策略从规则库种选择规则与综合数据库中的已知事实进行匹配。匹配是指把所选规则的前提与综合数据库中的已知事实进行比较，若事实库中存的事实与所选规则前提一致，则称匹配成功，该规则为可用；否则，称匹配失败，该规则不可用。 冲突消解：对匹配成功的规则，按照某种策略从中选出一条规则执行。 执行操作：对所执行的规则，若其后件为一个或多个结论，则把这些结论加入综合数据库；若其后件为一个或多个操作时，执行这些操作。 终止推理：检查综合数据库中是否包含有目标，若有，则停止推理。 路径解释：在问题求解过程中，记住应用过的规则序列，以便最终能够给出问题的解的路径 推理过程 正向推理： 从已知事实出发、正向使用规则，亦称为数据驱动推理或前向链推理 逆向推理： 从某个假设目标出发，逆向使用规则，亦称为目标驱动推理或逆向链推理 产生式系统的示例：略 推理过程的相关说明： 正向推理的特性 正向推理的主要优点是比较直观，主要缺点是推理无明确的目标，求解问题时可能会执行许多与解无关的操作，导致推理效率较低。 逆向推理的特性 逆向推理的主要优点是不必寻找和使用那些与假设目标无关的信息和规则，推理过程的目标明确，主要缺点是当用户对解的情况认识不清时，由系统自主选择假设目标的盲目性比较大，若选择不好，会影响系统效率。 双向推理方法 为互相取长补短，可以把正向和逆向结合起来使用，采用双向推理的方式。双向推理有多种不同的实现方法，可以采用先正向后逆向，也可以采用先逆向后正向，还可以采用随机选择正向和逆向的推理方法。 推理过程的不唯一性 从前面的推理算法可以看出，无论是正向推理还是逆向推理，当可用规则集中有多条规则可用时，不同的冲突消解策略将导致不同的规则使用顺序， 因此其推理过程是不唯一的。 ","date":"2022-02-24 14:06:32","objectID":"/ai_base_01/:3:2","tags":["AI"],"title":"AI_base_01","uri":"/ai_base_01/"},{"categories":["School courses"],"content":"自然演绎推理 一阶谓词逻辑基础： 谓词公式的永真性：如果谓词公式P对非空个体域D上的任一解释都取得真值T，则称P在D上是永真的；如果P在任何非空个体域上都是永真的，则称P是永真。 谓词公式的可满足性：对于谓词公式P，如果至少存在D上的一个解释，使公式P在此解释下的真值为T，则称公式P在D上是可满足的。 谓词公式的永假性：如果谓词公式P对非空个体域D上的任一解释都取真值F，则称P在D上是永假的；如果P在任何非空个体域上均是永假的，则称P永假。 谓词公式的等价性：设P与Q是D上的两个谓词公式，若对D上的任意解释，P与Q都有相同的真值，则称P与Q在D上是等价的。如果D是任意非空个体域，则称P与Q是等价的，记作P ⇔ Q。 永真蕴含式：对谓词公式P和Q，如果P→Q永真，则称P 永真蕴含Q，且称Q为P 的逻辑结论，P为Q的前提，记作P ⇒Q。 常用的等价式： 常用的永真蕴含式： 置换和合一： 在不同谓词公式中，往往会出现谓词名相同但其个体不同的情况，此时推理过程是不能直接进行匹配的，需要先进行置换 要使用假言推理，首先需要找到项a对变元x的置换，使W(a)与W (x)一致。这种寻找项对变元的置换，使谓词一致的过程叫做合一的过程。合一可理解为是寻找相对应变量的置换，使两个或多个谓词公式一致。 例如，{a/x, c/y, f(b)/z} 是一个置换。但{g(z)/x, f(x)/z}不是一个置换。原因是它在x与z之间出现了循环置换现象。即当用g(z)置换x, 用f(x)置换z时，既没有消去x，也没有消去z。若改为{g(a)/x, f(x)/z}即可，原因是用g(a)置换x ，用f(x)置换z ，若再用一次置换，用g(a)置换x, 最终原x和z被g(a)和f(g(a))置换,则经过有限次置换消去了x和z 。通常，置换是用希腊字母θ、σ、α、λ等来表示的 设θ={t1 /x1, t2 /x2 ,…, tn /xn }是一个置换，F是一个谓词公式，把公式F中出现的所有xi 换成ti (i=1, 2, …, n)，得到一个新的公式G，称G为F在置换θ 下的例示，记作$G=F_θ$ 合一： 最一般合一： 设σ是谓词公式集F 的一个合一置换，如果对F的任意一个合一置换θ都存在一个置换λ，使得 θ= σ· λ，则称σ是一个最一般(或最简单)合一(most general unifier，简记为mgu)置换 一个公式集的最一般合一是唯一的 最一般合一置换的求取算法：略 ","date":"2022-02-24 14:06:32","objectID":"/ai_base_01/:3:3","tags":["AI"],"title":"AI_base_01","uri":"/ai_base_01/"},{"categories":["School courses"],"content":"归结演绎推理 谓词公式的范式 子句集及其应用 鲁滨逊归结原理 归结演绎推理的方法 归结演绎推理的归结策略 用归结反演求取问题的答案 ","date":"2022-02-24 14:06:32","objectID":"/ai_base_01/:3:4","tags":["AI"],"title":"AI_base_01","uri":"/ai_base_01/"},{"categories":["School courses"],"content":"第4章 搜索策略 搜索策略： 搜索基本概念 搜索的分类 按是否使用启发式信息： 盲目搜索：按预定的控制策略进行搜索，在搜索过程中获得的中间信息并不改变控制策略。 启发式搜索：在搜索中加入了与问题有关的启发性信息，用于指导搜索朝着最有希望的方向前进，加速问题的求解过程并找到最优解。 按问题的表示方式： 状态空间搜索：用状态空间法来求解问题所进行的搜索 与或树搜索：用问题归约法来求解问题时所进行的搜索 状态空间搜索 盲目搜索 深度优先搜索 广度优先搜索 代价一致搜索 启发式搜索 A算法 A*算法 与或图搜索 博弈树搜索 极大极小分析法 a - b 剪枝技术 ","date":"2022-02-24 14:06:32","objectID":"/ai_base_01/:4:0","tags":["AI"],"title":"AI_base_01","uri":"/ai_base_01/"},{"categories":["School courses"],"content":"第5章 不确定性推理 ","date":"2022-02-24 14:06:32","objectID":"/ai_base_01/:5:0","tags":["AI"],"title":"AI_base_01","uri":"/ai_base_01/"},{"categories":["School courses"],"content":"概述 不确定性推理的含义 不确定性推理的基本问题： 不确定性的表示 不确定性的匹配 组合证据不确定性的计算 不确定性的更新 不确定性结论的合成 不确定性推理类型： 模型方法 数值方法 概率统计方法 绝对概率方法 贝叶斯方法 证据理论方法 HMM方法 可信度方法 模糊推理方法 粗糙集方法 非数值方法 发送率计算 控制方法 相关性制导回溯、机缘控制、启发式搜索等 ","date":"2022-02-24 14:06:32","objectID":"/ai_base_01/:5:1","tags":["AI"],"title":"AI_base_01","uri":"/ai_base_01/"},{"categories":["School courses"],"content":"可信度推理 知识不确定性的表示 证据不确定性的表示 组合证据不确定性的算法 不确定性的更新 不确定性结论的合成 ","date":"2022-02-24 14:06:32","objectID":"/ai_base_01/:5:2","tags":["AI"],"title":"AI_base_01","uri":"/ai_base_01/"},{"categories":["School courses"],"content":"主观Bayes推理 Bayes公式 知识不确定性的表示 证据不确定性的表示 组合证据不确定性的算法 不确定性的更新 不确定性结论的合成 ","date":"2022-02-24 14:06:32","objectID":"/ai_base_01/:5:3","tags":["AI"],"title":"AI_base_01","uri":"/ai_base_01/"},{"categories":["School courses"],"content":"证据理论推理 证据理论 证据理论的形式化描述 证据理论的推理模型 证据理论推理实例 ","date":"2022-02-24 14:06:32","objectID":"/ai_base_01/:5:4","tags":["AI"],"title":"AI_base_01","uri":"/ai_base_01/"},{"categories":["School courses"],"content":"模糊推理 模糊集及其运算 模糊关系及其运算 模糊知识表示 模糊概念的匹配 模糊推理的方法 ","date":"2022-02-24 14:06:32","objectID":"/ai_base_01/:5:5","tags":["AI"],"title":"AI_base_01","uri":"/ai_base_01/"},{"categories":["School courses"],"content":"Bayes网络 贝叶斯网络定义 贝叶斯网络的全联合概率分布表示 贝叶斯网络的条件独立关系 贝叶斯网络的构造 贝叶斯网络的精确推理 变量消元 贝叶斯网络的近似推理 马尔科夫链蒙特卡洛（MCMC）方法 ","date":"2022-02-24 14:06:32","objectID":"/ai_base_01/:5:6","tags":["AI"],"title":"AI_base_01","uri":"/ai_base_01/"},{"categories":["School courses"],"content":"机器学习 ","date":"2022-02-24 14:06:32","objectID":"/ai_base_01/:6:0","tags":["AI"],"title":"AI_base_01","uri":"/ai_base_01/"},{"categories":["School courses"],"content":"概述 ","date":"2022-02-24 14:06:32","objectID":"/ai_base_01/:6:1","tags":["AI"],"title":"AI_base_01","uri":"/ai_base_01/"},{"categories":["School courses"],"content":"线性回归 ","date":"2022-02-24 14:06:32","objectID":"/ai_base_01/:6:2","tags":["AI"],"title":"AI_base_01","uri":"/ai_base_01/"},{"categories":["School courses"],"content":"决策树学习 ","date":"2022-02-24 14:06:32","objectID":"/ai_base_01/:6:3","tags":["AI"],"title":"AI_base_01","uri":"/ai_base_01/"},{"categories":["School courses"],"content":"支持向量机 ","date":"2022-02-24 14:06:32","objectID":"/ai_base_01/:6:4","tags":["AI"],"title":"AI_base_01","uri":"/ai_base_01/"},{"categories":["School courses"],"content":"集成学习 ","date":"2022-02-24 14:06:32","objectID":"/ai_base_01/:6:5","tags":["AI"],"title":"AI_base_01","uri":"/ai_base_01/"},{"categories":["School courses"],"content":"深度学习 ","date":"2022-02-24 14:06:32","objectID":"/ai_base_01/:7:0","tags":["AI"],"title":"AI_base_01","uri":"/ai_base_01/"},{"categories":["School courses"],"content":"概述 ","date":"2022-02-24 14:06:32","objectID":"/ai_base_01/:7:1","tags":["AI"],"title":"AI_base_01","uri":"/ai_base_01/"},{"categories":["School courses"],"content":"前馈神经网络 ","date":"2022-02-24 14:06:32","objectID":"/ai_base_01/:7:2","tags":["AI"],"title":"AI_base_01","uri":"/ai_base_01/"},{"categories":["School courses"],"content":"几种常见的神经网络 ","date":"2022-02-24 14:06:32","objectID":"/ai_base_01/:7:3","tags":["AI"],"title":"AI_base_01","uri":"/ai_base_01/"},{"categories":["Coding"],"content":"查找算法 散列查找：也称哈希查找，有拉链法查找，也有线性探测法查找，拉链法使用数组链表结构，线性探测法使用数组。 树查找：有搜索二叉树，平衡查找树如：红黑树，B树，AVL树，B+等，使用链表树结构 ","date":"2022-02-20 19:52:33","objectID":"/algorithm_find/:0:0","tags":["data structure"],"title":"Algorithm_find","uri":"/algorithm_find/"},{"categories":["Coding"],"content":"哈希表：散列查找 线性查找：在链表上线性查找键更新值。 散列查找： 空间换时间的查找算法 依赖数据结构HashTable（Hash: 指压缩映射，它将一个比较大的域空间映射到一个比较小的域空间） 关于线性探测法与拉链法在go语法里map实现原理时有讲解。（go_base_01） 哈希算法非常多，随机分布性不同，当然，让得到的哈希值越均匀随机分布越好，拉链法形成的链表不会很长。 最好时间复杂度能达到： O(1) ，最坏情况下退化到查找链表： O(n) 。均匀性很好的哈希算法以及合适空间大小的数组，在很大概率避免了最坏情况。 ","date":"2022-02-20 19:52:33","objectID":"/algorithm_find/:1:0","tags":["data structure"],"title":"Algorithm_find","uri":"/algorithm_find/"},{"categories":["Coding"],"content":"二叉查找树 又称为二叉搜索树，二叉排序树。 二叉查找树不保证是一个平衡的二叉树，最坏情况下二叉查找树会退化成一个链表。 通用形式的二叉查找树实现甚少使用，大部分程序都使用了AVL树或红黑树。 二叉查找树中序遍历即可实现排序。 查找，添加，删除元素的时间复杂度取决于树的高度。查找，添加和删除的时间复杂度范围为 log(n)~n 。 AVL树和红黑树都是相对平衡的二叉查找树，因为特殊的旋转平衡操作，树的高度被大大压低。它们查找效率较高，添加，删除，查找操作的平均时间复杂度都为 log(n) 。 ","date":"2022-02-20 19:52:33","objectID":"/algorithm_find/:2:0","tags":["data structure"],"title":"Algorithm_find","uri":"/algorithm_find/"},{"categories":["Coding"],"content":"AVL树（平衡二叉搜索树） Adelson-Velsky and Landis。 添加和删除元素时的调整操作比较关键且重要。 // AVL树 type AVLTree struct { Root *AVLTreeNode // 树根节点 } // AVL节点 type AVLTreeNode struct { Value int64 // 值 Times int64 // 值出现的次数 Height int64 // 该节点作为树根节点，树的高度，方便计算平衡因子 Left *AVLTreeNode // 左子树 Right *AVLTreeNode // 右字树 } // 初始化一个AVL树 func NewAVLTree() *AVLTree { return new(AVLTree) } AVL树添加元素 插入节点后，需要满足所有节点的平衡因子在 [-1，0，1] 范围内，如果不在，需要进行旋转调整。旋转有四种情况： 在右子树上插上右儿子导致失衡，左旋，转一次。 在左子树上插上左儿子导致失衡，右旋，转一次。 在左子树上插上右儿子导致失衡，先左后右旋，转两次。 在右子树上插上左儿子导致失衡，先右后左旋，转两次。 右图来自维基百科： // 单右旋操作，看图说话 func RightRotation(Root *AVLTreeNode) *AVLTreeNode { // 只有Pivot和B，Root位置变了 Pivot := Root.Left B := Pivot.Right Pivot.Right = Root Root.Left = B // 只有Root和Pivot变化了高度 Root.UpdateHeight() Pivot.UpdateHeight() return Pivot } // 单左旋操作，看图说话 func LeftRotation(Root *AVLTreeNode) *AVLTreeNode { // 只有Pivot和B，Root位置变了 Pivot := Root.Right B := Pivot.Left Pivot.Left = Root Root.Right = B // 只有Root和Pivot变化了高度 Root.UpdateHeight() Pivot.UpdateHeight() return Pivot } 复用上面代码 // 先左后右旋操作，看图说话 func LeftRightRotation(node *AVLTreeNode) *AVLTreeNode { node.Left = LeftRotation(node.Left) return RightRotation(node) } // 先右后左旋操作，看图说话 func RightLeftRotation(node *AVLTreeNode) *AVLTreeNode { node.Right = RightRotation(node.Right) return LeftRotation(node) } 完成旋转操作，可以添加元素了~ // 添加元素 func (tree *AVLTree) Add(value int64) { // 往树根添加元素，会返回新的树根 tree.Root = tree.Root.Add(value) } func (node *AVLTreeNode) Add(value int64) *AVLTreeNode { // 添加值到根节点node，如果node为空，那么让值成为新的根节点，树的高度为1 if node == nil { return \u0026AVLTreeNode{Value: value, Height: 1} } // 如果值重复，什么都不用做，直接更新次数 if node.Value == value { node.Times = node.Times + 1 return node } // 辅助变量 var newTreeNode *AVLTreeNode if value \u003e node.Value { // 插入的值大于节点值，要从右子树继续插入 node.Right = node.Right.Add(value) // 平衡因子，插入右子树后，要确保树根左子树的高度不能比右子树低一层。 factor := node.BalanceFactor() // 右子树的高度变高了，导致左子树-右子树的高度从-1变成了-2。 if factor == -2 { if value \u003e node.Right.Value { // 表示在右子树上插上右儿子导致失衡，需要单左旋： newTreeNode = LeftRotation(node) } else { //表示在右子树上插上左儿子导致失衡，先右后左旋： newTreeNode = RightLeftRotation(node) } } } else { // 插入的值小于节点值，要从左子树继续插入 node.Left = node.Left.Add(value) // 平衡因子，插入左子树后，要确保树根左子树的高度不能比右子树高一层。 factor := node.BalanceFactor() // 左子树的高度变高了，导致左子树-右子树的高度从1变成了2。 if factor == 2 { if value \u003c node.Left.Value { // 表示在左子树上插上左儿子导致失衡，需要单右旋： newTreeNode = RightRotation(node) } else { //表示在左子树上插上右儿子导致失衡，先左后右旋： newTreeNode = LeftRightRotation(node) } } } if newTreeNode == nil { // 表示什么旋转都没有，根节点没变，直接刷新树高度 node.UpdateHeight() return node } else { // 旋转了，树根节点变了，需要刷新新的树根高度 newTreeNode.UpdateHeight() return newTreeNode } } 由于树的高度最高为 1.44log(n)，查找元素插入位置，最坏次数为 1.44log(n) 次。逐层更新子树高度并判断平衡是否被破坏，最坏需要 1.44log(n) 次，因此可以得知添加元素最坏时间复杂度为：2.88log(n) 当插入节点后，某子树不平衡时最多旋转 2次，也就是双旋该子树即可恢复平衡，该调整为局部特征，调整完后其父层不再需要旋转。也就是说，插入操作最坏旋转两次即可 AVL树查找元素与普通二叉查找树一致。 删除元素有四种情况： 删除的节点是叶子节点，没有儿子，直接删除后看离它最近的父亲节点是否失衡，做调整操作。 删除的节点下有两个子树，选择高度更高的子树下的节点来替换被删除的节点，如果左子树更高，选择左子树中最大的节点，也就是左子树最右边的叶子节点，如果右子树更高，选择右子树中最小的节点，也就是右子树最左边的叶子节点。最后，删除这个叶子节点，也就是变成情况1。 删除的节点只有左子树，可以知道左子树其实就只有一个节点，被删除节点本身（假设左子树多于2个节点，那么高度差就等于2了，不符合AVL树定义），将左节点替换被删除的节点，最后删除这个左节点，变成情况1。 删除的节点只有右子树，可以知道右子树其实就只有一个节点，被删除节点本身（假设右子树多于2个节点，那么高度差就等于2了，不符合AVL树定义），将右节点替换被删除的节点，最后删除这个右节点，变成情况1。 后面三种情况最后都变成情况1，就是将删除的节点变成叶子节点，然后可以直接删除该叶子节点，然后看其最近的父亲节点是否失衡，失衡时对树进行平衡。 实现代码： func (node *AVLTreeNode) Delete(value int64) *AVLTreeNode { if node == nil { // 如果是空树，直接返回 return nil } if value \u003c node.Value { // 从左子树开始删除 node.Left = node.Left.Delete(value) // 删除后要更新该子树高度 node.Left.UpdateHeight() } else if value \u003e node.Value { // 从右子树开始删除 node.Right = node.Right.Delete(value) // 删除后要更新该子树高度 node.Right.UpdateHeight() } else { // 找到该值对应的节点 // 该节点没有左右子树 // 第一种情况，删除的节点没有儿子，直接删除即可。 if node.Left == nil \u0026\u0026 node.Right == nil { return nil // 直接返回nil，表示直接该值删除 } // 该节点有两棵子树，选择更高的哪个来替换 // 第二种情况，删除的节点下有两个子树，选择高度更高的子树下的节点来替换被删除的节点，如果左子树更高，选择左子树中最大的节点，也就是左子树最右边的叶子节点，如果右子树更高，选择右子树中最小的节点，也就是右子树最左边的叶子节点。最后，删除这个叶子节点。 if node.Left != nil \u0026\u0026 node.Right != nil { // 左子树更高，拿左子树中最大值的节点替换 if node.Left.Height \u003e node.Right.Height { maxNode := node.","date":"2022-02-20 19:52:33","objectID":"/algorithm_find/:3:0","tags":["data structure"],"title":"Algorithm_find","uri":"/algorithm_find/"},{"categories":["Coding"],"content":"2-3树和左倾红黑树 左倾红黑树比普通红黑树实现更简单。 红黑树是一种近似平衡的二叉查找树，从 2-3 树或 2-3-4 树衍生而来。通过对二叉树节点进行染色，染色为红或黑节点，来模仿 2-3 树或 2-3-4 树的3节点和4节点，从而让树的高度减小。2-3-4 树对照实现的红黑树是普通的红黑树，而 2-3 树对照实现的红黑树是一种变种，称为左倾红黑树。 2-3树：它不是一棵二叉树，是一棵三叉树。具有以下特征： 内部节点要么有1个数据元素和2个孩子，要么有2个数据元素和3个孩子，叶子节点没有孩子，但有1或2个数据元素。 所有叶子节点到根节点的长度一致。这个特征保证了完全平衡，非常完美的平衡。 每个节点的数据元素保持从小到大排序，两个数据元素之间的子树的所有值大小介于两个数据元素之间。 2-3树插入元素： 先二分查找到要插入的位置 插入元素到一个2节点，直接插入即可，这样节点变成3节点。 插入元素到一个3节点，该3节点的父亲是一个2节点，先将节点变成临时的4节点，然后向上分裂调整一次。 插入元素到一个3节点，该3节点的父亲是一个3节点，先将节点变成临时的4节点，然后向上分裂调整，此时父亲节点变为临时4节点，继续向上分裂调整。 核心在于插入3节点后，该节点变为临时4节点，然后进行分裂恢复树的特征。最坏情况为插入节点后，每一次分裂后都导致上一层变为临时4节点，直到树根节点，这样需要不断向上分裂。 临时4节点的分裂，细分有六种情况： 2-3树删除元素： 情况1：删除中间节点 删除的是非叶子节点，该节点一定是有两棵或者三棵子树的，那么从子树中找到其最小后继节点，该节点是叶子节点，用该节点替换被删除的非叶子节点，然后再删除这个叶子节点，进入情况2。 如何找到最小后继节点，当有两棵子树时，那么从右子树一直往左下方找，如果有三棵子树，被删除节点在左边，那么从中子树一直往左下方找，否则从右子树一直往左下方找。 情况2：删除叶子节点 删除的是叶子节点，这时如果叶子节点是3节点，那么直接变为2节点即可，不影响平衡。但是，如果叶子节点是2节点，那么删除后，其父节点将会缺失一个儿子，破坏了满孩子的 2-3 树特征，需要进行调整后才能删除。 针对情况2，删除一个2节点的叶子节点，会导致父节点缺失一个儿子，破坏了 2-3 树的特征，我们可以进行调整变换，主要有两种调整： 重新分布：尝试从兄弟节点那里借值，然后重新调整节点。 合并：如果兄弟借不到值，合并节点（与父亲的元素），再向上递归处理。 左倾红黑树： 可以由2-3树实现 根节点的链接是黑色的。 红链接均为左链接。 没有任何一个结点同时和两条红链接相连 任意一个节点到达叶子节点的所有路径，经过的黑链接数量相同，也就是该树是完美黑色平衡的。比如，某一个节点，它可以到达5个叶子节点，那么这5条路径上的黑链接数量一样。 // 定义颜色 const ( RED = true BLACK = false ) // 左倾红黑树 type LLRBTree struct { Root *LLRBTNode // 树根节点 } // 左倾红黑树节点 type LLRBTNode struct { Value int64 // 值 Times int64 // 值出现的次数 Left *LLRBTNode // 左子树 Right *LLRBTNode // 右子树 Color bool // 父亲指向该节点的链接颜色 } // 新建一棵空树 func NewLLRBTree() *LLRBTree { return \u0026LLRBTree{} } // 节点的颜色 func IsRed(node *LLRBTNode) bool { if node == nil { return false } return node.Color == RED } 在元素添加和实现的过程中，需要做调整操作，有两种旋转操作，对某节点的右链接进行左旋转，或者左链接进行右旋转。 // 左旋转 func RotateLeft(h *LLRBTNode) *LLRBTNode { if h == nil { return nil } x := h.Right h.Right = x.Left x.Left = h x.Color = h.Color h.Color = RED return x } // 右旋转 func RotateRight(h *LLRBTNode) *LLRBTNode { if h == nil { return nil } x := h.Left h.Left = x.Right x.Right = h x.Color = h.Color h.Color = RED return x } // 颜色转换 func ColorChange(h *LLRBTNode) { if h == nil { return } h.Color = !h.Color h.Left.Color = !h.Left.Color h.Right.Color = !h.Right.Color } 添加元素实现： 每次添加元素节点时，都将该节点 Color 字段，也就是父亲指向它的链接设置为 RED 红色。接着判断其父亲是否有两个红链接（如连续的两个左红链接或者左右红色链接），或者有右红色链接，进行颜色变换或旋转操作。 几种情况： 插入元素到2节点，直接让节点变为3节点，不过当右插入时需要左旋使得红色链接在左边 插入元素到3节点，需要做旋转和颜色转换操作 // 左倾红黑树添加元素 func (tree *LLRBTree) Add(value int64) { // 跟节点开始添加元素，因为可能调整，所以需要将返回的节点赋值回根节点 tree.Root = tree.Root.Add(value) // 根节点的链接永远都是黑色的 tree.Root.Color = BLACK } // 往节点添加元素 func (node *LLRBTNode) Add(value int64) *LLRBTNode { // 插入的节点为空，将其链接颜色设置为红色，并返回 if node == nil { return \u0026LLRBTNode{ Value: value, Color: RED, } } // 插入的元素重复 if value == node.Value { node.Times = node.Times + 1 } else if value \u003e node.Value { // 插入的元素比节点值大，往右子树插入 node.Right = node.Right.Add(value) } else { // 插入的元素比节点值小，往左子树插入 node.Left = node.Left.Add(value) } // 辅助变量 nowNode := node // 右链接为红色，那么进行左旋，确保树是左倾的 // 这里做完操作后就可以结束了，因为插入操作，新插入的右红链接左旋后，nowNode节点不会出现连续两个红左链接，因为它只有一个左红链接 if IsRed(nowNode.Right) \u0026\u0026 !IsRed(nowNode.Left) { nowNode = RotateLeft(nowNode) } else { // 连续两个左链接为红色，那么进行右旋 if IsRed(nowNode.Left) \u0026\u0026 IsRed(nowNode.Left.Left) { nowNode = RotateRight(nowNode) } // 旋转后，可能左右链接都为红色，需要变色 if IsRed(nowNode.Left) \u0026\u0026 IsRed(nowNode.Right) { ColorChange(nowNode) } } return nowNode } 算法分析： 左倾红黑树的最坏树高度为 2log(n)，其中 n 为树的节点数量。为什么呢，我们先把左倾红黑树当作 2-3 树，也就是说最坏情况下沿着 2-3 树左边的节点都是3节点，其他节点都是2节点，这时树高近似 log(n)，再从 2-3 树转成左倾红黑树，当3节点不画平时，可以知道树高变成原来 2-3 树树高的两倍。虽然如此，构造一棵最坏的左倾红黑树很难。 AVL 树的最坏树高度为 1.44log(n)。由于左倾红黑树是近似平衡的二叉树，没有 AVL 树的严格平衡，树的高度会更高一点，因此查找操作效率比 AVL 树低，但时间复杂度只在于常数项的差别，去掉常数项，时间复杂度仍然是 log(n)。 我们的代码实现中，左倾红黑树的插入，需要逐层判断是否需要旋转和变色，复杂度为 log(n)，当旋转变色后导致上层存在连续的红左链接或者红色左右链接，那么需要继续旋转和变色，可能有多次这种调整操作，如图在箭头处添加新节点，出现了右红链接，要一直向上变色到根节点（实际上穿投到根节点的情况极少发生）：我们可以优化代码，使得在某一层旋转变色后，如果其父层没有连续的左红链接或者不需要变色，那么可以直接退出，不需要逐层判断是否需要旋转和变色。 对于 AVL 树来说，插入最多旋转两次，但其需要逐层更新树高度，复杂度也是为 log(n)。 按照插入效率来说，很多教程都说左倾红黑树会比 AVL 树好一点，因为其不要求严格的平衡，会插入得更快点，但根据我们实际上的递归代码，两者都需要逐层向上判断是否需要调整，只不过 AVL 树多了更新树高度的操作，此操作影响了一点点效率，但我觉得两种树的插入效率都差不多。 在此，我们不再纠结两种平衡树哪种更好","date":"2022-02-20 19:52:33","objectID":"/algorithm_find/:4:0","tags":["data structure"],"title":"Algorithm_find","uri":"/algorithm_find/"},{"categories":["Coding"],"content":"2-3-4树和普通红黑树 ","date":"2022-02-20 19:52:33","objectID":"/algorithm_find/:5:0","tags":["data structure"],"title":"Algorithm_find","uri":"/algorithm_find/"},{"categories":["Coding"],"content":"参考 https://www.cs.princeton.edu/~rs/talks/LLRB/LLRB.pdf ","date":"2022-02-20 19:52:33","objectID":"/algorithm_find/:6:0","tags":["data structure"],"title":"Algorithm_find","uri":"/algorithm_find/"},{"categories":["Coding"],"content":"排序算法 稳定性概念 定义：能保证两个相等的数，经过排序之后，其在序列的前后位置顺序不变。（A1=A2，排序前A1在A2前面，排序后A1还在A2前面） 意义：稳定性本质是维持具有相同属性的数据的插入顺序，如果后面需要使用该插入顺序排序，则稳定性排序可以避免这次排序。 冒泡排序，直接选择排序，直接插入排序被认为是初级的排序算法。中等规模用希尔排序，大规模排序使用快排、归并、堆排序这些高级排序算法。快排综合性能最好，甚至成为了很多编程库内置的排序算法。 ","date":"2022-02-20 19:52:01","objectID":"/algorithm_sort/:0:0","tags":["data structure"],"title":"Algorithm_sort","uri":"/algorithm_sort/"},{"categories":["Coding"],"content":"冒泡 自己实现时忘记考虑某一轮两两比较时已经排好序的情况，属实生疏了。时间复杂度一般是$O(n^2)$。冒泡排序是效率较低的排序算法，可以说是最慢的排序算法了，我们只需知道它是什么，在实际编程中一定不能使用如此之慢的排序算法! ","date":"2022-02-20 19:52:01","objectID":"/algorithm_sort/:1:0","tags":["data structure"],"title":"Algorithm_sort","uri":"/algorithm_sort/"},{"categories":["Coding"],"content":"选择 效率同样低下的排序算法。可以在每次循环选择时既选择最小的数，也选择最大的数，以减少循环次数达到优化的目的。工程上同样避免使用。 ","date":"2022-02-20 19:52:01","objectID":"/algorithm_sort/:2:0","tags":["data structure"],"title":"Algorithm_sort","uri":"/algorithm_sort/"},{"categories":["Coding"],"content":"插入 时间复杂度：$O(n)-O(n^2)$ 数组规模 n 较小的大多数情况下，我们可以使用插入排序，它比冒泡排序，选择排序都快，甚至比任何的排序算法都快。 数列中的有序性越高，插入排序的性能越高，因为待排序数组有序性越高，插入排序比较的次数越少。 数据规模较大时，效率也低。 ","date":"2022-02-20 19:52:01","objectID":"/algorithm_sort/:3:0","tags":["data structure"],"title":"Algorithm_sort","uri":"/algorithm_sort/"},{"categories":["Coding"],"content":"希尔 一个美国人1959年发明的，希尔排序是直接插入排序的改进版本。因为直接插入排序对那些几乎已经排好序的数列来说，排序效率极高，达到了 O(n) 的线性复杂度，但是每次只能将数据移动一位。希尔排序创造性的可以将数据移动 n 位，然后将 n 一直缩小，缩到与直接插入排序一样为 1 先取一个小于 N 的整数 d1 ，将位置是 d1 整数倍的数们分成一组，对这些数进行直接 插入排序。接着取一个小于 d1 的整数 d2 ，将位置是 d2 整数倍的数们分成一组，对这些数进行直接插入排序。接着取一个小于 d2 的整数 d3 ，将位置是 d3 整数倍的数们分成一组，对这些数进行直接插入排序。…直到取到的整数 d=1 ，接着使用直接插入排序。 这是一种分组插入方法，最后一次迭代就相当于是直接插入排序，其他迭代相当于每次移动 n 个距离的直接插入排序，这些整数是两个数之间的距离，我们称它们为增量。 我们取数列长度的一半为增量，以后每次减半，直到增量为1。 希尔排序通过分组使用直接插入排序，因为步长比 1大，在一开始可以很快将无序的数列变得不那么无序，比较和交换的次数也减少，直到最后使用步长为 1 的直接插入排序，数列已经是相对有序了，所以时间复杂度会稍好一点。 在最好情况下，也就是数列是有序时，希尔排序需要进行 logn 次增量的直接插入排序，因为每次直接插入排序最佳时间复杂度都为： O(n) ，因此希尔排序的最佳时间复杂度为： O(nlogn) 。 在最坏情况下，每一次迭代都是最坏的，假设增量序列为： d8 d7 d6 … d3 d2 1 ，那么每一轮直接插入排序的元素数量为： n/d8 n/d7 n/d6 …. n/d3 n/d2 n ，那么时间复杂度按照直接插入的最坏复杂度来计算为：$O( (n/d8)^2 + (n/d7)^2 + (n/d6)^2 + … + (n/d2)^2 + n^2) = O( \u003c 2 ) * O(n^2)$ 不同的分组增量序列，有不同的时间复杂度。Hibbard 增量序列： 1，3，7，···，2n−1 是被证明可广泛应用的分组序列，时间复杂度为： Θ(n^1.5) 。 希尔排序的时间复杂度大约在这个范围： O(n^1.3)~O(n^2) ，具体还无法用数学来严格证明它。 ","date":"2022-02-20 19:52:01","objectID":"/algorithm_sort/:4:0","tags":["data structure"],"title":"Algorithm_sort","uri":"/algorithm_sort/"},{"categories":["Coding"],"content":"归并 分治法 将两个有序数组进行合并，最多进行 N 次比较就可以生成一个新的有序数组， N 是两个数组长度之和。 归并操作最坏的时间复杂度为： O(n) ，其中 n 是较长数组的长度（因为$N\u003c2n$）。归并操作最好的时间复杂度为： O(n) ，其中 n 是较短数组的长度。正是利用这个特点，归并排序先排序较小的数组，再将有序的小数组合并形成更大有序的数组。 归并排序有两种递归做法，一种是自顶向下，一种是自底向上。 自顶向下：不断二分大数组直到无法切分，排序，不断两两合并，得到排序后数组。 每次都是一分为二，特别均匀，所以最差和最坏时间复杂度都一样。归并操作的时间复杂度为： O(n) ，因此总的时间复杂度为： T(n)=2T(n/2)+O(n) ，根据主定理公式可以知道时间复杂度为： O(nlogn) 因为不断地递归，程序栈层数会有 logn 层，所以递归栈的空间复杂度为： O(logn) ，对于排序十亿个整数，也只要： log(100 0000 0000)=29.897 ，占用的堆栈层数最多 30 层 自底向上：小数组排序合并成大数组。 时间复杂度同上 因不需要递归，没有程序栈占用，空间复杂度为O(1) 归并排序归并操作占用了额外的辅助数组，且归并操作是从一个元素的数组开始。 改进： 对于小规模数组，使用直接插入排序。 原地排序，节约掉辅助数组空间的占用。 建议使用自底向上非递归排序，不会有程序栈空间损耗 手摇算法（翻转算法）： 主要用来对数组两部分进行位置互换 eg:将字符串 abcde1234567 的前 5 个字符与后面的字符交换位置，那么手摇后变成： 1234567abcde 。 如何翻转： 将前部分逆序 将后部分逆序 对整体逆序 归并原地排序利用了手摇算法的特征，不需要额外的辅助数组。 我们自底开始，将元素按照数量为 blockSize 进行小数组排序，使用直接插入排序，然后我们对这些有序的数组向上进行原地归并操作。 归并排序是唯一一个有稳定性保证的高级排序算法，某些时候，为了寻求大规模数据下排序前后，相同元素位置不变，可以使用归并排序。 ","date":"2022-02-20 19:52:01","objectID":"/algorithm_sort/:5:0","tags":["data structure"],"title":"Algorithm_sort","uri":"/algorithm_sort/"},{"categories":["Coding"],"content":"优先队列及堆 堆排序属于选择类排序算法。 优先队列是一种能完成以下任务的队列：插入一个数值，取出最小或最大的数值（获取数值，并且删除）。 优先队列可以用二叉树来实现，我们称这种结构为二叉堆。 最小堆和最大堆是二叉堆的一种，是一棵完全二叉树（一种平衡树）。 最小堆的性质： 父节点的值都小于左右儿子节点。 这是一个递归的性质。 最大堆的性质： 父节点的值都大于左右儿子节点。 这是一个递归的性质。 最大堆和最小堆实现方式一样，只不过根节点一个是最大的，一个是最小的 最大堆特征： 最大堆实现细节(两个操作)： push：向堆中插入数据时，首先在堆的末尾插入数据，如果该数据比父亲节点还大，那么交换，然后不断向上提升，直到没有大小颠倒为止。 pop：从堆中删除最大值时，首先把最后一个值复制到根节点上，并且删除最后一个数值，然后和儿子节点比较，如果值小于儿子，与儿子节点交换，然后不断向下交换， 直到没有大小颠倒为止。在向下交换过程中，如果有两个子儿子都大于自己，就选择较大的。 最大堆有两个核心操作，一个是上浮，一个是下沉，分别对应 push 和 pop 。 上浮操作 操作一次 push 的最好时间复杂度为： O(1) ，因为第一次上浮时如果不大于父亲，那么就结束了。最坏的时间复杂度为： O(logn) ，相当于每次都大于父亲，会一直往上浮到根节点，翻转次数等于树的高度，而树的高度等于元素个数的对数： log(n) 。 构建一个最大堆，从空堆开始，每次添加元素到尾部后，需要向上翻转，最坏翻转次数是：近似 = log(1)+log(2)+log(3)+…+log(n) = log(n!) log(n!) 和 nlog(n) 是同阶的，故最坏时间复杂度便得到了。元素不全相同的情况下最好时间复杂度也是这个。 下沉操作 操作一次 pop 最好的时间复杂度也是： O(1) ，因为第一次比较时根节点就是最大的。最坏时间复杂度仍然是树的高度： O(logn) 。 从一个最大堆，逐一移除堆顶元素，然后将堆尾元素置于堆顶后，向下翻转恢复堆特征，最坏翻转次数是:近似 = log(1)+log(2)+log(3)+…+log(n) = log(n!)，同上浮。元素不全相同的情况下最好时间复杂度也是O(nlog(n)) 如果所有的元素都一样的情况下，建堆和移除堆的每一步都不需要翻转，最好时间复杂度 为： O(n) ，复杂度主要在于遍历元素。 根据最大堆，可以实现堆排序。 普通堆排序：先构建一个最小堆，然后依次把根节点元素 pop 出即可： 因为一开始会认为堆是空的，每次添加元素都需要添加到尾部，然后向上翻转，需要用 Heap.Size来记录堆的大小增长，这种堆构建，可以认为是非原地的构建，影响了效率 改进的原地自底向上的堆排序，不会从空堆开始，而是把待排序的数列当成一个混乱的最大堆，从底层逐层开始，对元素进行下沉操作，一直恢复最大堆的特征，直到根节点。 将构建堆的时间复杂度从 O(nlogn) 降为 O(n) ，总的堆排序时间复杂度从 O(2nlogn) 改进到 O(n+nlogn) 。 自底向上堆排序： 构建最大堆步骤： 先对最底部的所有非叶子节点进行下沉，即这些非叶子节点与它们的儿子节点比较，较大的儿子和父亲交换位置。 接着从次二层开始的非叶子节点重复这个操作，直到到达根节点最大堆就构建好了 ","date":"2022-02-20 19:52:01","objectID":"/algorithm_sort/:6:0","tags":["data structure"],"title":"Algorithm_sort","uri":"/algorithm_sort/"},{"categories":["Coding"],"content":"快速 亦使用了分治法。 对冒泡排序的改进。 快速排序通过一趟排序将要排序的数据分割成独立的两部分，其中一部分的所有数据都比另外一部分的所有数据都要小，然后再按此方法对这两部分数据分别进行快速排序，整个排序过程可以递归进行，以此达到整个数据变成有序序列。 步骤： 先从数列中取出一个数作为基准数。一般取第一个数。 分区过程，将比这个数大的数全放到它的右边，小于或等于它的数全放到它的左边。 再对左右区间重复第二步，直到各区间只有一个数。 在最好情况下，每一轮都能平均切分，这样遍历元素只要 n/2 次就可以把数列分成两部分，每一轮的时间复杂度都是： O(n) 。因为问题规模每次被折半，折半的数列继续递归进行切分，也就是总的时间复杂度计算公式为： T(n) = 2*T(n/2) + O(n) 。按照主定理公式计算，我们可以知道时间复杂度为： O(nlogn) 最差的情况下，每次都不能平均地切分，每次切分都因为基准数是最大的或者最小的，不能分成两个数列，这样时间复杂度变为了 T(n) = T(n-1) + O(n) ，按照主定理计算可以知道时间复杂度为： O(n^2) 数据规模越大越难以出现最差的情况，在综合情况下，快速排序的平均时间复杂度为： O(nlogn) 。对比之前介绍的排序算法，快速排序比那些动不动就是平方级别的初级排序算法更佳。 为了避免切分不均匀情况的发生，有几种方法改进： 每次进行快速排序切分时，先将数列随机打乱，再进行切分，这样随机加了个震荡，减少不均匀的情况。当然，也可以随机选择一个基准数，而不是选第一个数。 每次取数列头部，中部，尾部三个数，取三个数的中位数为基准数进行切分。 快速排序使用原地排序，存储空间复杂度为： O(1) 。而因为递归栈的影响，递归的程序栈开辟的层数范围在 $logn-n$ ，所以递归栈的空间复杂度为： $O(logn)-log(n)$ ，最坏为： log(n) ，当元素较多时，程序栈可能溢出。通过改进算法，使用伪尾递归进行优化，递归栈的空间复杂度可以减小到 O(logn) 改进： 在小规模数组的情况下，直接插入排序的效率最好，当快速排序递归部分进入小数组范围，可以 切换成直接插入排序。 排序数列可能存在大量重复值，使用三向切分快速排序，将数组分成三部分，大于基准数，等于 基准数，小于基准数，这个时候需要维护三个下标。 使用伪尾递归减少程序栈空间占用，使得栈空间复杂度从 O(logn)~log(n) 变 为： O(logn) 。 伪尾递归优化： // 伪尾递归快速排序 func QuickSort3(array []int, begin, end int) { for begin \u003c end { // 进行切分 loc := partition(array, begin, end) // 那边元素少先排哪边 if loc-begin \u003c end-loc { // 先排左边 QuickSort3(array, begin, loc-1) begin = loc + 1 } else { // 先排右边 QuickSort3(array, loc+1, end) end = loc - 1 } } } 解析：很多人以为这样子是尾递归。其实这样的快排写法是伪装的尾递归，不是真正的尾递归，因为有for 循环，不是直接 return QuickSort ，递归还是不断地压栈，栈的层次仍然不断地增长。但是，因为先让规模小的部分排序，栈的深度大大减少，程序栈最深不会超过 logn 层，这样堆栈最坏空间复杂度从 O(n) 降为 O(logn) 。这种优化也是一种很好的优化，因为栈的层数减少了，对于排序十亿个整数，也只要： log(100 00000000)=29.897 ，占用的堆栈层数最多 30 层，比不进行优化，可能出现的 O(n) 常数层好很多。 非递归写法仅仅是将之前的递归栈转化为自己维持的手工栈。 ","date":"2022-02-20 19:52:01","objectID":"/algorithm_sort/:7:0","tags":["data structure"],"title":"Algorithm_sort","uri":"/algorithm_sort/"},{"categories":["Coding"],"content":"内置库使用快排的原因 首先堆排序，归并排序最好最坏时间复杂度都是： O(nlogn) ，而快速排序最坏的时间复杂度是： O(n^2) ，但是很多编程语言内置的排序算法使用的仍然是快速排序，这是为什么？ 这个问题有偏颇，选择排序算法要看具体的场景， Linux 内核用的排序算法就是堆排序，而Java 对于数量比较多的复杂对象排序，内置排序使用的是归并排序，只是一般情况下，快速排序更快。 归并排序有两个稳定，第一个稳定是排序前后相同的元素位置不变，第二个稳定是，每次都是很平均地进行排序，读取数据也是顺序读取，能够利用存储器缓存的特征，比如从磁盘读取数据进行排序。因为排序过程需要占用额外的辅助数组空间，所以这部分有代价损耗，但是原地手摇的归并排序克服了这个缺陷。 复杂度中，大 O 有一个常数项被省略了，堆排序每次取最大的值之后，都需要进行节点翻转，重新恢复堆的特征，做了大量无用功，常数项比快速排序大，大部分情况下比快速排序慢很多。但是堆排序时间较稳定，不会出现快排最坏 O(n^2) 的情况，且省空间，不需要额外的存储空间和栈空间。 当待排序数量大于16000个元素时，使用自底向上的堆排序比快速排序还快，可见此：https://core.ac.uk/download/pdf/82350265.pdf。 快速排序最坏情况下复杂度高，主要在于切分不像归并排序一样平均，而是很依赖基准数的现在，我们通过改进，比如随机数，三切分等，这种最坏情况的概率极大的降低。大多数情况下，它并不会那么地坏，大多数快才是真的块。 归并排序和快速排序都是分治法，排序的数据都是相邻的，而堆排序比较的数可能跨越很大的范围，导致局部性命中率降低，不能利用现代存储器缓存的特征，加载数据过程会损失性能。 对稳定性有要求的，要求排序前后相同元素位置不变，可以使用归并排序， Java 中的复杂对象类型，要求排序前后位置不能发生变化，所以小规模数据下使用了直接插入排序，大规模数据下使用了归并排序。 对栈，存储空间有要求的可以使用堆排序，比如 Linux 内核栈小，快速排序占用程序栈太大了，使用快速排序可能栈溢出，所以使用了堆排序。 在 Golang 中，标准库 sort 中对切片进行稳定排序：会先按照 20 个元素的范围，对整个切片分段进行插入排序，因为小数组插入排序效率高，然后再对这些已排好序的小数组进行归并排序。其中归并排序还使用了原地排序，节约了辅助空间。 快速排序限制程序栈的层数为： 2*ceil(log(n+1)) ，当递归超过该层时表示程序栈过深，那么转为堆排序。 ","date":"2022-02-20 19:52:01","objectID":"/algorithm_sort/:8:0","tags":["data structure"],"title":"Algorithm_sort","uri":"/algorithm_sort/"},{"categories":["Coding"],"content":"facade API 为facade 模块的外观接口，大部分代码使用此接口简化对facade类的访问。 facade模块同时暴露了a和b 两个Module 的NewXXX和interface，其它代码如果需要使用细节功能时可以直接调用。 package facade import \"fmt\" func NewAPI() API { return \u0026apiImpl{ a: NewAModuleAPI(), b: NewBModuleAPI(), } } //API is facade interface of facade package type API interface { Test() string } //facade implement type apiImpl struct { a AModuleAPI b BModuleAPI } func (a *apiImpl) Test() string { aRet := a.a.TestA() bRet := a.b.TestB() return fmt.Sprintf(\"%s\\n%s\", aRet, bRet) } //NewAModuleAPI return new AModuleAPI func NewAModuleAPI() AModuleAPI { return \u0026aModuleImpl{} } //AModuleAPI ... type AModuleAPI interface { TestA() string } type aModuleImpl struct{} func (*aModuleImpl) TestA() string { return \"A module running\" } //NewBModuleAPI return new BModuleAPI func NewBModuleAPI() BModuleAPI { return \u0026bModuleImpl{} } //BModuleAPI ... type BModuleAPI interface { TestB() string } type bModuleImpl struct{} func (*bModuleImpl) TestB() string { return \"B module running\" } package facade import \"testing\" var expect = \"A module running\\nB module running\" // TestFacadeAPI ... func TestFacadeAPI(t *testing.T) { api := NewAPI() ret := api.Test() if ret != expect { t.Fatalf(\"expect %s, return %s\", expect, ret) } } ","date":"2022-01-22 09:21:00","objectID":"/structural_type/:1:0","tags":["design mode"],"title":"Structural_type","uri":"/structural_type/"},{"categories":["Coding"],"content":"adapter 适配器模式用于转换一种接口适配另一种接口。 实际使用中Adaptee一般为接口，并且使用工厂函数生成实例。 在Adapter中匿名组合Adaptee接口，所以Adapter类也拥有SpecificRequest实例方法，又因为Go语言中非入侵式接口特征，其实Adapter也适配Adaptee接口。 package adapter //Target 是适配的目标接口 type Target interface { Request() string } //Adaptee 是被适配的目标接口 type Adaptee interface { SpecificRequest() string } //NewAdaptee 是被适配接口的工厂函数 func NewAdaptee() Adaptee { return \u0026adapteeImpl{} } //AdapteeImpl 是被适配的目标类 type adapteeImpl struct{} //SpecificRequest 是目标类的一个方法 func (*adapteeImpl) SpecificRequest() string { return \"adaptee method\" } //NewAdapter 是Adapter的工厂函数 func NewAdapter(adaptee Adaptee) Target { return \u0026adapter{ Adaptee: adaptee, } } //Adapter 是转换Adaptee为Target接口的适配器 type adapter struct { Adaptee } //Request 实现Target接口 func (a *adapter) Request() string { return a.SpecificRequest() } package adapter import \"testing\" var expect = \"adaptee method\" func TestAdapter(t *testing.T) { adaptee := NewAdaptee() target := NewAdapter(adaptee) res := target.Request() if res != expect { t.Fatalf(\"expect: %s, actual: %s\", expect, res) } } ","date":"2022-01-22 09:21:00","objectID":"/structural_type/:2:0","tags":["design mode"],"title":"Structural_type","uri":"/structural_type/"},{"categories":["Coding"],"content":"proxy 代理模式用于延迟处理操作或者在进行实际操作前后进行其它处理。 代理模式的常见用法有 虚代理 COW代理 远程代理 保护代理 Cache 代理 防火墙代理 同步代理 智能指引 等。。。 package proxy type Subject interface { Do() string } type RealSubject struct{} func (RealSubject) Do() string { return \"real\" } type Proxy struct { real RealSubject } func (p Proxy) Do() string { var res string // 在调用真实对象之前的工作，检查缓存，判断权限，实例化真实对象等。。 res += \"pre:\" // 调用真实对象 res += p.real.Do() // 调用之后的操作，如缓存结果，对结果进行处理等。。 res += \":after\" return res } package proxy import \"testing\" func TestProxy(t *testing.T) { var sub Subject sub = \u0026Proxy{} res := sub.Do() if res != \"pre:real:after\" { t.Fail() } } ","date":"2022-01-22 09:21:00","objectID":"/structural_type/:3:0","tags":["design mode"],"title":"Structural_type","uri":"/structural_type/"},{"categories":["Coding"],"content":"composite 组合模式统一对象和对象集，使得使用相同接口使用对象和对象集。 组合模式常用于树状结构，用于统一叶子节点和树节点的访问，并且可以用于应用某一操作到所有子节点。 package composite import \"fmt\" type Component interface { Parent() Component SetParent(Component) Name() string SetName(string) AddChild(Component) Print(string) } const ( LeafNode = iota CompositeNode ) func NewComponent(kind int, name string) Component { var c Component switch kind { case LeafNode: c = NewLeaf() case CompositeNode: c = NewComposite() } c.SetName(name) return c } type component struct { parent Component name string } func (c *component) Parent() Component { return c.parent } func (c *component) SetParent(parent Component) { c.parent = parent } func (c *component) Name() string { return c.name } func (c *component) SetName(name string) { c.name = name } func (c *component) AddChild(Component) {} func (c *component) Print(string) {} type Leaf struct { component } func NewLeaf() *Leaf { return \u0026Leaf{} } func (c *Leaf) Print(pre string) { fmt.Printf(\"%s-%s\\n\", pre, c.Name()) } type Composite struct { component childs []Component } func NewComposite() *Composite { return \u0026Composite{ childs: make([]Component, 0), } } func (c *Composite) AddChild(child Component) { child.SetParent(c) c.childs = append(c.childs, child) } func (c *Composite) Print(pre string) { fmt.Printf(\"%s+%s\\n\", pre, c.Name()) pre += \" \" for _, comp := range c.childs { comp.Print(pre) } } package composite func ExampleComposite() { root := NewComponent(CompositeNode, \"root\") c1 := NewComponent(CompositeNode, \"c1\") c2 := NewComponent(CompositeNode, \"c2\") c3 := NewComponent(CompositeNode, \"c3\") l1 := NewComponent(LeafNode, \"l1\") l2 := NewComponent(LeafNode, \"l2\") l3 := NewComponent(LeafNode, \"l3\") root.AddChild(c1) root.AddChild(c2) c1.AddChild(c3) c1.AddChild(l1) c2.AddChild(l2) c2.AddChild(l3) root.Print(\"\") // Output: // +root // +c1 // +c3 // -l1 // +c2 // -l2 // -l3 } ","date":"2022-01-22 09:21:00","objectID":"/structural_type/:4:0","tags":["design mode"],"title":"Structural_type","uri":"/structural_type/"},{"categories":["Coding"],"content":"flyweight 享元模式从对象中剥离出不发生改变且多个实例需要的重复数据，独立出一个享元，使多个对象共享，从而节省内存以及减少对象数量。 package flyweight import \"fmt\" type ImageFlyweightFactory struct { maps map[string]*ImageFlyweight } var imageFactory *ImageFlyweightFactory func GetImageFlyweightFactory() *ImageFlyweightFactory { if imageFactory == nil { imageFactory = \u0026ImageFlyweightFactory{ maps: make(map[string]*ImageFlyweight), } } return imageFactory } func (f *ImageFlyweightFactory) Get(filename string) *ImageFlyweight { image := f.maps[filename] if image == nil { image = NewImageFlyweight(filename) f.maps[filename] = image } return image } type ImageFlyweight struct { data string } func NewImageFlyweight(filename string) *ImageFlyweight { // Load image file data := fmt.Sprintf(\"image data %s\", filename) return \u0026ImageFlyweight{ data: data, } } func (i *ImageFlyweight) Data() string { return i.data } type ImageViewer struct { *ImageFlyweight } func NewImageViewer(filename string) *ImageViewer { image := GetImageFlyweightFactory().Get(filename) return \u0026ImageViewer{ ImageFlyweight: image, } } func (i *ImageViewer) Display() { fmt.Printf(\"Display: %s\\n\", i.Data()) } package flyweight import \"testing\" func ExampleFlyweight() { viewer := NewImageViewer(\"image1.png\") viewer.Display() // Output: // Display: image data image1.png } func TestFlyweight(t *testing.T) { viewer1 := NewImageViewer(\"image1.png\") viewer2 := NewImageViewer(\"image1.png\") if viewer1.ImageFlyweight != viewer2.ImageFlyweight { t.Fail() } } ","date":"2022-01-22 09:21:00","objectID":"/structural_type/:5:0","tags":["design mode"],"title":"Structural_type","uri":"/structural_type/"},{"categories":["Coding"],"content":"decorator 装饰模式使用对象组合的方式动态改变或增加对象行为。 Go语言借助于匿名组合和非入侵式接口可以很方便实现装饰模式。 使用匿名组合，在装饰器中不必显式定义转调原对象方法。 package decorator type Component interface { Calc() int } type ConcreteComponent struct{} func (*ConcreteComponent) Calc() int { return 0 } type MulDecorator struct { Component num int } func WarpMulDecorator(c Component, num int) Component { return \u0026MulDecorator{ Component: c, num: num, } } func (d *MulDecorator) Calc() int { return d.Component.Calc() * d.num } type AddDecorator struct { Component num int } func WarpAddDecorator(c Component, num int) Component { return \u0026AddDecorator{ Component: c, num: num, } } func (d *AddDecorator) Calc() int { return d.Component.Calc() + d.num } package decorator import \"fmt\" func ExampleDecorator() { var c Component = \u0026ConcreteComponent{} c = WarpAddDecorator(c, 10) c = WarpMulDecorator(c, 8) res := c.Calc() fmt.Printf(\"res %d\\n\", res) // Output: // res 80 } ","date":"2022-01-22 09:21:00","objectID":"/structural_type/:6:0","tags":["design mode"],"title":"Structural_type","uri":"/structural_type/"},{"categories":["Coding"],"content":"bridge 桥接模式分离抽象部分和实现部分。使得两部分独立扩展。 桥接模式类似于策略模式，区别在于策略模式封装一系列算法使得算法可以互相替换。 策略模式使抽象部分和实现部分分离，可以独立变化。 package bridge import \"fmt\" type AbstractMessage interface { SendMessage(text, to string) } type MessageImplementer interface { Send(text, to string) } type MessageSMS struct{} func ViaSMS() MessageImplementer { return \u0026MessageSMS{} } func (*MessageSMS) Send(text, to string) { fmt.Printf(\"send %s to %s via SMS\", text, to) } type MessageEmail struct{} func ViaEmail() MessageImplementer { return \u0026MessageEmail{} } func (*MessageEmail) Send(text, to string) { fmt.Printf(\"send %s to %s via Email\", text, to) } type CommonMessage struct { method MessageImplementer } func NewCommonMessage(method MessageImplementer) *CommonMessage { return \u0026CommonMessage{ method: method, } } func (m *CommonMessage) SendMessage(text, to string) { m.method.Send(text, to) } type UrgencyMessage struct { method MessageImplementer } func NewUrgencyMessage(method MessageImplementer) *UrgencyMessage { return \u0026UrgencyMessage{ method: method, } } func (m *UrgencyMessage) SendMessage(text, to string) { m.method.Send(fmt.Sprintf(\"[Urgency] %s\", text), to) } package bridge func ExampleCommonSMS() { m := NewCommonMessage(ViaSMS()) m.SendMessage(\"have a drink?\", \"bob\") // Output: // send have a drink? to bob via SMS } func ExampleCommonEmail() { m := NewCommonMessage(ViaEmail()) m.SendMessage(\"have a drink?\", \"bob\") // Output: // send have a drink? to bob via Email } func ExampleUrgencySMS() { m := NewUrgencyMessage(ViaSMS()) m.SendMessage(\"have a drink?\", \"bob\") // Output: // send [Urgency] have a drink? to bob via SMS } func ExampleUrgencyEmail() { m := NewUrgencyMessage(ViaEmail()) m.SendMessage(\"have a drink?\", \"bob\") // Output: // send [Urgency] have a drink? to bob via Email } 参考 ","date":"2022-01-22 09:21:00","objectID":"/structural_type/:7:0","tags":["design mode"],"title":"Structural_type","uri":"/structural_type/"},{"categories":["Coding"],"content":"mediator 中介者模式封装对象之间互交，使依赖变的简单，并且使复杂互交简单化，封装在中介者中。 例子中的中介者使用单例模式生成中介者。 中介者的change使用switch判断类型。 package mediator import ( \"fmt\" \"strings\" ) type CDDriver struct { Data string } func (c *CDDriver) ReadData() { c.Data = \"music,image\" fmt.Printf(\"CDDriver: reading data %s\\n\", c.Data) GetMediatorInstance().changed(c) } type CPU struct { Video string Sound string } func (c *CPU) Process(data string) { sp := strings.Split(data, \",\") c.Sound = sp[0] c.Video = sp[1] fmt.Printf(\"CPU: split data with Sound %s, Video %s\\n\", c.Sound, c.Video) GetMediatorInstance().changed(c) } type VideoCard struct { Data string } func (v *VideoCard) Display(data string) { v.Data = data fmt.Printf(\"VideoCard: display %s\\n\", v.Data) GetMediatorInstance().changed(v) } type SoundCard struct { Data string } func (s *SoundCard) Play(data string) { s.Data = data fmt.Printf(\"SoundCard: play %s\\n\", s.Data) GetMediatorInstance().changed(s) } type Mediator struct { CD *CDDriver CPU *CPU Video *VideoCard Sound *SoundCard } var mediator *Mediator func GetMediatorInstance() *Mediator { if mediator == nil { mediator = \u0026Mediator{} } return mediator } func (m *Mediator) changed(i interface{}) { switch inst := i.(type) { case *CDDriver: m.CPU.Process(inst.Data) case *CPU: m.Sound.Play(inst.Sound) m.Video.Display(inst.Video) } } package mediator import \"testing\" func TestMediator(t *testing.T) { mediator := GetMediatorInstance() mediator.CD = \u0026CDDriver{} mediator.CPU = \u0026CPU{} mediator.Video = \u0026VideoCard{} mediator.Sound = \u0026SoundCard{} //Tiggle mediator.CD.ReadData() if mediator.CD.Data != \"music,image\" { t.Fatalf(\"CD unexpect data %s\", mediator.CD.Data) } if mediator.CPU.Sound != \"music\" { t.Fatalf(\"CPU unexpect sound data %s\", mediator.CPU.Sound) } if mediator.CPU.Video != \"image\" { t.Fatalf(\"CPU unexpect video data %s\", mediator.CPU.Video) } if mediator.Video.Data != \"image\" { t.Fatalf(\"VidoeCard unexpect data %s\", mediator.Video.Data) } if mediator.Sound.Data != \"music\" { t.Fatalf(\"SoundCard unexpect data %s\", mediator.Sound.Data) } } ","date":"2022-01-22 09:20:41","objectID":"/behavioral_type/:1:0","tags":["design mode"],"title":"Behavioral_type","uri":"/behavioral_type/"},{"categories":["Coding"],"content":"observer 观察者模式用于触发联动。 一个对象的改变会触发其它观察者的相关动作，而此对象无需关心连动对象的具体实现。 package observer import \"fmt\" type Subject struct { observers []Observer context string } func NewSubject() *Subject { return \u0026Subject{ observers: make([]Observer, 0), } } func (s *Subject) Attach(o Observer) { s.observers = append(s.observers, o) } func (s *Subject) notify() { for _, o := range s.observers { o.Update(s) } } func (s *Subject) UpdateContext(context string) { s.context = context s.notify() } type Observer interface { Update(*Subject) } type Reader struct { name string } func NewReader(name string) *Reader { return \u0026Reader{ name: name, } } func (r *Reader) Update(s *Subject) { fmt.Printf(\"%s receive %s\\n\", r.name, s.context) } package observer func ExampleObserver() { subject := NewSubject() reader1 := NewReader(\"reader1\") reader2 := NewReader(\"reader2\") reader3 := NewReader(\"reader3\") subject.Attach(reader1) subject.Attach(reader2) subject.Attach(reader3) subject.UpdateContext(\"observer mode\") // Output: // reader1 receive observer mode // reader2 receive observer mode // reader3 receive observer mode } ","date":"2022-01-22 09:20:41","objectID":"/behavioral_type/:2:0","tags":["design mode"],"title":"Behavioral_type","uri":"/behavioral_type/"},{"categories":["Coding"],"content":"command 命令模式本质是把某个对象的方法调用封装到对象中，方便传递、存储、调用。 示例中把主板单中的启动(start)方法和重启(reboot)方法封装为命令对象，再传递到主机(box)对象中。于两个按钮进行绑定： 第一个机箱(box1)设置按钮1(button1) 为开机按钮2(button2)为重启。 第二个机箱(box1)设置按钮2(button2) 为开机按钮1(button1)为重启。 从而得到配置灵活性。 除了配置灵活外，使用命令模式还可以用作： 批处理 任务队列 undo, redo 等把具体命令封装到对象中使用的场合 package command import \"fmt\" type Command interface { Execute() } type StartCommand struct { mb *MotherBoard } func NewStartCommand(mb *MotherBoard) *StartCommand { return \u0026StartCommand{ mb: mb, } } func (c *StartCommand) Execute() { c.mb.Start() } type RebootCommand struct { mb *MotherBoard } func NewRebootCommand(mb *MotherBoard) *RebootCommand { return \u0026RebootCommand{ mb: mb, } } func (c *RebootCommand) Execute() { c.mb.Reboot() } type MotherBoard struct{} func (*MotherBoard) Start() { fmt.Print(\"system starting\\n\") } func (*MotherBoard) Reboot() { fmt.Print(\"system rebooting\\n\") } type Box struct { button1 Command button2 Command } func NewBox(button1, button2 Command) *Box { return \u0026Box{ button1: button1, button2: button2, } } func (b *Box) PressButton1() { b.button1.Execute() } func (b *Box) PressButton2() { b.button2.Execute() } package command func ExampleCommand() { mb := \u0026MotherBoard{} startCommand := NewStartCommand(mb) rebootCommand := NewRebootCommand(mb) box1 := NewBox(startCommand, rebootCommand) box1.PressButton1() box1.PressButton2() box2 := NewBox(rebootCommand, startCommand) box2.PressButton1() box2.PressButton2() // Output: // system starting // system rebooting // system rebooting // system starting } ","date":"2022-01-22 09:20:41","objectID":"/behavioral_type/:3:0","tags":["design mode"],"title":"Behavioral_type","uri":"/behavioral_type/"},{"categories":["Coding"],"content":"iterator 迭代器模式用于使用相同方式送代不同类型集合或者隐藏集合类型的具体实现。 可以使用迭代器模式使遍历同时应用送代策略，如请求新对象、过滤、处理对象等。 package iterator import \"fmt\" type Aggregate interface { Iterator() Iterator } type Iterator interface { First() IsDone() bool Next() interface{} } type Numbers struct { start, end int } func NewNumbers(start, end int) *Numbers { return \u0026Numbers{ start: start, end: end, } } func (n *Numbers) Iterator() Iterator { return \u0026NumbersIterator{ numbers: n, next: n.start, } } type NumbersIterator struct { numbers *Numbers next int } func (i *NumbersIterator) First() { i.next = i.numbers.start } func (i *NumbersIterator) IsDone() bool { return i.next \u003e i.numbers.end } func (i *NumbersIterator) Next() interface{} { if !i.IsDone() { next := i.next i.next++ return next } return nil } func IteratorPrint(i Iterator) { for i.First(); !i.IsDone(); { c := i.Next() fmt.Printf(\"%#v\\n\", c) } } package iterator func ExampleIterator() { var aggregate Aggregate aggregate = NewNumbers(1, 10) IteratorPrint(aggregate.Iterator()) // Output: // 1 // 2 // 3 // 4 // 5 // 6 // 7 // 8 // 9 // 10 } ","date":"2022-01-22 09:20:41","objectID":"/behavioral_type/:4:0","tags":["design mode"],"title":"Behavioral_type","uri":"/behavioral_type/"},{"categories":["Coding"],"content":"template method 模版方法模式使用继承机制，把通用步骤和通用方法放到父类中，把具体实现延迟到子类中实现。使得实现符合开闭原则。 如实例代码中通用步骤在父类中实现（准备、下载、保存、收尾）下载和保存的具体实现留到子类中，并且提供 保存方法的默认实现。 因为Golang不提供继承机制，需要使用匿名组合模拟实现继承。 此处需要注意：因为父类需要调用子类方法，所以子类需要匿名组合父类的同时，父类需要持有子类的引用。 package templatemethod import \"fmt\" type Downloader interface { Download(uri string) } type template struct { implement uri string } type implement interface { download() save() } func newTemplate(impl implement) *template { return \u0026template{ implement: impl, } } func (t *template) Download(uri string) { t.uri = uri fmt.Print(\"prepare downloading\\n\") t.implement.download() t.implement.save() fmt.Print(\"finish downloading\\n\") } func (t *template) save() { fmt.Print(\"default save\\n\") } type HTTPDownloader struct { *template } func NewHTTPDownloader() Downloader { downloader := \u0026HTTPDownloader{} template := newTemplate(downloader) downloader.template = template return downloader } func (d *HTTPDownloader) download() { fmt.Printf(\"download %s via http\\n\", d.uri) } func (*HTTPDownloader) save() { fmt.Printf(\"http save\\n\") } type FTPDownloader struct { *template } func NewFTPDownloader() Downloader { downloader := \u0026FTPDownloader{} template := newTemplate(downloader) downloader.template = template return downloader } func (d *FTPDownloader) download() { fmt.Printf(\"download %s via ftp\\n\", d.uri) } package templatemethod func ExampleHTTPDownloader() { var downloader Downloader = NewHTTPDownloader() downloader.Download(\"http://example.com/abc.zip\") // Output: // prepare downloading // download http://example.com/abc.zip via http // http save // finish downloading } func ExampleFTPDownloader() { var downloader Downloader = NewFTPDownloader() downloader.Download(\"ftp://example.com/abc.zip\") // Output: // prepare downloading // download ftp://example.com/abc.zip via ftp // default save // finish downloading } ","date":"2022-01-22 09:20:41","objectID":"/behavioral_type/:5:0","tags":["design mode"],"title":"Behavioral_type","uri":"/behavioral_type/"},{"categories":["Coding"],"content":"strategy 定义一系列算法，让这些算法在运行时可以互换，使得分离算法，符合开闭原则。 package strategy import \"fmt\" type Payment struct { context *PaymentContext strategy PaymentStrategy } type PaymentContext struct { Name, CardID string Money int } func NewPayment(name, cardid string, money int, strategy PaymentStrategy) *Payment { return \u0026Payment{ context: \u0026PaymentContext{ Name: name, CardID: cardid, Money: money, }, strategy: strategy, } } func (p *Payment) Pay() { p.strategy.Pay(p.context) } type PaymentStrategy interface { Pay(*PaymentContext) } type Cash struct{} func (*Cash) Pay(ctx *PaymentContext) { fmt.Printf(\"Pay $%d to %s by cash\", ctx.Money, ctx.Name) } type Bank struct{} func (*Bank) Pay(ctx *PaymentContext) { fmt.Printf(\"Pay $%d to %s by bank account %s\", ctx.Money, ctx.Name, ctx.CardID) } package strategy func ExamplePayByCash() { payment := NewPayment(\"Ada\", \"\", 123, \u0026Cash{}) payment.Pay() // Output: // Pay $123 to Ada by cash } func ExamplePayByBank() { payment := NewPayment(\"Bob\", \"0002\", 888, \u0026Bank{}) payment.Pay() // Output: // Pay $888 to Bob by bank account 0002 } ","date":"2022-01-22 09:20:41","objectID":"/behavioral_type/:6:0","tags":["design mode"],"title":"Behavioral_type","uri":"/behavioral_type/"},{"categories":["Coding"],"content":"state 状态模式用于分离状态和行为。 package state import \"fmt\" type Week interface { Today() Next(*DayContext) } type DayContext struct { today Week } func NewDayContext() *DayContext { return \u0026DayContext{ today: \u0026Sunday{}, } } func (d *DayContext) Today() { d.today.Today() } func (d *DayContext) Next() { d.today.Next(d) } type Sunday struct{} func (*Sunday) Today() { fmt.Printf(\"Sunday\\n\") } func (*Sunday) Next(ctx *DayContext) { ctx.today = \u0026Monday{} } type Monday struct{} func (*Monday) Today() { fmt.Printf(\"Monday\\n\") } func (*Monday) Next(ctx *DayContext) { ctx.today = \u0026Tuesday{} } type Tuesday struct{} func (*Tuesday) Today() { fmt.Printf(\"Tuesday\\n\") } func (*Tuesday) Next(ctx *DayContext) { ctx.today = \u0026Wednesday{} } type Wednesday struct{} func (*Wednesday) Today() { fmt.Printf(\"Wednesday\\n\") } func (*Wednesday) Next(ctx *DayContext) { ctx.today = \u0026Thursday{} } type Thursday struct{} func (*Thursday) Today() { fmt.Printf(\"Thursday\\n\") } func (*Thursday) Next(ctx *DayContext) { ctx.today = \u0026Friday{} } type Friday struct{} func (*Friday) Today() { fmt.Printf(\"Friday\\n\") } func (*Friday) Next(ctx *DayContext) { ctx.today = \u0026Saturday{} } type Saturday struct{} func (*Saturday) Today() { fmt.Printf(\"Saturday\\n\") } func (*Saturday) Next(ctx *DayContext) { ctx.today = \u0026Sunday{} } package state func ExampleWeek() { ctx := NewDayContext() todayAndNext := func() { ctx.Today() ctx.Next() } for i := 0; i \u003c 8; i++ { todayAndNext() } // Output: // Sunday // Monday // Tuesday // Wednesday // Thursday // Friday // Saturday // Sunday } ","date":"2022-01-22 09:20:41","objectID":"/behavioral_type/:7:0","tags":["design mode"],"title":"Behavioral_type","uri":"/behavioral_type/"},{"categories":["Coding"],"content":"memento 备忘录模式用于保存程序内部状态到外部，又不希望暴露内部状态的情形。 程序内部状态使用窄接口传递给外部进行存储，从而不暴露程序实现细节。 备忘录模式同时可以离线保存内部状态，如保存到数据库，文件等。 package memento import \"fmt\" type Memento interface{} type Game struct { hp, mp int } type gameMemento struct { hp, mp int } func (g *Game) Play(mpDelta, hpDelta int) { g.mp += mpDelta g.hp += hpDelta } func (g *Game) Save() Memento { return \u0026gameMemento{ hp: g.hp, mp: g.mp, } } func (g *Game) Load(m Memento) { gm := m.(*gameMemento) g.mp = gm.mp g.hp = gm.hp } func (g *Game) Status() { fmt.Printf(\"Current HP:%d, MP:%d\\n\", g.hp, g.mp) } package memento func ExampleGame() { game := \u0026Game{ hp: 10, mp: 10, } game.Status() progress := game.Save() game.Play(-2, -3) game.Status() game.Load(progress) game.Status() // Output: // Current HP:10, MP:10 // Current HP:7, MP:8 // Current HP:10, MP:10 } ","date":"2022-01-22 09:20:41","objectID":"/behavioral_type/:8:0","tags":["design mode"],"title":"Behavioral_type","uri":"/behavioral_type/"},{"categories":["Coding"],"content":"iterpreter 解释器模式定义一套语言文法，并设计该语言解释器，使用户能使用特定文法控制解释器行为。 解释器模式的意义在于，它分离多种复杂功能的实现，每个功能只需关注自身的解释。 对于调用者不用关心内部的解释器的工作，只需要用简单的方式组合命令就可以。 package interpreter import ( \"strconv\" \"strings\" ) type Node interface { Interpret() int } type ValNode struct { val int } func (n *ValNode) Interpret() int { return n.val } type AddNode struct { left, right Node } func (n *AddNode) Interpret() int { return n.left.Interpret() + n.right.Interpret() } type MinNode struct { left, right Node } func (n *MinNode) Interpret() int { return n.left.Interpret() - n.right.Interpret() } type Parser struct { exp []string index int prev Node } func (p *Parser) Parse(exp string) { p.exp = strings.Split(exp, \" \") for { if p.index \u003e= len(p.exp) { return } switch p.exp[p.index] { case \"+\": p.prev = p.newAddNode() case \"-\": p.prev = p.newMinNode() default: p.prev = p.newValNode() } } } func (p *Parser) newAddNode() Node { p.index++ return \u0026AddNode{ left: p.prev, right: p.newValNode(), } } func (p *Parser) newMinNode() Node { p.index++ return \u0026MinNode{ left: p.prev, right: p.newValNode(), } } func (p *Parser) newValNode() Node { v, _ := strconv.Atoi(p.exp[p.index]) p.index++ return \u0026ValNode{ val: v, } } func (p *Parser) Result() Node { return p.prev } package interpreter import \"testing\" func TestInterpreter(t *testing.T) { p := \u0026Parser{} p.Parse(\"1 + 2 + 3 - 4 + 5 - 6\") res := p.Result().Interpret() expect := 1 if res != expect { t.Fatalf(\"expect %d got %d\", expect, res) } } ","date":"2022-01-22 09:20:41","objectID":"/behavioral_type/:9:0","tags":["design mode"],"title":"Behavioral_type","uri":"/behavioral_type/"},{"categories":["Coding"],"content":"chain of responsibility 职责链模式用于分离不同职责，并且动态组合相关职责。 Golang实现职责链模式时候，因为没有继承的支持，使用链对象包涵职责的方式，即： 链对象包含当前职责对象以及下一个职责链。 职责对象提供接口表示是否能处理对应请求。 职责对象提供处理函数处理相关职责。 同时可在职责链类中实现职责接口相关函数，使职责链对象可以当做一般职责对象是用。 package chain import \"fmt\" type Manager interface { HaveRight(money int) bool HandleFeeRequest(name string, money int) bool } type RequestChain struct { Manager successor *RequestChain } func (r *RequestChain) SetSuccessor(m *RequestChain) { r.successor = m } func (r *RequestChain) HandleFeeRequest(name string, money int) bool { if r.Manager.HaveRight(money) { return r.Manager.HandleFeeRequest(name, money) } if r.successor != nil { return r.successor.HandleFeeRequest(name, money) } return false } func (r *RequestChain) HaveRight(money int) bool { return true } type ProjectManager struct{} func NewProjectManagerChain() *RequestChain { return \u0026RequestChain{ Manager: \u0026ProjectManager{}, } } func (*ProjectManager) HaveRight(money int) bool { return money \u003c 500 } func (*ProjectManager) HandleFeeRequest(name string, money int) bool { if name == \"bob\" { fmt.Printf(\"Project manager permit %s %d fee request\\n\", name, money) return true } fmt.Printf(\"Project manager don't permit %s %d fee request\\n\", name, money) return false } type DepManager struct{} func NewDepManagerChain() *RequestChain { return \u0026RequestChain{ Manager: \u0026DepManager{}, } } func (*DepManager) HaveRight(money int) bool { return money \u003c 5000 } func (*DepManager) HandleFeeRequest(name string, money int) bool { if name == \"tom\" { fmt.Printf(\"Dep manager permit %s %d fee request\\n\", name, money) return true } fmt.Printf(\"Dep manager don't permit %s %d fee request\\n\", name, money) return false } type GeneralManager struct{} func NewGeneralManagerChain() *RequestChain { return \u0026RequestChain{ Manager: \u0026GeneralManager{}, } } func (*GeneralManager) HaveRight(money int) bool { return true } func (*GeneralManager) HandleFeeRequest(name string, money int) bool { if name == \"ada\" { fmt.Printf(\"General manager permit %s %d fee request\\n\", name, money) return true } fmt.Printf(\"General manager don't permit %s %d fee request\\n\", name, money) return false } package chain func ExampleChain() { c1 := NewProjectManagerChain() c2 := NewDepManagerChain() c3 := NewGeneralManagerChain() c1.SetSuccessor(c2) c2.SetSuccessor(c3) var c Manager = c1 c.HandleFeeRequest(\"bob\", 400) c.HandleFeeRequest(\"tom\", 1400) c.HandleFeeRequest(\"ada\", 10000) c.HandleFeeRequest(\"floar\", 400) // Output: // Project manager permit bob 400 fee request // Dep manager permit tom 1400 fee request // General manager permit ada 10000 fee request // Project manager don't permit floar 400 fee request } ","date":"2022-01-22 09:20:41","objectID":"/behavioral_type/:10:0","tags":["design mode"],"title":"Behavioral_type","uri":"/behavioral_type/"},{"categories":["Coding"],"content":"visitor 访问者模式可以给一系列对象透明的添加功能，并且把相关代码封装到一个类中。 对象只要预留访问者接口Accept则后期为对象添加功能的时候就不需要改动对象。 package visitor import \"fmt\" type Customer interface { Accept(Visitor) } type Visitor interface { Visit(Customer) } type EnterpriseCustomer struct { name string } type CustomerCol struct { customers []Customer } func (c *CustomerCol) Add(customer Customer) { c.customers = append(c.customers, customer) } func (c *CustomerCol) Accept(visitor Visitor) { for _, customer := range c.customers { customer.Accept(visitor) } } func NewEnterpriseCustomer(name string) *EnterpriseCustomer { return \u0026EnterpriseCustomer{ name: name, } } func (c *EnterpriseCustomer) Accept(visitor Visitor) { visitor.Visit(c) } type IndividualCustomer struct { name string } func NewIndividualCustomer(name string) *IndividualCustomer { return \u0026IndividualCustomer{ name: name, } } func (c *IndividualCustomer) Accept(visitor Visitor) { visitor.Visit(c) } type ServiceRequestVisitor struct{} func (*ServiceRequestVisitor) Visit(customer Customer) { switch c := customer.(type) { case *EnterpriseCustomer: fmt.Printf(\"serving enterprise customer %s\\n\", c.name) case *IndividualCustomer: fmt.Printf(\"serving individual customer %s\\n\", c.name) } } // only for enterprise type AnalysisVisitor struct{} func (*AnalysisVisitor) Visit(customer Customer) { switch c := customer.(type) { case *EnterpriseCustomer: fmt.Printf(\"analysis enterprise customer %s\\n\", c.name) } } package visitor func ExampleRequestVisitor() { c := \u0026CustomerCol{} c.Add(NewEnterpriseCustomer(\"A company\")) c.Add(NewEnterpriseCustomer(\"B company\")) c.Add(NewIndividualCustomer(\"bob\")) c.Accept(\u0026ServiceRequestVisitor{}) // Output: // serving enterprise customer A company // serving enterprise customer B company // serving individual customer bob } func ExampleAnalysis() { c := \u0026CustomerCol{} c.Add(NewEnterpriseCustomer(\"A company\")) c.Add(NewIndividualCustomer(\"bob\")) c.Add(NewEnterpriseCustomer(\"B company\")) c.Accept(\u0026AnalysisVisitor{}) // Output: // analysis enterprise customer A company // analysis enterprise customer B company } 参考 ","date":"2022-01-22 09:20:41","objectID":"/behavioral_type/:11:0","tags":["design mode"],"title":"Behavioral_type","uri":"/behavioral_type/"},{"categories":["Coding"],"content":"simple factory go 语言没有构造函数一说，所以一般会定义NewXXX函数来初始化相关类。 NewXXX 函数根据参数返回不同接口时就是简单工厂模式。 在这个simplefactory包中只有API 接口和NewAPI函数为包外可见，封装了实现细节。 package simplefactory import \"fmt\" //API is interface type API interface { Say(name string) string } //NewAPI return Api instance by type func NewAPI(t int) API { if t == 1 { return \u0026hiAPI{} } else if t == 2 { return \u0026helloAPI{} } return nil } //hiAPI is one of API implement type hiAPI struct{} //Say hi to name func (*hiAPI) Say(name string) string { return fmt.Sprintf(\"Hi, %s\", name) } //HelloAPI is another API implement type helloAPI struct{} //Say hello to name func (*helloAPI) Say(name string) string { return fmt.Sprintf(\"Hello, %s\", name) } package simplefactory import \"testing\" //TestType1 test get hiapi with factory func TestType1(t *testing.T) { api := NewAPI(1) s := api.Say(\"Tom\") if s != \"Hi, Tom\" { t.Fatal(\"Type1 test fail\") } } func TestType2(t *testing.T) { api := NewAPI(2) s := api.Say(\"Tom\") if s != \"Hello, Tom\" { t.Fatal(\"Type2 test fail\") } } ","date":"2022-01-22 09:19:30","objectID":"/create_type/:1:0","tags":["design mode"],"title":"Create_type","uri":"/create_type/"},{"categories":["Coding"],"content":"factory method 工厂方法模式使用子类的方式延迟生成对象到子类中实现。 Go中不存在继承 所以使用匿名组合来实现 package factorymethod //Operator 是被封装的实际类接口 type Operator interface { SetA(int) SetB(int) Result() int } //OperatorFactory 是工厂接口 type OperatorFactory interface { Create() Operator } //OperatorBase 是Operator 接口实现的基类，封装公用方法 type OperatorBase struct { a, b int } //SetA 设置 A func (o *OperatorBase) SetA(a int) { o.a = a } //SetB 设置 B func (o *OperatorBase) SetB(b int) { o.b = b } //PlusOperatorFactory 是 PlusOperator 的工厂类 type PlusOperatorFactory struct{} func (PlusOperatorFactory) Create() Operator { return \u0026PlusOperator{ OperatorBase: \u0026OperatorBase{}, } } //PlusOperator Operator 的实际加法实现 type PlusOperator struct { *OperatorBase } //Result 获取结果 func (o PlusOperator) Result() int { return o.a + o.b } //MinusOperatorFactory 是 MinusOperator 的工厂类 type MinusOperatorFactory struct{} func (MinusOperatorFactory) Create() Operator { return \u0026MinusOperator{ OperatorBase: \u0026OperatorBase{}, } } //MinusOperator Operator 的实际减法实现 type MinusOperator struct { *OperatorBase } //Result 获取结果 func (o MinusOperator) Result() int { return o.a - o.b } package factorymethod import \"testing\" func compute(factory OperatorFactory, a, b int) int { op := factory.Create() op.SetA(a) op.SetB(b) return op.Result() } func TestOperator(t *testing.T) { var ( factory OperatorFactory ) factory = PlusOperatorFactory{} if compute(factory, 1, 2) != 3 { t.Fatal(\"error with factory method pattern\") } factory = MinusOperatorFactory{} if compute(factory, 4, 2) != 2 { t.Fatal(\"error with factory method pattern\") } } ","date":"2022-01-22 09:19:30","objectID":"/create_type/:2:0","tags":["design mode"],"title":"Create_type","uri":"/create_type/"},{"categories":["Coding"],"content":"abstract factory 抽象工厂模式用于生成产品族的工厂，所生成的对象是有关联的。 如果抽象工厂退化成生成的对象无关联则成为工厂函数模式。 比如本例子中使用RDB和XML存储订单信息，抽象工厂分别能生成相关的主订单信息和订单详情信息。 如果业务逻辑中需要替换使用的时候只需要改动工厂函数相关的类就能替换使用不同的存储方式了。 package abstractfactory import \"fmt\" //OrderMainDAO 为订单主记录 type OrderMainDAO interface { SaveOrderMain() } //OrderDetailDAO 为订单详情纪录 type OrderDetailDAO interface { SaveOrderDetail() } //DAOFactory DAO 抽象模式工厂接口 type DAOFactory interface { CreateOrderMainDAO() OrderMainDAO CreateOrderDetailDAO() OrderDetailDAO } //RDBMainDAP 为关系型数据库的OrderMainDAO实现 type RDBMainDAO struct{} //SaveOrderMain ... func (*RDBMainDAO) SaveOrderMain() { fmt.Print(\"rdb main save\\n\") } //RDBDetailDAO 为关系型数据库的OrderDetailDAO实现 type RDBDetailDAO struct{} // SaveOrderDetail ... func (*RDBDetailDAO) SaveOrderDetail() { fmt.Print(\"rdb detail save\\n\") } //RDBDAOFactory 是RDB 抽象工厂实现 type RDBDAOFactory struct{} func (*RDBDAOFactory) CreateOrderMainDAO() OrderMainDAO { return \u0026RDBMainDAO{} } func (*RDBDAOFactory) CreateOrderDetailDAO() OrderDetailDAO { return \u0026RDBDetailDAO{} } //XMLMainDAO XML存储 type XMLMainDAO struct{} //SaveOrderMain ... func (*XMLMainDAO) SaveOrderMain() { fmt.Print(\"xml main save\\n\") } //XMLDetailDAO XML存储 type XMLDetailDAO struct{} // SaveOrderDetail ... func (*XMLDetailDAO) SaveOrderDetail() { fmt.Print(\"xml detail save\") } //XMLDAOFactory 是RDB 抽象工厂实现 type XMLDAOFactory struct{} func (*XMLDAOFactory) CreateOrderMainDAO() OrderMainDAO { return \u0026XMLMainDAO{} } func (*XMLDAOFactory) CreateOrderDetailDAO() OrderDetailDAO { return \u0026XMLDetailDAO{} } package abstractfactory func getMainAndDetail(factory DAOFactory) { factory.CreateOrderMainDAO().SaveOrderMain() factory.CreateOrderDetailDAO().SaveOrderDetail() } func ExampleRdbFactory() { var factory DAOFactory factory = \u0026RDBDAOFactory{} getMainAndDetail(factory) // Output: // rdb main save // rdb detail save } func ExampleXmlFactory() { var factory DAOFactory factory = \u0026XMLDAOFactory{} getMainAndDetail(factory) // Output: // xml main save // xml detail save } ","date":"2022-01-22 09:19:30","objectID":"/create_type/:3:0","tags":["design mode"],"title":"Create_type","uri":"/create_type/"},{"categories":["Coding"],"content":"builder package builder //Builder 是生成器接口 type Builder interface { Part1() Part2() Part3() } type Director struct { builder Builder } // NewDirector ... func NewDirector(builder Builder) *Director { return \u0026Director{ builder: builder, } } //Construct Product func (d *Director) Construct() { d.builder.Part1() d.builder.Part2() d.builder.Part3() } type Builder1 struct { result string } func (b *Builder1) Part1() { b.result += \"1\" } func (b *Builder1) Part2() { b.result += \"2\" } func (b *Builder1) Part3() { b.result += \"3\" } func (b *Builder1) GetResult() string { return b.result } type Builder2 struct { result int } func (b *Builder2) Part1() { b.result += 1 } func (b *Builder2) Part2() { b.result += 2 } func (b *Builder2) Part3() { b.result += 3 } func (b *Builder2) GetResult() int { return b.result } package builder import \"testing\" func TestBuilder1(t *testing.T) { builder := \u0026Builder1{} director := NewDirector(builder) director.Construct() res := builder.GetResult() if res != \"123\" { t.Fatalf(\"Builder1 fail expect 123 acture %s\", res) } } func TestBuilder2(t *testing.T) { builder := \u0026Builder2{} director := NewDirector(builder) director.Construct() res := builder.GetResult() if res != 6 { t.Fatalf(\"Builder2 fail expect 6 acture %d\", res) } } ","date":"2022-01-22 09:19:30","objectID":"/create_type/:4:0","tags":["design mode"],"title":"Create_type","uri":"/create_type/"},{"categories":["Coding"],"content":"prototype 原型模式使对象能复制自身，并且暴露到接口中，使客户端面向接口编程时，不知道接口实际对象的情况下生成新的对象。 原型模式配合原型管理器使用，使得客户端在不知道具体类的情况下，通过接口管理器得到新的实例，并且包含部分预设定配置。 package prototype //Cloneable 是原型对象需要实现的接口 type Cloneable interface { Clone() Cloneable } type PrototypeManager struct { prototypes map[string]Cloneable } func NewPrototypeManager() *PrototypeManager { return \u0026PrototypeManager{ prototypes: make(map[string]Cloneable), } } func (p *PrototypeManager) Get(name string) Cloneable { return p.prototypes[name].Clone() } func (p *PrototypeManager) Set(name string, prototype Cloneable) { p.prototypes[name] = prototype } package prototype import \"testing\" var manager *PrototypeManager type Type1 struct { name string } func (t *Type1) Clone() Cloneable { tc := *t return \u0026tc } type Type2 struct { name string } func (t *Type2) Clone() Cloneable { tc := *t return \u0026tc } func TestClone(t *testing.T) { t1 := manager.Get(\"t1\") t2 := t1.Clone() if t1 == t2 { t.Fatal(\"error! get clone not working\") } } func TestCloneFromManager(t *testing.T) { c := manager.Get(\"t1\").Clone() t1 := c.(*Type1) if t1.name != \"type1\" { t.Fatal(\"error\") } } func init() { manager = NewPrototypeManager() t1 := \u0026Type1{ name: \"type1\", } manager.Set(\"t1\", t1) } ","date":"2022-01-22 09:19:30","objectID":"/create_type/:5:0","tags":["design mode"],"title":"Create_type","uri":"/create_type/"},{"categories":["Coding"],"content":"singleton 使用懒惰模式的单例模式，使用双重检查加锁保证线程安全 package singleton import \"sync\" // Singleton 是单例模式接口，导出的 // 通过该接口可以避免 GetInstance 返回一个包私有类型的指针 type Singleton interface { foo() } // singleton 是单例模式类，包私有的 type singleton struct{} func (s singleton) foo() {} var ( instance *singleton once sync.Once ) //GetInstance 用于获取单例模式对象 func GetInstance() Singleton { once.Do(func() { instance = \u0026singleton{} }) return instance } package singleton import ( \"sync\" \"testing\" ) const parCount = 100 func TestSingleton(t *testing.T) { ins1 := GetInstance() ins2 := GetInstance() if ins1 != ins2 { t.Fatal(\"instance is not equal\") } } func TestParallelSingleton(t *testing.T) { start := make(chan struct{}) wg := sync.WaitGroup{} wg.Add(parCount) instances := [parCount]Singleton{} for i := 0; i \u003c parCount; i++ { go func(index int) { //协程阻塞，等待channel被关闭才能继续运行 \u003c-start instances[index] = GetInstance() wg.Done() }(i) } //关闭channel，所有协程同时开始运行，实现并行(parallel) close(start) wg.Wait() for i := 1; i \u003c parCount; i++ { if instances[i] != instances[i-1] { t.Fatal(\"instance is not equal\") } } } 参考 ","date":"2022-01-22 09:19:30","objectID":"/create_type/:6:0","tags":["design mode"],"title":"Create_type","uri":"/create_type/"},{"categories":["Catalogue","Coding"],"content":" 学习代码随想录笔记 ","date":"2022-01-19 10:06:22","objectID":"/algo_catalogue/:0:0","tags":["catalogue","data structure"],"title":"Algo_catalogue","uri":"/algo_catalogue/"},{"categories":["Catalogue","Coding"],"content":"algorithm_array algorithm_backTracking algorithm_binaryTree algorithm_doublePointer algorithm_dp algorithm_find algorithm_greedy algorithm_hashTable algorithm_linkedList algorithm_sort algorithm_stackAndQueue algorithm_string ","date":"2022-01-19 10:06:22","objectID":"/algo_catalogue/:1:0","tags":["catalogue","data structure"],"title":"Algo_catalogue","uri":"/algo_catalogue/"},{"categories":["Go"],"content":"常用标准库 ","date":"2022-01-12 22:17:07","objectID":"/go_base_09/:0:0","tags":["go grammar"],"title":"Go_base_09","uri":"/go_base_09/"},{"categories":["Go"],"content":"前言 ","date":"2022-01-12 22:17:07","objectID":"/go_base_09/:1:0","tags":["go grammar"],"title":"Go_base_09","uri":"/go_base_09/"},{"categories":["Go"],"content":"包复用 package: 基本复⽤模块单元：以⾸字⺟⼤写来表明可被包外代码访问 代码的 package 可以和所在的⽬录不⼀致 同⼀⽬录⾥的 Go 代码的 package 要保持⼀致 ","date":"2022-01-12 22:17:07","objectID":"/go_base_09/:1:1","tags":["go grammar"],"title":"Go_base_09","uri":"/go_base_09/"},{"categories":["Go"],"content":"依赖管理 Go 未解决的依赖问题： 同⼀环境下，不同项⽬使⽤同⼀包的不同版本 ⽆法管理对包的特定版本的依赖 vendor 路径 随着 Go 1.5 release 版本的发布，vendor ⽬录被添加到除了 GOPATH 和GOROOT 之外的依赖⽬录查找的解决⽅案。在 Go 1.6 之前，你需要⼿动的设置环境变量查找依赖包路径的解决⽅案如下： 当前包下的 vendor ⽬录 向上级⽬录查找，直到找到 src 下的 vendor ⽬录 在 GOPATH 下⾯查找依赖包 在 GOROOT ⽬录下查找 常用依赖管理工具： godep glide dep ","date":"2022-01-12 22:17:07","objectID":"/go_base_09/:1:2","tags":["go grammar"],"title":"Go_base_09","uri":"/go_base_09/"},{"categories":["Go"],"content":"fmt ","date":"2022-01-12 22:17:07","objectID":"/go_base_09/:2:0","tags":["go grammar"],"title":"Go_base_09","uri":"/go_base_09/"},{"categories":["Go"],"content":"Time ","date":"2022-01-12 22:17:07","objectID":"/go_base_09/:3:0","tags":["go grammar"],"title":"Go_base_09","uri":"/go_base_09/"},{"categories":["Go"],"content":"Flag ","date":"2022-01-12 22:17:07","objectID":"/go_base_09/:4:0","tags":["go grammar"],"title":"Go_base_09","uri":"/go_base_09/"},{"categories":["Go"],"content":"Log ","date":"2022-01-12 22:17:07","objectID":"/go_base_09/:5:0","tags":["go grammar"],"title":"Go_base_09","uri":"/go_base_09/"},{"categories":["Go"],"content":"IO操作 ","date":"2022-01-12 22:17:07","objectID":"/go_base_09/:6:0","tags":["go grammar"],"title":"Go_base_09","uri":"/go_base_09/"},{"categories":["Go"],"content":"Strconv ","date":"2022-01-12 22:17:07","objectID":"/go_base_09/:7:0","tags":["go grammar"],"title":"Go_base_09","uri":"/go_base_09/"},{"categories":["Go"],"content":"Template ","date":"2022-01-12 22:17:07","objectID":"/go_base_09/:8:0","tags":["go grammar"],"title":"Go_base_09","uri":"/go_base_09/"},{"categories":["Go"],"content":"Http ","date":"2022-01-12 22:17:07","objectID":"/go_base_09/:9:0","tags":["go grammar"],"title":"Go_base_09","uri":"/go_base_09/"},{"categories":["Go"],"content":"Context ","date":"2022-01-12 22:17:07","objectID":"/go_base_09/:10:0","tags":["go grammar"],"title":"Go_base_09","uri":"/go_base_09/"},{"categories":["Go"],"content":"数据格式 ","date":"2022-01-12 22:17:07","objectID":"/go_base_09/:11:0","tags":["go grammar"],"title":"Go_base_09","uri":"/go_base_09/"},{"categories":["Go"],"content":"反射和unsafe ","date":"2022-01-12 22:17:07","objectID":"/go_base_09/:12:0","tags":["go grammar"],"title":"Go_base_09","uri":"/go_base_09/"},{"categories":["Go"],"content":"反射编程 反射是指在程序运行期对程序本身进行访问和修改的能力 变量的内在机制 变量包含类型信息和值信息 var arr [10]int arr[0] = 10 类型信息：是静态的元信息，是预先定义好的 值信息：是程序运行过程中动态改变的 反射的使用 reflect包封装了反射相关的方法 获取类型信息：reflect.TypeOf，是静态的 获取值信息：reflect.ValueOf，是动态的 空接口与反射 反射可以在运行时动态获取程序的各种详细信息 反射获取interface类型信息 package main import ( \"fmt\" \"reflect\" ) //反射获取interface类型信息 func reflect_type(a interface{}) { t := reflect.TypeOf(a) fmt.Println(\"类型是：\", t) // kind()可以获取具体类型 k := t.Kind() fmt.Println(k) switch k { case reflect.Float64: fmt.Printf(\"a is float64\\n\") case reflect.String: fmt.Println(\"string\") } } func main() { var x float64 = 3.4 reflect_type(x) } 反射获取interface值信息 package main import ( \"fmt\" \"reflect\" ) //反射获取interface值信息 func reflect_value(a interface{}) { v := reflect.ValueOf(a) fmt.Println(v) k := v.Kind() fmt.Println(k) switch k { case reflect.Float64: fmt.Println(\"a是：\", v.Float()) } } func main() { var x float64 = 3.4 reflect_value(x) } 反射修改值信息 package main import ( \"fmt\" \"reflect\" ) //反射修改值 func reflect_set_value(a interface{}) { v := reflect.ValueOf(a) k := v.Kind() switch k { case reflect.Float64: // 反射修改值 v.SetFloat(6.9) fmt.Println(\"a is \", v.Float()) case reflect.Ptr: // Elem()获取地址指向的值 v.Elem().SetFloat(7.9) fmt.Println(\"case:\", v.Elem().Float()) // 地址 fmt.Println(v.Pointer()) } } func main() { var x float64 = 3.4 // 反射认为下面是指针类型，不是float类型 reflect_set_value(\u0026x) fmt.Println(\"main:\", x) } 结构体与反射 查看类型字段和方法 package main import ( \"fmt\" \"reflect\" ) // 定义结构体 type User struct { Id int Name string Age int } // 绑方法 func (u User) Hello() { fmt.Println(\"Hello\") } // 传入interface{} func Poni(o interface{}) { t := reflect.TypeOf(o) fmt.Println(\"类型：\", t) fmt.Println(\"字符串类型：\", t.Name()) // 获取值 v := reflect.ValueOf(o) fmt.Println(v) // 可以获取所有属性 // 获取结构体字段个数：t.NumField() for i := 0; i \u003c t.NumField(); i++ { // 取每个字段 f := t.Field(i) fmt.Printf(\"%s : %v\", f.Name, f.Type) // 获取字段的值信息 // Interface()：获取字段对应的值 val := v.Field(i).Interface() fmt.Println(\"val :\", val) } fmt.Println(\"=================方法====================\") for i := 0; i \u003c t.NumMethod(); i++ { m := t.Method(i) fmt.Println(m.Name) fmt.Println(m.Type) } } func main() { u := User{1, \"zs\", 20} Poni(u) } 查看匿名字段 package main import ( \"fmt\" \"reflect\" ) // 定义结构体 type User struct { Id int Name string Age int } // 匿名字段 type Boy struct { User Addr string } func main() { m := Boy{User{1, \"zs\", 20}, \"bj\"} t := reflect.TypeOf(m) fmt.Println(t) // Anonymous：匿名 fmt.Printf(\"%#v\\n\", t.Field(0)) // 值信息 fmt.Printf(\"%#v\\n\", reflect.ValueOf(m).Field(0)) } 修改结构体的值： package main import ( \"fmt\" \"reflect\" ) // 定义结构体 type User struct { Id int Name string Age int } // 修改结构体值 func SetValue(o interface{}) { v := reflect.ValueOf(o) // 获取指针指向的元素 v = v.Elem() // 取字段 f := v.FieldByName(\"Name\") if f.Kind() == reflect.String { f.SetString(\"kuteng\") } } func main() { u := User{1, \"5lmh.com\", 20} SetValue(\u0026u) fmt.Println(u) } 调用方法 package main import ( \"fmt\" \"reflect\" ) // 定义结构体 type User struct { Id int Name string Age int } func (u User) Hello(name string) { fmt.Println(\"Hello：\", name) } func main() { u := User{1, \"5lmh.com\", 20} v := reflect.ValueOf(u) // 获取方法 m := v.MethodByName(\"Hello\") // 构建一些参数 args := []reflect.Value{reflect.ValueOf(\"6666\")} // 没参数的情况下：var args2 []reflect.Value // 调用方法，需要传入方法的参数 m.Call(args) } 获取字段的tag package main import ( \"fmt\" \"reflect\" ) type Student struct { Name string `json:\"name1\" db:\"name2\"` } func main() { var s Student v := reflect.ValueOf(\u0026s) // 类型 t := v.Type() // 获取字段 f := t.Elem().Field(0) fmt.Println(f.Tag.Get(\"json\")) fmt.Println(f.Tag.Get(\"db\")) } 万能程序 DeepEqual比较切片和map func TestDeepEqual(t *testing.T) { a := map[int]string{1: \"one\", 2: \"two\", 3: \"three\"} b := map[int]string{1: \"one\", 2: \"two\", 3: \"three\"} // t.Log(a == b) t.Log(\"a==b?\", reflect.DeepEqual(a, b)) s1 := []int{1, 2, 3} s2 := []int{1, 2, 3} s3 := []int{2, 3, 1} t.Log(\"s1 == s2?\", reflect.DeepEqual(s1, s2)) t.Log(\"s1 == s3?\", reflect.DeepEqual(s1, s3)) c1 := Customer{\"1\", \"Mike\", 40} c2 := Customer{\"1\", \"Mike\", 40} fmt.Println(c1 == c2) fmt.Println(reflect.DeepEqual(c1, c2","date":"2022-01-12 22:17:07","objectID":"/go_base_09/:12:1","tags":["go grammar"],"title":"Go_base_09","uri":"/go_base_09/"},{"categories":["Go"],"content":"文件操作 ","date":"2022-01-12 22:17:07","objectID":"/go_base_09/:13:0","tags":["go grammar"],"title":"Go_base_09","uri":"/go_base_09/"},{"categories":["Go"],"content":"go module ","date":"2022-01-12 22:17:07","objectID":"/go_base_09/:14:0","tags":["go grammar"],"title":"Go_base_09","uri":"/go_base_09/"},{"categories":["Go"],"content":"String ","date":"2022-01-12 22:17:07","objectID":"/go_base_09/:15:0","tags":["go grammar"],"title":"Go_base_09","uri":"/go_base_09/"},{"categories":["Go"],"content":" 参考学习go语言中文网、C语言中文网、golang官方文档等 ","date":"2022-01-06 20:55:59","objectID":"/go_base_01/:0:0","tags":["go grammar"],"title":"Go_base_01","uri":"/go_base_01/"},{"categories":["Go"],"content":"前言 计算机里的数据都是以字节形式进行存储和处理，从而需要编码来表达信息。ASCII是简单字符集编码模型，定义了这个字符集里包含的字符以及其映射成的8位比特值。 关于现代编码模型： 一个字符如何映射成有限长度的比特值？ 需要表示字符的范围–字符表（character repertoire） CR映射到一个整数集，称映射为编码字符集（coded character set），也就是Unicode的概念，那些整数称为码点（code point） 将CCS里的整数映射成有限长度的比特值，这个对应关系称为字符编码方式或字符编码表（character encoding form），比如UTF-8，UTF-16。（Unicode Transformation Format，8或者16是指码元的大小，码元是一个已编码文本中具有最短的比特组合的单元，即最小单位是一个字节或者两个字节） UTF-8是完全兼容ASCII的，多字节表示一个字符时Unicode码点范围以及对应的bit组合： 一字节：U+00~U+7F————–UTF-8字节流（二进制）：0xxxxxxx 二字节：U+80~U+7FF————-UTF-8字节流（二进制）：110xxxxx 10xxxxxx 三字节：U+800~U+7FFF———–UTF-8字节流（二进制）：1110xxxx 10xxxxxx 10xxxxxx 四字节：U+10000~U+10FFFF——-UTF-8字节流（二进制）：11110xxx 10xxxxxx 10xxxxxx 10xxxxxx 汉字大多是三字节 关于Unicode和UTF-8： func TestString(t *testing.T) { var s string t.Log(s) //初始化为默认零值“” s = \"hello\" t.Log(len(s)) //s[1] = '3' //string是不可变的byte slice //s = \"\\xE4\\xB8\\xA5\" //可以存储任何二进制数据 s = \"\\xE4\\xBA\\xBB\\xFF\" t.Log(s) t.Log(len(s)) s = \"中\" t.Log(len(s)) //是byte数 c := []rune(s) t.Log(len(c)) // t.Log(\"rune size:\", unsafe.Sizeof(c[0])) t.Logf(\"中 unicode %x\", c[0]) t.Logf(\"中 UTF8 %x\", s) } Running tool: D:\\go\\bin\\go.exe test -timeout 30s -run ^TestString$ code/code/ch9/string === RUN TestString d:\\Go\\Go_WorkSpace\\go_learning-master\\code\\ch9\\string\\string_test.go:9: d:\\Go\\Go_WorkSpace\\go_learning-master\\code\\ch9\\string\\string_test.go:11: 5 d:\\Go\\Go_WorkSpace\\go_learning-master\\code\\ch9\\string\\string_test.go:15: 亻� d:\\Go\\Go_WorkSpace\\go_learning-master\\code\\ch9\\string\\string_test.go:16: 4 d:\\Go\\Go_WorkSpace\\go_learning-master\\code\\ch9\\string\\string_test.go:18: 3 d:\\Go\\Go_WorkSpace\\go_learning-master\\code\\ch9\\string\\string_test.go:21: 1 d:\\Go\\Go_WorkSpace\\go_learning-master\\code\\ch9\\string\\string_test.go:23: 中 unicode 4e2d d:\\Go\\Go_WorkSpace\\go_learning-master\\code\\ch9\\string\\string_test.go:24: 中 UTF8 e4b8ad --- PASS: TestString (0.00s) PASS ok code/code/ch9/string 0.514s ","date":"2022-01-06 20:55:59","objectID":"/go_base_01/:1:0","tags":["go grammar"],"title":"Go_base_01","uri":"/go_base_01/"},{"categories":["Go"],"content":"go起源 07年 三位大牛 解决三个困难：多核硬件架构、超大规模的分布式计算集群、如今使用的web开发模式导致的前所未有的开发规模和更新速度 Go 官方下载站点是 golang.org/dl，但我们可以用针对中国大陆的镜像站点 golang.google.cn/dl 来下载 ","date":"2022-01-06 20:55:59","objectID":"/go_base_01/:2:0","tags":["go grammar"],"title":"Go_base_01","uri":"/go_base_01/"},{"categories":["Go"],"content":"主要特征 自动立即回收。(自带GC) 更丰富的内置类型。 函数多返回值。 错误处理。 匿名函数和闭包。 类型和接口。 并发编程。 反射。 有复合，无继承（因为复合大于继承，干脆不要继承） Go的函数、变量、常量、自定义类型、包(package)的命名方式遵循以下规则： 1）首字符可以是任意的Unicode字符（一种字符集，一个字符两个字节，表示包括了每种语言）或者下划线 2）剩余字符可以是Unicode字符、下划线、数字 3）字符长度不限 25关键字： break default func interface select case defer go map struct chan else goto package switch const fallthrough if range type continue for import return var select和switch:select只能应用于channel的操作，既可以用于channel的数据接收，也可以用于channel的数据发送。如果select的多个分支都满足条件，则会随机的选取其中一个满足条件的分支。而switch用于一般的分支判断，顺序执行。 continue 如果循环体中的代码执行到一半，要中断当前迭代，忽略此迭代循环体中的后续代码（循环后置语句eg:x++不会被忽略），并回到 for 循环条件判断，尝试开启下一次迭代,使用continue关键字。 看上去好像和C没区别，其实Go的continue增加了对lable的支持，label 语句的作用，是标记跳转的目标。在中断内层 for 循环，回到外层 for 循环继续执行的场景应用比较合适 func main() { var sum int var sl = []int{1, 2, 3, 4, 5, 6} loop: for i := 0; i \u003c len(sl); i++ { if sl[i]%2 == 0 { // 忽略切片中值为偶数的元素 continue loop } sum += sl[i] } println(sum) // 9 } 注意与goto的区别： 一旦使用 goto 跳转，那么不管是内层循环还是外层循环都会被终结，代码将会从 outerloop 这个 label 处，开始重新执行我们的嵌套循环语句，这与带 label 的 continue 的跳转语义是完全不同的。 goto 是一种公认的、难于驾驭的语法元素，应用 goto 的代码可读性差、代码难于维护还易错。而 Go 语言保留了 goto，具体我不得而知 break Go 语言规范中明确规定，不带 label 的 break 语句中断执行并跳出的，是同一函数内 break 语句所在的最内层的 for、switch 或 select package main import \"time\" import \"fmt\" func main() { c1 := make(chan string) c2 := make(chan string) go func() { time.Sleep(time.Second * 1) c1 \u003c- \"one\" }() go func() { time.Sleep(time.Second * 2) c2 \u003c- \"two\" }() for i := 0; i \u003c 2; i++ { select { case msg1 := \u003c-c1: fmt.Println(\"received\", msg1) case msg2 := \u003c-c2: fmt.Println(\"received\", msg2) } } } fallthrough:可以使用fallthrough强制执行该case执行完下一条case代码，fallthrough不会判断下一条case的判断结果是否为true。 37个保留字： Constants: true false iota nil Types: int int8 int16 int32 int64 uint uint8 uint16 uint32 uint64 uintptr float32 float64 complex128 complex64 bool byte rune string error Functions: make len cap new append copy close delete complex real imag panic recover new\u0026make func new(Type) *Type func make(t Type, size …IntegerType) Type go声明：var,const,type,func ","date":"2022-01-06 20:55:59","objectID":"/go_base_01/:3:0","tags":["go grammar"],"title":"Go_base_01","uri":"/go_base_01/"},{"categories":["Go"],"content":"内置类型与函数 值类型： bool int(32 or 64), int8, int16, int32, int64 uint(32 or 64), uint8(byte), uint16, uint32, uint64 float32, float64 string complex64, complex128 array 引用类型（声明的同时需要分配内存空间，不然会引发panic，分配内存可以用new或者make）： slice map chan …… 内置函数（无需导入即可使用）: append -- 用来追加元素到数组、slice中,返回修改后的数组、slice close -- 主要用来关闭channel delete -- 从map中删除key对应的value panic -- 停止常规的goroutine （panic和recover：用来做错误处理） recover -- 允许程序定义goroutine的panic动作 real -- 返回complex的实部 （complex、real imag：用于创建和操作复数） imag -- 返回complex的虚部 make -- 用来分配内存，返回Type本身(只能应用于slice, map, channel) make 函数允许在运行期动态指定数组长度，绕开了数组类型必须使用编译期常量的限制。 new -- 用来分配内存，主要用来分配值类型，比如int、struct。返回指向Type的指针 cap -- capacity用于返回某个类型的最大容量（只能用于切片和 map） copy -- 用于复制和连接slice，返回复制的数目，copy(a,b) 只有 min(len(a),len(b))个元素会被成功拷贝。 len -- 用来求长度，比如string、array、slice、map、channel ，返回长度 print、println -- 底层打印函数，在部署环境中建议使用 fmt 包 内置接口error： type error interface { //只要实现了Error()函数，返回值为String的都实现了err接口（鸭子类型） Error() String } ","date":"2022-01-06 20:55:59","objectID":"/go_base_01/:4:0","tags":["go grammar"],"title":"Go_base_01","uri":"/go_base_01/"},{"categories":["Go"],"content":"init \u0026 main 以及Go包的初始化顺序 go语言中init函数用于包(package)的初始化，该函数是go语言的一个重要特性。有下面的特征： init函数是用于程序执行前做包的初始化的函数，比如初始化包里的变量等 每个包可以拥有多个init函数 包的每个源文件也可以拥有多个init函数 同一个包中多个init函数的执行顺序go语言没有明确的定义(说明) 不同包的init函数按照包导入的依赖关系决定该初始化函数的执行顺序 init函数不能被其他函数调用，而是在main函数执行之前，自动被调用 init()函数的用途： 重置包级变量值，对包内部以及暴露到外部的包级数据（主要是包级变量）的初始状态进行检查 实现对包级变量的复杂初始化，有些包级变量需要一个比较复杂的初始化过程，使用init()比较合适 在 init 函数中实现“注册模式”，如下 import ( \"database/sql\" _ \"github.com/lib/pq\" ) func main() { db, err := sql.Open(\"postgres\", \"user=pqgotest dbname=pqgotest sslmode=verify-full\") if err != nil { log.Fatal(err) } age := 21 rows, err := db.Query(\"SELECT name FROM users WHERE age = $1\", age) ... } //pq包的init() func init() { sql.Register(\"postgres\", \u0026Driver{}) } pq 包将自己实现的 sql 驱动注册到了 sql 包中。这样只要应用层代码在 Open 数据库的时候，传入驱动的名字（这里是“postgres”)，那么通过 sql.Open 函数，返回的数据库实例句柄对数据库进行的操作，实际上调用的都是 pq 包中相应的驱动实现。 从标准库 database/sql 包的角度来看，这种“注册模式”实质是一种工厂设计模式的实现，sql.Open 函数就是这个模式中的工厂方法，它根据外部传入的驱动名称“生产”出不同类别的数据库实例句柄。 这种“注册模式”在标准库的其他包中也有广泛应用，比如说，使用标准库 image 包获取各种格式图片的宽和高： package main import ( \"fmt\" \"image\" _ \"image/gif\" // 以空导入方式注入gif图片格式驱动 _ \"image/jpeg\" // 以空导入方式注入jpeg图片格式驱动 _ \"image/png\" // 以空导入方式注入png图片格式驱动 \"os\" ) func main() { // 支持png, jpeg, gif width, height, err := imageSize(os.Args[1]) // 获取传入的图片文件的宽与高 if err != nil { fmt.Println(\"get image size error:\", err) return } fmt.Printf(\"image size: [%d, %d]\\n\", width, height) } func imageSize(imageFile string) (int, int, error) { f, _ := os.Open(imageFile) // 打开图文文件 defer f.Close() img, _, err := image.Decode(f) // 对文件进行解码，得到图片实例 if err != nil { return 0, 0, err } b := img.Bounds() // 返回图片区域 return b.Max.X, b.Max.Y, nil } // $GOROOT/src/image/png/reader.go func init() { image.RegisterFormat(\"png\", pngHeader, Decode, DecodeConfig) } // $GOROOT/src/image/jpeg/reader.go func init() { image.RegisterFormat(\"jpeg\", \"\\xff\\xd8\", Decode, DecodeConfig) } // $GOROOT/src/image/gif/reader.go func init() { image.RegisterFormat(\"gif\", \"GIF8?a\", Decode, DecodeConfig) } Go语言程序的默认入口函数(主函数)： func main(){ …… //通过os.Args获取参数eg:os.Args[0] //不支持返回值，可以通过os.Exit()来返回状态 } 在启动了多个 Goroutine 的 Go 应用中，main.main 函数将在 Go 应用的主 Goroutine 中执行。 init函数和main函数的异同： 同 两个函数在定义时不能有任何的参数和返回值，且Go程序自动调用。 异 init可以应用于任意包中，且可以重复定义多个。 main函数只能用于main包中，且只能定义一个。 init()执行顺序： 对同一个go文件的init()调用顺序是从上到下的。 对同一个package中不同文件是按文件名字符串比较“从小到大”顺序调用各文件中的init()函数。 对于不同的package，如果不相互依赖的话，按照main包中”先import的后调用”的顺序调用其包中的init()，如果package存在依赖，则先调用最早被依赖的package中的init()，最后调用main函数。 同一个包被多次调用只会执行一次init()函数 如果init函数中使用了println()或者print()你会发现在执行过程中这两个不会按照你想象中的顺序执行。这两个函数官方只推荐在测试环境中使用，对于正式环境不要使用。 Go包初始化：从main包开始按照深度优先初始化main包的依赖包，初始化一个包时的顺序是初始化依赖包、常量、变量、init()，回到main包时同样初始化常量、变量、init()，再执行main()函数。 ","date":"2022-01-06 20:55:59","objectID":"/go_base_01/:5:0","tags":["go grammar"],"title":"Go_base_01","uri":"/go_base_01/"},{"categories":["Go"],"content":"go命令 PS D:\\Blog\\qizhengzou.github.io-blog\\content\\posts\u003e go Go is a tool for managing Go source code. Usage: go \u003ccommand\u003e [arguments] The commands are: bug start a bug report build compile packages and dependencies clean remove object files and cached files doc show documentation for package or symbol env print Go environment information fix update packages to use new APIs fmt gofmt (reformat) package sources generate generate Go files by processing source get add dependencies to current module and install them install compile and install packages and dependencies list list packages or modules mod module maintenance run compile and run Go program test test packages tool run specified go tool version print Go version vet report likely mistakes in packages Use \"go help \u003ccommand\u003e\" for more information about a command. Additional help topics: buildconstraint build constraints buildmode build modes c calling between Go and C cache build and test caching environment environment variables filetype file types go.mod the go.mod file gopath GOPATH environment variable gopath-get legacy GOPATH go get goproxy module proxy protocol importpath import path syntax modules modules, module versions, and more module-get module-aware go get module-auth module authentication using go.sum packages package lists and patterns private configuration for downloading non-public code testflag testing flags testfunc testing functions vcs controlling version control with GOVCS Use \"go help \u003ctopic\u003e\" for more information about that topic. ","date":"2022-01-06 20:55:59","objectID":"/go_base_01/:6:0","tags":["go grammar"],"title":"Go_base_01","uri":"/go_base_01/"},{"categories":["Go"],"content":"go env 用于打印Go语言的环境信息。 ","date":"2022-01-06 20:55:59","objectID":"/go_base_01/:6:1","tags":["go grammar"],"title":"Go_base_01","uri":"/go_base_01/"},{"categories":["Go"],"content":"go run命令 可以编译并运行命令源码文件。 ","date":"2022-01-06 20:55:59","objectID":"/go_base_01/:6:2","tags":["go grammar"],"title":"Go_base_01","uri":"/go_base_01/"},{"categories":["Go"],"content":"go get 可以根据要求和实际情况从互联网上下载或更新指定的代码包及其依赖包，并对它们进行编译和安装。 ","date":"2022-01-06 20:55:59","objectID":"/go_base_01/:6:3","tags":["go grammar"],"title":"Go_base_01","uri":"/go_base_01/"},{"categories":["Go"],"content":"go build命令 用于编译我们指定的源码文件或代码包以及它们的依赖包。 ","date":"2022-01-06 20:55:59","objectID":"/go_base_01/:6:4","tags":["go grammar"],"title":"Go_base_01","uri":"/go_base_01/"},{"categories":["Go"],"content":"go install 用于编译并安装指定的代码包及它们的依赖包。 ","date":"2022-01-06 20:55:59","objectID":"/go_base_01/:6:5","tags":["go grammar"],"title":"Go_base_01","uri":"/go_base_01/"},{"categories":["Go"],"content":"go clean命令 会删除掉执行其它命令时产生的一些文件和目录。 ","date":"2022-01-06 20:55:59","objectID":"/go_base_01/:6:6","tags":["go grammar"],"title":"Go_base_01","uri":"/go_base_01/"},{"categories":["Go"],"content":"go doc命令 可以打印附于Go语言程序实体上的文档。我们可以通过把程序实体的标识符作为该命令的参数来达到查看其文档的目的。 ","date":"2022-01-06 20:55:59","objectID":"/go_base_01/:6:7","tags":["go grammar"],"title":"Go_base_01","uri":"/go_base_01/"},{"categories":["Go"],"content":"go test命令 用于对Go语言编写的程序进行测试。 在命名文件时需要让文件必须以_test结尾。默认的情况下，go test命令不需要任何的参数，它会自动把你源码包下面所有 test 文件测试完毕，当然你也可以带上参数。 这里介绍几个常用的参数： -bench regexp 执行相应的 benchmarks，例如 -bench=.； -cover 开启测试覆盖率； -run regexp 只运行 regexp 匹配的函数，例如 -run=Array 那么就执行包含有 Array 开头的函数； -v 显示测试的详细命令。 单元测试源码文件可以由多个测试用例组成，每个测试用例函数需要以Test为前缀。**测试用例文件不会参与正常源码编译，不会被包含到可执行文件中。**测试用例文件使用go test指令来执行，没有也不需要 main() 作为函数入口。所有在以_test结尾的源码内以Test开头的函数会自动被执行。测试用例可以不传入 *testing.T 参数。单元（功能）测试以testing.T为参数，性能（压力）测试以testing.B为参数。 运行指定示例： PS D:\\Go\\Go_WorkSpace\\go_learning-master\\code\\ch2\\constant_test\u003e go test -v -run TestTest test_test.go === RUN TestTest test_test.go:6: kk --- PASS: TestTest (0.00s) PASS ok command-line-arguments 0.425s t.FailNow()———–标记错误并终止当前测试用例 t.Fail()————–仅标记错误 每个测试用例可能并发执行，使用 testing.T 提供的日志输出可以保证日志跟随这个测试上下文一起打印输出。testing.T 提供了几种日志输出方法： Log 打印日志 Logf 格式化打印日志 Error 打印错误日志 Errorf 格式化打印错误日志 Fatal 打印致命日志 Fatalf 格式化打印致命日志 单元测试使用示例： //demo.go package demo // 冒泡排序 func BubbleSort(list []int) []int { n := len(list) for i := n - 1; i \u003e 0; i-- { for j := 0; j \u003c i; j++ { if list[j] \u003e list[j+1] { list[j], list[j+1] = list[j+1], list[j] } } } return list } //demo_test.go package demo import \"testing\" func TestBubbleSort(t *testing.T) { area := BubbleSort([]list{2,1,3,4,65,13,22}) if area != {1,2,3,4,13,22,65} { t.Error(\"测试失败\") } } 执行： PS D:\\code\u003e go test -v === RUN TestGetArea --- PASS: TestGetArea (0.00s) PASS ok _/D_/code 0.435s 基准测试——获得代码内存占用和运行效率的性能数据 使用者无须准备高精度的计时器和各种分析工具，基准测试本身即可以打印出非常标准的测试报告。 package code import \"testing\" func Benchmark_Add(b *testing.B) { var n int for i := 0; i \u003c b.N; i++ { n++ } } 这段代码使用基准测试框架测试加法性能。第 7 行中的 b.N 由基准测试框架提供。测试代码需要保证函数可重入性及无状态，也就是说，测试代码不使用全局变量等带有记忆性质的数据结构。避免多次运行同一段代码时的环境不一致，不能假设 N 值范围。 //-bench=.相当于-run。在windows下使用-bench=\".\" $ go test -v -bench=. benchmark_test.go goos: linux goarch: amd64 Benchmark_Add-4 20000000 0.33 ns/op // 20000000指的是测试执行次数 PASS ok command-line-arguments 0.700s 基准测试原理：基准测试框架对一个测试用例的默认测试时间是 1 秒。开始测试时，当以 Benchmark 开头的基准测试用例函数返回时还不到 1 秒，那么 testing.B 中的 N 值将按 1、2、5、10、20、50……递增，同时以递增后的值重新调用基准测试用例函数。 通过-benchtime参数可以自定义测试时间，例如： $ go test -v -bench=. -benchtime=5s benchmark_test.go goos: linux goarch: amd64 Benchmark_Add-4 10000000000 0.33 ns/op PASS ok command-line-arguments 3.380s 基准测试可以对一段代码可能存在的内存分配进行统计，下面是一段使用字符串格式化的函数，内部会进行一些分配操作。 func Benchmark_Alloc(b *testing.B) { for i := 0; i \u003c b.N; i++ { fmt.Sprintf(\"%d\", i) } } $ go test -v -bench=Alloc -benchmem benchmark_test.go goos: linux goarch: amd64 Benchmark_Alloc-4 20000000 109 ns/op 16 B/op 2 allocs/op PASS ok command-line-arguments 2.311s 第 1 行的代码中-bench后添加了 Alloc，指定只测试 Benchmark_Alloc() 函数。 第 4 行代码的“16 B/op”表示每一次调用需要分配 16 个字节，“2 allocs/op”表示每一次调用有两次分配 开发者根据这些信息可以迅速找到可能的分配点，进行优化和调整。 控制计时器：有些测试需要一定的启动和初始化时间，如果从 Benchmark() 函数开始计时会很大程度上影响测试结果的精准性。testing.B 提供了一系列的方法可以方便地控制计时器，从而让计时器只在需要的区间进行测试。我们通过下面的代码来了解计时器的控制。 func Benchmark_Add_TimerControl(b *testing.B) { // 重置计时器 b.ResetTimer() // 停止计时器 b.StopTimer() // 开始计时器 b.StartTimer() var n int for i := 0; i \u003c b.N; i++ { n++ } } 从 Benchmark() 函数开始，Timer 就开始计数。计数器内部不仅包含耗时数据，还包括内存分配的数据。 ","date":"2022-01-06 20:55:59","objectID":"/go_base_01/:6:8","tags":["go grammar"],"title":"Go_base_01","uri":"/go_base_01/"},{"categories":["Go"],"content":"go list命令 作用是列出指定的代码包的信息。 ","date":"2022-01-06 20:55:59","objectID":"/go_base_01/:6:9","tags":["go grammar"],"title":"Go_base_01","uri":"/go_base_01/"},{"categories":["Go"],"content":"go fix 会把指定代码包的所有Go语言源码文件中的旧版本代码修正为新版本的代码。 ","date":"2022-01-06 20:55:59","objectID":"/go_base_01/:6:10","tags":["go grammar"],"title":"Go_base_01","uri":"/go_base_01/"},{"categories":["Go"],"content":"go vet 是一个用于检查Go语言源码中静态错误的简单工具。比如是否存在变量遮蔽。 $go install golang.org/x/tools/go/analysis/passes/shadow/cmd/shadow@latest go: downloading golang.org/x/tools v0.1.5 go: downloading golang.org/x/mod v0.4.2 $go vet -vettool=$(which shadow) -strict complex.go ./complex.go:13:12: declaration of \"err\" shadows declaration at line 11 ","date":"2022-01-06 20:55:59","objectID":"/go_base_01/:6:11","tags":["go grammar"],"title":"Go_base_01","uri":"/go_base_01/"},{"categories":["Go"],"content":"go tool pprof命令 交互式的访问概要文件的内容。监测进程的运行数据，用于监控程序的性能，对内存使用和CPU使用的情况统信息进行分析。官方提供了两个包**：runtime/pprof和net/http/pprof**，前者用于普通代码的性能分析，后者用于web服务器的性能分析。 runtime/pprof PS D:\\Go\\Go_WorkSpace\\go_learning-master\\code\\ch46\\tools\\file\u003e go tool pprof prof cpu.prof prof: open prof: The system cannot find the file specified. Fetched 1 source profiles out of 2 Type: cpu Time: Mar 10, 2022 at 10:41am (CST) Duration: 2.77s, Total samples = 1.70s (61.39%) Entering interactive mode (type \"help\" for commands, \"o\" for options) (pprof) top Showing nodes accounting for 1.67s, 98.24% of 1.70s total Showing top 10 nodes out of 34 flat flat% sum% cum cum% 0.49s 28.82% 28.82% 1.62s 95.29% main.fillMatrix 0.35s 20.59% 49.41% 1.13s 66.47% math/rand.(*Rand).Intn 0.33s 19.41% 68.82% 0.78s 45.88% math/rand.(*Rand).Int31n 0.14s 8.24% 77.06% 0.14s 8.24% math/rand.(*rngSource).Uint64 (inline) 0.13s 7.65% 84.71% 0.45s 26.47% math/rand.(*Rand).Int31 (inline) 0.10s 5.88% 90.59% 0.24s 14.12% math/rand.(*rngSource).Int63 0.08s 4.71% 95.29% 0.32s 18.82% math/rand.(*Rand).Int63 0.02s 1.18% 96.47% 0.02s 1.18% main.calculate 0.02s 1.18% 97.65% 0.02s 1.18% runtime.stdcall1 0.01s 0.59% 98.24% 0.01s 0.59% runtime.lock2 (pprof) list fillMatrix Total: 1.70s ROUTINE ======================== main.fillMatrix in D:\\go\\Go_WorkSpace\\go_learning-master\\code\\ch46\\tools\\file\\prof.go 490ms 1.62s (flat, cum) 95.29% of Total . . 16: . . 17:func fillMatrix(m *[row][col]int) { . . 18: s := rand.New(rand.NewSource(time.Now().UnixNano())) . . 19: . . 20: for i := 0; i \u003c row; i++ { 20ms 20ms 21: for j := 0; j \u003c col; j++ { 470ms 1.60s 22: m[i][j] = s.Intn(100000) . . 23: } . . 24: } . . 25:} . . 26: . . 27:func calculate(m *[row][col]int) { (pprof) top/tree/web top [n]，查看排名前n个数据，默认为10。（这个函数本身占用的时间，以及这个函数包括调用其他函数的时间） tree [n]，以树状图形式显示，默认显示10个。 net/http/pprof ","date":"2022-01-06 20:55:59","objectID":"/go_base_01/:6:12","tags":["go grammar"],"title":"Go_base_01","uri":"/go_base_01/"},{"categories":["Go"],"content":"go modules go modules 是 golang 1.11 新加的特性 go mod download download modules to local cache(下载依赖包) edit edit go.mod from tools or scripts（编辑go.mod） graph print module requirement graph (打印模块依赖图) init initialize new module in current directory（在当前目录初始化mod） tidy add missing and remove unused modules(拉取缺少的模块，移除不用的模块,常用) vendor make vendored copy of dependencies(将依赖复制到vendor下) verify verify dependencies have expected content (验证依赖是否正确 why explain why packages or modules are needed(解释为什么需要依赖) go.mod文件一旦创建后，它的内容将会被go toolchain全面掌控。go toolchain会在各类命令执行时，比如go get、go build、go mod等修改和维护go.mod文件。 ","date":"2022-01-06 20:55:59","objectID":"/go_base_01/:6:13","tags":["go grammar"],"title":"Go_base_01","uri":"/go_base_01/"},{"categories":["Go"],"content":"运算符 Go 语言内置的运算符有： 算术运算符 ++ 和 – 只有后置的，没有前置的，只作为语句，不作为表达式 关系运算符 逻辑运算符 位运算符 按位置零运算符 x \u0026^ y ——如果y非零，则z为0；如果y为零，则z为x（先非再与），注意按位运算 赋值运算符（一个赋值语句可以给多个变量进行赋值，多重赋值时，变量的左值和右值按从左到右的顺序赋值。多重赋值在 Go 语言的错误处理和函数返回值中会大量地使用。） = 简单的赋值运算符，将一个表达式的值赋给一个左值 += 相加后再赋值 -= 相减后再赋值 *= 相乘后再赋值 /= 相除后再赋值 %= 求余后再赋值 «= 左移后赋值 \u003e\u003e= 右移后赋值 \u0026= 按位与后赋值 l= 按位或后赋值 ^= 按位异或后赋值 ","date":"2022-01-06 20:55:59","objectID":"/go_base_01/:7:0","tags":["go grammar"],"title":"Go_base_01","uri":"/go_base_01/"},{"categories":["Go"],"content":"下划线 “_”是特殊标识符，用来忽略结果。 下划线在import中 import 下划线（如：import _ hello/imp）的作用：当导入一个包时，该包下的文件里所有init()函数都会被执行，然而，有些时候我们并不需要把整个包都导入进来，仅仅是是希望它执行init()函数而已。这个时候就可以使用 import _ 引用该包。即使用【import _ 包路径】只是引用该包，仅仅是为了调用init()函数，所以无法通过包名来调用包中的其他函数。 注意区别于.在import中，它是指import进来的package里的所有的方法是在当前的名字空间的，使用其方法时直接使用即可。 下划线在代码中 作为占位符 ","date":"2022-01-06 20:55:59","objectID":"/go_base_01/:8:0","tags":["go grammar"],"title":"Go_base_01","uri":"/go_base_01/"},{"categories":["Go"],"content":"格式占位符%… 普通占位符 占位符 说明 举例 输出 %v 相应值的默认格式。 Printf(\"%v\", people) {zhangsan}， %+v 打印结构体时，会添加字段名 Printf(\"%+v\", people) {Name:zhangsan} %#v 相应值的Go语法表示 Printf(\"#v\", people) main.Human{Name:\"zhangsan\"} %T 相应值的类型的Go语法表示 Printf(\"%T\", people) main.Human %% 字面上的百分号，并非值的占位符 Printf(\"%%\") % 布尔占位符 占位符 说明 举例 输出 %t true 或 false。 Printf(\"%t\", true) true 整数占位符 占位符 说明 举例 输出 %b 二进制表示 Printf(\"%b\", 5) 101 %c 相应Unicode码点所表示的字符 Printf(\"%c\", 0x4E2D) 中 %d 十进制表示 Printf(\"%d\", 0x12) 18 %o 八进制表示 Printf(\"%o\", 10) 12 %q 单引号围绕的字符字面值，由Go语法安全地转义 Printf(\"%q\", 0x4E2D) '中' %x 十六进制表示，字母形式为小写 a-f Printf(\"%x\", 13) d %X 十六进制表示，字母形式为大写 A-F Printf(\"%x\", 13) D %U Unicode格式：U+1234，等同于 \"U+%04X\" Printf(\"%U\", 0x4E2D) U+4E2D 浮点数和复数的组成部分（实部和虚部） 占位符 说明 举例 输出 %b 无小数部分的，指数为二的幂的科学计数法， 与 strconv.FormatFloat 的 'b' 转换格式一致。例如 -123456p-78 %e 科学计数法，例如 -1234.456e+78 Printf(\"%e\", 10.2) 1.020000e+01 %E 科学计数法，例如 -1234.456E+78 Printf(\"%e\", 10.2) 1.020000E+01 %f 有小数点而无指数，例如 123.456 Printf(\"%f\", 10.2) 10.200000 %g 根据情况选择 %e 或 %f 以产生更紧凑的（无末尾的0）输出 Printf(\"%g\", 10.20) 10.2 %G 根据情况选择 %E 或 %f 以产生更紧凑的（无末尾的0）输出 Printf(\"%G\", 10.20+2i) (10.2+2i) 字符串与字节切片 占位符 说明 举例 输出 %s 输出字符串表示（string类型或[]byte) Printf(\"%s\", []byte(\"Go语言\")) Go语言 %q 双引号围绕的字符串，由Go语法安全地转义 Printf(\"%q\", \"Go语言\") \"Go语言\" %x 十六进制，小写字母，每字节两个字符 Printf(\"%x\", \"golang\") 676f6c616e67 %X 十六进制，大写字母，每字节两个字符 Printf(\"%X\", \"golang\") 676F6C616E67 指针 占位符 说明 举例 输出 %p 十六进制表示，前缀 0x Printf(\"%p\", \u0026people) 0x4f57f0 其它标记 占位符 说明 举例 输出 + 总打印数值的正负号；对于%q（%+q）保证只输出ASCII编码的字符。 Printf(\"%+q\", \"中文\") \"\\u4e2d\\u6587\" - 在右侧而非左侧填充空格（左对齐该区域） # 备用格式：为八进制添加前导 0（%#o） Printf(\"%#U\", '中') U+4E2D 为十六进制添加前导 0x（%#x）或 0X（%#X），为 %p（%#p）去掉前导 0x； 如果可能的话，%q（%#q）会打印原始 （即反引号围绕的）字符串； 如果是可打印字符，%U（%#U）会写出该字符的 Unicode 编码形式（如字符 x 会被打印成 U+0078 'x'）。 ' ' (空格)为数值中省略的正负号留出空白（% d）； 以十六进制（% x, % X）打印字符串或切片时，在字节之间用空格隔开 0 填充前导的0而非空格；对于数字，这会将填充移到正负号之后 golang没有 ‘%u’ 点位符，若整数为无符号类型，默认就会被打印成无符号的。 宽度与精度的控制格式以Unicode码点为单位。宽度为该数值占用区域的最小宽度；精度为小数点之后的位数。 操作数的类型为int时，宽度与精度都可用字符 ‘*’ 表示。 对于 %g/%G 而言，精度为所有数字的总数，例如：123.45，%.4g 会打印123.5，（而 %6.2f 会打印123.45）。 %e 和 %f 的默认精度为6 对大多数的数值类型而言，宽度为输出的最小字符数，如果必要的话会为已格式化的形式填充空格。 而以字符串类型，精度为输出的最大字符数，如果必要的话会直接截断。 ","date":"2022-01-06 20:55:59","objectID":"/go_base_01/:9:0","tags":["go grammar"],"title":"Go_base_01","uri":"/go_base_01/"},{"categories":["Go"],"content":"变量和常量 变量： 为什么要有变量：程序运行过程中的数据都是保存在内存中，我们想要在代码中操作某个数据时就需要去内存上找到这个变量，但是如果我们直接在代码中通过内存地址去操作变量的话，代码的可读性会非常差而且还容易出错，所以我们就利用变量将这个数据的内存地址保存起来，以后直接通过这个变量就能找到内存上对应的数据了。 Go语言中的变量需要声明后才能使用，同一作用域内不支持重复声明。并且Go语言的变量声明后必须使用。 批量声明变量：（“声明聚类”） var ( a string b int c bool d float32 ) 建议将延迟初始化的变量声明放在一个 var 声明块，将声明且显式初始化的变量放在另一个 var 块中 变量声明咱们一般采用就近原则，以实现变量的作用域最小化 在函数内部，可以使用更简略的 := 方式声明并初始化变量。 匿名变量_ 常量: const ( pi = 3.1415 e = 2.7182 ) 如果省略了值则表示和上面一行的值相同 iota是go语言的常量计数器，只能在常量的表达式中使用。iota在const关键字出现时将被重置为0。const中每新增一行常量声明将使iota计数一次(iota可理解为const语句块中的行索引)。 使用iota能简化定义，在定义枚举时很有用。 可以使用_跳过某些值 const ( _ = iota KB = 1 \u003c\u003c (10 * iota) MB = 1 \u003c\u003c (10 * iota) GB = 1 \u003c\u003c (10 * iota) TB = 1 \u003c\u003c (10 * iota) PB = 1 \u003c\u003c (10 * iota) ) const ( a, b = iota + 1, iota + 2 //1,2 c, d //2,3 e, f //3,4 ) Go在常量上还是有一定创新的： 与C对比： C 语言中，原生不支持常量，字面值担负着常量的角色，C 语言的常用实践是使用宏（macro）定义记号来指代这些字面值，但它是一种仅在预编译阶段进行替换的字面值，继承了宏替换的复杂性和易错性，而且还有类型不安全、无法在调试时通过宏名字输出常量的值，等等问题。后续 C 标准中提供的 const 关键字修饰的标识符也不够完美，因为 const 关键字修饰的标识符本质上依旧是变量，它甚至无法用作数组变量声明中的初始长度（除非用 GNU 扩展 C）。 Go 原生提供的用 const 关键字定义的常量，整合了 C 语言中宏定义常量、const 修饰的“只读变量”，以及枚举常量这三种形式，并消除了每种形式的不足，使得 Go 常量是类型安全的，而且对编译器优化友好。 支持无类型常量； eg: const n = 13 常量 n 在声明时并没有显式地被赋予类型（但并非真的无类型，由初始值给予默认类型），在 Go 中，这样的常量就被称为无类型常量（Untyped Constant） 但下面的例子里边为啥编译器不报错？ type myInt int const n = 13 func main() { var a myInt = 5 fmt.Println(a + n) // 输出：18 } 支持常量隐式自动转型； 对于无类型常量参与的表达式求值，Go 编译器会根据上下文中的类型信息，把无类型常量自动转换为相应的类型后，再参与求值计算，这一转型动作是隐式进行的。但由于转型的对象是一个常量，所以这并不会引发类型安全问题，Go 编译器会保证这一转型的安全性。 这就很好地解释了上面的问题。 注意：如果 Go 编译器在做隐式转型时，发现无法将常量转换为目标类型，Go 编译器也会报错，比如转型后溢出了。 以及前面有提到的可用于实现枚举。 ","date":"2022-01-06 20:55:59","objectID":"/go_base_01/:10:0","tags":["go grammar"],"title":"Go_base_01","uri":"/go_base_01/"},{"categories":["Go"],"content":"基本类型 类型 长度(字节) 默认值 说明 bool 1 false byte 1 0 uint8，一个ASCII字符 rune 4 0 Unicode Code Point，int32，一个utf-8字符,c:=[]rune(s)//指将字符串s转化为rune的切片 int, uint 4或8 0 由操作系统位数(32/64)决定 int8, uint8 1 0 -128 ~ 127, 0 ~ 255，byte是uint8 的别名 int16, uint16 2 0 -32768 ~ 32767, 0 ~ 65535 int32, uint32 4 0 -21亿~ 21亿, 0 ~ 42亿，rune是int32 的别名 int64, uint64 8 0 float32 4 0.0 float64 8 0.0 complex64 8 complex128 16 uintptr 4或8 以存储指针的 uint32 或 uint64 整数 array 值类型 struct 值类型 string “” UTF-8 字符串 slice nil 引用类型 map nil 引用类型 channel nil 引用类型 interface nil 接口 function nil 函数 uintptr 实际上就是一个 uint 用来表示地址，go 的指针和 c 不一样不能进行偏移操作，如果非要偏移的话就需要 unsafe.Pointer 和 uintptr 配合来实现。uintptr 不是一个指针 所以 GC 时也不会处理 uintptr 的引用。如果不涉及地址偏移时没有必要使用 uintptr 。——来自知乎回答 标准库 math 定义了各数字类型取值范围。 空指针值 nil，而非C/C++ NULL。golang中有多种引用类型：pointer、interface、slice、map、channel、function。go作为一个强类型语言（类型是定义好的无法改变，不像c，你定义一个short可以当成char用，因为可以直接操作内存），不同引用类型的判空（nil）规则是不同的；比如：interface的判空规则是，需要判断类型和值是否都为nil(interface的底层是有类型和值构成的)slice的判空，需要判断slice引用底层数组的指针为空，容量和size均为0。 不允许将整型强制转换为布尔型。 字符串的内部实现使用UTF-8编码。（UTF-8是Unicode的存储实现，转化为有限长度比特组合的规则） 只有显示类型转化。 ","date":"2022-01-06 20:55:59","objectID":"/go_base_01/:11:0","tags":["go grammar"],"title":"Go_base_01","uri":"/go_base_01/"},{"categories":["Go"],"content":"string 值类型，空值是空字符串而不是nil。本质就是个只读的（不可变的）byte切片。 // $GOROOT/src/reflect/value.go // StringHeader是一个string的运行时表示 type StringHeader struct { Data uintptr Len int } string 类型其实是一个“描述符”，它本身并不真正存储字符串数据，而仅是由一个指向底层存储的指针和字符串的长度字段组成的 多行字符串，用反引号` s1 := `第一行 第二行 第三行 ` fmt.Println(s1) 方法 介绍 len(str) 求长度 +或fmt.Sprintf 拼接字符串 strings.Split 分割 strings.Contains 判断是否包含 strings.HasPrefix,strings.HasSuffix 前缀/后缀判断 strings.Index(),strings.LastIndex() 子串出现的位置 strings.Join(a[]string, sep string) join操作 遍历字符串： // Traversal strings func traversalString() { s := \"JeFo的博客\" for i := 0; i \u003c len(s); i++ { //Traversal by byte fmt.Printf(\"%v(%c) \", s[i], s[i]) } fmt.Println() for _, r := range s { //Traversal by rune fmt.Printf(\"%v(%c) \", r, r) } fmt.Println() } 74(J) 101(e) 70(F) 111(o) 231(ç) 154() 132( ) 229(å) 141() 154() 229(å) 174(®) 162(¢) 74(J) 101(e) 70(F) 111(o) 30340(的) 21338(博) 23458(客) 可以调用标准库 UTF-8 包中的 RuneCountInString 函数获取字符串字符个数。（len只能获得字节个数） Go 原生支持通过 +/+= 操作符进行字符串连接。以及，Go 还提供了 strings.Builder、strings.Join、fmt.Sprintf 等函数来进行字符串连接操作。 如果能知道拼接字符串的个数，那么使用bytes.Buffer和strings.Builder的Grows申请空间后，性能是最好的；如果不能确定长度，那么bytes.Buffer和strings.Builder也比“+”和fmt.Sprintf性能好很多。 bytes.Buffer与strings.Builder，strings.Builder更合适，因为bytes.Buffer 转化为字符串时重新申请了一块空间，存放生成的字符串变量，而 strings.Builder 直接将底层的 []byte 转换成了字符串类型返回了回来。 字符串比较： Go 字符串类型支持各种比较关系操作符，包括 = =、!= 、\u003e=、\u003c=、\u003e 和 \u003c。在字符串的比较上，Go 采用字典序的比较策略，分别从每个字符串的起始处，开始逐个字节地对两个字符串类型变量进行比较。 Go 支持字符串与字节切片、字符串与 rune 切片的双向转换，并且这种转换无需调用任何函数，只需使用显式类型转换就可以了 修改字符串： 要修改字符串，需要先将其转换成[]rune或[]byte，完成后再转换为string。无论哪种转换，都会重新分配内存，并复制字节数组。 为什么go要原生支持字符串类型？ c语言并没有内置字符串类型 不是原生类型，编译器不会对它进行类型校验，导致类型安全性差； 字符串操作时要时刻考虑结尾的’\\0’，防止缓冲区溢出； 以字符数组形式定义的“字符串”，它的值是可变的，在并发场景中需要考虑同步问题； 获取一个字符串的长度代价较大，通常是 O(n) 时间复杂度； C 语言没有内置对非 ASCII 字符（如中文字符）的支持。 go内置了字符串类型的好处 string 类型的数据是不可变的，提高了字符串的并发安全性和存储利用率。 没有结尾’\\0’，而且获取长度的时间复杂度是常数时间，消除了获取字符串长度的开销。 原生支持“所见即所得”的原始字符串，大大降低构造多行字符串时的心智负担。 在 C 语言中构造多行字符串，一般就是两个方法：要么使用多个字符串的自然拼接，要么需要结合续行符”\"。但因为有转义字符的存在，我们很难控制好格式。Go 语言就简单多了，通过一对反引号原生支持构造“所见即所得”的原始字符串（Raw String）。而且，Go 语言原始字符串中的任意转义字符都不会起到转义的作用 对非 ASCII 字符提供原生支持，消除了源码在不同环境下显示乱码的可能。 ","date":"2022-01-06 20:55:59","objectID":"/go_base_01/:12:0","tags":["go grammar"],"title":"Go_base_01","uri":"/go_base_01/"},{"categories":["Go"],"content":"数组 数组可以通过下标进行访问，下标是从0开始，最后一个元素下标是：len-1 支持 “==”、\"!=” 操作符，因为内存总是被初始化过的。 相同类型的数组之间可以使用 == 或 != 进行比较，但不可以使用 \u003c 或 \u003e，也可以相互赋值。 长度不同类型也不同。 指针数组 [n]*T，数组指针 *[n]T。 多维数组除了第一维，初始化时都不能用[…]省略长度声明。 值拷贝行为会造成性能问题，通常会建议使用 slice，或数组指针。 ","date":"2022-01-06 20:55:59","objectID":"/go_base_01/:13:0","tags":["go grammar"],"title":"Go_base_01","uri":"/go_base_01/"},{"categories":["Go"],"content":"列表list 初始化： 通过 container/list 包的 New() 函数初始化 list。变量名 := list.New() 通过 var 关键字声明初始化 list 。var 变量名 list.List 列表与切片和 map 不同的是，列表并没有具体元素类型的限制，因此，列表的元素可以是任意类型，这既带来了便利，也引来一些问题，例如给列表中放入了一个 interface{} 类型的值，取出值后，如果要将 interface{} 转换为其他类型将会发生宕机。 源码数据结构： type Element struct { // Next and previous pointers in the doubly-linked list of elements. // To simplify the implementation, internally a list l is implemented // as a ring, such that \u0026l.root is both the next element of the last // list element (l.Back()) and the previous element of the first list // element (l.Front()). next, prev *Element // The list to which this element belongs. list *List // The value stored with this element. Value interface{} } type List struct { root Element // sentinel list element, only \u0026root, root.prev, and root.next are used len int // current list length excluding (this) sentinel element } 其他源码列出稍冗长，root.prev可以视作尾节点，root.next相当于头节点，不过这些是透明的，被封装好的。 在列表中插入元素删除元素都有简便方法。 ","date":"2022-01-06 20:55:59","objectID":"/go_base_01/:14:0","tags":["go grammar"],"title":"Go_base_01","uri":"/go_base_01/"},{"categories":["Go"],"content":"切片 只能和nil进行比较。 //用make()初始化 var s3 []int = make([]int, 0, 10)//len=0,cap=10 var s4 []int = make([]int, 5)//len=5 //先初始化一个数组，再截取相应部分得到切片 arr := [5]int{1, 2, 3, 4, 5} var s6 []int //数组的切片化 s6 = arr[1:4] // 左闭右开 s7 = arr[1:3:5]//arr[low,high,max]，len=high-low cap=max-low 切片追加append（内置函数）： // The append built-in function appends elements to the end of a slice. If // it has sufficient capacity, the destination is resliced to accommodate the // new elements. If it does not, a new underlying array will be allocated. // Append returns the updated slice. It is therefore necessary to store the // result of append, often in the variable holding the slice itself: // slice = append(slice, elem1, elem2) // slice = append(slice, anotherSlice...) // As a special case, it is legal to append a string to a byte slice, like this: // slice = append([]byte(\"hello \"), \"world\"...) var a = []int{1, 2, 3} fmt.Printf(\"slice a : %v\\n\", a) var b = []int{4, 5, 6} fmt.Printf(\"slice b : %v\\n\", b) c := append(a, b...) fmt.Printf(\"slice c : %v\\n\", c) d := append(c, 7) fmt.Printf(\"slice d : %v\\n\", d) e := append(d, 8, 9, 10) fmt.Printf(\"slice e : %v\\n\", e) slice := append([]byte(\"hello \"), \"world\"...)//**注意**是字节数组和字符串。 fmt.Printf(\"slice slice : %v\\n\", slice) func TestOne(t *testing.T) { q := make([]int, 3, 10) w := append(q, 1) t.Log(len(w), len(q)) e := append(q, 2) //append是加在该切片的len后面，但不是最后一个元素后面，因为底层数组会被改变，而切片变量的结构体所记录的信息是固定的。 t.Log(q, w, e) } 超出原 slice.cap 限制，就会重新分配底层数组，即便原数组并未填满。 通常以 2 倍容量重新分配底层数组。在大批量添加数据时，建议一次性分配足够大的空间，以减少内存分配和数据复制开销。或初始化足够长的 len 属性，改用索引号进行操作。 及时释放不再使用的 slice 对象，避免持有过期数组，造成 GC 无法回收。 切片resize: package main import ( \"fmt\" ) func main() { var a = []int{1, 3, 4, 5} fmt.Printf(\"slice a : %v , len(a) : %v\\n\", a, len(a)) b := a[1:2] fmt.Printf(\"slice b : %v , len(b) : %v\\n\", b, len(b)) c := b[1:3] fmt.Printf(\"slice c : %v , len(c) : %v\\n\", c, len(c)) } slice a : [1 3 4 5] , len(a) : 4 slice b : [3] , len(b) : 1 slice c : [4 5] , len(c) : 2 string \u0026 slice : string底层就是一个byte的数组，因此，也可以进行切片操作。 二维切片： …… a := make([][]int,dy) for i = range a { a[i] = make([]int, dx) } ","date":"2022-01-06 20:55:59","objectID":"/go_base_01/:15:0","tags":["go grammar"],"title":"Go_base_01","uri":"/go_base_01/"},{"categories":["Go"],"content":"slice切片底层实现 切片的设计想法是由动态数组概念而来，为了开发者可以更加方便的使一个数据结构可以自动增加和减少。但是切片本身并不是动态数据或者数组指针。切片常见的操作有 reslice、append、copy。与此同时，切片还具有可索引，可迭代的优秀特性。 func main() { arrayA := []int{1, 2} testArrayPoint(\u0026arrayA) // 1.传数组指针 arrayB := arrayA[:] testArrayPoint(\u0026arrayB) // 2.传切片 fmt.Printf(\"arrayA : %p , %v\\n\", \u0026arrayA, arrayA) } func testArrayPoint(x *[]int) { fmt.Printf(\"func Array : %p , %v\\n\", x, *x) (*x)[1] += 1 } func Array : 0xc4200b0140 , [1 2] func Array : 0xc4200b0180 , [1 3] arrayA : 0xc4200b0140 , [1 4] 传指针会有一个弊端，从打印结果可以看到，第一行和第三行指针地址都是同一个，万一原数组的指针指向更改了，那么函数里面的指针指向都会跟着更改。 用切片传数组参数，既可以达到节约内存的目的，也可以达到合理处理好共享内存的问题。打印结果第二行就是切片，切片的指针和原来数组的指针是不同的。 slice数据结构源码： // runtime/slice.go type slice struct { array unsafe.Pointer // 指向一个数组的指针 len int // 切片长度 cap int // 切片容量 } 注意：Golang 语言是没有操作原始内存的指针的，所以 unsafe 包提供相关的对内存指针的操作，一般情况下非专业人员勿用 如果想从 slice 中得到一块内存地址，可以这样做： s := make([]byte, 200) ptr := unsafe.Pointer(\u0026s[0]) 自己构造一个slice: var ptr unsafe.Pointer var s1 = struct { addr uintptr len int cap int }{ptr, length, length} s := *(*[]byte)(unsafe.Pointer(\u0026s1)) 在 Go 的反射中就存在一个与之对应的数据结构 SliceHeader，我们可以用它来构造一个 slice： var o []byte sliceHeader := (*reflect.SliceHeader)((unsafe.Pointer(\u0026o))) sliceHeader.Cap = length sliceHeader.Len = length sliceHeader.Data = uintptr(ptr) 此外，unsafe的Sizeof函数： var a, b = int(5), uint(6) var p uintptr = 0x12345678 fmt.Println(\"signed integer a's length is\", unsafe.Sizeof(a)) // 8 fmt.Println(\"unsigned integer b's length is\", unsafe.Sizeof(b)) // 8 fmt.Println(\"uintptr's length is\", unsafe.Sizeof(p)) // 8 并非所有时候都适合用切片代替数组：因为切片底层数组可能会在堆上分配内存，而且小数组在栈上拷贝的消耗也未必比make 消耗大。 ","date":"2022-01-06 20:55:59","objectID":"/go_base_01/:15:1","tags":["go grammar"],"title":"Go_base_01","uri":"/go_base_01/"},{"categories":["Go"],"content":"创建切片 make 函数允许在运行期动态指定数组长度，绕开了数组类型必须使用编译期常量的限制。 创建切片有两种形式，make 创建切片，字面量创建切片（既可以初始化一个新的，也可以截取一个数组,截取一个数组的时候cap未声明时为数组容量）。 空切片和nil切片 nil切片：| nil (Pointer) | Len(int) | Cap(int) | var slice []int nil 切片被用在很多标准库和内置函数中，描述一个不存在的切片的时候，就需要用到 nil 切片。比如函数在发生异常的时候，返回的切片就是 nil 切片。nil 切片的指针指向 nil。 空切片： | Array (Pointer) | Len(int) | Cap(int) | silce := make( []int , 0 ) slice := []int{ } 空切片一般会用来表示一个空的集合。比如数据库查询，一条结果也没有查到，那么就可以返回一个空切片。 空切片和 nil 切片的区别在于，空切片指向的地址不是nil，指向的是一个内存地址，但是它没有分配任何内存空间，即底层元素包含0个元素。 不管是使用 nil 切片还是空切片，对其调用内置函数 append，len 和 cap 的效果都是一样的。 扩容策略 如果切片的容量小于 1024 个元素，于是扩容的时候就翻倍增加容量。 一旦元素个数超过 1024 个元素，那么增长因子就变成 1.25 ，即每次增加原来容量的四分之一。 注意：扩容扩大的容量都是针对原来的容量而言的，而不是针对原来数组的长度而言的。 扩容后的数组是新数组还是老数组？ 如果如果切片扩容后容量比原来数组的容量最大值还大，扩容切片需要另开一片内存区域，把原来的值拷贝过来，再执行append()操作。 否则，不会开辟新数组，这种情况很危险，因为这种情况下，扩容以后的数组还是指向原来的数组,多个原来的数组上的切片会受新切片所影响！ ","date":"2022-01-06 20:55:59","objectID":"/go_base_01/:15:2","tags":["go grammar"],"title":"Go_base_01","uri":"/go_base_01/"},{"categories":["Go"],"content":"切片拷贝 slicecopy 方法会把源切片值(即 fm Slice )中的元素复制到目标切片(即 to Slice )中，并返回被复制的元素个数，copy 的两个类型必须一致。slicecopy 方法最终的复制结果取决于较短的那个切片，当较短的切片复制完成，整个复制过程就全部完成了。 如果用 range 的方式去遍历一个切片，拿到的 Value 其实是切片里面的值拷贝。所以每次打印 Value 的地址都不变。由于 Value 是值拷贝的，并非引用传递，所以直接改 Value 是达不到更改原切片值的目的的，需要通过 \u0026slice[index] 获取真实的地址。 ","date":"2022-01-06 20:55:59","objectID":"/go_base_01/:15:3","tags":["go grammar"],"title":"Go_base_01","uri":"/go_base_01/"},{"categories":["Go"],"content":"指针 区别于C/C++中的指针，Go语言中的指针不能进行偏移和运算，是安全指针。 首先需要知道指针地址、指针类型和指针取值。 指针地址和指针类型： Go语言中的值类型（int、float、bool、string、array、struct）都有对应的指针类型 指针取值：* 空指针： 当一个指针被定义后没有分配到任何变量时，它的值为 nil new和make: 在Go语言中对于引用类型的变量，我们在使用的时候不仅要声明它，还要为它分配内存空间，否则我们的值就没办法存储。 new 函数签名和举例 func new(Type) *Type func main() { var a *int a = new(int) *a = 10 fmt.Println(*a) } new函数不太常用，使用new函数得到的是一个类型的指针，并且该指针对应的值为该类型的零值。 make 只用于slice、map以及chan的内存创建，而且它返回的类型就是这三个类型本身,因为这三种类型就是引用类型，所以就没有必要返回他们的指针了。 以map举例： func main() { var b map[string]int b = make(map[string]int, 10) b[\"测试\"] = 100 fmt.Println(b) } ","date":"2022-01-06 20:55:59","objectID":"/go_base_01/:16:0","tags":["go grammar"],"title":"Go_base_01","uri":"/go_base_01/"},{"categories":["Go"],"content":"map map[KeyType]ValueType 初始化时用make申请内存（或者直接填充元素）： make(map[KeyType]ValueType, [cap])//cap不是必须的，但最好一开始就申请一个合适的容量 // // 初始化 + 赋值一体化 m3 := map[string]string{ \"a\": \"aa\", \"b\": \"bb\", } 判断某个key是否存在： //map[key]会返回两个值，第二个是该键是否存在 value, ok := map[key] map的遍历还是正常用for range，但有一点需要注意：遍历map时元素顺序与添加键值对的顺序无关。 按照指定顺序遍历map:思路是将map的key取出另存为切片再排序，再按照切片的顺序进行遍历即可。 因为 map 类型要保证 key 的唯一性。key 的类型必须支持“==”和“!=”两种比较操作符，比如函数类型、map 类型自身，以及切片类型是不能作为 map 的 key 类型的。value类型则没有限制。 map \u0026 切片： 元素为map的切片： func main() { var mapSlice = make([]map[string]string, 3) for index, value := range mapSlice { fmt.Printf(\"index:%d value:%v\\n\", index, value) } fmt.Println(\"after init\") // 对切片中的map元素进行初始化 mapSlice[0] = make(map[string]string, 10) mapSlice[0][\"name\"] = \"王五\" mapSlice[0][\"password\"] = \"123456\" mapSlice[0][\"address\"] = \"红旗大街\" } value为切片的map: func main() { var sliceMap = make(map[string][]string, 3) fmt.Println(sliceMap) fmt.Println(\"after init\") key := \"中国\" value, ok := sliceMap[key] if !ok { value = make([]string, 0, 2) } value = append(value, \"北京\", \"上海\") sliceMap[key] = value fmt.Println(sliceMap) } 获取键值对数量： m := map[string]int { \"key1\" : 1, \"key2\" : 2, } fmt.Println(len(m)) // 2 m[\"key3\"] = 3 fmt.Println(len(m)) // 3 注意：不能对 map 类型变量调用 cap，来获取当前容量 map删除键值对：（即便传给 delete 的键在 map 中并不存在，delete 函数的执行也不会失败，更不会抛出运行时的异常。） delete(map,key) ","date":"2022-01-06 20:55:59","objectID":"/go_base_01/:17:0","tags":["go grammar"],"title":"Go_base_01","uri":"/go_base_01/"},{"categories":["Go"],"content":"map实现原理 map底层存储方式为（结构体）数组，在存储时key不能重复，当key重复时，value进行覆盖，我们通过key进行hash运算（可以简单理解为把key转化为一个整形数字）然后对数组的长度取余，得到key存储在数组的哪个下标位置，最后将key和value组装为一个结构体，放入数组下标处。 哈希冲突：即不同key经哈希映射后得到相同的数组下标。 解决办法：开放定址法： 发现hashkey(key)的下标已经被别key占用的时候，在这个数组中空间中重新找一个没被占用的存储这个冲突的key。寻找方式有很多。常见的有线性探测法，线性补偿探测法，随机探测法。 线性探测法： 从冲突的下标处开始往后探测，到达数组末尾时，从数组开始处探测，直到找到一个空位置存储这个key，当数组都找不到的情况下会扩容（事实上当数组容量快满的时候就会扩容了） 查找某一个key的时候，找到key对应的下标，比较key是否相等，如果相等直接取出来，否则按照顺序探测直到碰到一个空位置，说明key不存在。 拉链法： 当key的hash冲突时，我们在冲突位置的元素上形成一个链表，通过指针互连接。 当查找时，发现key冲突，顺着链表一直往下找，直到链表的尾节点，找不到则返回空 开放定址法的优缺点： 由上面可以看出拉链法比线性探测处理简单 线性探测查找是会被拉链法会更消耗时间 线性探测会更加容易导致扩容，而拉链不会 拉链存储了指针，所以空间上会比线性探测占用多一点 拉链是动态申请存储空间的，所以更适合链长不确定的 ","date":"2022-01-06 20:55:59","objectID":"/go_base_01/:17:1","tags":["go grammar"],"title":"Go_base_01","uri":"/go_base_01/"},{"categories":["Go"],"content":"go中map的实现原理 go里面map并不是线程安全的，在1.9版本之前用map加互斥锁来解决此问题，之后加了sync.map性能稍微高了一些，因为它有一块只读的buffer，相当于由两个buffer组成，一块只读，一块读写。sync.map更适合于读非常多，能够占到90%以上的情况。如果读写差不多或者说写更多的话，sync.map的性能就比较差了。 在读写对半的情况下，可以考虑引入concurrent_map包。（go get -u +包地址） 在go1.16中，map也是数组存储的的，每个数组下标处存储的是一个bucket,这个bucket的类型见下面代码，每个bucket中可以存储8个kv键值对，当每个bucket存储的kv对到达8个之后，会通过overflow指针指向一个新的bucket，从而形成一个链表,看bmap的结构 // A bucket for a Go map. type bmap struct { // tophash generally contains the top byte of the hash value // for each key in this bucket. If tophash[0] \u003c minTopHash, // tophash[0] is a bucket evacuation state instead.即桶疏散状态 tophash [bucketCnt]uint8 // Followed by bucketCnt keys and then bucketCnt elems. // NOTE: packing all the keys together and then all the elems together makes the // code a bit more complicated than alternating key/elem/key/elem/... but it allows // us to eliminate padding which would be needed for, e.g., map[int64]int8. // Followed by an overflow pointer. } tophash用来快速查找key值是否在该bucket中，而不同每次都通过真值进行比较。 map[int64]int8,key是int64（8个字节），value是int8（一个字节），kv的长度不同，如果按照kv格式存放，则考虑内存对齐v也会占用int64，而按照后者存储时，8个v刚好占用一个int64 当往map中存储一个kv对时，通过k获取hash值，hash值的低八位和bucket数组长度取余，定位到在数组中的那个下标，hash值的高八位存储在bucket中的tophash中，用来快速判断key是否存在，key和value的具体值则通过指针运算存储，当一个bucket满时，通过overfolw指针链接到下一个bucket。 map的存储源码： // Like mapaccess, but allocates a slot for the key if it is not present in the map.如果key不在map里为其分配一个插槽（狭槽） func mapassign(t *maptype, h *hmap, key unsafe.Pointer) unsafe.Pointer { if h == nil { panic(plainError(\"assignment to entry in nil map\")) } if raceenabled { callerpc := getcallerpc() pc := funcPC(mapassign) racewritepc(unsafe.Pointer(h), callerpc, pc) raceReadObjectPC(t.key, key, callerpc, pc) } if msanenabled { //获取hash算法 msanread(key, t.key.size) } if h.flags\u0026hashWriting != 0 { throw(\"concurrent map writes\") } //计算哈希值 hash := t.hasher(key, uintptr(h.hash0)) // Set hashWriting after calling t.hasher, since t.hasher may panic, // in which case we have not actually done a write. h.flags ^= hashWriting //如果bucket数组一开始为空，则初始化 if h.buckets == nil { h.buckets = newobject(t.bucket) // newarray(t.bucket, 1) } again: //定位在哪一个bucket中 bucket := hash \u0026 bucketMask(h.B) if h.growing() { growWork(t, h, bucket) } //得到bucket的结构体 b := (*bmap)(add(h.buckets, bucket*uintptr(t.bucketsize))) //获取高八位的哈希值 top := tophash(hash) var inserti *uint8 var insertk unsafe.Pointer var elem unsafe.Pointer bucketloop: //死循环 for { //循环bucket中的tophash数组 for i := uintptr(0); i \u003c bucketCnt; i++ { //如果hash不相等 if b.tophash[i] != top { //判断是否为空，为空则插入 if isEmpty(b.tophash[i]) \u0026\u0026 inserti == nil { inserti = \u0026b.tophash[i] insertk = add(unsafe.Pointer(b), dataOffset+i*uintptr(t.keysize)) elem = add(unsafe.Pointer(b), dataOffset+bucketCnt*uintptr(t.keysize)+i*uintptr(t.elemsize)) } //插入成功，终止外层循环 if b.tophash[i] == emptyRest { break bucketloop } continue } //高八位哈希值一样，获取已存在的kay k := add(unsafe.Pointer(b), dataOffset+i*uintptr(t.keysize)) if t.indirectkey() { k = *((*unsafe.Pointer)(k)) } //判断两个key是否相等，不相等就循环下一个 if !t.key.equal(key, k) { continue } // already have a mapping for key. Update it. if t.needkeyupdate() { typedmemmove(t.key, k, key) } //获取已存在的value elem = add(unsafe.Pointer(b), dataOffset+bucketCnt*uintptr(t.keysize)+i*uintptr(t.elemsize)) goto done } //如果上一个bucket没能找到插入，则通过overflow获取链表上的下一个bucket ovf := b.overflow(t) if ovf == nil { break } b = ovf } // Did not find mapping for key. Allocate new cell \u0026 add entry. // If we hit the max load factor or we have too many overflow buckets, // and we're not already in the middle of growing, start growing. if !h.growing() \u0026\u0026 (overLoadFactor(h.count+1, h.B) || tooManyOverflowBuckets(h.noverflow, h.B)) { hashGrow(t, h) goto again // Growing the table invalidates everything, so try again } if inserti == nil { // The current bucket and all the overflow buckets connected to it are full, allocate a new one. newb := h.newoverflow(t, b) inserti = \u0026newb.tophash[0] insertk = add(unsafe.Pointer(newb), dataOffset) elem = add(insertk, bucketCnt*uintptr(t.keysize)) } // ","date":"2022-01-06 20:55:59","objectID":"/go_base_01/:17:2","tags":["go grammar"],"title":"Go_base_01","uri":"/go_base_01/"},{"categories":["Go"],"content":"map与工厂模式 map的value可以是一个方法。 与 Go 的 Dock type 接⼝⽅式⼀起，可以⽅便的实现单⼀⽅法对象的⼯⼚模式 func TestMapWithFunValue(t *testing.T) { m := map[int]func(op int) int{} m[1] = func(op int) int { return op } m[2] = func(op int) int { return op * op } m[3] = func(op int) int { return op * op * op } t.Log(m[1](2), m[2](2), m[3](2)) } ","date":"2022-01-06 20:55:59","objectID":"/go_base_01/:17:3","tags":["go grammar"],"title":"Go_base_01","uri":"/go_base_01/"},{"categories":["Go"],"content":"set Go 的内置集合中没有 Set 实现， 可以 map[type]bool。 map可以保证添加元素的唯⼀性，方便判断唯一元素的个数 基本操作 添加元素 判断元素是否存在 删除元素 元素个数 func TestMapForSet(t *testing.T) { mySet := map[int]bool{} mySet[1] = true n := 3 if mySet[n] { t.Logf(\"%d is existing\", n) } else { t.Logf(\"%d is not existing\", n) } mySet[3] = true t.Log(len(mySet)) delete(mySet, 1) n = 1 if mySet[n] { t.Logf(\"%d is existing\", n) } else { t.Logf(\"%d is not existing\", n) } } ","date":"2022-01-06 20:55:59","objectID":"/go_base_01/:18:0","tags":["go grammar"],"title":"Go_base_01","uri":"/go_base_01/"},{"categories":["Go"],"content":"结构体 Go语言中通过结构体的内嵌再配合接口比面向对象具有更高的扩展性和灵活性。 ","date":"2022-01-06 20:55:59","objectID":"/go_base_01/:19:0","tags":["go grammar"],"title":"Go_base_01","uri":"/go_base_01/"},{"categories":["Go"],"content":"类型别名和自定义类型 自定义类型（新类型）： //将MyInt定义为int类型 type MyInt int 可以基于内置的基本类型定义，也可以通过struct定义。 类型别名（Go1.9添加的新功能，注意编译后是原来的类型）： //将MyInt作为为int类型的昵称 type MyInt = int ","date":"2022-01-06 20:55:59","objectID":"/go_base_01/:19:1","tags":["go grammar"],"title":"Go_base_01","uri":"/go_base_01/"},{"categories":["Go"],"content":"结构体 本质上是一种聚合型的数据类型。 通过struct可以实现面向对象。 定义的时候，同样类型的字段也可以写在一行。 只有结构体被实例化时，才会真正被分配内存。 匿名结构体：定义临时数据结构时可能会用到。 语法糖：Go语言中支持对结构体指针直接使用.来访问结构体的成员，在使用new分配内存后得到的便是结构体指针。 对于包含结构体类型字段的结构体类型来说,可以无需提供字段的名字: 以这种方式定义的结构体字段，我们叫做嵌入字段（Embedded Field）。我们也可以将这种字段称为匿名字段，或者把类型名看作是这个字段的名字 type Book struct { Title string Person ... ... } func main(){ var book Book println(book.Person.Phone) // 将类型名当作嵌入字段的名字 println(book.Phone) // 支持直接访问嵌入字段所属类型中字段 } 注意： 结构体类型 T 定义中，不能有以自身类型 T 定义的字段，但却可以拥有自身类型的指针类型、以自身类型为元素类型的切片类型，以及以自身类型作为 value 类型的 map 类型的字段，因为指针、map、切片的变量元数据的内存占用大小是固定的。 type T struct { a T // wrong t *T // ok st []T // ok m map[string]T // ok } sync.Mutex和bytes.Buffer的“零值可用”： var mu sync.Mutex mu.Lock() mu.Unlock() var b bytes.Buffer b.Write([]byte(\"Hello, Go\")) fmt.Println(b.String()) // 输出：Hello, Go 使用\u0026对结构体进行取地址操作相当于对该结构体类型进行了一次new实例化操作。 p := \u0026person{} 初始化（没有指定初始值的字段的值就是该字段类型的零值）： p := person{ a: \"1a\", b: \"2b\", } //结构体指针 q := \u0026ss{ a: \"1a\", b: \"2b\", } //简写需**注意**三点：1.必须初始化结构体的所有字段。2.初始值的填充顺序必须与字段在结构体中的声明顺序一致。3.该方式不能和键值初始化方式混用。 s := \u0026d{ \"aq\", \"sw\", } Go 语言并不推荐我们按字段顺序对一个结构体类型变量进行显式初始化，甚至 Go 官方还在提供的 go vet 工具中专门内置了一条检查规则：“composites”，用来静态检查代码中结构体变量初始化是否使用了这种方法，一旦发现，就会给出警告。 Go 推荐我们用“field:value”形式的复合字面值，对结构体类型变量进行显式初始化，这种方式可以降低结构体类型使用者和结构体类型设计者之间的耦合 var t = T{ F2: \"hello\", F1: 11, F4: 14, } 空结构体的作用： 空结构体类型变量内存占用为0 使用空结构体类型元素，作为一种“事件”信息进行 Goroutine 之间的通信 var c = make(chan Empty) // 声明一个元素类型为Empty的channel c\u003c-Empty{} // 向channel写入一个“事件” 这种以空结构体为元素类建立的 channel，是目前能实现的、内存占用最小的 Goroutine 间通信方式。 空标识符“_”作为结构体类型定义中的字段名称的作用： 自己实现一个结构体构造函数： //值拷贝开销太大，返回结构体指针 func newPerson(name, city string, age int8) *person { return \u0026person{ name: name, city: city, age: age, } } 如果一个结构体类型中包含未导出字段，并且这个字段的零值还不可用时,又或是一个结构体类型中的某些字段，需要一个复杂的初始化逻辑时：需要使用一个特定的构造函数，来创建并初始化结构体变量了。 结构体类型的内存布局： 在真实情况下，虽然 Go 编译器没有在结构体变量占用的内存空间中插入额外字段，但结构体字段实际上可能并不是紧密相连的，中间可能存在“缝隙”。这些“缝隙”同样是结构体变量占用的内存空间的一部分，它们是 Go 编译器插入的“填充物（Padding）” 这是为了内存对齐。 方法和接收者 Go语言中的方法（Method）是一种作用于特定类型变量的函数。这种特定类型变量叫做接收者（Receiver）。接收者的概念就类似于其他语言中的this或者 self。 //接收者变量：接收者中的参数变量名在命名时，官方建议使用接收者类型名的第一个小写字母，而不是self、this之类的命名。例如，Person类型的接收者变量应该命名为 p，Connector类型的接收者变量应该命名为c等。 func (接收者变量 接收者类型) 方法名(参数列表) (返回参数) { 函数体 } 指针类型的接收者： 指针类型的接收者由一个结构体的指针组成，由于指针的特性，调用方法时修改接收者指针的任意成员变量，在方法结束后，修改都是有效的。这种方式就十分接近于其他语言中面向对象中的this或者self。 例如我们为Person添加一个SetAge方法，来修改实例变量的年龄。 // SetAge 设置p的年龄 // 使用指针接收者 func (p *Person) SetAge(newAge int8) { p.age = newAge } //调用该方法 func main() { p1 := NewPerson(\"测试\", 25) fmt.Println(p1.age) // 25 p1.SetAge(30) fmt.Println(p1.age) // 30 } 值类型的接收者： // SetAge2 设置p的年龄 // 使用值接收者 func (p Person) SetAge2(newAge int8) { p.age = newAge } func main() { p1 := NewPerson(\"测试\", 25) p1.Dream() fmt.Println(p1.age) // 25 p1.SetAge2(30) // (*p1).SetAge2(30) fmt.Println(p1.age) // 25 } 什么时候应该使用指针类型接收者： 需要修改接收者中的值 接收者是拷贝代价比较大的大对象 保证一致性，如果有某个方法使用了指针接收者，那么其他的方法也应该使用指针接收者。 为任意类型添加方法： 在Go语言中，接收者的类型可以是任何类型，不仅仅是结构体，任何类型都可以拥有方法。 举个例子，我们基于内置的int类型使用type关键字可以定义新的自定义类型，然后为我们的自定义类型添加方法。 注意事项：非本地类型不能定义方法，也就是说我们不能给别的包的类型定义方法。 结构体的匿名字段： 结构体允许其成员字段在声明时没有字段名而只有类型，这种没有名字的字段就称为匿名字段。 匿名字段默认采用类型名作为字段名，结构体要求字段名称必须唯一，因此一个结构体中同种类型的匿名字段只能有一个。 嵌套结构体 一个结构体中可以嵌套包含另一个结构体或结构体指针。 嵌套结构体内部可能存在相同的字段名。这个时候为了避免歧义需要指定具体的内嵌结构体的字段。 类型的“继承”： 即通过类型嵌入，包括接口类型的类型嵌入和结构体类型的类型嵌入。 结构体类型中嵌入接口类型：结构体类型的方法集合，包含嵌入的接口类型的方法集合。 结构体类型嵌入接口类型在日常编码中有一个妙用，就是可以简化单元测试的编写：见下面例子： 由于嵌入某接口类型的结构体类型的方法集合包含了这个接口类型的方法集合，这就意味着，这个结构体类型也是它嵌入的接口类型的一个实现 package employee type Result struct { Count int } func (r Result) Int() int { return r.Count } type Rows []struct{} type Stmt interface { Close() error NumInput() int Exec(stmt string, args ...string) (Result, error) Query(args []string) (Rows, error) } // 返回男性员工总数 func MaleCount(s Stmt) (int, error) { result, err := s.Exec(\"select count(*) from employee_tab where gender=?\", \"1\") if err != nil { return 0, err } return result.Int(), nil } package employee import \"testing\" type fakeStmtForMaleCount struct { Stmt } func (fakeStmtForMaleCount) Exec(stmt string, args ...string) (Result, error) { return Result{Count: 5}, nil } func TestEmployeeMaleCount(t *testing.T) { f :=","date":"2022-01-06 20:55:59","objectID":"/go_base_01/:19:2","tags":["go grammar"],"title":"Go_base_01","uri":"/go_base_01/"},{"categories":["Go"],"content":"go项目的标准布局演进 官方并没有给标准布局，但社区还是有的。随着go版本的不断更新，Go源码比例不断增大，Go 1.0时还占比32%的C语言现在也只不过占比不到1%。而项目布局一直保持了下来。 Go 1.3 src 目录下面的结构： 以 all.bash 为代表的代码构建的脚本源文件放在了 src 下面的顶层目录下。 src 下的二级目录 cmd 下面存放着 Go 相关可执行文件的相关目录 每个子目录都是一个 Go 工具链命令或子命令对应的可执行文件 src 下的二级目录 pkg 下面存放着运行时实现、标准库包实现，这些包既可以被上面 cmd 下各程序所导入，也可以被 Go 语言项目之外的 Go 程序依赖并导入。 Go 1.4 版本删除 pkg 这一中间层目录并引入 internal 目录。“src/pkg/xxx”-\u003e“src/xxx” 根据 internal 机制的定义，一个 Go 项目里的 internal 目录下的 Go 包，只可以被本项目内部的包导入。项目外部是无法导入这个 internal 目录下面的包的。 internal 目录的引入，让一个 Go 项目中 Go 包的分类与用途变得更加清晰 Go 1.6 版本增加 vendor 目录 为了解决 Go 包依赖版本管理的问题，Go 核心团队在 Go 1.5 版本中做了第一次改进。增加了 vendor 构建机制，也就是 Go 源码的编译可以不在 GOPATH 环境变量下面搜索依赖包的路径，而在 vendor 目录下查找对应的依赖包 Go 1.7 版本，Go 在 vendor 下缓存了其依赖的外部包。这些依赖包主要是 golang.org/x 下面的包 vendor 机制与目录的引入，让 Go 项目第一次具有了可重现构建（Reproducible Build）的能力。 Go 1.13 版本引入 go.mod 和 go.sum 在 Go 1.11 版本中，Go 核心团队做出了第二次改进尝试：引入了 Go Module 构建机制，也就是在项目引入 go.mod 以及在 go.mod 中明确项目所依赖的第三方包和版本，项目的构建就将摆脱 GOPATH 的束缚，实现精准的可重现构建。 Go 语言项目自身在 Go 1.13 版本引入 go.mod 和 go.sum 以支持 Go Module 构建机制 Go 可执行程序项目的典型结构布局： ├── cmd/ │ ├── app1/ │ │ └── main.go │ └── app2/ │ └── main.go ├── go.mod ├── go.sum ├── internal/ │ ├── pkga/ │ │ └── pkg_a.go │ └── pkgb/ │ └── pkg_b.go ├── pkg1/ │ └── pkg1.go ├── pkg2/ │ └── pkg2.go └── vendor/ cmd（也可以是app或者其他名字） 存放项目要编译构建的可执行文件对应的 main 包的源文件。如果你的项目中有多个可执行文件需要构建，每个可执行文件的 main 包单独放在一个子目录中，cmd 目录下的各 app 的 main 包将整个项目的依赖连接在一起 通常来说，main 包应该很简洁。我们在 main 包中会做一些命令行参数解析、资源初始化、日志设施初始化、数据库连接初始化等工作，之后就会将程序的执行权限交给更高级的执行控制对象 pkgN 一个存放项目自身要使用、同样也是可执行文件对应 main 包所要依赖的库文件，同时这些目录下的包还可以被外部项目引用。 go.mod和go.sum 包依赖管理的配置文件 vendor（可选） 前面有说，vendor 是 Go 1.5 版本引入的用于在项目本地缓存特定版本依赖包的机制 Go Module 机制也保留了 vendor 目录（通过 go mod vendor 可以生成 vendor 下的依赖包，通过 go build -mod=vendor 可以实现基于 vendor 的构建）。一般我们仅保留项目根目录下的 vendor 目录，否则会造成不必要的依赖选择的复杂性。 etc 如若喜欢借助一些第三方的构建工具辅助构建，比如：make、bazel 等。你可以将这类外部辅助构建工具涉及的诸多脚本文件（比如 Makefile）放置在项目的顶层目录下，就像 Go 1.3中的 all.bash 那样。 如果app1，app2的发布版本不总是同步的，建议将每个项目单独作为一个 module 进行单独的版本管理和演进，避免版本管理的“分歧”带来更大的复杂性。当然新版Go命令较好地解决了这一点，可以采用如下结构： ├── go.mod // mainmodule ├── module1 │ └── go.mod // module1 └── module2 └── go.mod // module2 可以通过 git tag 名字来区分不同 module 的版本。其中 vX.Y.Z 形式的 tag 名字用于代码仓库下的 mainmodule；而 module1/vX.Y.Z 形式的 tag 名字用于指示 module1 的版本。 Go 库项目的典型结构布局：Go 库项目主要作用还是对外暴露 API。 ├── go.mod ├── internal/ │ ├── pkga/ │ │ └── pkg_a.go │ └── pkgb/ │ └── pkg_b.go ├── pkg1/ │ └── pkg1.go └── pkg2/ └── pkg2.go 不需要构建可执行程序 仅限项目内部使用而不想暴露到外部的包，可以放在项目顶层的 internal 目录下面。当然 internal 也可以有多个并存在于项目结构中的任一目录层级中，关键是项目结构设计人员要明确各级 internal 包的应用层次和范围。 ","date":"2022-01-06 20:55:59","objectID":"/go_base_01/:20:0","tags":["go grammar"],"title":"Go_base_01","uri":"/go_base_01/"},{"categories":["Go"],"content":"go应用构建模式的演进 Go 程序的构建过程就是确定包版本、编译包以及将编译后得到的目标文件链接在一起的过程。 包依赖管理演进： GOPATH模式。 Go 编译器可以在本地 GOPATH 环境变量配置的路径下，搜寻 Go 程序依赖的第三方包。如果存在，就使用这个本地包进行编译；如果不存在，就会报编译错误 如果你没有显式设置 GOPATH 环境变量，Go 会将 GOPATH 设置为默认值，不同操作系统下默认值的路径不同 可以通过 go get 命令将本地缺失的第三方依赖包下载到本地。不仅能将包下载到 GOPATH 环境变量配置的目录下，它还会检查该包的依赖包在本地是否存在，如果不存在，go get 也会一并将它们下载到本地。但是，go get只能得到最新主线版本的依赖包，不能保证Reproduceable Build 总之，在 GOPATH 构建模式下，Go 编译器实质上并没有关注 Go 项目所依赖的第三方包的版本。从而有了Vendor机制控制依赖包版本。 Vendor机制 本质上就是在 Go 项目的某个特定目录下，将项目的所有依赖包缓存起来，这个特定目录名就是 vendor。 Go 编译器会优先感知和使用 vendor 目录下缓存的第三方包版本，而不是 GOPATH 环境变量所配置的路径下的第三方包版本。 目录示例： ├── main.go └── vendor/ ├── github.com/ │ └── sirupsen/ │ └── logrus/ └── golang.org/ └── x/ └── sys/ └── unix/ 开启 vendor 机制，你的 Go 项目必须位于 GOPATH 环境变量配置的某个路径的 src 目录下面 不足之处主要在于需要手工管理 vendor 下面的 Go 依赖包（Go 社区先后开发了诸如 gb、glide、dep 等工具，都是用来进行依赖分析管理的，自身却都存在某些问题）、庞大的vendor目录也得提交到代码仓库，以及项目路径的限制。 Go Module 如何创建？ go mod init go mod tidy go build go.sum是由 go mod 相关命令维护的一个文件，它存放了特定版本 module 内容的哈希值。这是 Go Module 的一个安全措施。当将来这里的某个 module 的特定版本被再次下载的时候，go 命令会使用 go.sum 文件中对应的哈希值，和新下载的内容的哈希值进行比对，只有哈希值比对一致才是合法的，这样可以确保你的项目所依赖的 module 内容，不会被恶意或意外篡改。 项目所依赖的包有很多版本，Go Module 是如何选出最适合的那个版本？ Go Module 的语义导入版本机制 go.mod 的 require 段中依赖的版本号，都符合 vX.Y.Z 的格式。语义版本号分成 3 部分：主版本号 (major)、次版本号 (minor) 和补丁版本号 (patch)。分别对应XYZ。 按照语义版本规范，主版本号不同的两个版本是相互不兼容的。而且，在主版本号相同的情况下，次版本号大都是向后兼容次版本号小的版本。补丁版本号也不影响兼容性 Go Module 规定：如果同一个包的新旧版本是兼容的，那么它们的包导入路径应该是相同的。 如果不兼容，Go Module 创新性地给出了一个方法：将包主版本号引入到包导入路径中，我们可以像下面这样导入 logrus v2.0.0 版本依赖包： import \"github.com/sirupsen/logrus/v2\" 关于主版本号为0时，按照语义版本规范的说法，v0.y.z 这样的版本号是用于项目初始开发阶段的版本号。在这个阶段任何事情都有可能发生，其 API 也不应该被认为是稳定的。Go Module 将这样的版本 (v0) 与主版本号 v1 做同等对待，也就是采用不带主版本号的包导入路径 总之，通过在包导入路径中引入主版本号的方式，来区别同一个包的不兼容版本。 最小版本选择原则 包依赖关系比较复杂时，一个包A的两个依赖包BC可能依赖于不同版本的某个包D，那A依赖于D的哪个版本？ Go 会在该项目依赖项的所有版本中，选出符合项目整体要求的“最小版本”。 明确具体版本下 Go Module 的实际表现行为还是比较重要的，方便应用时选择切换。 Go 各版本构建模式机制和切换： Go 1.11后一段时间GOPATH 构建模式与 Go Modules 构建模式各自独立工作，我们可以通过设置环境变量 GO111MODULE 的值在两种构建模式间切换。 Go 1.16 版本，Go Module 构建模式成为了默认模式。 目前我觉得GOPATH似乎可以抛弃了 Go Module常规操作： 为当前 module 添加一个依赖： go get 命令将我们新增的依赖包下载到了本地 module 缓存里，并在 go.mod 文件的 require 段中新增相关内容。 使用 go mod tidy 命令，在执行构建前自动分析源码中的依赖变化，识别新增依赖项并下载它们 两种方法都行，复杂的项目用go mod tidy/go get .（注意二者还是有区别的,看到后面你就知道了） 升级、降级依赖版本： 查询某个依赖包的版本 PS D:\\Blog\\qizhengzou.github.io-blog\\content\\posts\u003e go list -m -versions github.com/sirupsen/logrus github.com/sirupsen/logrus v0.1.0 v0.1.1 v0.2.0 v0.3.0 v0.4.0 v0.4.1 v0.5.0 v0.5.1 v0.6.0 v0.6.1 v0.6.2 v0.6.3 v0.6.4 v0.6.5 v0.6.6 v0.7.0 v0.7.1 v0.7.2 v0.7.3 v0.8.0 v0.8.1 v0.8.2 v0.8.3 v0.8.4 v0.8.5 v0.8.6 v0.8.7 v0.9.0 v0.10.0 v0.11.0 v0.11.1 v0.11.2 v0.11.3 v0.11.4 v0.11.5 v1.0.0 v1.0.1 v1.0.3 v1.0.4 v1.0.5 v1.0.6 v1.1.0 v1.1.1 v1.2.0 v1.3.0 v1.4.0 v1.4.1 v1.4.2 v1.5.0 v1.6.0 v1.7.0 v1.7.1 v1.8.0 v1.8.1 //go mod tidy帮我们选择了v1.8.1 我们可以在项目的 module 根目录下，执行带有版本号的 go get 命令eg: go get github.com/sirupsen/logrus@v1.7.0 或者，用 go mod edit 命令，明确告知我们要依赖 v1.7.0 版本，而不是 v1.8.1。执行go mod edit -require=github.com/sirupsen/logrus@v1.7.0再go mod tidy。 添加一个主版本号大于 1 的依赖： 在声明它的导入路径的基础上，加上版本号信息 升级依赖版本到一个不兼容版本： 需要注意一点，可能需要移除对某个包的依赖 移除一个依赖 要想彻底从项目中移除 go.mod 中的依赖项，仅从源码中删除对依赖项的导入语句还不够。 还得用 go mod tidy 命令，将这个依赖项彻底从 Go Module 构建上下文中清除掉。go mod tidy 会自动分析源码依赖，而且将不再使用的依赖从 go.mod 和 go.sum 中移除。 特殊情况：借用Vendor Vendor机制其实可以作为Go Module的一个很好的补充。 在一些不方便访问外部网络，并且对 Go 应用构建性能敏感的环境，比如在一些内部的持续集成或持续交付环境（CI/CD）中，使用 vendor 机制可以实现与 Go Module 等价的构建。 Go 提供了可以快速建立和更新 vendor 的命令： go mod vendor make vendored copy of dependencies(将依赖复制到vendor下) 在 go build 后面加上 -mod=vendor 参数，可以快速基于 vendor 构建项目。 在 Go 1.14 及以后版本中，如果 Go 项目的顶层目录下存在 vendor 目录，那么 go build 默认也会优先基于 vendor 构建，除非你给 go build 传入 -mod=mod 的参数 ","date":"2022-01-06 20:55:59","objectID":"/go_base_01/:21:0","tags":["go grammar"],"title":"Go_base_01","uri":"/go_base_01/"},{"categories":["Go"],"content":"参考 https://cloud.tencent.com/developer/article/1526095 https://www.topgoer.cn/docs/golang/chapter02 https://time.geekbang.org/column/article/429143 ","date":"2022-01-06 20:55:59","objectID":"/go_base_01/:22:0","tags":["go grammar"],"title":"Go_base_01","uri":"/go_base_01/"},{"categories":["Catalogue","Go"],"content":" 参考学习go语言中文网、C语言中文网、golang官方文档等 go_base_01 go起源 环境相关 主要特征 内置类型与函数 init \u0026 main以及Go包的初始化顺序 go命令 go env go run命令 go get go build命令 go install go clean命令 go doc命令 go test命令 go list命令 go fix go vet go tool pprof命令 go modules 运算符 下划线 格式占位符%…… 变量和常量 基本类型 string： 数组 切片 切片底层实现 创建切片 切片拷贝 指针 map map实现原理 go中map的实现原理 map与工厂模式 set 结构体 类型别名和自定义类型 结构体 方法和接收者 go项目的标准布局演进 go应用构建模式的演进 参考 go_base_02 if switch Type Switch select 基本使用 典型用法 for range Goto Break Continue go_base_03 函数定义 参数 不定参 返回值 匿名函数 闭包、递归 闭包 go递归函数 延迟调用（defer） defer陷阱 异常处理，错误处理 单元测试 go test工具 测试函数 测试组 子测试 测试覆盖率 基准测试 x性能比较函数 重置时间 并行测试 Setup与TearDown 示例函数 func ToUpper 压力测试 Go怎么写测试用例 如何编写测试用例 如何编写压力测试 小结 BDD go_base_04 方法定义 匿名字段 方法集 表达式 自定义error 抛异常和处理异常 系统抛 返回异常 自定义error go_base_05 匿名字段 接口 接口 类型与接口的关系 空接口 空接口的应用 类型断言 go_base_06 互联网协议介绍 互联网分层模型 socket编程 http编程 WebSocket编程 go_base_07 并发介绍 Goroutine runtime包 Channel Goroutine池 定时器 select 并发安全和锁 Sync 原子操作 GMP原理和调度 爬虫小案例 ","date":"2022-01-06 09:17:19","objectID":"/go_grammar_catalogue/:0:0","tags":["catalogue","go grammar"],"title":"Go_grammar_catalogue","uri":"/go_grammar_catalogue/"},{"categories":["Go"],"content":" 参考学习go语言中文网、C语言中文网、golang官方文档等 并发编程 ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:0:0","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"并发介绍 进程和线程 进程是程序在操作系统中的一次执行过程，系统进行资源分配和调度的一个独立单位。 线程是进程的一个执行实体,是CPU调度和分派的基本单位,它是比进程更小的能独立运行的基本单位。 一个进程可以创建和撤销多个线程;同一个进程中的多个线程之间可以并发执行。 并发和并行 多线程程序在一个核的cpu上运行，就是并发。（一段时间内都有运行） 多线程程序在多个核的cpu上运行，就是并行。（同时运行） 协程和线程 协程：独立的栈空间，共享堆空间，调度由用户自己控制，本质上有点类似于用户级线程，这些用户级线程的调度也是自己实现的。 线程：一个线程上可以跑多个协程，协程是轻量级的线程。 goroutine 只是由官方实现的超级”线程池”。 每个实力4~5KB的栈内存占用和由于实现机制而大幅减少的创建和销毁开销是go高并发的根本原因。 并发不是并行： 并发主要由切换时间片来实现”同时”运行，并行则是直接利用多核实现多线程的运行，go可以设置使用核数，以发挥多核计算机的能力。 goroutine 奉行通过通信来共享内存，而不是共享内存来通信。 ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:1:0","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"Goroutine 在java/c++中我们要实现并发编程的时候，我们通常需要自己维护一个线程池，并且需要自己去包装一个又一个的任务，同时需要自己去调度线程执行任务并维护上下文切换，这一切通常会耗费程序员大量的心智。那么能不能有一种机制，程序员只需要定义很多个任务，让系统去帮助我们把这些任务分配到CPU上实现并发执行呢？ Go语言中的goroutine就是这样一种机制，goroutine的概念类似于线程，但 goroutine是由Go的运行时（runtime）调度和管理的。Go程序会智能地将 goroutine 中的任务合理地分配给每个CPU。Go语言之所以被称为现代化的编程语言，就是因为它在语言层面已经内置了调度和上下文切换的机制。 在Go语言编程中你不需要去自己写进程、线程、协程，你的技能包里只有一个技能–goroutine，当你需要让某个任务并发执行的时候，你只需要把这个任务包装成一个函数，开启一个goroutine去执行这个函数就可以了，就是这么简单粗暴。 ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:2:0","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"使用goroutine Go语言中使用goroutine非常简单，只需要在调用函数的时候在前面加上go关键字，就可以为一个函数创建一个goroutine。 一个goroutine必定对应一个函数，可以创建多个goroutine去执行相同的函数。 启动单个goroutine 启动goroutine的方式非常简单，只需要在调用的函数（普通函数和匿名函数）前面加上一个go关键字。 举个例子如下： func hello() { fmt.Println(\"Hello Goroutine!\") } func main() { hello() fmt.Println(\"main goroutine done!\") } 这个示例中hello函数和下面的语句是串行的，执行的结果是打印完Hello Goroutine!后打印main goroutine done!。 接下来我们在调用hello函数前面加上关键字go，也就是启动一个goroutine去执行hello这个函数。 func main() { go hello() // 启动另外一个goroutine去执行hello函数 fmt.Println(\"main goroutine done!\") } 这一次的执行结果只打印了main goroutine done!，并没有打印Hello Goroutine!。为什么呢？ 在程序启动时，Go程序就会为main()函数创建一个默认的goroutine。 当main()函数返回的时候该goroutine就结束了，所有在main()函数中启动的goroutine会一同结束，main函数所在的goroutine就像是权利的游戏中的夜王，其他的goroutine都是异鬼，夜王一死它转化的那些异鬼也就全部GG了。 所以我们要想办法让main函数等一等hello函数，最简单粗暴的方式就是time.Sleep了。 func main() { go hello() // 启动另外一个goroutine去执行hello函数 fmt.Println(\"main goroutine done!\") time.Sleep(time.Second) } 执行上面的代码你会发现，这一次先打印main goroutine done!，然后紧接着打印Hello Goroutine!。 首先为什么会先打印main goroutine done!是因为我们在创建新的goroutine的时候需要花费一些时间，而此时main函数所在的goroutine是继续执行的。 启动多个goroutine 在Go语言中实现并发就是这样简单，我们还可以启动多个goroutine。让我们再来一个例子： （这里使用了sync.WaitGroup来实现goroutine的同步） var wg sync.WaitGroup func hello(i int) { defer wg.Done() // goroutine结束就登记-1 fmt.Println(\"Hello Goroutine!\", i) } func main() { for i := 0; i \u003c 10; i++ { wg.Add(1) // 启动一个goroutine就登记+1 go hello(i) } wg.Wait() // 等待所有登记的goroutine都结束 } 多次执行上面的代码，会发现每次打印的数字的顺序都不一致。这是因为10个goroutine是并发执行的，而goroutine的调度是随机的。 注意 如果主协程退出了，其他任务还执行吗（运行下面的代码测试一下吧） package main import ( \"fmt\" \"time\" ) func main() { // 合起来写 go func() { i := 0 for { i++ fmt.Printf(\"new goroutine: i = %d\\n\", i) time.Sleep(time.Second) } }() i := 0 for { i++ fmt.Printf(\"main goroutine: i = %d\\n\", i) time.Sleep(time.Second) if i == 2 { break } } } ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:2:1","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"goroutine与线程 可增长的栈 OS线程（操作系统线程）一般都有固定的栈内存（通常为2MB）,一个goroutine的栈在其生命周期开始时只有很小的栈（典型情况下2KB），goroutine的栈不是固定的，他可以按需增大和缩小，goroutine的栈大小限制可以达到1GB，虽然极少会用到这么大。所以在Go语言中一次创建十万左右的goroutine也是可以的。 goroutine调度 GPM是Go语言运行时（runtime）层面的实现，是go语言自己实现的一套调度系统。区别于操作系统调度OS线程。 G很好理解，就是个goroutine的，里面除了存放本goroutine信息外 还有与所在P的绑定等信息。 P是go语言实现的协程处理器，管理着一组goroutine队列，P里面会存储当前goroutine运行的上下文环境（函数指针，堆栈地址及地址边界），P会对自己管理的goroutine队列做一些调度（比如把占用CPU时间较长的goroutine暂停，或者阻塞的协程进行跳过（有一个守护线程会记录每个processor完成的协程的数量，如果有个P完成的协程的数量一直不变，说明被阻塞了，守护线程会往这个协程的用户栈里插入一个特殊的标记，当协程运行内敛函数时会读到这个标记，会把自己中断下来，然后查找等候协程队列的队尾，然后切换成别的线程进一步运行）、运行后续的goroutine等等）当自己的队列消费完了就去全局队列里取，如果全局队列里也消费完了会去其他P的队列里抢任务。 另一个提高整个并发能力的机制：当某一个协程被系统中断了，比如一些IO操作，需要等待的时候，为了提高整体的并发，processor会把自己移到另一个可使用的系统进程当中，继续执行它所挂的队列里的其他的协程，当被中断的协程被唤醒，会把自己加入到某一个pocessor的协程等待队列里，或者加入到全局等待队列当中。需要注意的一点：当一个协程被中断的时候，其在寄存器里的运行状态也会保存到这个协程对象里，当协程再次获得运行的机会，这些又会重新写入寄存器继续运行。 go的协程机制与系统线程的这种多对多的关系以及它是如何来高效地利用系统线程来尽量多的运行并发的协程任务的。 M（machine）是Go运行时（runtime）对操作系统内核线程的虚拟， M与内核线程一般是一一映射的关系，也可以是多对一， 一个groutine最终是要放到M上执行的； P与M一般也是一一对应的。他们关系是： P管理着一组G挂载在M上运行。当一个G长久阻塞在一个M上时，runtime会新建一个M，阻塞G所在的P会把其他的G 挂载在新建的M上。当旧的G阻塞完成或者认为其已经死掉时回收旧的M。 P的个数是通过runtime.GOMAXPROCS设定（最大256），Go1.5版本之后默认为物理线程数。 在并发量大的时候会增加一些P和M，但不会太多，切换太频繁的话得不偿失。 单从线程调度讲，Go语言相比起其他语言的优势在于OS线程是由OS内核来调度的，goroutine则是由Go运行时（runtime）自己的调度器调度的，这个调度器使用一个称为m:n调度的技术（复用/调度m个goroutine到n个OS线程）。 其一大特点是goroutine的调度是在用户态下完成的， 不涉及内核态与用户态之间的频繁切换，包括内存的分配与释放，都是在用户态维护着一块大的内存池， 不直接调用系统的malloc函数（除非内存池需要改变），成本比调度OS线程低很多。 另一方面充分利用了多核的硬件资源，近似的把若干goroutine均分在物理线程上， 再加上本身goroutine的超轻量，以上种种保证了go调度方面的性能。 kernel space entity 系统线程或者说内核对象，内核对象切换的消耗很大。 多个协程对应于一个内核线程时，这些协程切换时消耗较少。 package groutine_test import ( \"fmt\" \"testing\" \"time\" ) func TestGroutine(t *testing.T) { for i := 0; i \u003c 10; i++ { go func(i int) { time.Sleep(time.Second * 1) fmt.Println(i) }(i) } time.Sleep(time.Millisecond * 50) } ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:2:2","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"runtime包 ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:3:0","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"runtime.Gosched() 让出CPU时间片，重新等待安排任务(大概意思就是本来计划的好好的周末出去烧烤，但是你妈让你去相亲,两种情况第一就是你相亲速度非常快，见面就黄不耽误你继续烧烤，第二种情况就是你相亲速度特别慢，见面就是你侬我侬的，耽误了烧烤，但是还馋就是耽误了烧烤你还得去烧烤) package main import ( \"fmt\" \"runtime\" ) func main() { go func(s string) { for i := 0; i \u003c 2; i++ { fmt.Println(s) } }(\"world\") // 主协程 for i := 0; i \u003c 2; i++ { // 切一下，再次分配任务 runtime.Gosched() fmt.Println(\"hello\") } } ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:3:1","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"runtime.Goexit() 退出当前协程(一边烧烤一边相亲，突然发现相亲对象太丑影响烧烤，果断让她滚蛋，然后也就没有然后了) package main import ( \"fmt\" \"runtime\" ) func main() { go func() { defer fmt.Println(\"A.defer\") func() { defer fmt.Println(\"B.defer\") // 结束协程 runtime.Goexit() defer fmt.Println(\"C.defer\") fmt.Println(\"B\") }() fmt.Println(\"A\") }() for { } } ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:3:2","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"runtime.GOMAXPROCS Go运行时的调度器使用GOMAXPROCS参数来确定需要使用多少个OS线程来同时执行Go代码。默认值是机器上的CPU核心数。例如在一个8核心的机器上，调度器会把Go代码同时调度到8个OS线程上（GOMAXPROCS是m:n调度中的n）。 Go语言中可以通过runtime.GOMAXPROCS()函数设置当前程序并发时占用的CPU逻辑核心数。 Go1.5版本之前，默认使用的是单核心执行。Go1.5版本之后，默认使用全部的CPU逻辑核心数。 我们可以通过将任务分配到不同的CPU逻辑核心上实现并行的效果，这里举个例子： func a() { for i := 1; i \u003c 10; i++ { fmt.Println(\"A:\", i) } } func b() { for i := 1; i \u003c 10; i++ { fmt.Println(\"B:\", i) } } func main() { runtime.GOMAXPROCS(1) go a() go b() time.Sleep(time.Second) } 两个任务只有一个逻辑核心，此时是做完一个任务再做另一个任务。 将逻辑核心数设为2，此时两个任务并行执行，代码如下。 func a() { for i := 1; i \u003c 10; i++ { fmt.Println(\"A:\", i) } } func b() { for i := 1; i \u003c 10; i++ { fmt.Println(\"B:\", i) } } func main() { runtime.GOMAXPROCS(2) go a() go b() time.Sleep(time.Second) } Go语言中的操作系统线程和goroutine的关系： 一个操作系统线程对应用户态多个goroutine。 go程序可以同时使用多个操作系统线程。 goroutine和OS线程是多对多的关系，即m:n。 ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:3:3","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"共享内存机制 使用锁来实现并发控制 package share_mem import ( \"sync\" \"testing\" \"time\" ) func TestCounter(t *testing.T) { counter := 0 for i := 0; i \u003c 5000; i++ { go func() { counter++ }() } time.Sleep(1 * time.Second) t.Logf(\"counter = %d\", counter) } //使用锁实现线程安全 func TestCounterThreadSafe(t *testing.T) { var mut sync.Mutex counter := 0 for i := 0; i \u003c 5000; i++ { go func() { defer func() { mut.Unlock() }() mut.Lock() counter++ }() } time.Sleep(1 * time.Second) t.Logf(\"counter = %d\", counter) } //是要弄WaitGroup实现 func TestCounterWaitGroup(t *testing.T) { var mut sync.Mutex var wg sync.WaitGroup counter := 0 for i := 0; i \u003c 5000; i++ { wg.Add(1) go func() { defer func() { mut.Unlock() }() mut.Lock() counter++ wg.Done() }() } wg.Wait() t.Logf(\"counter = %d\", counter) } ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:4:0","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"CSP 并发机制 communicating sequential processes通信顺序进程。 依赖于通道来完成两个通信实体之间的协调。 CSP VS Actor Model 和Actor的直接通讯不同，CSP模式则是通过Channel进⾏通讯的，更松耦合⼀些。 Go中channel是有容量限制并且独⽴于处理Groutine，⽽如Erlang，Actor模式中的mailbox容量是⽆限的，接收进程也总是被动地处理消息 可利用channel实现异步返回。 ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:5:0","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"Channel ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:6:0","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"channel 单纯地将函数并发执行是没有意义的。函数与函数间需要交换数据才能体现并发执行函数的意义。 虽然可以使用共享内存进行数据交换，但是共享内存在不同的goroutine中容易发生竞态问题。为了保证数据交换的正确性，必须使用互斥量对内存进行加锁，这种做法势必造成性能问题。 Go语言的并发模型是CSP（Communicating Sequential Processes），提倡通过通信共享内存而不是通过共享内存而实现通信。 如果说goroutine是Go程序并发的执行体，channel就是它们之间的连接。channel是可以让一个goroutine发送特定值到另一个goroutine的通信机制。 Go 语言中的通道（channel）是一种特殊的类型。通道像一个传送带或者队列，总是遵循先入先出（First In First Out）的规则，保证收发数据的顺序。每一个通道都是一个具体类型的导管，也就是声明channel的时候需要为其指定元素类型。 ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:6:1","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"channel类型 channel是一种类型，一种引用类型。声明通道类型的格式如下： var 变量 chan 元素类型 举几个例子： var ch1 chan int // 声明一个传递整型的通道 var ch2 chan bool // 声明一个传递布尔型的通道 var ch3 chan []int // 声明一个传递int切片的通道 ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:6:2","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"创建channel 通道是引用类型，通道类型的空值是nil。 var ch chan int fmt.Println(ch) // \u003cnil\u003e 声明的通道后需要使用make函数初始化之后才能使用。 创建channel的格式如下： make(chan 元素类型, [缓冲大小]) channel的缓冲大小是可选的。 举几个例子： ch4 := make(chan int) ch5 := make(chan bool) ch6 := make(chan []int) ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:6:3","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"channel操作 通道有发送（send）、接收(receive）和关闭（close）三种操作。 发送和接收都使用\u003c-符号。 现在我们先使用以下语句定义一个通道： ch := make(chan int) 发送 将一个值发送到通道中。 ch \u003c- 10 // 把10发送到ch中 接收 从一个通道中接收值。 x := \u003c- ch // 从ch中接收值并赋值给变量x \u003c-ch // 从ch中接收值，忽略结果 关闭 我们通过调用内置的close函数来关闭通道。 close(ch) 关于关闭通道需要注意的事情是，只有在通知接收方goroutine所有的数据都发送完毕的时候才需要关闭通道。通道是可以被垃圾回收机制回收的，它和关闭文件是不一样的，在结束操作之后关闭文件是必须要做的，但关闭通道不是必须的。 关闭后的通道有以下特点： 对一个关闭的通道再发送值就会导致panic。 对一个关闭的通道进行接收会一直获取值直到通道为空。 对一个关闭的并且没有值的通道执行接收操作会得到对应类型的零值。 关闭一个已经关闭的通道会导致panic。 v, ok \u003c-ch; ok 为 bool 值，true 表示正常接受，false 表示通道关闭 所有的 channel 接收者都会在 channel 关闭时，⽴刻从阻塞等待中返回且上 述 ok 值为 false。这个⼴播机制常被利⽤，进⾏向多个订阅者同时发送信号。如：退出信号 package channel_close import ( \"fmt\" \"sync\" \"testing\" ) func dataProducer(ch chan int, wg *sync.WaitGroup) { go func() { for i := 0; i \u003c 10; i++ { ch \u003c- i } close(ch) wg.Done() }() } func dataReceiver(ch chan int, wg *sync.WaitGroup) { go func() { for { if data, ok := \u003c-ch; ok { fmt.Println(data) } else { break } } wg.Done() }() } func TestCloseChannel(t *testing.T) { var wg sync.WaitGroup ch := make(chan int) wg.Add(1) dataProducer(ch, \u0026wg) wg.Add(1) dataReceiver(ch, \u0026wg) // wg.Add(1) // dataReceiver(ch, \u0026wg) wg.Wait() } ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:6:4","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"任务的返回 package concurrency import ( \"fmt\" \"testing\" \"time\" ) func isCancelled(cancelChan chan struct{}) bool { select { case \u003c-cancelChan: return true default: return false } } func cancel_1(cancelChan chan struct{}) { cancelChan \u003c- struct{}{} } func cancel_2(cancelChan chan struct{}) { close(cancelChan) } func TestCancel(t *testing.T) { cancelChan := make(chan struct{}, 0) for i := 0; i \u003c 5; i++ { go func(i int, cancelCh chan struct{}) { for { if isCancelled(cancelCh) { break } time.Sleep(time.Millisecond * 5) } fmt.Println(i, \"Cancelled\") }(i, cancelChan) } cancel_2(cancelChan) time.Sleep(time.Second * 1) } ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:6:5","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"context与任务的取消 关联任务的取消。 go1.9以后，把context正式并入到golang正式的一个包里面了。 context： 根 Context：通过 context.Background () 创建 ⼦ Context：context.WithCancel(parentContext) 创建 ctx, cancel := context.WithCancel(context.Background()) 当前 Context 被取消时，基于他的⼦ context 都会被取消 接收取消通知 \u003c-ctx.Done() package cancel import ( \"context\" \"fmt\" \"testing\" \"time\" ) func isCancelled(ctx context.Context) bool { select { case \u003c-ctx.Done(): return true default: return false } } func TestCancel(t *testing.T) { ctx, cancel := context.WithCancel(context.Background()) for i := 0; i \u003c 5; i++ { go func(i int, ctx context.Context) { for { if isCancelled(ctx) { break } time.Sleep(time.Millisecond * 5) } fmt.Println(i, \"Cancelled\") }(i, ctx) } cancel() time.Sleep(time.Second * 1) } ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:6:6","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"无缓冲的通道 无缓冲的通道又称为阻塞的通道。我们来看一下下面的代码： func main() { ch := make(chan int) ch \u003c- 10 fmt.Println(\"发送成功\") } 上面这段代码能够通过编译，但是执行的时候会出现以下错误： fatal error: all goroutines are asleep - deadlock! goroutine 1 [chan send]: main.main() .../src/github.com/pprof/studygo/day06/channel02/main.go:8 +0x54 为什么会出现deadlock错误呢？ 因为我们使用ch := make(chan int)创建的是无缓冲的通道，无缓冲的通道只有在有人接收值的时候才能发送值。就像你住的小区没有快递柜和代收点，快递员给你打电话必须要把这个物品送到你的手中，简单来说就是无缓冲的通道必须有接收才能发送。 上面的代码会阻塞在ch \u003c- 10这一行代码形成死锁，那如何解决这个问题呢？ 一种方法是启用一个goroutine去接收值，例如： func recv(c chan int) { ret := \u003c-c fmt.Println(\"接收成功\", ret) } func main() { ch := make(chan int) go recv(ch) // 启用goroutine从通道接收值 ch \u003c- 10 fmt.Println(\"发送成功\") } 无缓冲通道上的发送操作会阻塞，直到另一个goroutine在该通道上执行接收操作，这时值才能发送成功，两个goroutine将继续执行。相反，如果接收操作先执行，接收方的goroutine将阻塞，直到另一个goroutine在该通道上发送一个值。 使用无缓冲通道进行通信将导致发送和接收的goroutine同步化。因此，无缓冲通道也被称为同步通道。 有缓冲的通道 解决上面问题的方法还有一种就是使用有缓冲区的通道。 我们可以在使用make函数初始化通道的时候为其指定通道的容量，例如： func main() { ch := make(chan int, 1) // 创建一个容量为1的有缓冲区通道 ch \u003c- 10 fmt.Println(\"发送成功\") } 只要通道的容量大于零，那么该通道就是有缓冲的通道，通道的容量表示通道中能存放元素的数量。就像你小区的快递柜只有那么个多格子，格子满了就装不下了，就阻塞了，等到别人取走一个快递员就能往里面放一个。 我们可以使用内置的len函数获取通道内元素的数量，使用cap函数获取通道的容量，虽然我们很少会这么做。 ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:6:7","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"close() 可以通过内置的close()函数关闭channel（如果你的管道不往里存值或者取值的时候一定记得关闭管道） package main import \"fmt\" func main() { c := make(chan int) go func() { for i := 0; i \u003c 5; i++ { c \u003c- i } close(c) }() for { if data, ok := \u003c-c; ok { fmt.Println(data) } else { break } } fmt.Println(\"main结束\") } ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:6:8","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"如何优雅的从通道循环取值 当通过通道发送有限的数据时，我们可以通过close函数关闭通道来告知从该通道接收值的goroutine停止等待。当通道被关闭时，往该通道发送值会引发panic，从该通道里接收的值一直都是类型零值。那如何判断一个通道是否被关闭了呢？ 我们来看下面这个例子： // channel 练习 func main() { ch1 := make(chan int) ch2 := make(chan int) // 开启goroutine将0~100的数发送到ch1中 go func() { for i := 0; i \u003c 100; i++ { ch1 \u003c- i } close(ch1) }() // 开启goroutine从ch1中接收值，并将该值的平方发送到ch2中 go func() { for { i, ok := \u003c-ch1 // 通道关闭后再取值ok=false if !ok { break } ch2 \u003c- i * i } close(ch2) }() // 在主goroutine中从ch2中接收值打印 for i := range ch2 { // 通道关闭后会退出for range循环 fmt.Println(i) } } 从上面的例子中我们看到有两种方式在接收值的时候判断通道是否被关闭，我们通常使用的是for range的方式。 ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:6:9","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"单向通道 有的时候我们会将通道作为参数在多个任务函数间传递，很多时候我们在不同的任务函数中使用通道都会对其进行限制，比如限制通道在函数中只能发送或只能接收。 Go语言中提供了单向通道来处理这种情况。例如，我们把上面的例子改造如下： func counter(out chan\u003c- int) { for i := 0; i \u003c 100; i++ { out \u003c- i } close(out) } func squarer(out chan\u003c- int, in \u003c-chan int) { for i := range in { out \u003c- i * i } close(out) } func printer(in \u003c-chan int) { for i := range in { fmt.Println(i) } } func main() { ch1 := make(chan int) ch2 := make(chan int) go counter(ch1) go squarer(ch2, ch1) printer(ch2) } 其中， chan\u003c- int是一个只能发送的通道，可以发送但是不能接收； \u003c-chan int是一个只能接收的通道，可以接收但是不能发送。 在函数传参及任何赋值操作中将双向通道转换为单向通道是可以的，但反过来是不可以的。 ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:6:10","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"通道总结 channel常见的异常总结，如下图： 注意:关闭已经关闭的channel也会引发panic。 ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:6:11","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"用多路选择实现超时控制 多路选择select package select_test import ( \"fmt\" \"testing\" \"time\" ) func service() string { time.Sleep(time.Millisecond * 500) return \"Done\" } func AsyncService() chan string { retCh := make(chan string, 1) //retCh := make(chan string, 1) go func() { ret := service() fmt.Println(\"returned result.\") retCh \u003c- ret fmt.Println(\"service exited.\") }() return retCh } func TestSelect(t *testing.T) { select { case ret := \u003c-AsyncService(): t.Log(ret) case \u003c-time.After(time.Millisecond * 100): t.Error(\"time out\") } } ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:7:0","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"Goroutine池 ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:8:0","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"worker pool（goroutine池） 本质上是生产者消费者模型 可以有效控制goroutine数量，防止暴涨 需求： 计算一个数字的各个位数之和，例如数字123，结果为1+2+3=6 随机生成数字进行计算 控制台输出结果如下： package main import ( \"fmt\" \"math/rand\" ) type Job struct { // id Id int // 需要计算的随机数 RandNum int } type Result struct { // 这里必须传对象实例 job *Job // 求和 sum int } func main() { // 需要2个管道 // 1.job管道 jobChan := make(chan *Job, 128) // 2.结果管道 resultChan := make(chan *Result, 128) // 3.创建工作池 createPool(64, jobChan, resultChan) // 4.开个打印的协程 go func(resultChan chan *Result) { // 遍历结果管道打印 for result := range resultChan { fmt.Printf(\"job id:%v randnum:%v result:%d\\n\", result.job.Id, result.job.RandNum, result.sum) } }(resultChan) var id int // 循环创建job，输入到管道 for { id++ // 生成随机数 r_num := rand.Int() job := \u0026Job{ Id: id, RandNum: r_num, } jobChan \u003c- job } } // 创建工作池 // 参数1：开几个协程 func createPool(num int, jobChan chan *Job, resultChan chan *Result) { // 根据开协程个数，去跑运行 for i := 0; i \u003c num; i++ { go func(jobChan chan *Job, resultChan chan *Result) { // 执行运算 // 遍历job管道所有数据，进行相加 for job := range jobChan { // 随机数接过来 r_num := job.RandNum // 随机数每一位相加 // 定义返回值 var sum int for r_num != 0 { tmp := r_num % 10 sum += tmp r_num /= 10 } // 想要的结果是Result r := \u0026Result{ job: job, sum: sum, } //运算结果扔到管道 resultChan \u003c- r } }(jobChan, resultChan) } } ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:8:1","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"定时器 ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:9:0","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"定时器 Timer：时间到了，执行只执行1次 package main import ( \"fmt\" \"time\" ) func main() { // 1.timer基本使用 //timer1 := time.NewTimer(2 * time.Second) //t1 := time.Now() //fmt.Printf(\"t1:%v\\n\", t1) //t2 := \u003c-timer1.C //fmt.Printf(\"t2:%v\\n\", t2) // 2.验证timer只能响应1次 //timer2 := time.NewTimer(time.Second) //for { // \u003c-timer2.C // fmt.Println(\"时间到\") //} // 3.timer实现延时的功能 //(1) //time.Sleep(time.Second) //(2) //timer3 := time.NewTimer(2 * time.Second) //\u003c-timer3.C //fmt.Println(\"2秒到\") //(3) //\u003c-time.After(2*time.Second) //fmt.Println(\"2秒到\") // 4.停止定时器 //timer4 := time.NewTimer(2 * time.Second) //go func() { // \u003c-timer4.C // fmt.Println(\"定时器执行了\") //}() //b := timer4.Stop() //if b { // fmt.Println(\"timer4已经关闭\") //} // 5.重置定时器 timer5 := time.NewTimer(3 * time.Second) timer5.Reset(1 * time.Second) fmt.Println(time.Now()) fmt.Println(\u003c-timer5.C) for { } } Ticker：时间到了，多次执行 package main import ( \"fmt\" \"time\" ) func main() { // 1.获取ticker对象 ticker := time.NewTicker(1 * time.Second) i := 0 // 子协程 go func() { for { //\u003c-ticker.C i++ fmt.Println(\u003c-ticker.C) if i == 5 { //停止 ticker.Stop() } } }() for { } } ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:9:1","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"select ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:10:0","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"select多路复用 在某些场景下我们需要同时从多个通道接收数据。通道在接收数据时，如果没有数据可以接收将会发生阻塞。你也许会写出如下代码使用遍历的方式来实现： for{ // 尝试从ch1接收值 data, ok := \u003c-ch1 // 尝试从ch2接收值 data, ok := \u003c-ch2 … } 这种方式虽然可以实现从多个通道接收值的需求，但是运行性能会差很多。为了应对这种场景，Go内置了select关键字，可以同时响应多个通道的操作。 select的使用类似于switch语句，它有一系列case分支和一个默认的分支。每个case会对应一个通道的通信（接收或发送）过程。select会一直等待，直到某个case的通信操作完成时，就会执行case分支对应的语句。具体格式如下： select { case \u003c-chan1: // 如果chan1成功读到数据，则进行该case处理语句 case chan2 \u003c- 1: // 如果成功向chan2写入数据，则进行该case处理语句 default: // 如果上面都没有成功，则进入default处理流程 } select可以同时监听一个或多个channel，直到其中一个channel ready package main import ( \"fmt\" \"time\" ) func test1(ch chan string) { time.Sleep(time.Second * 5) ch \u003c- \"test1\" } func test2(ch chan string) { time.Sleep(time.Second * 2) ch \u003c- \"test2\" } func main() { // 2个管道 output1 := make(chan string) output2 := make(chan string) // 跑2个子协程，写数据 go test1(output1) go test2(output2) // 用select监控 select { case s1 := \u003c-output1: fmt.Println(\"s1=\", s1) case s2 := \u003c-output2: fmt.Println(\"s2=\", s2) } } 如果多个channel同时ready，则随机选择一个执行 package main import ( \"fmt\" ) func main() { // 创建2个管道 int_chan := make(chan int, 1) string_chan := make(chan string, 1) go func() { //time.Sleep(2 * time.Second) int_chan \u003c- 1 }() go func() { string_chan \u003c- \"hello\" }() select { case value := \u003c-int_chan: fmt.Println(\"int:\", value) case value := \u003c-string_chan: fmt.Println(\"string:\", value) } fmt.Println(\"main结束\") } 可以用于判断管道是否存满 package main import ( \"fmt\" \"time\" ) // 判断管道有没有存满 func main() { // 创建管道 output1 := make(chan string, 10) // 子协程写数据 go write(output1) // 取数据 for s := range output1 { fmt.Println(\"res:\", s) time.Sleep(time.Second) } } func write(ch chan string) { for { select { // 写数据 case ch \u003c- \"hello\": fmt.Println(\"write hello\") default: fmt.Println(\"channel full\") } time.Sleep(time.Millisecond * 500) } } ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:10:1","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"并发安全和锁 有时候在Go代码中可能会存在多个goroutine同时操作一个资源（临界区），这种情况会发生竞态问题（数据竞态）。类比现实生活中的例子有十字路口被各个方向的的汽车竞争；还有火车上的卫生间被车厢里的人竞争。 举个例子： var x int64 var wg sync.WaitGroup func add() { for i := 0; i \u003c 5000; i++ { x = x + 1 } wg.Done() } func main() { wg.Add(2) go add() go add() wg.Wait() fmt.Println(x) } 上面的代码中我们开启了两个goroutine去累加变量x的值，这两个goroutine在访问和修改x变量的时候就会存在数据竞争，导致最后的结果与期待的不符。 ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:11:0","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"互斥锁 互斥锁是一种常用的控制共享资源访问的方法，它能够保证同时只有一个goroutine可以访问共享资源。Go语言中使用sync包的Mutex类型来实现互斥锁。 使用互斥锁来修复上面代码的问题： var x int64 var wg sync.WaitGroup var lock sync.Mutex func add() { for i := 0; i \u003c 5000; i++ { lock.Lock() // 加锁 x = x + 1 lock.Unlock() // 解锁 } wg.Done() } func main() { wg.Add(2) go add() go add() wg.Wait() fmt.Println(x) } 使用互斥锁能够保证同一时间有且只有一个goroutine进入临界区，其他的goroutine则在等待锁；当互斥锁释放后，等待的goroutine才可以获取锁进入临界区，多个goroutine同时等待一个锁时，唤醒的策略是随机的。 ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:11:1","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"读写互斥锁 互斥锁是完全互斥的，但是有很多实际的场景下是读多写少的，当我们并发的去读取一个资源不涉及资源修改的时候是没有必要加锁的，这种场景下使用读写锁是更好的一种选择。读写锁在Go语言中使用sync包中的RWMutex类型。 读写锁分为两种：读锁和写锁。当一个goroutine获取读锁之后，其他的goroutine如果是获取读锁会继续获得锁，如果是获取写锁就会等待；当一个goroutine获取写锁之后，其他的goroutine无论是获取读锁还是写锁都会等待。 读写锁示例： var ( x int64 wg sync.WaitGroup lock sync.Mutex rwlock sync.RWMutex ) func write() { // lock.Lock() // 加互斥锁 rwlock.Lock() // 加写锁 x = x + 1 time.Sleep(10 * time.Millisecond) // 假设读操作耗时10毫秒 rwlock.Unlock() // 解写锁 // lock.Unlock() // 解互斥锁 wg.Done() } func read() { // lock.Lock() // 加互斥锁 rwlock.RLock() // 加读锁 time.Sleep(time.Millisecond) // 假设读操作耗时1毫秒 rwlock.RUnlock() // 解读锁 // lock.Unlock() // 解互斥锁 wg.Done() } func main() { start := time.Now() for i := 0; i \u003c 10; i++ { wg.Add(1) go write() } for i := 0; i \u003c 1000; i++ { wg.Add(1) go read() } wg.Wait() end := time.Now() fmt.Println(end.Sub(start)) } 需要注意的是读写锁非常适合读多写少的场景，如果读和写的操作差别不大，读写锁的优势就发挥不出来。 ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:11:2","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"Sync ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:12:0","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"sync.WaitGroup 在代码中生硬的使用time.Sleep肯定是不合适的，Go语言中可以使用sync.WaitGroup来实现并发任务的同步。 sync.WaitGroup有以下几个方法： 方法名 功能 (wg * WaitGroup) Add(delta int) 计数器+delta (wg *WaitGroup) Done() 计数器-1 (wg *WaitGroup) Wait() 阻塞直到计数器变为0 sync.WaitGroup内部维护着一个计数器，计数器的值可以增加和减少。例如当我们启动了N 个并发任务时，就将计数器值增加N。每个任务完成时通过调用Done()方法将计数器减1。通过调用Wait()来等待并发任务执行完，当计数器值为0时，表示所有并发任务已经完成。 我们利用sync.WaitGroup将上面的代码优化一下： var wg sync.WaitGroup func hello() { defer wg.Done() fmt.Println(\"Hello Goroutine!\") } func main() { wg.Add(1) go hello() // 启动另外一个goroutine去执行hello函数 fmt.Println(\"main goroutine done!\") wg.Wait() } 需要注意sync.WaitGroup是一个结构体，传递的时候要传递指针。 ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:12:1","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"sync.Once 说在前面的话：这是一个进阶知识点。 在编程的很多场景下我们需要确保某些操作在高并发的场景下只执行一次，例如只加载一次配置文件、只关闭一次通道等。 Go语言中的sync包中提供了一个针对只执行一次场景的解决方案–sync.Once。 sync.Once只有一个Do方法，其签名如下： func (o *Once) Do(f func()) {} 注意：如果要执行的函数f需要传递参数就需要搭配闭包来使用。 ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:12:2","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"加载配置文件示例 延迟一个开销很大的初始化操作到真正用到它的时候再执行是一个很好的实践。因为预先初始化一个变量（比如在init函数中完成初始化）会增加程序的启动耗时，而且有可能实际执行过程中这个变量没有用上，那么这个初始化操作就不是必须要做的。我们来看一个例子： var icons map[string]image.Image func loadIcons() { icons = map[string]image.Image{ \"left\": loadIcon(\"left.png\"), \"up\": loadIcon(\"up.png\"), \"right\": loadIcon(\"right.png\"), \"down\": loadIcon(\"down.png\"), } } // Icon 被多个goroutine调用时不是并发安全的 func Icon(name string) image.Image { if icons == nil { loadIcons() } return icons[name] } 多个goroutine并发调用Icon函数时不是并发安全的，现代的编译器和CPU可能会在保证每个goroutine都满足串行一致的基础上自由地重排访问内存的顺序。loadIcons函数可能会被重排为以下结果： func loadIcons() { icons = make(map[string]image.Image) icons[\"left\"] = loadIcon(\"left.png\") icons[\"up\"] = loadIcon(\"up.png\") icons[\"right\"] = loadIcon(\"right.png\") icons[\"down\"] = loadIcon(\"down.png\") } 在这种情况下就会出现即使判断了icons不是nil也不意味着变量初始化完成了。考虑到这种情况，我们能想到的办法就是添加互斥锁，保证初始化icons的时候不会被其他的goroutine操作，但是这样做又会引发性能问题。 使用sync.Once改造的示例代码如下： var icons map[string]image.Image var loadIconsOnce sync.Once func loadIcons() { icons = map[string]image.Image{ \"left\": loadIcon(\"left.png\"), \"up\": loadIcon(\"up.png\"), \"right\": loadIcon(\"right.png\"), \"down\": loadIcon(\"down.png\"), } } // Icon 是并发安全的 func Icon(name string) image.Image { loadIconsOnce.Do(loadIcons) return icons[name] } sync.Once其实内部包含一个互斥锁和一个布尔值，互斥锁保证布尔值和数据的安全，而布尔值用来记录初始化是否完成。这样设计就能保证初始化操作的时候是并发安全的并且初始化操作也不会被执行多次。 ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:12:3","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"sync.Map Go语言中内置的map不是并发安全的。请看下面的示例： var m = make(map[string]int) func get(key string) int { return m[key] } func set(key string, value int) { m[key] = value } func main() { wg := sync.WaitGroup{} for i := 0; i \u003c 20; i++ { wg.Add(1) go func(n int) { key := strconv.Itoa(n) set(key, n) fmt.Printf(\"k=:%v,v:=%v\\n\", key, get(key)) wg.Done() }(i) } wg.Wait() } 上面的代码开启少量几个goroutine的时候可能没什么问题，当并发多了之后执行上面的代码就会报fatal error: concurrent map writes错误。 像这种场景下就需要为map加锁来保证并发的安全性了，Go语言的sync包中提供了一个开箱即用的并发安全版map–sync.Map。开箱即用表示不用像内置的map一样使用make函数初始化就能直接使用。同时sync.Map内置了诸如Store、Load、LoadOrStore、Delete、Range等操作方法。 var m = sync.Map{} func main() { wg := sync.WaitGroup{} for i := 0; i \u003c 20; i++ { wg.Add(1) go func(n int) { key := strconv.Itoa(n) m.Store(key, n) value, _ := m.Load(key) fmt.Printf(\"k=:%v,v:=%v\\n\", key, value) wg.Done() }(i) } wg.Wait() } ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:12:4","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"原子操作 ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:13:0","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"原子操作 代码中的加锁操作因为涉及内核态的上下文切换会比较耗时、代价比较高。针对基本数据类型我们还可以使用原子操作来保证并发安全，因为原子操作是Go语言提供的方法它在用户态就可以完成，因此性能比加锁操作更好。Go语言中原子操作由内置的标准库sync/atomic提供。 ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:13:1","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"atomic包 方法 解释 func LoadInt32(addr int32) (val int32) func LoadInt64(addr int64) (val int64)\u003cbr\u003efunc LoadUint32(addruint32) (val uint32)\u003cbr\u003efunc LoadUint64(addruint64) (val uint64)\u003cbr\u003efunc LoadUintptr(addruintptr) (val uintptr)\u003cbr\u003efunc LoadPointer(addrunsafe.Pointer) (val unsafe.Pointer) 读取操作 func StoreInt32(addr *int32, val int32) func StoreInt64(addr *int64, val int64) func StoreUint32(addr *uint32, val uint32) func StoreUint64(addr *uint64, val uint64) func StoreUintptr(addr *uintptr, val uintptr) func StorePointer(addr *unsafe.Pointer, val unsafe.Pointer) 写入操作 func AddInt32(addr *int32, delta int32) (new int32) func AddInt64(addr *int64, delta int64) (new int64) func AddUint32(addr *uint32, delta uint32) (new uint32) func AddUint64(addr *uint64, delta uint64) (new uint64) func AddUintptr(addr *uintptr, delta uintptr) (new uintptr) 修改操作 func SwapInt32(addr *int32, new int32) (old int32) func SwapInt64(addr *int64, new int64) (old int64) func SwapUint32(addr *uint32, new uint32) (old uint32) func SwapUint64(addr *uint64, new uint64) (old uint64) func SwapUintptr(addr *uintptr, new uintptr) (old uintptr) func SwapPointer(addr *unsafe.Pointer, new unsafe.Pointer) (old unsafe.Pointer) 交换操作 func CompareAndSwapInt32(addr *int32, old, new int32) (swapped bool) func CompareAndSwapInt64(addr *int64, old, new int64) (swapped bool) func CompareAndSwapUint32(addr *uint32, old, new uint32) (swapped bool) func CompareAndSwapUint64(addr *uint64, old, new uint64) (swapped bool) func CompareAndSwapUintptr(addr *uintptr, old, new uintptr) (swapped bool) func CompareAndSwapPointer(addr *unsafe.Pointer, old, new unsafe.Pointer) (swapped bool) 比较并交换操作 ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:13:2","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"示例 我们填写一个示例来比较下互斥锁和原子操作的性能。 var x int64 var l sync.Mutex var wg sync.WaitGroup // 普通版加函数 func add() { // x = x + 1 x++ // 等价于上面的操作 wg.Done() } // 互斥锁版加函数 func mutexAdd() { l.Lock() x++ l.Unlock() wg.Done() } // 原子操作版加函数 func atomicAdd() { atomic.AddInt64(\u0026x, 1) wg.Done() } func main() { start := time.Now() for i := 0; i \u003c 10000; i++ { wg.Add(1) // go add() // 普通版add函数 不是并发安全的 // go mutexAdd() // 加锁版add函数 是并发安全的，但是加锁性能开销大 go atomicAdd() // 原子操作版add函数 是并发安全，性能优于加锁版 } wg.Wait() end := time.Now() fmt.Println(x) fmt.Println(end.Sub(start)) } atomic包提供了底层的原子级内存操作，对于同步算法的实现很有用。这些函数必须谨慎地保证正确使用。除了某些特殊的底层应用，使用通道或者sync包的函数/类型实现同步更好。 ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:13:3","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"GMP原理和调度 ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:14:0","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"一、Golang “调度器” 的由来？ 单进程时代不需要调度器 我们知道，一切的软件都是跑在操作系统上，真正用来干活 (计算) 的是 CPU。早期的操作系统每个程序就是一个进程，知道一个程序运行完，才能进行下一个进程，就是 “单进程时代” 一切的程序只能串行发生。 早期的单进程操作系统，面临 2 个问题： 单一的执行流程，计算机只能一个任务一个任务处理。 进程阻塞所带来的 CPU 时间浪费。 那么能不能有多个进程来宏观一起来执行多个任务呢？ 后来操作系统就具有了最早的并发能力：多进程并发，当一个进程阻塞的时候，切换到另外等待执行的进程，这样就能尽量把 CPU 利用起来，CPU 就不浪费了。 多进程 / 线程时代有了调度器需求 在多进程 / 多线程的操作系统中，就解决了阻塞的问题，因为一个进程阻塞 cpu 可以立刻切换到其他进程中去执行，而且调度 cpu 的算法可以保证在运行的进程都可以被分配到 cpu 的运行时间片。这样从宏观来看，似乎多个进程是在同时被运行。 但新的问题就又出现了，进程拥有太多的资源，进程的创建、切换、销毁，都会占用很长的时间，CPU 虽然利用起来了，但如果进程过多，CPU 有很大的一部分都被用来进行进程调度了。 怎么才能提高 CPU 的利用率呢？ 但是对于 Linux 操作系统来讲，cpu 对进程的态度和线程的态度是一样的。 很明显，CPU 调度切换的是进程和线程。尽管线程看起来很美好，但实际上多线程开发设计会变得更加复杂，要考虑很多同步竞争等问题，如锁、竞争冲突等。 协程来提高 CPU 利用率 多进程、多线程已经提高了系统的并发能力，但是在当今互联网高并发场景下，为每个任务都创建一个线程是不现实的，因为会消耗大量的内存 (进程虚拟内存会占用 4GB [32 位操作系统], 而线程也要大约 4MB)。 大量的进程 / 线程出现了新的问题 高内存占用 调度的高消耗 CPU 好了，然后工程师们就发现，其实一个线程分为 “内核态 “线程和” 用户态 “线程。 一个 “用户态线程” 必须要绑定一个 “内核态线程”，但是 CPU 并不知道有 “用户态线程” 的存在，它只知道它运行的是一个 “内核态线程”(Linux 的 PCB 进程控制块)。 这样，我们再去细化去分类一下，内核线程依然叫 “线程 (thread)”，用户线程叫 “协程 (co-routine)”. 看到这里，我们就要开脑洞了，既然一个协程 (co-routine) 可以绑定一个线程 (thread)，那么能不能多个协程 (co-routine) 绑定一个或者多个线程 (thread) 上呢。 之后，我们就看到了有 3 中协程和线程的映射关系： s=\"default\"\u003e N:1 关系 N 个协程绑定 1 个线程，优点就是协程在用户态线程即完成切换，不会陷入到内核态，这种切换非常的轻量快速。但也有很大的缺点，1 个进程的所有协程都绑定在 1 个线程上 缺点： 某个程序用不了硬件的多核加速能力 一旦某协程阻塞，造成线程阻塞，本进程的其他协程都无法执行了，根本就没有并发的能力了。 s=\"default\"\u003e 1:1 关系 1 个协程绑定 1 个线程，这种最容易实现。协程的调度都由 CPU 完成了，不存在 N:1 缺点， 缺点： 协程的创建、删除和切换的代价都由 CPU 完成，有点略显昂贵了。 s=\"default\"\u003e M:N 关系 M 个协程绑定 1 个线程，是 N:1 和 1:1 类型的结合，克服了以上 2 种模型的缺点，但实现起来最为复杂。 协程跟线程是有区别的，线程由 CPU 调度是抢占式的，协程由用户态调度是协作式的，一个协程让出 CPU 后，才执行下一个协程。 Go 语言的协程 goroutine Go 为了提供更容易使用的并发方法，使用了 goroutine 和 channel。goroutine 来自协程的概念，让一组可复用的函数运行在一组线程之上，即使有协程阻塞，该线程的其他协程也可以被 runtime 调度，转移到其他可运行的线程上。最关键的是，程序员看不到这些底层的细节，这就降低了编程的难度，提供了更容易的并发。 Go 中，协程被称为 goroutine，它非常轻量，一个 goroutine 只占几 KB，并且这几 KB 就足够 goroutine 运行完，这就能在有限的内存空间内支持大量 goroutine，支持了更多的并发。虽然一个 goroutine 的栈只占几 KB，但实际是可伸缩的，如果需要更多内容，runtime 会自动为 goroutine 分配。 Goroutine 特点： 占用内存更小（几 kb） 调度更灵活 (runtime 调度) 被废弃的 goroutine 调度器 好了，既然我们知道了协程和线程的关系，那么最关键的一点就是调度协程的调度器的实现了。 Go 目前使用的调度器是 2012 年重新设计的，因为之前的调度器性能存在问题，所以使用 4 年就被废弃了，那么我们先来分析一下被废弃的调度器是如何运作的？ 大部分文章都是会用 G 来表示 Goroutine，用 M 来表示线程，那么我们也会用这种表达的对应关系。 下面我们来看看被废弃的 golang 调度器是如何实现的？ M 想要执行、放回 G 都必须访问全局 G 队列，并且 M 有多个，即多线程访问同一资源需要加锁进行保证互斥 / 同步，所以全局 G 队列是有互斥锁进行保护的。 老调度器有几个缺点： 创建、销毁、调度 G 都需要每个 M 获取锁，这就形成了激烈的锁竞争。 M 转移 G 会造成延迟和额外的系统负载。比如当 G 中包含创建新协程的时候，M 创建了 G’，为了继续执行 G，需要把 G’交给 M’执行，也造成了很差的局部性，因为 G’和 G 是相关的，最好放在 M 上执行，而不是其他 M’。 系统调用 (CPU 在 M 之间的切换) 导致频繁的线程阻塞和取消阻塞操作增加了系统开销。 ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:14:1","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"二、Goroutine 调度器的 GMP 模型的设计思想 面对之前调度器的问题，Go 设计了新的调度器。 在新调度器中，出列 M (thread) 和 G (goroutine)，又引进了 P (Processor)。 Processor，它包含了运行 goroutine 的资源，如果线程想运行 goroutine，必须先获取 P，P 中还包含了可运行的 G 队列。 GMP 模型 在 Go 中，线程是运行 goroutine 的实体，调度器的功能是把可运行的 goroutine 分配到工作线程上。 全局队列（Global Queue）：存放等待运行的 G。 P 的本地队列：同全局队列类似，存放的也是等待运行的 G，存的数量有限，不超过 256 个。新建 G’时，G’优先加入到 P 的本地队列，如果队列满了，则会把本地队列中一半的 G 移动到全局队列。 P 列表：所有的 P 都在程序启动时创建，并保存在数组中，最多有 GOMAXPROCS(可配置) 个。 M：线程想运行任务就得获取 P，从 P 的本地队列获取 G，P 队列为空时，M 也会尝试从全局队列拿一批 G 放到 P 的本地队列，或从其他 P 的本地队列偷一半放到自己 P 的本地队列。M 运行 G，G 执行之后，M 会从 P 获取下一个 G，不断重复下去。 Goroutine 调度器和 OS 调度器是通过 M 结合起来的，每个 M 都代表了 1 个内核线程，OS 调度器负责把内核线程分配到 CPU 的核上执行。 有关 P 和 M 的个数问题： P 的数量： 由启动时环境变量 $GOMAXPROCS 或者是由 runtime 的方法 GOMAXPROCS() 决定。这意味着在程序执行的任意时刻都只有 $GOMAXPROCS 个 goroutine 在同时运行。 M 的数量: go 语言本身的限制：go 程序启动时，会设置 M 的最大数量，默认 10000. 但是内核很难支持这么多的线程数，所以这个限制可以忽略。 runtime/debug 中的 SetMaxThreads 函数，设置 M 的最大数量 一个 M 阻塞了，会创建新的 M。 M 与 P 的数量没有绝对关系，一个 M 阻塞，P 就会去创建或者切换另一个 M，所以，即使 P 的默认数量是 1，也有可能会创建很多个 M 出来。 P 和 M 何时会被创建： P 何时创建：在确定了 P 的最大数量 n 后，运行时系统会根据这个数量创建 n 个 P。 M 何时创建：没有足够的 M 来关联 P 并运行其中的可运行的 G。比如所有的 M 此时都阻塞住了，而 P 中还有很多就绪任务，就会去寻找空闲的 M，而没有空闲的，就会去创建新的 M。 调度器的设计策略 复用线程：避免频繁的创建、销毁线程，而是对线程的复用。 1）work stealing 机制 当本线程无可运行的 G 时，尝试从其他线程绑定的 P 偷取 G，而不是销毁线程。 2）hand off 机制 当本线程因为 G 进行系统调用阻塞时，线程释放绑定的 P，把 P 转移给其他空闲的线程执行。 利用并行：GOMAXPROCS 设置 P 的数量，最多有 GOMAXPROCS 个线程分布在多个 CPU 上同时运行。GOMAXPROCS 也限制了并发的程度，比如 GOMAXPROCS = 核数/2，则最多利用了一半的 CPU 核进行并行。 抢占：在 coroutine 中要等待一个协程主动让出 CPU 才执行下一个协程，在 Go 中，一个 goroutine 最多占用 CPU 10ms，防止其他 goroutine 被饿死，这就是 goroutine 不同于 coroutine 的一个地方。 全局 G 队列：在新的调度器中依然有全局 G 队列，但功能已经被弱化了，当 M 执行 work stealing 从其他 P 偷不到 G 时，它可以从全局 G 队列获取 G。 go func () 调度流程 从上图我们可以分析出几个结论： 我们通过 go func () 来创建一个 goroutine； 有两个存储 G 的队列，一个是局部调度器 P 的本地队列、一个是全局 G 队列。新创建的 G 会先保存在 P 的本地队列中，如果 P 的本地队列已经满了就会保存在全局的队列中； G 只能运行在 M 中，一个 M 必须持有一个 P，M 与 P 是 1：1 的关系。M 会从 P 的本地队列弹出一个可执行状态的 G 来执行，如果 P 的本地队列为空，就会想其他的 MP 组合偷取一个可执行的 G 来执行； 一个 M 调度 G 执行的过程是一个循环机制； 当 M 执行某一个 G 时候如果发生了 syscall 或则其余阻塞操作，M 会阻塞，如果当前有一些 G 在执行，runtime 会把这个线程 M 从 P 中摘除 (detach)，然后再创建一个新的操作系统的线程 (如果有空闲的线程可用就复用空闲线程) 来服务于这个 P； 当 M 系统调用结束时候，这个 G 会尝试获取一个空闲的 P 执行，并放入到这个 P 的本地队列。如果获取不到 P，那么这个线程 M 变成休眠状态， 加入到空闲线程中，然后这个 G 会被放入全局队列中。 调度器的生命周期 特殊的 M0 和 G0 M0 M0 是启动程序后的编号为 0 的主线程，这个 M 对应的实例会在全局变量 runtime.m0 中，不需要在 heap 上分配，M0 负责执行初始化操作和启动第一个 G， 在之后 M0 就和其他的 M 一样了。 G0 G0 是每次启动一个 M 都会第一个创建的 goroutine，G0 仅用于负责调度的 G，G0 不指向任何可执行的函数，每个 M 都会有一个自己的 G0。在调度或系统调用时会使用 G0 的栈空间，全局变量的 G0 是 M0 的 G0。 我们来跟踪一段代码 package main import \"fmt\" func main() { fmt.Println(\"Hello world\") } 接下来我们来针对上面的代码对调度器里面的结构做一个分析。 也会经历如上图所示的过程： runtime 创建最初的线程 m0 和 goroutine g0，并把 2 者关联。 调度器初始化：初始化 m0、栈、垃圾回收，以及创建和初始化由 GOMAXPROCS 个 P 构成的 P 列表。 示例代码中的 main 函数是 main.main，runtime 中也有 1 个 main 函数 ——runtime.main，代码经过编译后，runtime.main 会调用 main.main，程序启动时会为 runtime.main 创建 goroutine，称它为 main goroutine 吧，然后把 main goroutine 加入到 P 的本地队列。 启动 m0，m0 已经绑定了 P，会从 P 的本地队列获取 G，获取到 main goroutine。 G 拥有栈，M 根据 G 中的栈信息和调度信息设置运行环境 M 运行 G G 退出，再次回到 M 获取可运行的 G，这样重复下去，直到 main.main 退出，runtime.main 执行 Defer 和 Panic 处理，或调用 runtime.exit 退出程序。 调度器的生命周期几乎占满了一个 Go 程序的一生，runtime.main 的 goroutine 执行之前都是为调度器做准备工作，runtime.main 的 goroutine 运行，才是调度器的真正开始，直到 runtime.main 结束而结束。 可视化 GMP 编程 有 2 种方式可以查看一个程序的 GMP 的数据。 方式 1：go tool trace trace 记录了运行时的信息，能提供可视化的 Web 页面。 简单测试代码：main 函数创建 trace，trace 会运行在单独的 goroutine 中，然后 main 打印”Hello World” 退出。 trace.go package main import ( \"os\" \"fmt\" \"runtime/trace\" ) func main() { //创建trace文件 f, err := os.Create(\"trace.out\") if err != nil { panic(err) } defer f.Close() //启动trace goroutine err = trace.Start(f) if err != nil { panic(err) } defer trace.Stop() //main fmt.Println(\"Hello World\") } 运行程序 $ go run trace.go Hello World 会得到一个 trace.out 文件，然后我们可以用一个工具打开，来分析这个文件。 $ go tool trace trace.out 2020/02/23 10:44:11 Parsing trace... 2020/02/23 10:44:11 Splitting trace... 2020/02/23 10:44:11 Opening browser. Trace viewer is listening on http://127.0.0.1:33479 我们可以通过浏览器打开 http://127.0.0.1:33479 网址，点击 view trace 能够看见可视化的调度流程。 G 信息 点击 Goroutines 那一行可视化的数据条，我们会看到一些详细的信息。 一共有两个G在程序中，一个是特殊的G0，是每个M必","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:14:2","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"三、Go 调度器调度场景过程全解析 场景 1 P 拥有 G1，M1 获取 P 后开始运行 G1，G1 使用 go func() 创建了 G2，为了局部性 G2 优先加入到 P1 的本地队列。 场景 2 G1 运行完成后 (函数：goexit)，M 上运行的 goroutine 切换为 G0，G0 负责调度时协程的切换（函数：schedule）。从 P 的本地队列取 G2，从 G0 切换到 G2，并开始运行 G2 (函数：execute)。实现了线程 M1 的复用。 场景 3 假设每个 P 的本地队列只能存 3 个 G。G2 要创建了 6 个 G，前 3 个 G（G3, G4, G5）已经加入 p1 的本地队列，p1 本地队列满了。 场景 4 G2 在创建 G7 的时候，发现 P1 的本地队列已满，需要执行负载均衡 (把 P1 中本地队列中前一半的 G，还有新创建 G 转移到全局队列) （实现中并不一定是新的 G，如果 G 是 G2 之后就执行的，会被保存在本地队列，利用某个老的 G 替换新 G 加入全局队列） 这些 G 被转移到全局队列时，会被打乱顺序。所以 G3,G4,G7 被转移到全局队列。 场景 5 G2 创建 G8 时，P1 的本地队列未满，所以 G8 会被加入到 P1 的本地队列。 G8 加入到 P1 点本地队列的原因还是因为 P1 此时在与 M1 绑定，而 G2 此时是 M1 在执行。所以 G2 创建的新的 G 会优先放置到自己的 M 绑定的 P 上。 场景 6 规定：在创建 G 时，运行的 G 会尝试唤醒其他空闲的 P 和 M 组合去执行。 假定 G2 唤醒了 M2，M2 绑定了 P2，并运行 G0，但 P2 本地队列没有 G，M2 此时为自旋线程（没有 G 但为运行状态的线程，不断寻找 G）。 (7) 场景 7 M2 尝试从全局队列 (简称 “GQ”) 取一批 G 放到 P2 的本地队列（函数：findrunnable()）。M2 从全局队列取的 G 数量符合下面的公式： n = min(len(GQ)/GOMAXPROCS + 1, len(GQ/2)) 至少从全局队列取 1 个 g，但每次不要从全局队列移动太多的 g 到 p 本地队列，给其他 p 留点。这是从全局队列到 P 本地队列的负载均衡。 假定我们场景中一共有 4 个 P（GOMAXPROCS 设置为 4，那么我们允许最多就能用 4 个 P 来供 M 使用）。所以 M2 只从能从全局队列取 1 个 G（即 G3）移动 P2 本地队列，然后完成从 G0 到 G3 的切换，运行 G3。 场景 8 假设 G2 一直在 M1 上运行，经过 2 轮后，M2 已经把 G7、G4 从全局队列获取到了 P2 的本地队列并完成运行，全局队列和 P2 的本地队列都空了，如场景 8 图的左半部分。 全局队列已经没有 G，那 m 就要执行 work stealing (偷取)：从其他有 G 的 P 哪里偷取一半 G 过来，放到自己的 P 本地队列。P2 从 P1 的本地队列尾部取一半的 G，本例中一半则只有 1 个 G8，放到 P2 的本地队列并执行。 场景 9 G1 本地队列 G5、G6 已经被其他 M 偷走并运行完成，当前 M1 和 M2 分别在运行 G2 和 G8，M3 和 M4 没有 goroutine 可以运行，M3 和 M4 处于自旋状态，它们不断寻找 goroutine。 为什么要让 m3 和 m4 自旋，自旋本质是在运行，线程在运行却没有执行 G，就变成了浪费 CPU. 为什么不销毁现场，来节约 CPU 资源。因为创建和销毁 CPU 也会浪费时间，我们希望当有新 goroutine 创建时，立刻能有 M 运行它，如果销毁再新建就增加了时延，降低了效率。当然也考虑了过多的自旋线程是浪费 CPU，所以系统中最多有 GOMAXPROCS 个自旋的线程 (当前例子中的 GOMAXPROCS=4，所以一共 4 个 P)，多余的没事做线程会让他们休眠。 场景 10 假定当前除了 M3 和 M4 为自旋线程，还有 M5 和 M6 为空闲的线程 (没有得到 P 的绑定，注意我们这里最多就只能够存在 4 个 P，所以 P 的数量应该永远是 M\u003e=P, 大部分都是 M 在抢占需要运行的 P)，G8 创建了 G9，G8 进行了阻塞的系统调用，M2 和 P2 立即解绑，P2 会执行以下判断：如果 P2 本地队列有 G、全局队列有 G 或有空闲的 M，P2 都会立马唤醒 1 个 M 和它绑定，否则 P2 则会加入到空闲 P 列表，等待 M 来获取可用的 p。本场景中，P2 本地队列有 G9，可以和其他空闲的线程 M5 绑定。 (11) 场景 11 G8 创建了 G9，假如 G8 进行了非阻塞系统调用。 M2 和 P2 会解绑，但 M2 会记住 P2，然后 G8 和 M2 进入系统调用状态。当 G8 和 M2 退出系统调用时，会尝试获取 P2，如果无法获取，则获取空闲的 P，如果依然没有，G8 会被记为可运行状态，并加入到全局队列，M2 因为没有 P 的绑定而变成休眠状态 (长时间休眠等待 GC 回收销毁)。 ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:14:3","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"四、小结 总结，Go 调度器很轻量也很简单，足以撑起 goroutine 的调度工作，并且让 Go 具有了原生（强大）并发的能力。Go 调度本质是把大量的 goroutine 分配到少量线程上去执行，并利用多核并行，实现更强大的并发。 ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:14:4","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"典型并发任务 ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:15:0","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"只运行一次 在多线程的环境下，某一段代码只执行一次。 也就是单例模式。 在go语言里有个专门的方法 sync.Once package once_test import ( \"fmt\" \"sync\" \"testing\" \"unsafe\" ) type Singleton struct { data string } var singleInstance *Singleton var once sync.Once func GetSingletonObj() *Singleton { once.Do(func() { fmt.Println(\"Create Obj\") singleInstance = new(Singleton) }) return singleInstance } func TestGetSingletonObj(t *testing.T) { var wg sync.WaitGroup for i := 0; i \u003c 10; i++ { wg.Add(1) go func() { obj := GetSingletonObj() fmt.Printf(\"%X\\n\", unsafe.Pointer(obj)) wg.Done() }() } wg.Wait() } PS D:\\Go\\Go_WorkSpace\\go_learning-master\\code\\ch23\\singleton\u003e go test -v -run TestGetSingletonObj once_test.go === RUN TestGetSingletonObj Create Obj C00008A000 C00008A000 C00008A000 C00008A000 C00008A000 C00008A000 C00008A000 C00008A000 C00008A000 C00008A000 --- PASS: TestGetSingletonObj (0.00s) PASS ok command-line-arguments 0.550s ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:15:1","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"仅需任意任务完成 eg:并行执行很多任务，当一个任务返回的时候就可以返回给用户了。像搜索引擎返回搜索结果。 go的CSP的并发控制机制能够简单快速地实现这样的模式。 package concurrency import ( \"fmt\" \"runtime\" \"testing\" \"time\" ) func runTask(id int) string { time.Sleep(10 * time.Millisecond) return fmt.Sprintf(\"The result is from %d\", id) } func FirstResponse() string { numOfRunner := 10 ch := make(chan string, numOfRunner) for i := 0; i \u003c numOfRunner; i++ { go func(i int) { ret := runTask(i) ch \u003c- ret }(i) } return \u003c-ch } func TestFirstResponse(t *testing.T) { t.Log(\"Before:\", runtime.NumGoroutine()) t.Log(FirstResponse()) time.Sleep(time.Second * 1) t.Log(\"After:\", runtime.NumGoroutine()) } ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:15:2","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"所有任务完成 sync package 里的WaitGroup即可实现。 另一种方式：在CSP模式下如何利用channel实现？ package util_all_done import ( \"fmt\" \"runtime\" \"testing\" \"time\" ) func runTask(id int) string { time.Sleep(10 * time.Millisecond) return fmt.Sprintf(\"The result is from %d\", id) } func AllResponse() string { numOfRunner := 10 ch := make(chan string, numOfRunner) for i := 0; i \u003c numOfRunner; i++ { go func(i int) { ret := runTask(i) ch \u003c- ret }(i) } finalRet := \"\" for j := 0; j \u003c numOfRunner; j++ { finalRet += \u003c-ch + \"\\n\" } return finalRet } func TestAllResponse(t *testing.T) { t.Log(\"Before:\", runtime.NumGoroutine()) t.Log(AllResponse()) time.Sleep(time.Second * 1) t.Log(\"After:\", runtime.NumGoroutine()) } ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:15:3","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"对象池 很多场景可能会遇到对象池，比如创建一些代价比较高的对象。数据库连接、网络连接。通常会将这些对象进行池化以避免重复创建 简单地可以使用buffered channel实现对象池。在select时需要有一个超时控制（高可用系统里有个金句：slow response比quick failure更糟糕） 当然可以用空接口来实现对象池里有不同类型的对象，但每次取出对象时需要断言来确认对象的类型。实际运用时建议不同类型用不同缓冲池。 package object_pool import ( \"errors\" \"time\" ) type ReusableObj struct { } type ObjPool struct { bufChan chan *ReusableObj //用于缓冲可重用对象 } func NewObjPool(numOfObj int) *ObjPool { objPool := ObjPool{} objPool.bufChan = make(chan *ReusableObj, numOfObj) for i := 0; i \u003c numOfObj; i++ { objPool.bufChan \u003c- \u0026ReusableObj{} } return \u0026objPool } func (p *ObjPool) GetObj(timeout time.Duration) (*ReusableObj, error) { select { case ret := \u003c-p.bufChan: return ret, nil case \u003c-time.After(timeout): //超时控制 return nil, errors.New(\"time out\") } } func (p *ObjPool) ReleaseObj(obj *ReusableObj) error { select { //channel被阻塞会立即返回default case p.bufChan \u003c- obj: return nil default: return errors.New(\"overflow\") } } ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:15:4","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"sync.pool对象缓存 注意与buffered channel区别。 sync.Pool对象获取： 尝试从私有对象获取 私有对象不存在，尝试从当前 Processor 的共享池获取 如果当前 Processor 共享池也是空的，那么就尝试去其他Processor 的共享池获取 如果所有⼦池都是空的，最后就⽤⽤户指定的 New 函数产⽣⼀个新的对象返回 对象放回： 如果私有对象不存在则保存为私有对象 如果私有对象存在，放⼊当前 Processor ⼦池的共享池中 …… pool := \u0026sync.Pool{ New: func() interface{} { return 0 }, } arry := pool.Get().(int) … pool.Put(10) 为什么sync.Pool不能拿来当对象池用？ sync.Pool对象的生命周期 GC 会清除 sync.pool 缓存的对象（GC是通过系统来调度的，没办法去干预，如果要长时间的去控制一个连接的生命周期就难以做到） 对象的缓存有效期为下⼀次GC 之前 package object_pool import ( \"fmt\" \"runtime\" \"sync\" \"testing\" ) func TestSyncPool(t *testing.T) { pool := \u0026sync.Pool{ New: func() interface{} { fmt.Println(\"Create a new object.\") return 100 }, } v := pool.Get().(int) fmt.Println(v) pool.Put(3) runtime.GC() //GC 会清除sync.pool中缓存的对象 v1, _ := pool.Get().(int) fmt.Println(v1) } func TestSyncPoolInMultiGroutine(t *testing.T) { pool := \u0026sync.Pool{ New: func() interface{} { fmt.Println(\"Create a new object.\") return 10 }, } pool.Put(100) pool.Put(100) pool.Put(100) var wg sync.WaitGroup for i := 0; i \u003c 10; i++ { wg.Add(1) go func(id int) { fmt.Println(pool.Get()) wg.Done() }(i) } wg.Wait() } sync.Pool总结： 适合于通过复⽤，降低复杂对象的创建和 GC 代价 协程安全，会有锁的开销 ⽣命周期受 GC 影响，不适合于做连接池等，需⾃⼰管理⽣命周期的资源的池化 ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:15:5","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"爬虫小案例 ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:16:0","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"爬虫步骤 明确目标（确定在哪个网站搜索） 爬（爬下内容） 取（筛选想要的） 处理数据（按照你的想法去处理） package main import ( \"fmt\" \"io/ioutil\" \"net/http\" \"regexp\" ) //这个只是一个简单的版本只是获取QQ邮箱并且没有进行封装操作，另外爬出来的数据也没有进行去重操作 var ( // \\d是数字 reQQEmail = `(\\d+)@qq.com` ) // 爬邮箱 func GetEmail() { // 1.去网站拿数据 resp, err := http.Get(\"https://tieba.baidu.com/p/6051076813?red_tag=1573533731\") HandleError(err, \"http.Get url\") defer resp.Body.Close() // 2.读取页面内容 pageBytes, err := ioutil.ReadAll(resp.Body) HandleError(err, \"ioutil.ReadAll\") // 字节转字符串 pageStr := string(pageBytes) //fmt.Println(pageStr) // 3.过滤数据，过滤qq邮箱 re := regexp.MustCompile(reQQEmail) // -1代表取全部 results := re.FindAllStringSubmatch(pageStr, -1) //fmt.Println(results) // 遍历结果 for _, result := range results { fmt.Println(\"email:\", result[0]) fmt.Println(\"qq:\", result[1]) } } // 处理异常 func HandleError(err error, why string) { if err != nil { fmt.Println(why, err) } } func main() { GetEmail() } ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:16:1","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"正则表达式 文档：https://studygolang.com/pkgdoc API re := regexp.MustCompile(reStr)，传入正则表达式，得到正则表达式对象 ret := re.FindAllStringSubmatch(srcStr,-1)：用正则对象，获取页面页面，srcStr是页面内容，-1代表取全部 爬邮箱 方法抽取 爬超链接 爬手机号 http://www.zhaohaowang.com/ 如果连接失效了自己找一个有手机号的就好了 爬身份证号 http://henan.qq.com/a/20171107/069413.htm 如果连接失效了自己找一个就好了 爬图片链接 package main import ( \"fmt\" \"io/ioutil\" \"net/http\" \"regexp\" ) var ( // w代表大小写字母+数字+下划线 reEmail = `\\w+@\\w+\\.\\w+` // s?有或者没有s // +代表出1次或多次 //\\s\\S各种字符 // +?代表贪婪模式 reLinke = `href=\"(https?://[\\s\\S]+?)\"` rePhone = `1[3456789]\\d\\s?\\d{4}\\s?\\d{4}` reIdcard = `[123456789]\\d{5}((19\\d{2})|(20[01]\\d))((0[1-9])|(1[012]))((0[1-9])|([12]\\d)|(3[01]))\\d{3}[\\dXx]` reImg = `https?://[^\"]+?(\\.((jpg)|(png)|(jpeg)|(gif)|(bmp)))` ) // 处理异常 func HandleError(err error, why string) { if err != nil { fmt.Println(why, err) } } func GetEmail2(url string) { pageStr := GetPageStr(url) re := regexp.MustCompile(reEmail) results := re.FindAllStringSubmatch(pageStr, -1) for _, result := range results { fmt.Println(result) } } // 抽取根据url获取内容 func GetPageStr(url string) (pageStr string) { resp, err := http.Get(url) HandleError(err, \"http.Get url\") defer resp.Body.Close() // 2.读取页面内容 pageBytes, err := ioutil.ReadAll(resp.Body) HandleError(err, \"ioutil.ReadAll\") // 字节转字符串 pageStr = string(pageBytes) return pageStr } func main() { // 2.抽取的爬邮箱 // GetEmail2(\"https://tieba.baidu.com/p/6051076813?red_tag=1573533731\") // 3.爬链接 //GetLink(\"http://www.baidu.com/s?wd=%E8%B4%B4%E5%90%A7%20%E7%95%99%E4%B8%8B%E9%82%AE%E7%AE%B1\u0026rsv_spt=1\u0026rsv_iqid=0x98ace53400003985\u0026issp=1\u0026f=8\u0026rsv_bp=1\u0026rsv_idx=2\u0026ie=utf-8\u0026tn=baiduhome_pg\u0026rsv_enter=1\u0026rsv_dl=ib\u0026rsv_sug2=0\u0026inputT=5197\u0026rsv_sug4=6345\") // 4.爬手机号 //GetPhone(\"https://www.zhaohaowang.com/\") // 5.爬身份证号 //GetIdCard(\"https://henan.qq.com/a/20171107/069413.htm\") // 6.爬图片 // GetImg(\"http://image.baidu.com/search/index?tn=baiduimage\u0026ps=1\u0026ct=201326592\u0026lm=-1\u0026cl=2\u0026nc=1\u0026ie=utf-8\u0026word=%E7%BE%8E%E5%A5%B3\") } func GetIdCard(url string) { pageStr := GetPageStr(url) re := regexp.MustCompile(reIdcard) results := re.FindAllStringSubmatch(pageStr, -1) for _, result := range results { fmt.Println(result) } } // 爬链接 func GetLink(url string) { pageStr := GetPageStr(url) re := regexp.MustCompile(reLinke) results := re.FindAllStringSubmatch(pageStr, -1) for _, result := range results { fmt.Println(result[1]) } } //爬手机号 func GetPhone(url string) { pageStr := GetPageStr(url) re := regexp.MustCompile(rePhone) results := re.FindAllStringSubmatch(pageStr, -1) for _, result := range results { fmt.Println(result) } } func GetImg(url string) { pageStr := GetPageStr(url) re := regexp.MustCompile(reImg) results := re.FindAllStringSubmatch(pageStr, -1) for _, result := range results { fmt.Println(result[0]) } } ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:16:2","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":"并发爬取美图 下面的两个是即将要爬的网站，如果网址失效自己换一个就好了 https://www.bizhizu.cn/shouji/tag-%E5%8F%AF%E7%88%B1/1.html package main import ( \"fmt\" \"io/ioutil\" \"net/http\" \"regexp\" \"strconv\" \"strings\" \"sync\" \"time\" ) func HandleError(err error, why string) { if err != nil { fmt.Println(why, err) } } // 下载图片，传入的是图片叫什么 func DownloadFile(url string, filename string) (ok bool) { resp, err := http.Get(url) HandleError(err, \"http.get.url\") defer resp.Body.Close() bytes, err := ioutil.ReadAll(resp.Body) HandleError(err, \"resp.body\") filename = \"E:/topgoer.com/src/github.com/student/3.0/img/\" + filename // 写出数据 err = ioutil.WriteFile(filename, bytes, 0666) if err != nil { return false } else { return true } } // 并发爬思路： // 1.初始化数据管道 // 2.爬虫写出：26个协程向管道中添加图片链接 // 3.任务统计协程：检查26个任务是否都完成，完成则关闭数据管道 // 4.下载协程：从管道里读取链接并下载 var ( // 存放图片链接的数据管道 chanImageUrls chan string waitGroup sync.WaitGroup // 用于监控协程 chanTask chan string reImg = `https?://[^\"]+?(\\.((jpg)|(png)|(jpeg)|(gif)|(bmp)))` ) func main() { // myTest() // DownloadFile(\"http://i1.shaodiyejin.com/uploads/tu/201909/10242/e5794daf58_4.jpg\", \"1.jpg\") // 1.初始化管道 chanImageUrls = make(chan string, 1000000) chanTask = make(chan string, 26) // 2.爬虫协程 for i := 1; i \u003c 27; i++ { waitGroup.Add(1) go getImgUrls(\"https://www.bizhizu.cn/shouji/tag-%E5%8F%AF%E7%88%B1/\" + strconv.Itoa(i) + \".html\") } // 3.任务统计协程，统计26个任务是否都完成，完成则关闭管道 waitGroup.Add(1) go CheckOK() // 4.下载协程：从管道中读取链接并下载 for i := 0; i \u003c 5; i++ { waitGroup.Add(1) go DownloadImg() } waitGroup.Wait() } // 下载图片 func DownloadImg() { for url := range chanImageUrls { filename := GetFilenameFromUrl(url) ok := DownloadFile(url, filename) if ok { fmt.Printf(\"%s 下载成功\\n\", filename) } else { fmt.Printf(\"%s 下载失败\\n\", filename) } } waitGroup.Done() } // 截取url名字 func GetFilenameFromUrl(url string) (filename string) { // 返回最后一个/的位置 lastIndex := strings.LastIndex(url, \"/\") // 切出来 filename = url[lastIndex+1:] // 时间戳解决重名 timePrefix := strconv.Itoa(int(time.Now().UnixNano())) filename = timePrefix + \"_\" + filename return } // 任务统计协程 func CheckOK() { var count int for { url := \u003c-chanTask fmt.Printf(\"%s 完成了爬取任务\\n\", url) count++ if count == 26 { close(chanImageUrls) break } } waitGroup.Done() } // 爬图片链接到管道 // url是传的整页链接 func getImgUrls(url string) { urls := getImgs(url) // 遍历切片里所有链接，存入数据管道 for _, url := range urls { chanImageUrls \u003c- url } // 标识当前协程完成 // 每完成一个任务，写一条数据 // 用于监控协程知道已经完成了几个任务 chanTask \u003c- url waitGroup.Done() } // 获取当前页图片链接 func getImgs(url string) (urls []string) { pageStr := GetPageStr(url) re := regexp.MustCompile(reImg) results := re.FindAllStringSubmatch(pageStr, -1) fmt.Printf(\"共找到%d条结果\\n\", len(results)) for _, result := range results { url := result[0] urls = append(urls, url) } return } // 抽取根据url获取内容 func GetPageStr(url string) (pageStr string) { resp, err := http.Get(url) HandleError(err, \"http.Get url\") defer resp.Body.Close() // 2.读取页面内容 pageBytes, err := ioutil.ReadAll(resp.Body) HandleError(err, \"ioutil.ReadAll\") // 字节转字符串 pageStr = string(pageBytes) return pageStr } ","date":"2022-01-06 09:16:49","objectID":"/go_base_07/:16:3","tags":["go grammar"],"title":"Go_base_07","uri":"/go_base_07/"},{"categories":["Go"],"content":" 参考学习go语言中文网、C语言中文网、golang官方文档等 网络编程 ","date":"2022-01-06 09:16:45","objectID":"/go_base_06/:0:0","tags":["go grammar"],"title":"Go_base_06","uri":"/go_base_06/"},{"categories":["Go"],"content":"互联网协议介绍 互联网的核心是一系列协议，总称为”互联网协议”（Internet Protocol Suite），正是这一些协议规定了电脑如何连接和组网。我们理解了这些协议，就理解了互联网的原理。由于这些协议太过庞大和复杂，没有办法在这里一概而全，只能介绍一下我们日常开发中接触较多的几个协议。 ","date":"2022-01-06 09:16:45","objectID":"/go_base_06/:1:0","tags":["go grammar"],"title":"Go_base_06","uri":"/go_base_06/"},{"categories":["Go"],"content":"互联网分层模型 互联网的逻辑实现被分为好几层。每一层都有自己的功能，就像建筑物一样，每一层都靠下一层支持。用户接触到的只是最上面的那一层，根本不会感觉到下面的几层。要理解互联网就需要自下而上理解每一层的实现的功能。 如上图所示，互联网按照不同的模型划分会有不用的分层，但是不论按照什么模型去划分，越往上的层越靠近用户，越往下的层越靠近硬件。在软件开发中我们使用最多的是上图中将互联网划分为五个分层的模型。 接下来我们一层一层的自底向上介绍一下每一层。 物理层 我们的电脑要与外界互联网通信，需要先把电脑连接网络，我们可以用双绞线、光纤、无线电波等方式。这就叫做”实物理层”，它就是把电脑连接起来的物理手段。它主要规定了网络的一些电气特性，作用是负责传送0和1的电信号。 数据链路层 单纯的0和1没有任何意义，所以我们使用者会为其赋予一些特定的含义，规定解读电信号的方式：例如：多少个电信号算一组？每个信号位有何意义？这就是”数据链接层”的功能，它在”物理层”的上方，确定了物理层传输的0和1的分组方式及代表的意义。早期的时候，每家公司都有自己的电信号分组方式。逐渐地，一种叫做”以太网”（Ethernet）的协议，占据了主导地位。 以太网规定，一组电信号构成一个数据包，叫做”帧”（Frame）。每一帧分成两个部分：标头（Head）和数据（Data）。其中”标头”包含数据包的一些说明项，比如发送者、接受者、数据类型等等；”数据”则是数据包的具体内容。”标头”的长度，固定为18字节。”数据”的长度，最短为46字节，最长为1500字节。因此，整个”帧”最短为64字节，最长为1518字节。如果数据很长，就必须分割成多个帧进行发送。 那么，发送者和接受者是如何标识呢？以太网规定，连入网络的所有设备都必须具有”网卡”接口。数据包必须是从一块网卡，传送到另一块网卡。网卡的地址，就是数据包的发送地址和接收地址，这叫做MAC地址。每块网卡出厂的时候，都有一个全世界独一无二的MAC地址，长度是48个二进制位，通常用12个十六进制数表示。前6个十六进制数是厂商编号，后6个是该厂商的网卡流水号。有了MAC地址，就可以定位网卡和数据包的路径了。 我们会通过ARP协议来获取接受方的MAC地址，有了MAC地址之后，如何把数据准确的发送给接收方呢？其实这里以太网采用了一种很”原始”的方式，它不是把数据包准确送到接收方，而是向本网络内所有计算机都发送，让每台计算机读取这个包的”标头”，找到接收方的MAC地址，然后与自身的MAC地址相比较，如果两者相同，就接受这个包，做进一步处理，否则就丢弃这个包。这种发送方式就叫做”广播”（broadcasting）。 网络层 按照以太网协议的规则我们可以依靠MAC地址来向外发送数据。理论上依靠MAC地址，你电脑的网卡就可以找到身在世界另一个角落的某台电脑的网卡了，但是这种做法有一个重大缺陷就是以太网采用广播方式发送数据包，所有成员人手一”包”，不仅效率低，而且发送的数据只能局限在发送者所在的子网络。也就是说如果两台计算机不在同一个子网络，广播是传不过去的。这种设计是合理且必要的，因为如果互联网上每一台计算机都会收到互联网上收发的所有数据包，那是不现实的。 因此，必须找到一种方法区分哪些MAC地址属于同一个子网络，哪些不是。如果是同一个子网络，就采用广播方式发送，否则就采用”路由”方式发送。这就导致了”网络层”的诞生。它的作用是引进一套新的地址，使得我们能够区分不同的计算机是否属于同一个子网络。这套地址就叫做”网络地址”，简称”网址”。 “网络层”出现以后，每台计算机有了两种地址，一种是MAC地址，另一种是网络地址。两种地址之间没有任何联系，MAC地址是绑定在网卡上的，网络地址则是网络管理员分配的。网络地址帮助我们确定计算机所在的子网络，MAC地址则将数据包送到该子网络中的目标网卡。因此，从逻辑上可以推断，必定是先处理网络地址，然后再处理MAC地址。 规定网络地址的协议，叫做IP协议。它所定义的地址，就被称为IP地址。目前，广泛采用的是IP协议第四版，简称IPv4。IPv4这个版本规定，网络地址由32个二进制位组成，我们通常习惯用分成四段的十进制数表示IP地址，从0.0.0.0一直到255.255.255.255。 根据IP协议发送的数据，就叫做IP数据包。IP数据包也分为”标头”和”数据”两个部分：”标头”部分主要包括版本、长度、IP地址等信息，”数据”部分则是IP数据包的具体内容。IP数据包的”标头”部分的长度为20到60字节，整个数据包的总长度最大为65535字节。 传输层 有了MAC地址和IP地址，我们已经可以在互联网上任意两台主机上建立通信。但问题是同一台主机上会有许多程序都需要用网络收发数据，比如QQ和浏览器这两个程序都需要连接互联网并收发数据，我们如何区分某个数据包到底是归哪个程序的呢？也就是说，我们还需要一个参数，表示这个数据包到底供哪个程序（进程）使用。这个参数就叫做”端口”（port），它其实是每一个使用网卡的程序的编号。每个数据包都发到主机的特定端口，所以不同的程序就能取到自己所需要的数据。 “端口”是0到65535之间的一个整数，正好16个二进制位。0到1023的端口被系统占用，用户只能选用大于1023的端口。有了IP和端口我们就能实现唯一确定互联网上一个程序，进而实现网络间的程序通信。 我们必须在数据包中加入端口信息，这就需要新的协议。最简单的实现叫做UDP协议，它的格式几乎就是在数据前面，加上端口号。UDP数据包，也是由”标头”和”数据”两部分组成：”标头”部分主要定义了发出端口和接收端口，”数据”部分就是具体的内容。UDP数据包非常简单，”标头”部分一共只有8个字节，总长度不超过65,535字节，正好放进一个IP数据包。 UDP协议的优点是比较简单，容易实现，但是缺点是可靠性较差，一旦数据包发出，无法知道对方是否收到。为了解决这个问题，提高网络可靠性，TCP协议就诞生了。TCP协议能够确保数据不会遗失。它的缺点是过程复杂、实现困难、消耗较多的资源。TCP数据包没有长度限制，理论上可以无限长，但是为了保证网络的效率，通常TCP数据包的长度不会超过IP数据包的长度，以确保单个TCP数据包不必再分割。 应用层 应用程序收到”传输层”的数据，接下来就要对数据进行解包。由于互联网是开放架构，数据来源五花八门，必须事先规定好通信的数据格式，否则接收方根本无法获得真正发送的数据内容。”应用层”的作用就是规定应用程序使用的数据格式，例如我们TCP协议之上常见的Email、HTTP、FTP等协议，这些协议就组成了互联网协议的应用层。 如下图所示，发送方的HTTP数据经过互联网的传输过程中会依次添加各层协议的标头信息，接收方收到数据包之后再依次根据协议解包得到数据。 ","date":"2022-01-06 09:16:45","objectID":"/go_base_06/:1:1","tags":["go grammar"],"title":"Go_base_06","uri":"/go_base_06/"},{"categories":["Go"],"content":"socket编程 ","date":"2022-01-06 09:16:45","objectID":"/go_base_06/:2:0","tags":["go grammar"],"title":"Go_base_06","uri":"/go_base_06/"},{"categories":["Go"],"content":"socket 图解 Socket是BSD UNIX的进程通信机制，通常也称作”套接字”，用于描述IP地址和端口，是一个通信链的句柄。Socket可以理解为TCP/IP网络的API，它定义了许多函数或例程，程序员可以用它们来开发TCP/IP网络上的应用程序。电脑上运行的应用程序通常通过”套接字”向网络发出请求或者应答网络请求。 socket图解 Socket是应用层与TCP/IP协议族通信的中间软件抽象层。在设计模式中，Socket其实就是一个门面模式，它把复杂的TCP/IP协议族隐藏在Socket后面，对用户来说只需要调用Socket规定的相关函数，让Socket去组织符合指定的协议数据然后进行通信。 Socket又称“套接字”，应用程序通常通过“套接字”向网络发出请求或者应答网络请求 常用的Socket类型有两种：流式Socket和数据报式Socket，流式是一种面向连接的Socket，针对于面向连接的TCP服务应用，数据报式Socket是一种无连接的Socket，针对于无连接的UDP服务应用 TCP：比较靠谱，面向连接，比较慢 UDP：不是太靠谱，比较快 举个例子：TCP就像货到付款的快递，送到家还必须见到你人才算一整套流程。UDP就像某快递快递柜一扔就走管你收到收不到，一般直播用UDP。 ","date":"2022-01-06 09:16:45","objectID":"/go_base_06/:2:1","tags":["go grammar"],"title":"Go_base_06","uri":"/go_base_06/"},{"categories":["Go"],"content":"TCP编程 Go语言实现TCP通信 TCP协议 TCP/IP(Transmission Control Protocol/Internet Protocol) 即传输控制协议/网际协议，是一种面向连接（连接导向）的、可靠的、基于字节流的传输层（Transport layer）通信协议，因为是面向连接的协议，数据像水流一样传输，会存在黏包问题。 TCP服务端 一个TCP服务端可以同时连接很多个客户端，例如世界各地的用户使用自己电脑上的浏览器访问淘宝网。因为Go语言中创建多个goroutine实现并发非常方便和高效，所以我们可以每建立一次链接就创建一个goroutine去处理。 TCP服务端程序的处理流程： 监听端口 接收客户端请求建立链接 创建goroutine处理链接。 我们使用Go语言的net包实现的TCP服务端代码如下： // tcp/server/main.go // TCP server端 // 处理函数 func process(conn net.Conn) { defer conn.Close() // 关闭连接 for { reader := bufio.NewReader(conn) var buf [128]byte n, err := reader.Read(buf[:]) // 读取数据 if err != nil { fmt.Println(\"read from client failed, err:\", err) break } recvStr := string(buf[:n]) fmt.Println(\"收到client端发来的数据：\", recvStr) conn.Write([]byte(recvStr)) // 发送数据 } } func main() { listen, err := net.Listen(\"tcp\", \"127.0.0.1:20000\") if err != nil { fmt.Println(\"listen failed, err:\", err) return } for { conn, err := listen.Accept() // 建立连接 if err != nil { fmt.Println(\"accept failed, err:\", err) continue } go process(conn) // 启动一个goroutine处理连接 } } 将上面的代码保存之后编译成server或server.exe可执行文件。 TCP客户端 一个TCP客户端进行TCP通信的流程如下： 建立与服务端的链接 进行数据收发 关闭链接 使用Go语言的net包实现的TCP客户端代码如下： // tcp/client/main.go // 客户端 func main() { conn, err := net.Dial(\"tcp\", \"127.0.0.1:20000\") if err != nil { fmt.Println(\"err :\", err) return } defer conn.Close() // 关闭连接 inputReader := bufio.NewReader(os.Stdin) for { input, _ := inputReader.ReadString('\\n') // 读取用户输入 inputInfo := strings.Trim(input, \"\\r\\n\") if strings.ToUpper(inputInfo) == \"Q\" { // 如果输入q就退出 return } _, err = conn.Write([]byte(inputInfo)) // 发送数据 if err != nil { return } buf := [512]byte{} n, err := conn.Read(buf[:]) if err != nil { fmt.Println(\"recv failed, err:\", err) return } fmt.Println(string(buf[:n])) } } 将上面的代码编译成client或client.exe可执行文件，先启动server端再启动client端，在client端输入任意内容回车之后就能够在server端看到client端发送的数据，从而实现TCP通信。 ","date":"2022-01-06 09:16:45","objectID":"/go_base_06/:2:2","tags":["go grammar"],"title":"Go_base_06","uri":"/go_base_06/"},{"categories":["Go"],"content":"UDP编程 Go语言实现UDP通信 UDP协议 UDP协议（User Datagram Protocol）中文名称是用户数据报协议，是OSI（Open System Interconnection，开放式系统互联）参考模型中一种无连接的传输层协议，不需要建立连接就能直接进行数据发送和接收，属于不可靠的、没有时序的通信，但是UDP协议的实时性比较好，通常用于视频直播相关领域。 UDP服务端 使用Go语言的net包实现的UDP服务端代码如下： // UDP/server/main.go // UDP server端 func main() { listen, err := net.ListenUDP(\"udp\", \u0026net.UDPAddr{ IP: net.IPv4(0, 0, 0, 0), Port: 30000, }) if err != nil { fmt.Println(\"listen failed, err:\", err) return } defer listen.Close() for { var data [1024]byte n, addr, err := listen.ReadFromUDP(data[:]) // 接收数据 if err != nil { fmt.Println(\"read udp failed, err:\", err) continue } fmt.Printf(\"data:%v addr:%v count:%v\\n\", string(data[:n]), addr, n) _, err = listen.WriteToUDP(data[:n], addr) // 发送数据 if err != nil { fmt.Println(\"write to udp failed, err:\", err) continue } } } UDP客户端 使用Go语言的net包实现的UDP客户端代码如下： // UDP 客户端 func main() { socket, err := net.DialUDP(\"udp\", nil, \u0026net.UDPAddr{ IP: net.IPv4(0, 0, 0, 0), Port: 30000, }) if err != nil { fmt.Println(\"连接服务端失败，err:\", err) return } defer socket.Close() sendData := []byte(\"Hello server\") _, err = socket.Write(sendData) // 发送数据 if err != nil { fmt.Println(\"发送数据失败，err:\", err) return } data := make([]byte, 4096) n, remoteAddr, err := socket.ReadFromUDP(data) // 接收数据 if err != nil { fmt.Println(\"接收数据失败，err:\", err) return } fmt.Printf(\"recv:%v addr:%v count:%v\\n\", string(data[:n]), remoteAddr, n) } ","date":"2022-01-06 09:16:45","objectID":"/go_base_06/:2:3","tags":["go grammar"],"title":"Go_base_06","uri":"/go_base_06/"},{"categories":["Go"],"content":"TCP黏包 服务端代码如下： // socket_stick/server/main.go func process(conn net.Conn) { defer conn.Close() reader := bufio.NewReader(conn) var buf [1024]byte for { n, err := reader.Read(buf[:]) if err == io.EOF { break } if err != nil { fmt.Println(\"read from client failed, err:\", err) break } recvStr := string(buf[:n]) fmt.Println(\"收到client发来的数据：\", recvStr) } } func main() { listen, err := net.Listen(\"tcp\", \"127.0.0.1:30000\") if err != nil { fmt.Println(\"listen failed, err:\", err) return } defer listen.Close() for { conn, err := listen.Accept() if err != nil { fmt.Println(\"accept failed, err:\", err) continue } go process(conn) } } 客户端代码如下： // socket_stick/client/main.go func main() { conn, err := net.Dial(\"tcp\", \"127.0.0.1:30000\") if err != nil { fmt.Println(\"dial failed, err\", err) return } defer conn.Close() for i := 0; i \u003c 20; i++ { msg := `Hello, Hello. How are you?` conn.Write([]byte(msg)) } } 将上面的代码保存后，分别编译。先启动服务端再启动客户端，可以看到服务端输出结果如下： 收到client发来的数据： Hello, Hello. How are you?Hello, Hello. How are you?Hello, Hello. How are you?Hello, Hello. How are you?Hello, Hello. How are you? 收到client发来的数据： Hello, Hello. How are you?Hello, Hello. How are you?Hello, Hello. How are you?Hello, Hello. How are you?Hello, Hello. How are you?Hello, Hello. How are you?Hello, Hello. How are you?Hello, Hello. How are you? 收到client发来的数据： Hello, Hello. How are you?Hello, Hello. How are you? 收到client发来的数据： Hello, Hello. How are you?Hello, Hello. How are you?Hello, Hello. How are you? 收到client发来的数据： Hello, Hello. How are you?Hello, Hello. How are you? 客户端分10次发送的数据，在服务端并没有成功的输出10次，而是多条数据“粘”到了一起。 ","date":"2022-01-06 09:16:45","objectID":"/go_base_06/:2:4","tags":["go grammar"],"title":"Go_base_06","uri":"/go_base_06/"},{"categories":["Go"],"content":"为什么会出现粘包 主要原因就是tcp数据传递模式是流模式，在保持长连接的时候可以进行多次的收和发。 “粘包”可发生在发送端也可发生在接收端： 由Nagle算法造成的发送端的粘包：Nagle算法是一种改善网络传输效率的算法。简单来说就是当我们提交一段数据给TCP发送时，TCP并不立刻发送此段数据，而是等待一小段时间看看在等待期间是否还有要发送的数据，若有则会一次把这两段数据发送出去。 接收端接收不及时造成的接收端粘包：TCP会把接收到的数据存在自己的缓冲区中，然后通知应用层取数据。当应用层由于某些原因不能及时的把TCP的数据取出来，就会造成TCP缓冲区中存放了几段数据。 ","date":"2022-01-06 09:16:45","objectID":"/go_base_06/:2:5","tags":["go grammar"],"title":"Go_base_06","uri":"/go_base_06/"},{"categories":["Go"],"content":"解决办法 出现”粘包”的关键在于接收方不确定将要传输的数据包的大小，因此我们可以对数据包进行封包和拆包的操作。 封包：封包就是给一段数据加上包头，这样一来数据包就分为包头和包体两部分内容了(过滤非法包时封包会加入”包尾”内容)。包头部分的长度是固定的，并且它存储了包体的长度，根据包头长度固定以及包头中含有包体长度的变量就能正确的拆分出一个完整的数据包。 我们可以自己定义一个协议，比如数据包的前4个字节为包头，里面存储的是发送的数据的长度。 // socket_stick/proto/proto.go package proto import ( \"bufio\" \"bytes\" \"encoding/binary\" ) // Encode 将消息编码 func Encode(message string) ([]byte, error) { // 读取消息的长度，转换成int32类型（占4个字节） var length = int32(len(message)) var pkg = new(bytes.Buffer) // 写入消息头 err := binary.Write(pkg, binary.LittleEndian, length) if err != nil { return nil, err } // 写入消息实体 err = binary.Write(pkg, binary.LittleEndian, []byte(message)) if err != nil { return nil, err } return pkg.Bytes(), nil } // Decode 解码消息 func Decode(reader *bufio.Reader) (string, error) { // 读取消息的长度 lengthByte, _ := reader.Peek(4) // 读取前4个字节的数据 lengthBuff := bytes.NewBuffer(lengthByte) var length int32 err := binary.Read(lengthBuff, binary.LittleEndian, \u0026length) if err != nil { return \"\", err } // Buffered返回缓冲中现有的可读取的字节数。 if int32(reader.Buffered()) \u003c length+4 { return \"\", err } // 读取真正的消息数据 pack := make([]byte, int(4+length)) _, err = reader.Read(pack) if err != nil { return \"\", err } return string(pack[4:]), nil } 接下来在服务端和客户端分别使用上面定义的proto包的Decode和Encode函数处理数据。 服务端代码如下： // socket_stick/server2/main.go func process(conn net.Conn) { defer conn.Close() reader := bufio.NewReader(conn) for { msg, err := proto.Decode(reader) if err == io.EOF { return } if err != nil { fmt.Println(\"decode msg failed, err:\", err) return } fmt.Println(\"收到client发来的数据：\", msg) } } func main() { listen, err := net.Listen(\"tcp\", \"127.0.0.1:30000\") if err != nil { fmt.Println(\"listen failed, err:\", err) return } defer listen.Close() for { conn, err := listen.Accept() if err != nil { fmt.Println(\"accept failed, err:\", err) continue } go process(conn) } } 客户端代码如下： // socket_stick/client2/main.go func main() { conn, err := net.Dial(\"tcp\", \"127.0.0.1:30000\") if err != nil { fmt.Println(\"dial failed, err\", err) return } defer conn.Close() for i := 0; i \u003c 20; i++ { msg := `Hello, Hello. How are you?` data, err := proto.Encode(msg) if err != nil { fmt.Println(\"encode msg failed, err:\", err) return } conn.Write(data) } } ","date":"2022-01-06 09:16:45","objectID":"/go_base_06/:2:6","tags":["go grammar"],"title":"Go_base_06","uri":"/go_base_06/"},{"categories":["Go"],"content":"http编程 ","date":"2022-01-06 09:16:45","objectID":"/go_base_06/:3:0","tags":["go grammar"],"title":"Go_base_06","uri":"/go_base_06/"},{"categories":["Go"],"content":"web工作流程 Web服务器的工作原理可以简单地归纳为 客户机通过TCP/IP协议建立到服务器的TCP连接 客户端向服务器发送HTTP协议请求包，请求服务器里的资源文档 服务器向客户机发送HTTP协议应答包，如果请求的资源包含有动态语言的内容，那么服务器会调用动态语言的解释引擎负责处理“动态内容”，并将处理得到的数据返回给客户端 客户机与服务器断开。由客户端解释HTML文档，在客户端屏幕上渲染图形结果 ","date":"2022-01-06 09:16:45","objectID":"/go_base_06/:3:1","tags":["go grammar"],"title":"Go_base_06","uri":"/go_base_06/"},{"categories":["Go"],"content":"HTTP协议 超文本传输协议(HTTP，HyperText Transfer Protocol)是互联网上应用最为广泛的一种网络协议，它详细规定了浏览器和万维网服务器之间互相通信的规则，通过因特网传送万维网文档的数据传送协议 HTTP协议通常承载于TCP协议之上 ","date":"2022-01-06 09:16:45","objectID":"/go_base_06/:3:2","tags":["go grammar"],"title":"Go_base_06","uri":"/go_base_06/"},{"categories":["Go"],"content":"HTTP服务端 package main import ( \"fmt\" \"net/http\" ) func main() { //http://127.0.0.1:8000/go // 单独写回调函数 http.HandleFunc(\"/go\", myHandler) //http.HandleFunc(\"/ungo\",myHandler2 ) // addr：监听的地址 // handler：回调函数 http.ListenAndServe(\"127.0.0.1:8000\", nil) } // handler函数 func myHandler(w http.ResponseWriter, r *http.Request) { fmt.Println(r.RemoteAddr, \"连接成功\") // 请求方式：GET POST DELETE PUT UPDATE fmt.Println(\"method:\", r.Method) // /go fmt.Println(\"url:\", r.URL.Path) fmt.Println(\"header:\", r.Header) fmt.Println(\"body:\", r.Body) // 回复 w.Write([]byte(\"www.5lmh.com\")) } go的默认路由规则： URL 分为两种，末尾是 /：表示⼀个⼦树，后⾯可以跟其他⼦路径； 末尾不是 /，表示⼀个叶⼦，固定的路径 以/ 结尾的 URL 可以匹配它的任何⼦路径，⽐如 /images 会匹配 /images/cute-cat.jpg 它采⽤最⻓匹配原则，如果有多个匹配，⼀定采⽤匹配路径最⻓的那个进⾏处理 如果没有找到任何匹配项，会返回 404 错误 //Default Router func (sh serverHandler) ServeHTTP(rw ResponseWriter, req *Request) { handler := sh.srv.Handler if handler == nil { handler = DefaultServeMux //使⽤缺省的Router } if req.RequestURI == \"*\" \u0026\u0026 req.Method == \"OPTIONS\" { handler = globalOptionsHandler{} } handler.ServeHTTP(rw, req) } ","date":"2022-01-06 09:16:45","objectID":"/go_base_06/:3:3","tags":["go grammar"],"title":"Go_base_06","uri":"/go_base_06/"},{"categories":["Go"],"content":"HTTP客户端 package main import ( \"fmt\" \"io\" \"net/http\" ) func main() { //resp, _ := http.Get(\"http://www.baidu.com\") //fmt.Println(resp) resp, _ := http.Get(\"http://127.0.0.1:8000/go\") defer resp.Body.Close() // 200 OK fmt.Println(resp.Status) fmt.Println(resp.Header) buf := make([]byte, 1024) for { // 接收服务端信息 n, err := resp.Body.Read(buf) if err != nil \u0026\u0026 err != io.EOF { fmt.Println(err) return } else { fmt.Println(\"读取完毕\") res := string(buf[:n]) fmt.Println(res) break } } } ","date":"2022-01-06 09:16:45","objectID":"/go_base_06/:3:4","tags":["go grammar"],"title":"Go_base_06","uri":"/go_base_06/"},{"categories":["Go"],"content":"构建RESTful服务 第三方的handler 更好的router //详情见https://github.com/julienschmidt/httprouter //handler多了一个参数 func Hello(w http.ResponseWriter, r *http.Request, ps httprouter.Params) { fmt.Fprintf(w, \"hello, %s!\\n\", ps.ByName(\"name\")) } func main() { router := httprouter.New() router.GET(\"/\", Index) router.GET(\"/hello/:name\", Hello) log.Fatal(http.ListenAndServe(\":8080\", router)) } RESTful程序设计很多时候会基于面向资源的架构（resource oriented architecture） package main import ( \"encoding/json\" \"fmt\" \"log\" \"net/http\" \"github.com/julienschmidt/httprouter\" ) type Employee struct { ID string `json:\"id\"` Name string `json:\"name\"` Age int `json:\"age\"` } var employeeDB map[string]*Employee func init() { employeeDB = map[string]*Employee{} employeeDB[\"Mike\"] = \u0026Employee{\"e-1\", \"Mike\", 35} employeeDB[\"Rose\"] = \u0026Employee{\"e-2\", \"Rose\", 45} } func Index(w http.ResponseWriter, r *http.Request, _ httprouter.Params) { fmt.Fprint(w, \"Welcome!\\n\") } func GetEmployeeByName(w http.ResponseWriter, r *http.Request, ps httprouter.Params) { qName := ps.ByName(\"name\") var ( ok bool info *Employee infoJson []byte err error ) if info, ok = employeeDB[qName]; !ok { w.Write([]byte(\"{\\\"error\\\":\\\"Not Found\\\"}\")) return } if infoJson, err = json.Marshal(info); err != nil { w.Write([]byte(fmt.Sprintf(\"{\\\"error\\\":,\\\"%s\\\"}\", err))) return } w.Write(infoJson) } func main() { router := httprouter.New() router.GET(\"/\", Index) router.GET(\"/employee/:name\", GetEmployeeByName) log.Fatal(http.ListenAndServe(\":8080\", router)) } ","date":"2022-01-06 09:16:45","objectID":"/go_base_06/:3:5","tags":["go grammar"],"title":"Go_base_06","uri":"/go_base_06/"},{"categories":["Go"],"content":"WebSocket编程 ","date":"2022-01-06 09:16:45","objectID":"/go_base_06/:4:0","tags":["go grammar"],"title":"Go_base_06","uri":"/go_base_06/"},{"categories":["Go"],"content":"webSocket是什么 WebSocket是一种在单个TCP连接上进行全双工通信的协议 WebSocket使得客户端和服务器之间的数据交换变得更加简单，允许服务端主动向客户端推送数据 在WebSocket API中，浏览器和服务器只需要完成一次握手，两者之间就直接可以创建持久性的连接，并进行双向数据传输 需要安装第三方包： cmd中：go get -u -v github.com/gorilla/websocket ","date":"2022-01-06 09:16:45","objectID":"/go_base_06/:4:1","tags":["go grammar"],"title":"Go_base_06","uri":"/go_base_06/"},{"categories":["Go"],"content":"举个聊天室的小例子 在同一级目录下新建四个go文件connection.go|data.go|hub.go|server.go 运行 go run server.go hub.go data.go connection.go 运行之后执行local.html文件 server.go文件代码 package main import ( \"fmt\" \"net/http\" \"github.com/gorilla/mux\" ) func main() { router := mux.NewRouter() go h.run() router.HandleFunc(\"/ws\", myws) if err := http.ListenAndServe(\"127.0.0.1:8080\", router); err != nil { fmt.Println(\"err:\", err) } } hub.go文件代码 package main import \"encoding/json\" var h = hub{ c: make(map[*connection]bool), u: make(chan *connection), b: make(chan []byte), r: make(chan *connection), } type hub struct { c map[*connection]bool b chan []byte r chan *connection u chan *connection } func (h *hub) run() { for { select { case c := \u003c-h.r: h.c[c] = true c.data.Ip = c.ws.RemoteAddr().String() c.data.Type = \"handshake\" c.data.UserList = user_list data_b, _ := json.Marshal(c.data) c.sc \u003c- data_b case c := \u003c-h.u: if _, ok := h.c[c]; ok { delete(h.c, c) close(c.sc) } case data := \u003c-h.b: for c := range h.c { select { case c.sc \u003c- data: default: delete(h.c, c) close(c.sc) } } } } } data.go文件代码 package main type Data struct { Ip string `json:\"ip\"` User string `json:\"user\"` From string `json:\"from\"` Type string `json:\"type\"` Content string `json:\"content\"` UserList []string `json:\"user_list\"` } connection.go文件代码 package main import ( \"encoding/json\" \"fmt\" \"net/http\" \"github.com/gorilla/websocket\" ) type connection struct { ws *websocket.Conn sc chan []byte data *Data } var wu = \u0026websocket.Upgrader{ReadBufferSize: 512, WriteBufferSize: 512, CheckOrigin: func(r *http.Request) bool { return true } } func myws(w http.ResponseWriter, r *http.Request) { ws, err := wu.Upgrade(w, r, nil) if err != nil { return } c := \u0026connection{sc: make(chan []byte, 256), ws: ws, data: \u0026Data{} } h.r \u003c- c go c.writer() c.reader() defer func() { c.data.Type = \"logout\" user_list = del(user_list, c.data.User) c.data.UserList = user_list c.data.Content = c.data.User data_b, _ := json.Marshal(c.data) h.b \u003c- data_b h.r \u003c- c }() } func (c *connection) writer() { for message := range c.sc { c.ws.WriteMessage(websocket.TextMessage, message) } c.ws.Close() } var user_list = []string{} func (c *connection) reader() { for { _, message, err := c.ws.ReadMessage() if err != nil { h.r \u003c- c break } json.Unmarshal(message, \u0026c.data) switch c.data.Type { case \"login\": c.data.User = c.data.Content c.data.From = c.data.User user_list = append(user_list, c.data.User) c.data.UserList = user_list data_b, _ := json.Marshal(c.data) h.b \u003c- data_b case \"user\": c.data.Type = \"user\" data_b, _ := json.Marshal(c.data) h.b \u003c- data_b case \"logout\": c.data.Type = \"logout\" user_list = del(user_list, c.data.User) data_b, _ := json.Marshal(c.data) h.b \u003c- data_b h.r \u003c- c default: fmt.Print(\"========default================\") } } } func del(slice []string, user string) []string { count := len(slice) if count == 0 { return slice } if count == 1 \u0026\u0026 slice[0] == user { return []string{} } var n_slice = []string{} for i := range slice { if slice[i] == user \u0026\u0026 i == count { return slice[:count] } else if slice[i] == user { n_slice = append(slice[:i], slice[i+1:]...) break } } fmt.Println(n_slice) return n_slice } local.html文件代码 \u003c!DOCTYPE html\u003e \u003chtml\u003e \u003chead\u003e \u003ctitle\u003e\u003c/title\u003e \u003cmeta http-equiv=\"content-type\" content=\"text/html;charset=utf-8\"\u003e \u003cstyle\u003e p { text-align: left; padding-left: 20px; } \u003c/style\u003e \u003c/head\u003e \u003cbody\u003e \u003cdiv style=\"width: 800px;height: 600px;margin: 30px auto;text-align: center\"\u003e \u003ch1\u003ewww.5lmh.comy演示聊天室\u003c/h1\u003e \u003cdiv style=\"width: 800px;border: 1px solid gray;height: 300px;\"\u003e \u003cdiv style=\"width: 200px;height: 300px;float: left;text-align: left;\"\u003e \u003cp\u003e\u003cspan\u003e当前在线:\u003c/span\u003e\u003cspan id=\"user_num\"\u003e0\u003c/span\u003e\u003c/p\u003e \u003cdiv id=\"user_list\" style=\"overflow: auto;\"\u003e \u003c/div\u003e \u003c/div\u003e \u003cdiv id=\"msg_list\" style=\"width: 598px;border: 1px solid gray; height: 300px;overflow: scroll;float: left;\"\u003e \u003c/div\u003e \u003c/div\u003e \u003cbr\u003e \u003ctextarea id=\"msg_box\" rows=\"6\" cols=\"50\" onkeydown=\"confirm(event)\"\u003e\u003c/textarea\u003e\u003cbr\u003e \u003cinput type=\"button\" value=\"发送\" onclick=\"send()\"\u003e \u003c/div\u003e \u003c/body\u003e \u003c/html\u003e \u003cscr","date":"2022-01-06 09:16:45","objectID":"/go_base_06/:4:2","tags":["go grammar"],"title":"Go_base_06","uri":"/go_base_06/"},{"categories":["Go"],"content":" 参考学习go语言中文网、C语言中文网、golang官方文档等 面向对象 Is Go an object-oriented language? Yes and no. Although Go has types and methods and allows an object-oriented style of programming, there is no type hierarchy. The concept of “interface” in Go provides a different approach that we believe is easy to use and in some ways more general. Also, the lack of a type hierarchy makes “objects” in Go feel much more lightweight than in languages such as C++ or Java. 个人认为官网的大致意思就是go算是面向对象语言，没有继承，但有更通用的“接口”，没有继承也使对象比某些语言更轻量。 ","date":"2022-01-06 09:16:40","objectID":"/go_base_05/:0:0","tags":["go grammar"],"title":"Go_base_05","uri":"/go_base_05/"},{"categories":["Go"],"content":"匿名字段 go支持只提供类型而不写字段名的方式，也就是匿名字段，也称为嵌入字段 package main import \"fmt\" //- go支持只提供类型而不写字段名的方式，也就是匿名字段，也称为嵌入字段 //人 type Person struct { name string sex string age int } type Student struct { Person id int addr string } func main() { // 初始化 s1 := Student{Person{\"5lmh\", \"man\", 20}, 1, \"bj\"} fmt.Println(s1) s2 := Student{Person: Person{\"5lmh\", \"man\", 20} } fmt.Println(s2) s3 := Student{Person: Person{name: \"5lmh\"} } fmt.Println(s3) } output： { {5lmh man 20} 1 bj} { {5lmh man 20} 0 } { {5lmh 0} 0 } 同名字段的情况 package main import \"fmt\" //人 type Person struct { name string sex string age int } type Student struct { Person id int addr string //同名字段 name string } func main() { var s Student // 给自己字段赋值了 s.name = \"5lmh\" fmt.Println(s) // 若给父类同名字段赋值，如下 s.Person.name = \"枯藤\" fmt.Println(s) } output： { { 0} 0 5lmh} { {枯藤 0} 0 5lmh} 所有的内置类型和自定义类型都是可以作为匿名字段去使用 package main import \"fmt\" //人 type Person struct { name string sex string age int } // 自定义类型 type mystr string // 学生 type Student struct { Person int mystr } func main() { s1 := Student{Person{\"5lmh\", \"man\", 18}, 1, \"bj\"} fmt.Println(s1) } output： { {5lmh man 18} 1 bj} 指针类型匿名字段 package main import \"fmt\" //人 type Person struct { name string sex string age int } // 学生 type Student struct { *Person id int addr string } func main() { s1 := Student{\u0026Person{\"5lmh\", \"man\", 18}, 1, \"bj\"} fmt.Println(s1) fmt.Println(s1.name) fmt.Println(s1.Person.name) } output： {0xc00006a360 1 bj} 5lmh 5lmh ","date":"2022-01-06 09:16:40","objectID":"/go_base_05/:1:0","tags":["go grammar"],"title":"Go_base_05","uri":"/go_base_05/"},{"categories":["Go"],"content":"接口 接口（interface）定义了一个对象的行为规范，只定义规范不实现，由具体的对象来实现规范的细节。（也可以说是定义对象之间交互的协议的） ","date":"2022-01-06 09:16:40","objectID":"/go_base_05/:2:0","tags":["go grammar"],"title":"Go_base_05","uri":"/go_base_05/"},{"categories":["Go"],"content":"接口 接口类型 在Go语言中接口（interface）是一种类型，一种抽象的类型。 interface是一组method的集合，是duck-type programming的一种体现。接口做的事情就像是定义一个协议（规则），只要一台机器有洗衣服和甩干的功能，我就称它为洗衣机。不关心属性（数据），只关心行为（方法）。 为什么要使用接口 type Cat struct{} func (c Cat) Say() string { return \"喵喵喵\" } type Dog struct{} func (d Dog) Say() string { return \"汪汪汪\" } func main() { c := Cat{} fmt.Println(\"猫:\", c.Say()) d := Dog{} fmt.Println(\"狗:\", d.Say()) } 上面的代码中定义了猫和狗，然后它们都会叫，你会发现main函数中明显有重复的代码，如果我们后续再加上猪、青蛙等动物的话，我们的代码还会一直重复下去。那我们能不能把它们当成“能叫的动物”来处理呢？ 像类似的例子在我们编程过程中会经常遇到： 比如一个网上商城可能使用支付宝、微信、银联等方式去在线支付，我们能不能把它们当成“支付方式”来处理呢？ 比如三角形，四边形，圆形都能计算周长和面积，我们能不能把它们当成“图形”来处理呢？ 比如销售、行政、程序员都能计算月薪，我们能不能把他们当成“员工”来处理呢？ Go语言中为了解决类似上面的问题，就设计了接口这个概念。接口区别于我们之前所有的具体类型，接口是一种抽象的类型。当你看到一个接口类型的值时，你不知道它是什么，唯一知道的是通过它的方法能做什么。 接口的定义： （Go语言提倡面向接口编程。） 接口是一个或多个方法签名的集合。 任何类型的方法集中只要拥有该接口'对应的全部方法'签名。 就表示它 \"实现\" 了该接口，无须在该类型上显式声明实现了哪个接口。 这称为Structural Typing。 所谓对应方法，是指有相同名称、参数列表 (不包括参数名) 以及返回值。 当然，该类型还可以有其他方法。 接口只有方法声明，没有实现，没有数据字段。 接口可以匿名嵌入其他接口，或嵌入到结构中。 对象赋值给接口时，会发生拷贝，而接口内部存储的是指向这个复制品的指针，既无法修改复制品的状态，也无法获取指针。 只有当接口存储的类型和对象都为nil时，接口才等于nil。 接口调用不会做receiver的自动转换。 接口同样支持匿名字段方法。 接口也可实现类似OOP中的多态。 空接口可以作为任何类型数据的容器。 一个类型可实现多个接口。 接口命名习惯以 er 结尾。 每个接口由数个方法组成，接口的定义格式如下： type 接口类型名 interface{ 方法名1( 参数列表1 ) 返回值列表1 方法名2( 参数列表2 ) 返回值列表2 … } 其中： 接口名：使用type将接口定义为自定义的类型名。Go语言的接口在命名时，一般会在单词后面添加er，如有写操作的接口叫Writer，有字符串功能的接口叫Stringer等。接口名最好要能突出该接口的类型含义。 方法名：当方法名首字母是大写且这个接口类型名首字母也是大写时，这个方法可以被接口所在的包（package）之外的代码访问。 参数列表、返回值列表：参数列表和返回值列表中的参数变量名可以省略。 举个例子： type writer interface{ Write([]byte) error } 当你看到这个接口类型的值时，你不知道它是什么，唯一知道的就是可以通过它的Write方法来做一些事情。 实现接口的条件 一个对象只要全部实现了接口中的方法，那么就实现了这个接口。换句话说，接口就是一个需要实现的方法列表。 我们来定义一个Sayer接口： // Sayer 接口 type Sayer interface { say() } 定义dog和cat两个结构体： type dog struct {} type cat struct {} 因为Sayer接口里只有一个say方法，所以我们只需要给dog和cat 分别实现say方法就可以实现Sayer接口了。 // dog实现了Sayer接口 func (d dog) say() { fmt.Println(\"汪汪汪\") } // cat实现了Sayer接口 func (c cat) say() { fmt.Println(\"喵喵喵\") } 接口的实现就是这么简单，只要实现了接口中的所有方法，就实现了这个接口。 接口类型变量 那实现了接口有什么用呢？ 接口类型变量能够存储所有实现了该接口的实例。 例如上面的示例中，Sayer类型的变量能够存储dog和cat类型的变量。 func main() { var x Sayer // 声明一个Sayer类型的变量x a := cat{} // 实例化一个cat b := dog{} // 实例化一个dog x = a // 可以把cat实例直接赋值给x x.say() // 喵喵喵 x = b // 可以把dog实例直接赋值给x x.say() // 汪汪汪 } 值接收者和指针接收者实现接口的区别 使用值接收者实现接口和使用指针接收者实现接口有什么区别呢？接下来我们通过一个例子看一下其中的区别。 我们有一个Mover接口和一个dog结构体。 type Mover interface { move() } type dog struct {} 值接收者实现接口 func (d dog) move() { fmt.Println(\"狗会动\") } 此时实现接口的是dog类型： func main() { var x Mover var wangcai = dog{} // 旺财是dog类型 x = wangcai // x可以接收dog类型 var fugui = \u0026dog{} // 富贵是*dog类型 x = fugui // x可以接收*dog类型 x.move() } 从上面的代码中我们可以发现，使用值接收者实现接口之后，不管是dog结构体还是结构体指针*dog类型的变量都可以赋值给该接口变量。因为Go语言中有对指针类型变量求值的语法糖，dog指针fugui内部会自动求值*fugui。 指针接收者实现接口 同样的代码我们再来测试一下使用指针接收者有什么区别： func (d *dog) move() { fmt.Println(\"狗会动\") } func main() { var x Mover var wangcai = dog{} // 旺财是dog类型 x = wangcai // x不可以接收dog类型 var fugui = \u0026dog{} // 富贵是*dog类型 x = fugui // x可以接收*dog类型 } 此时实现Mover接口的是*dog类型，所以不能给x传入dog类型的wangcai，此时x只能存储*dog类型的值。 用接口实现多态 接口的最佳实践 倾向于使⽤⼩的接⼝定义，很多接⼝只包含⼀个⽅法 较⼤的接⼝定义，可以由多个⼩接⼝定义组合⽽成 只依赖于必要功能的最小接口，这样方法的复用性更强。 下面的代码是一个比较好的面试题 请问下面的代码是否能通过编译？ type People interface { Speak(string) string } type Student struct{} func (stu *Student) Speak(think string) (talk string) { if think == \"sb\" { talk = \"你是个大帅比\" } else { talk = \"您好\" } return } func main() { var peo People = Student{} think := \"bitch\" fmt.Println(peo.Speak(think)) } ","date":"2022-01-06 09:16:40","objectID":"/go_base_05/:2:1","tags":["go grammar"],"title":"Go_base_05","uri":"/go_base_05/"},{"categories":["Go"],"content":"类型与接口的关系 一个类型实现多个接口 一个类型可以同时实现多个接口，而接口间彼此独立，不知道对方的实现。 例如，狗可以叫，也可以动。我们就分别定义Sayer接口和Mover接口，如下： Mover接口。 // Sayer 接口 type Sayer interface { say() } // Mover 接口 type Mover interface { move() } dog既可以实现Sayer接口，也可以实现Mover接口。 type dog struct { name string } // 实现Sayer接口 func (d dog) say() { fmt.Printf(\"%s会叫汪汪汪\\n\", d.name) } // 实现Mover接口 func (d dog) move() { fmt.Printf(\"%s会动\\n\", d.name) } func main() { var x Sayer var y Mover var a = dog{name: \"旺财\"} x = a y = a x.say() y.move() } 多个类型实现同一接口 Go语言中不同的类型还可以实现同一接口 首先我们定义一个Mover接口，它要求必须由一个move方法。 // Mover 接口 type Mover interface { move() } 例如狗可以动，汽车也可以动，可以使用如下代码实现这个关系： type dog struct { name string } type car struct { brand string } // dog类型实现Mover接口 func (d dog) move() { fmt.Printf(\"%s会跑\\n\", d.name) } // car类型实现Mover接口 func (c car) move() { fmt.Printf(\"%s速度70迈\\n\", c.brand) } 这个时候我们在代码中就可以把狗和汽车当成一个会动的物体来处理了，不再需要关注它们具体是什么，只需要调用它们的move方法就可以了。 func main() { var x Mover var a = dog{name: \"旺财\"} var b = car{brand: \"保时捷\"} x = a x.move() x = b x.move() } 上面的代码执行结果如下： 旺财会跑 保时捷速度70迈 并且一个接口的方法，不一定需要由一个类型完全实现，接口的方法可以通过在类型中嵌入其他类型或者结构体来实现。 // WashingMachine 洗衣机 type WashingMachine interface { wash() dry() } // 甩干器 type dryer struct{} // 实现WashingMachine接口的dry()方法 func (d dryer) dry() { fmt.Println(\"甩一甩\") } // 海尔洗衣机 type haier struct { dryer //嵌入甩干器 } // 实现WashingMachine接口的wash()方法 func (h haier) wash() { fmt.Println(\"洗刷刷\") } 接口嵌套 接口与接口间可以通过嵌套创造出新的接口。 // Sayer 接口 type Sayer interface { say() } // Mover 接口 type Mover interface { move() } // 接口嵌套 type animal interface { Sayer Mover } 嵌套得到的接口的使用与普通接口一样，这里我们让cat实现animal接口： type cat struct { name string } func (c cat) say() { fmt.Println(\"喵喵喵\") } func (c cat) move() { fmt.Println(\"猫会动\") } func main() { var x animal x = cat{name: \"花花\"} x.move() x.say() } ","date":"2022-01-06 09:16:40","objectID":"/go_base_05/:2:2","tags":["go grammar"],"title":"Go_base_05","uri":"/go_base_05/"},{"categories":["Go"],"content":"空接口 空接口的定义 空接口是指没有定义任何方法的接口。因此任何类型都实现了空接口。 空接口类型的变量可以存储任意类型的变量。 func main() { // 定义一个空接口x var x interface{} s := \"pprof.cn\" x = s fmt.Printf(\"type:%T value:%v\\n\", x, x) i := 100 x = i fmt.Printf(\"type:%T value:%v\\n\", x, x) b := true x = b fmt.Printf(\"type:%T value:%v\\n\", x, x) } ","date":"2022-01-06 09:16:40","objectID":"/go_base_05/:2:3","tags":["go grammar"],"title":"Go_base_05","uri":"/go_base_05/"},{"categories":["Go"],"content":"空接口的应用 空接口作为函数的参数 使用空接口实现可以接收任意类型的函数参数。 // 空接口作为函数参数 func show(a interface{}) { fmt.Printf(\"type:%T value:%v\\n\", a, a) } 空接口作为map的值 使用空接口实现可以保存任意值的字典。 // 空接口作为map值 var studentInfo = make(map[string]interface{}) studentInfo[\"name\"] = \"李白\" studentInfo[\"age\"] = 18 studentInfo[\"married\"] = false fmt.Println(studentInfo) ","date":"2022-01-06 09:16:40","objectID":"/go_base_05/:2:4","tags":["go grammar"],"title":"Go_base_05","uri":"/go_base_05/"},{"categories":["Go"],"content":"类型断言 空接口可以存储任意类型的值，那我们如何获取其存储的具体数据呢？ 接口值 一个接口的值（简称接口值）是由一个具体类型和具体类型的值两部分组成的。这两部分分别称为接口的动态类型和动态值。 我们来看一个具体的例子： var w io.Writer w = os.Stdout w = new(bytes.Buffer) w = nil 请看下图分解（来自go中文文档）：想要判断空接口中的值这个时候就可以使用类型断言，其语法格式： x.(T) 其中： x：表示类型为interface{}的变量 T：表示断言x可能是的类型。 该语法返回两个参数，第一个参数是x转化为T类型后的变量，第二个值是一个布尔值，若为true则表示断言成功，为false则表示断言失败。 举个例子： func main() { var x interface{} x = \"pprof.cn\" v, ok := x.(string) if ok { fmt.Println(v) } else { fmt.Println(\"类型断言失败\") } } 上面的示例中如果要断言多次就需要写多个if判断，这个时候我们可以使用switch语句来实现： func justifyType(x interface{}) { switch v := x.(type) { case string: fmt.Printf(\"x is a string，value is %v\\n\", v) case int: fmt.Printf(\"x is a int is %v\\n\", v) case bool: fmt.Printf(\"x is a bool is %v\\n\", v) default: fmt.Println(\"unsupport type！\") } } 因为空接口可以存储任意类型值的特点，所以空接口在Go语言中的使用十分广泛。 关于接口需要注意的是，只有当有两个或两个以上的具体类型必须以相同的方式进行处理时才需要定义接口。不要为了接口而写接口，那样只会增加不必要的抽象，导致不必要的运行时损耗。 ","date":"2022-01-06 09:16:40","objectID":"/go_base_05/:2:5","tags":["go grammar"],"title":"Go_base_05","uri":"/go_base_05/"},{"categories":["Go"],"content":" 参考学习go语言中文网、C语言中文网、golang官方文档等 方法 ","date":"2022-01-06 09:16:37","objectID":"/go_base_04/:0:0","tags":["go grammar"],"title":"Go_base_04","uri":"/go_base_04/"},{"categories":["Go"],"content":"方法本质 一个以方法的 receiver 参数作为第一个参数的普通函数。 package main import ( \"fmt\" \"time\" ) type field struct { name string } func (p *field) print() { fmt.Println(p.name) } func main() { data1 := []*field{ {\"one\"}, {\"two\"}, {\"three\"} } for _, v := range data1 { go v.print() } data2 := []field{ {\"four\"}, {\"five\"}, {\"six\"} } for _, v := range data2 { go v.print() } time.Sleep(3 * time.Second) } one two three six six six 为什么会是这样的输出？ 根据 Go 方法的本质，也就是一个以方法的 receiver 参数作为第一个参数的普通函数，对这个程序做个等价变换 type field struct { name string } func (p *field) print() { fmt.Println(p.name) } func main() { data1 := []*field{ {\"one\"}, {\"two\"}, {\"three\"} } for _, v := range data1 { go (*field).print(v) } data2 := []field{ {\"four\"}, {\"five\"}, {\"six\"} } for _, v := range data2 { go (*field).print(\u0026v) } time.Sleep(3 * time.Second) } 我们把对 field 的方法 print 的调用，替换为 Method Expression 形式，替换前后的程序输出结果是一致的 使用 go 关键字启动一个新 Goroutine 时，method expression 形式的 print 函数是如何绑定参数的： 迭代 data1 时，由于 data1 中的元素类型是 field 指针 (*field)，因此赋值后 v 就是元素地址，与 print 的 receiver 参数类型相同，每次调用 (*field).print 函数时直接传入的 v 即可，实际上传入的也是各个 field 元素的地址； 迭代 data2 时，由于 data2 中的元素类型是 field（非指针），与 print 的 receiver 参数类型不同，因此需要将其取地址后再传入 (*field).print 函数。这样每次传入的 \u0026v 实际上是变量 v 的地址，而不是切片 data2 中各元素的地址。 ","date":"2022-01-06 09:16:37","objectID":"/go_base_04/:1:0","tags":["go grammar"],"title":"Go_base_04","uri":"/go_base_04/"},{"categories":["Go"],"content":"方法定义 Golang 方法总是绑定对象实例，并隐式将实例作为第一实参 (receiver)。 只能为当前包内命名类型定义方法。 参数 receiver 可任意命名。如方法中未曾使用 ，可省略参数名。 参数 receiver 类型可以是 T 或 *T。基类型 T 不能是接口或指针。 不支持方法重载，receiver 只是参数签名的组成部分。 可用实例 value 或 pointer 调用全部方法，编译器自动转换。 方法表达式（Method Expression）: 直接以类型名 T 调用方法的表达方式，被称为 Method Expression。通过 Method Expression 这种形式，类型 T 只能调用 T 的方法集合（Method Set）中的方法，同理类型 *T 也只能调用 *T 的方法集合中的方法 一个方法就是一个包含了接受者的函数，接受者可以是命名类型或者结构体类型的一个值或者是一个指针。 所有给定类型的方法属于该类型的方法集。 func (recevier type) methodName(参数列表)(返回值列表){} //参数和返回值可以省略 package main type Test struct{} // 无参数、无返回值 func (t Test) method0() { } // 单参数、无返回值 func (t Test) method1(i int) { } // 多参数、无返回值 func (t Test) method2(x, y int) { } // 无参数、单返回值 func (t Test) method3() (i int) { return } // 多参数、多返回值 func (t Test) method4(x, y int) (z int, err error) { return } // 无参数、无返回值 func (t *Test) method5() { } // 单参数、无返回值 func (t *Test) method6(i int) { } // 多参数、无返回值 func (t *Test) method7(x, y int) { } // 无参数、单返回值 func (t *Test) method8() (i int) { return } // 多参数、多返回值 func (t *Test) method9(x, y int) (z int, err error) { return } func main() {} 下面定义一个结构体类型和该类型的一个方法： package main import ( \"fmt\" ) //结构体 type User struct { Name string Email string } //方法 func (u User) Notify() { fmt.Printf(\"%v : %v \\n\", u.Name, u.Email) } func main() { // 值类型调用方法 u1 := User{\"golang\", \"golang@golang.com\"} u1.Notify() // 指针类型调用方法 u2 := User{\"go\", \"go@go.com\"} u3 := \u0026u2 u3.Notify() } output： golang : golang@golang.com go : go@go.com 解释： 首先我们定义了一个叫做 User 的结构体类型，然后定义了一个该类型的方法叫做 Notify，该方法的接受者是一个 User 类型的值。要调用 Notify 方法我们需要一个 User 类型的值或者指针。 在这个例子中当我们使用指针时，Go 调整和解引用指针使得调用可以被执行。注意，当接受者不是一个指针时，该方法操作对应接受者的值的副本(意思就是即使你使用了指针调用函数，但是函数的接受者是值类型，所以函数内部操作还是对副本的操作，而不是指针操作。 我们修改 Notify 方法，让它的接受者使用指针类型： package main import ( \"fmt\" ) //结构体 type User struct { Name string Email string } //方法 func (u *User) Notify() { fmt.Printf(\"%v : %v \\n\", u.Name, u.Email) } func main() { // 值类型调用方法 u1 := User{\"golang\", \"golang@golang.com\"} u1.Notify() // 指针类型调用方法 u2 := User{\"go\", \"go@go.com\"} u3 := \u0026u2 u3.Notify() } output： golang : golang@golang.com go : go@go.com 注意：当接受者是指针时，即使用值类型调用那么函数内部也是对指针的操作。 方法不过是一种特殊的函数，只需将其还原，就知道 receiver T 和 *T 的差别。 package main import \"fmt\" type Data struct { x int } func (self Data) ValueTest() { // func ValueTest(self Data); fmt.Printf(\"Value: %p\\n\", \u0026self) } func (self *Data) PointerTest() { // func PointerTest(self *Data); fmt.Printf(\"Pointer: %p\\n\", self) } func main() { d := Data{} p := \u0026d fmt.Printf(\"Data: %p\\n\", p) d.ValueTest() // ValueTest(d) d.PointerTest() // PointerTest(\u0026d) p.ValueTest() // ValueTest(*p) p.PointerTest() // PointerTest(p) } output: Data: 0xc42007c008 Value: 0xc42007c018 Pointer: 0xc42007c008 Value: 0xc42007c020 Pointer: 0xc42007c008 普通函数与方法的区别 1.对于普通函数，接收者为值类型时，不能将指针类型的数据直接传递，反之亦然。 2.对于方法（如struct的方法），接收者为值类型时，可以直接用指针类型的变量调用方法，反过来同样也可以。 package main //普通函数与方法的区别（在接收者分别为值类型和指针类型的时候） import ( \"fmt\" ) //1.普通函数 //接收值类型参数的函数 func valueIntTest(a int) int { return a + 10 } //接收指针类型参数的函数 func pointerIntTest(a *int) int { return *a + 10 } func structTestValue() { a := 2 fmt.Println(\"valueIntTest:\", valueIntTest(a)) //函数的参数为值类型，则不能直接将指针作为参数传递 //fmt.Println(\"valueIntTest:\", valueIntTest(\u0026a)) //compile error: cannot use \u0026a (type *int) as type int in function argument b := 5 fmt.Println(\"pointerIntTest:\", pointerIntTest(\u0026b)) //同样，当函数的参数为指针类型时，也不能直接将值类型作为参数传递 //fmt.Println(\"pointerIntTest:\", pointerIntTest(b)) //compile error:cannot use b (type int) as type *int in function argument } //2.方法 type PersonD struct { id int name string } //接收者为值类型 func (p PersonD) valueShowName() { fmt.Println(p.name) } //接收者为指针类型 func (p *PersonD) pointShowName() { fmt.Println(p.name) } func structTestFunc() { //值类型调用方法 personValue := PersonD{101, \"hello world\"} personValue.valueShowName() personValue.pointShowName() //指针类型调用方法 personPointer := \u0026PersonD{102, \"hello golang\"} personPointer.valueShowName() personPointer.pointShowName() //与普通函数不同，接收者为指针类型和值类型的方法，指针类型和值类型的变量均可相互调用 } func main() { structTestValue() structTestFunc() } output： valueIntTest: 12 ","date":"2022-01-06 09:16:37","objectID":"/go_base_04/:2:0","tags":["go grammar"],"title":"Go_base_04","uri":"/go_base_04/"},{"categories":["Go"],"content":"匿名字段 Golang匿名字段 ：可以像字段成员那样访问匿名字段方法，编译器负责查找。 package main import \"fmt\" type User struct { id int name string } type Manager struct { User } func (self *User) ToString() string { // receiver = \u0026(Manager.User) return fmt.Sprintf(\"User: %p, %v\", self, self) } func main() { m := Manager{User{1, \"Tom\"} } fmt.Printf(\"Manager: %p\\n\", \u0026m) fmt.Println(m.ToString()) } output: Manager: 0xc42000a060 User: 0xc42000a060, \u0026{1 Tom} 通过匿名字段，可获得和继承类似的复用能力。依据编译器查找次序，只需在外层定义同名方法，就可以实现 “override”。 package main import \"fmt\" type User struct { id int name string } type Manager struct { User title string } func (self *User) ToString() string { return fmt.Sprintf(\"User: %p, %v\", self, self) } func (self *Manager) ToString() string { return fmt.Sprintf(\"Manager: %p, %v\", self, self) } func main() { m := Manager{User{1, \"Tom\"}, \"Administrator\"} fmt.Println(m.ToString()) fmt.Println(m.User.ToString()) } output: Manager: 0xc420074180, \u0026\\{\\{1 Tom} Administrator} User: 0xc420074180, \u0026{1 Tom} ","date":"2022-01-06 09:16:37","objectID":"/go_base_04/:3:0","tags":["go grammar"],"title":"Go_base_04","uri":"/go_base_04/"},{"categories":["Go"],"content":"方法集以及如何选择 receiver 参数的类型。 Golang方法集 ：每个类型都有与之关联的方法集，这会影响到接口实现规则。 类型 T 方法集包含全部 receiver T 方法。 类型 *T 方法集包含全部 receiver T + *T 方法。 如类型 S 包含匿名字段 T，则 S 和 *S 方法集包含 T 方法。 如类型 S 包含匿名字段 *T，则 S 和 *S 方法集包含 T + *T 方法。 不管嵌入 T 或 *T，*S 方法集总是包含 T + *T 方法。 用实例 value 和 pointer 调用方法 (含匿名字段) 不受方法集约束，编译器总是查找全部方法，并自动转换 receiver 实参。 Go 语言中内部类型方法集提升的规则： 类型 T 方法集包含全部 receiver T 方法。 package main import ( \"fmt\" ) type T struct { int } func (t T) test() { fmt.Println(\"类型 T 方法集包含全部 receiver T 方法。\") } func main() { t1 := T{1} fmt.Printf(\"t1 is : %v\\n\", t1) t1.test() } output： t1 is : {1} 类型 T 方法集包含全部 receiver T 方法。 类型 *T 方法集包含全部 receiver T + *T 方法。 package main import ( \"fmt\" ) type T struct { int } func (t T) testT() { fmt.Println(\"类型 *T 方法集包含全部 receiver T 方法。\") } func (t *T) testP() { fmt.Println(\"类型 *T 方法集包含全部 receiver *T 方法。\") } func main() { t1 := T{1} t2 := \u0026t1 fmt.Printf(\"t2 is : %v\\n\", t2) t2.testT() t2.testP() } output： t2 is : \u0026{1} 类型 *T 方法集包含全部 receiver T 方法。 类型 *T 方法集包含全部 receiver *T 方法。 给定一个结构体类型 S 和一个命名为 T 的类型，方法提升像下面规定的这样被包含在结构体方法集中： 如类型 S 包含匿名字段 T，则 S 和 *S 方法集包含 T 方法。 这条规则说的是当我们嵌入一个类型，嵌入类型的接受者为值类型的方法将被提升，可以被外部类型的值和指针调用。 package main import ( \"fmt\" ) type S struct { T } type T struct { int } func (t T) testT() { fmt.Println(\"如类型 S 包含匿名字段 T，则 S 和 *S 方法集包含 T 方法。\") } func main() { s1 := S{T{1} } s2 := \u0026s1 fmt.Printf(\"s1 is : %v\\n\", s1) s1.testT() fmt.Printf(\"s2 is : %v\\n\", s2) s2.testT() } output： s1 is : { {1} } 如类型 S 包含匿名字段 T，则 S 和 *S 方法集包含 T 方法。 s2 is : \u0026{ {1} } 如类型 S 包含匿名字段 T，则 S 和 *S 方法集包含 T 方法。 如类型 S 包含匿名字段 *T，则 S 和 *S 方法集包含 T + *T 方法。 这条规则说的是当我们嵌入一个类型的指针，嵌入类型的接受者为值类型或指针类型的方法将被提升，可以被外部类型的值或者指针调用。 package main import ( \"fmt\" ) type S struct { T } type T struct { int } func (t T) testT() { fmt.Println(\"如类型 S 包含匿名字段 *T，则 S 和 *S 方法集包含 T 方法\") } func (t *T) testP() { fmt.Println(\"如类型 S 包含匿名字段 *T，则 S 和 *S 方法集包含 *T 方法\") } func main() { s1 := S{T{1} } s2 := \u0026s1 fmt.Printf(\"s1 is : %v\\n\", s1) s1.testT() s1.testP() fmt.Printf(\"s2 is : %v\\n\", s2) s2.testT() s2.testP() } output： s1 is : { {1} } 如类型 S 包含匿名字段 *T，则 S 和 *S 方法集包含 T 方法 如类型 S 包含匿名字段 *T，则 S 和 *S 方法集包含 *T 方法 s2 is : \u0026{ {1} } 如类型 S 包含匿名字段 *T，则 S 和 *S 方法集包含 T 方法 如类型 S 包含匿名字段 *T，则 S 和 *S 方法集包含 *T 方法 func (t T) M1() \u003c=\u003e F1(t T) func (t *T) M2() \u003c=\u003e F2(t *T) 当 receiver 参数的类型为 T 时： 当我们的方法 M1 采用类型为 T 的 receiver 参数时，代表 T 类型实例的 receiver 参数以值传递方式传递到 M1 方法体中的，实际上是 T 类型实例的副本，M1 方法体中对副本的任何修改操作，都不会影响到原 T 类型实例。 当 receiver 参数的类型为 *T 时： 方法 M2 采用类型为 *T 的 receiver 参数时，代表 *T 类型实例的 receiver 参数以值传递方式传递到 M2 方法体中的，实际上是 T 类型实例的地址，M2 方法体通过该地址可以对原 T 类型实例进行任何修改操作。 选择 receiver 参数类型的第一个原则：*如果 Go 方法要把对 receiver 参数代表的类型实例的修改，反映到原类型实例上，那么我们应该选择 T 作为 receiver 参数的类型。 无论是 T 类型实例，还是 *T 类型实例，都既可以调用 receiver 为 T 类型的方法，也可以调用 receiver 为 *T 类型的方法。 选择 receiver 参数类型的第二个原则： 一般情况下，我们通常会为 receiver 参数选择 T 类型，因为这样可以缩窄外部修改类型实例内部状态的“接触面”，也就是尽量少暴露可以修改类型内部状态的方法。 考虑到 Go 方法调用时，receiver 参数是以值拷贝的形式传入方法中的。那么，如果 receiver 参数类型的 size 较大，以值拷贝形式传入就会导致较大的性能开销，这时我们选择 *T 作为 receiver 类型可能更好些。 选择 receiver 参数类型的第三个原则：（其实是应该首先考虑的一点） 如果 T 类型需要实现某个接口，那我们就要使用 T 作为 receiver 参数的类型，来满足接口类型方法集合中的所有方法。 如果 T 不需要实现某一接口，但 *T 需要实现该接口，那么根据方法集合概念，*T 的方法集合是包含 T 的方法集合的，这样我们在确定 Go 方法的 receiver 的类型时，参考原则一和原则二就可以了。 ","date":"2022-01-06 09:16:37","objectID":"/go_base_04/:4:0","tags":["go grammar"],"title":"Go_base_04","uri":"/go_base_04/"},{"categories":["Go"],"content":"表达式 Golang 表达式 ：根据调用者不同，方法分为两种表现形式: instance.method(args...) ---\u003e \u003ctype\u003e.func(instance, args...) 前者称为 method value，后者 method expression。 两者都可像普通函数那样赋值和传参，区别在于 method value 绑定实例，而 method expression 则须显式传参。 package main import \"fmt\" type User struct { id int name string } func (self *User) Test() { fmt.Printf(\"%p, %v\\n\", self, self) } func main() { u := User{1, \"Tom\"} u.Test() mValue := u.Test mValue() // 隐式传递 receiver mExpression := (*User).Test mExpression(\u0026u) // 显式传递 receiver } output: 0xc42000a060, \u0026{1 Tom} 0xc42000a060, \u0026{1 Tom} 0xc42000a060, \u0026{1 Tom} 需要注意，method value 会复制 receiver。 package main import \"fmt\" type User struct { id int name string } func (self User) Test() { fmt.Println(self) } func main() { u := User{1, \"Tom\"} mValue := u.Test // 立即复制 receiver，因为不是指针类型，不受后续修改影响。 u.id, u.name = 2, \"Jack\" u.Test() mValue() } output: {2 Jack} {1 Tom} 在汇编层面，method value 和闭包的实现方式相同，实际返回 FuncVal 类型对象。 FuncVal { method_address, receiver_copy } 可依据方法集转换 method expression，注意 receiver 类型的差异。 package main import \"fmt\" type User struct { id int name string } func (self *User) TestPointer() { fmt.Printf(\"TestPointer: %p, %v\\n\", self, self) } func (self User) TestValue() { fmt.Printf(\"TestValue: %p, %v\\n\", \u0026self, self) } func main() { u := User{1, \"Tom\"} fmt.Printf(\"User: %p, %v\\n\", \u0026u, u) mv := User.TestValue mv(u) mp := (*User).TestPointer mp(\u0026u) mp2 := (*User).TestValue // *User 方法集包含 TestValue。签名变为 func TestValue(self *User)。实际依然是 receiver value copy。 mp2(\u0026u) } output: User: 0xc42000a060, {1 Tom} TestValue: 0xc42000a0a0, {1 Tom} TestPointer: 0xc42000a060, \u0026{1 Tom} TestValue: 0xc42000a100, {1 Tom} 将方法 “还原” 成函数，就容易理解下面的代码了。 package main type Data struct{} func (Data) TestValue() {} func (*Data) TestPointer() {} func main() { var p *Data = nil p.TestPointer() (*Data)(nil).TestPointer() // method value (*Data).TestPointer(nil) // method expression // p.TestValue() // invalid memory address or nil pointer dereference // (Data)(nil).TestValue() // cannot convert nil to type Data // Data.TestValue(nil) // cannot use nil as type Data in function argument } ","date":"2022-01-06 09:16:37","objectID":"/go_base_04/:5:0","tags":["go grammar"],"title":"Go_base_04","uri":"/go_base_04/"},{"categories":["Go"],"content":"自定义error ","date":"2022-01-06 09:16:37","objectID":"/go_base_04/:6:0","tags":["go grammar"],"title":"Go_base_04","uri":"/go_base_04/"},{"categories":["Go"],"content":"抛异常和处理异常 系统抛 package main import \"fmt\" // 系统抛 func test01() { a := [5]int{0, 1, 2, 3, 4} a[1] = 123 fmt.Println(a) //a[10] = 11 index := 10 a[index] = 10 fmt.Println(a) } func getCircleArea(radius float32) (area float32) { if radius \u003c 0 { // 自己抛 panic(\"半径不能为负\") } return 3.14 * radius * radius } func test02() { getCircleArea(-5) } // func test03() { // 延时执行匿名函数 // 延时到何时？（1）程序正常结束 （2）发生异常时 defer func() { // recover() 恢复 // 会返回程序为什么挂了 if err := recover(); err != nil { fmt.Println(err) } }() getCircleArea(-5) fmt.Println(\"这里有没有执行\") } func test04() { test03() fmt.Println(\"test04\") } func main() { test04() } 返回异常 package main import ( \"errors\" \"fmt\" ) func getCircleArea(radius float32) (area float32, err error) { if radius \u003c 0 { // 构建个异常对象 err = errors.New(\"半径不能为负\") return } area = 3.14 * radius * radius return } func main() { area, err := getCircleArea(-5) if err != nil { fmt.Println(err) } else { fmt.Println(area) } } 自定义error： package main import ( \"fmt\" \"os\" \"time\" ) type PathError struct { path string op string createTime string message string } func (p *PathError) Error() string { return fmt.Sprintf(\"path=%s \\nop=%s \\ncreateTime=%s \\nmessage=%s\", p.path, p.op, p.createTime, p.message) } func Open(filename string) error { file, err := os.Open(filename) if err != nil { return \u0026PathError{ path: filename, op: \"read\", message: err.Error(), createTime: fmt.Sprintf(\"%v\", time.Now()), } } defer file.Close() return nil } func main() { err := Open(\"/Users/5lmh/Desktop/go/src/test.txt\") switch v := err.(type) { case *PathError: fmt.Println(\"get path error,\", v) default: } } output： get path error, path=/Users/pprof/Desktop/go/src/test.txt op=read createTime=2018-04-05 11:25:17.331915 +0800 CST m=+0.000441790 message=open /Users/pprof/Desktop/go/src/test.txt: no such file or directory ","date":"2022-01-06 09:16:37","objectID":"/go_base_04/:6:1","tags":["go grammar"],"title":"Go_base_04","uri":"/go_base_04/"},{"categories":["Go"],"content":" 参考学习go语言中文网、C语言中文网、golang官方文档等 函数 ","date":"2022-01-06 09:16:34","objectID":"/go_base_03/:0:0","tags":["go grammar"],"title":"Go_base_03","uri":"/go_base_03/"},{"categories":["Go"],"content":"函数定义 go的函数特点： 无需声明原型。 支持不定参、变参。 支持多返回值。 支持命名返回参数。 支持匿名函数和闭包。（闭包详见后文） 函数也是一种类型，一个函数可以赋值给变量。 有返回值的函数，必须有明确的终止语句，否则会引发编译错误。 不支持 嵌套 (nested) 一个包不能有两个名字一样的函数。 不支持 重载 (overload) （区别于重写，重载(overloading) 是在一个类里面，方法名字相同，而参数不同。返回类型可以相同也可以不同。每个重载的方法（或者构造函数）都必须有一个独一无二的参数类型列表。最常用的地方就是构造器的重载。来自菜鸟教程java。） 不支持 默认参数 (default parameter)。 没有函数体的函数声明，表示该函数不是以Go实现的。这样的声明定义了函数标识符。 所有参数都是值传递：slice，map，channel 会有传引⽤的错觉 string、切片、map 这些类型它们的内存表示对应的是它们数据内容的“描述符”。当这些类型作为实参类型时，值传递拷贝的也是它们数据内容的“描述符”，不包括数据内容本身，所以这些类型传递的开销是固定的，与数据内容大小无关。这种只拷贝“描述符”，不拷贝实际数据内容的拷贝过程，也被称为“浅拷贝”。 当函数的形参为接口类型，或者形参是变长参数时，简单的值传递就不能满足要求了，这时 Go 编译器会介入：对于类型为接口类型的形参，Go 编译器会把传递的实参赋值给对应的接口类型形参；对于为变长参数的形参，Go 编译器会将零个或多个实参按一定形式转换为对应的变长形参。 在 Go 中，变长参数实际上是通过切片来实现的。所以，我们在函数体中，就可以使用切片支持的所有操作来操作变长参数 关于函数的返回值： Go 标准库以及大多数项目代码中的函数，都选择了使用普通的非具名返回值形式。但在一些特定场景下，具名返回值也会得到应用。比如，当函数使用 defer，而且还在 defer 函数中修改外部函数返回值时，具名返回值可以让代码显得更优雅清晰。当函数的返回值个数较多时，每次显式使用 return 语句时都会接一长串返回值，这时，我们用具名返回值可以让函数实现的可读性更好一些 // $GOROOT/src/time/format.go func parseNanoseconds(value string, nbytes int) (ns int, rangeErrString string, err error) { if !commaOrPeriod(value[0]) { err = errBad return } if ns, err = atoi(value[1:nbytes]); err != nil { return } if ns \u003c 0 || 1e9 \u003c= ns { rangeErrString = \"fractional second\" return } scaleDigits := 10 - nbytes for i := 0; i \u003c scaleDigits; i++ { ns *= 10 } return } 函数是第一类对象，可作为参数传递。建议将复杂签名定义为函数类型，以便于阅读。 import \"fmt\" func test(fn func() int) int { return fn() } // 定义函数类型。 type FormatFunc func(s string, x, y int) string func format(fn FormatFunc, s string, x, y int) string { return fn(s, x, y) } func main() { s1 := test(func() int { return 100 }) // 直接将匿名函数当参数。 s2 := format(func(s string, x, y int) string { return fmt.Sprintf(s, x, y) }, \"%d, %d\", 10, 20) println(s1, s2) } output: 100 10,20 fmt里的一些格式化输入输出函数： func Printf(format string, a ...interface{}) (n int, err error) func Fprintf(w io.Writer, format string, a ...interface{}) (n int, err error) func Sprintf(format string, a ...interface{}) string func Print(a ...interface{}) (n int, err error) func Fprint(w io.Writer, a ...interface{}) (n int, err error) func Sprint(a ...interface{}) string func Println(a ...interface{}) (n int, err error) func Fprintln(w io.Writer, a ...interface{}) (n int, err error) func Sprintln(a ...interface{}) string func Errorf(format string, a ...interface{}) error func Scanf(format string, a ...interface{}) (n int, err error) func Fscanf(r io.Reader, format string, a ...interface{}) (n int, err error) func Sscanf(str string, format string, a ...interface{}) (n int, err error) func Scan(a ...interface{}) (n int, err error) func Fscan(r io.Reader, a ...interface{}) (n int, err error) func Sscan(str string, a ...interface{}) (n int, err error) func Scanln(a ...interface{}) (n int, err error) func Fscanln(r io.Reader, a ...interface{}) (n int, err error) func Sscanln(str string, a ...interface{}) (n int, err error) ","date":"2022-01-06 09:16:34","objectID":"/go_base_03/:1:0","tags":["go grammar"],"title":"Go_base_03","uri":"/go_base_03/"},{"categories":["Go"],"content":"参数 map、slice、chan、指针、interface默认以引用的方式传递，其他的在默认情况下，使用的是值传递. 无论是值传递，还是引用传递，传递给函数的都是变量的副本，不过，值传递是值的拷贝。引用传递是地址的拷贝，一般来说，地址拷贝更为高效。而值拷贝取决于拷贝的对象大小，对象越大，则性能越低。 ","date":"2022-01-06 09:16:34","objectID":"/go_base_03/:2:0","tags":["go grammar"],"title":"Go_base_03","uri":"/go_base_03/"},{"categories":["Go"],"content":"不定参 就是函数的参数不是固定的，后面的类型是固定的。（可变参数） Golang 可变参数本质上就是 slice。只能有一个，且必须是最后一个。 在参数赋值时可以不用用一个一个的赋值，可以直接传递一个数组或者切片，特别注意的是在参数后加上“…”即可。 func myfunc(args ...int) { //0个或多个参数 } func add(a int, args…int) int { //1个或多个参数 } func add(a int, b int, args…int) int { //2个或多个参数 } 注意：其中args是一个slice，我们可以通过arg[index]依次访问所有参数,通过len(arg)来判断传递参数的个数. 任意类型的不定参数：就是函数的参数和每个参数的类型都不是固定的。 用interface{}传递任意类型数据是Go语言的惯例用法，而且interface{}是类型安全的。 func myfunc(args ...interface{}) { } func test(s string, n ...int) string { var x int for _, i := range n { x += i } return fmt.Sprintf(s, x) } func main() { s := []int{1, 2, 3} res := test(\"sum: %d\", s...) // slice... 展开slice,而不是只写变量名 println(res) } ","date":"2022-01-06 09:16:34","objectID":"/go_base_03/:2:1","tags":["go grammar"],"title":"Go_base_03","uri":"/go_base_03/"},{"categories":["Go"],"content":"返回值 Go 的返回值可以被命名，并且就像在函数体开头声明的变量那样使用。 返回值的名称应当具有一定的意义，可以作为文档使用。 没有参数的 return 语句返回各个返回变量的当前值。这种用法被称作 “裸”返回。 package main import ( \"fmt\" ) func add(a, b int) (c int) { c = a + b return } func calc(a, b int) (sum int, avg int) { sum = a + b avg = (a + b) / 2 return } func main() { var a, b int = 1, 2 c := add(a, b) sum, avg := calc(a, b) fmt.Println(a, b, c, sum, avg) } 命名返回参数可被同名局部变量遮蔽，此时需要显式返回。 Golang返回值不能用容器对象接收多返回值。只能用多个变量，或 “_” 忽略。或者多返回值可直接作为其他函数调用实参。 func test() (int, int) { return 1, 2 } func add(x, y int) int { return x + y } func sum(n ...int) int { var x int for _, i := range n { x += i } return x } func main() { println(add(test())) println(sum(test())) } 命名返回参数允许 defer 延迟调用通过闭包读取和修改。 package main func add(x, y int) (z int) { defer func() { z += 100 }() z = x + y return } func main() { println(add(1, 2)) } output:103 ","date":"2022-01-06 09:16:34","objectID":"/go_base_03/:3:0","tags":["go grammar"],"title":"Go_base_03","uri":"/go_base_03/"},{"categories":["Go"],"content":"利用多返回值进行错误处理： ","date":"2022-01-06 09:16:34","objectID":"/go_base_03/:4:0","tags":["go grammar"],"title":"Go_base_03","uri":"/go_base_03/"},{"categories":["Go"],"content":"匿名函数 匿名函数的优越性在于可以直接使用函数内的变量，不必申明。Golang匿名函数可赋值给变量，做为结构字段，或者在 channel 里传送。 package main func main() { // --- function variable --- fn := func() { println(\"Hello, World!\") } fn() // --- function collection --- fns := [](func(x int) int){ func(x int) int { return x + 1 }, func(x int) int { return x + 2 }, } println(fns[0](100)) // --- function as field --- d := struct { fn func() string }{ fn: func() string { return \"Hello, World!\" }, } println(d.fn()) // --- channel of function --- fc := make(chan func() string, 2) fc \u003c- func() string { return \"Hello, World!\" } println((\u003c-fc)()) output: Hello, World! 101 Hello, World! Hello, World! } ","date":"2022-01-06 09:16:34","objectID":"/go_base_03/:5:0","tags":["go grammar"],"title":"Go_base_03","uri":"/go_base_03/"},{"categories":["Go"],"content":"闭包、递归 ","date":"2022-01-06 09:16:34","objectID":"/go_base_03/:6:0","tags":["go grammar"],"title":"Go_base_03","uri":"/go_base_03/"},{"categories":["Go"],"content":"闭包 闭包是由函数及其相关引用环境组合而成的实体。 “官方”的解释是：所谓“闭包”，指的是一个拥有许多变量和绑定了这些变量的环境的表达式（通常是一个函数），因而这些变量也是该表达式的一部分。 维基百科讲，闭包（Closure），是引用了自由变量的函数。这个被引用的自由变量将和这个函数一同存在，即使已经离开了创造它的环境也不例外。所以，有另一种说法认为闭包是由函数和与其相关的引用环境组合而成的实体。闭包在运行时可以有多个实例，不同的引用环境和相同的函数组合可以产生不同的实例。 目前在JavaScript、Go、PHP、Scala、Scheme、Common Lisp、Smalltalk、Groovy、Ruby、 Python、Lua、objective c、Swift 以及Java8以上等语言中都能找到对闭包不同程度的支持。 通过支持闭包的语法可以发现一个特点，他们都有垃圾回收(GC)机制。 go的闭包： package main import ( \"fmt\" ) func a() func() int { i := 0 b := func() int { i++ fmt.Println(i) return i } return b } func main() { c := a() c() c() c() a() //不会输出i } output: 1 2 3 当函数a()的内部函数b()被函数a()外的一个变量引用的时候，就创建了一个闭包。 闭包复制的是原对象指针，这就很容易解释延迟引用现象。（延迟引用，引用的只是某个变量的“最终值”，延迟闭包里引用的变量是原变量指针，这解释了后面为什么derfer碰上闭包的输出都是同一值） package main import \"fmt\" func test() func() { x := 100 fmt.Printf(\"x (%p) = %d\\n\", \u0026x, x) return func() { fmt.Printf(\"x (%p) = %d\\n\", \u0026x, x) } } func main() { f := test() f() } output: x (0xc42007c008) = 100 x (0xc42007c008) = 100 在汇编层 ，test 实际返回的是 FuncVal 对象，其中包含了匿名函数地址、闭包对象指针。当调用匿名函数时，只需以某个寄存器传递该对象即可。 Funcval对象： FuncVal { func_address, closure_var_pointer ... } 外部引用函数参数局部变量: package main import \"fmt\" // 外部引用函数参数局部变量 func add(base int) func(int) int { return func(i int) int { base += i return base } } func main() { tmp1 := add(10) fmt.Println(tmp1(1), tmp1(2)) // 此时tmp1和tmp2不是一个实体了 tmp2 := add(100) fmt.Println(tmp2(1), tmp2(2)) } 返回两个闭包： package main import \"fmt\" // 返回2个函数类型的返回值 func test01(base int) (func(int) int, func(int) int) { // 定义2个函数，并返回 // 相加 add := func(i int) int { base += i return base } // 相减 sub := func(i int) int { base -= i return base } // 返回 return add, sub } func main() { f1, f2 := test01(10) // base一直是没有消 fmt.Println(f1(1), f2(2)) // 此时base是9 fmt.Println(f1(3), f2(4)) } ","date":"2022-01-06 09:16:34","objectID":"/go_base_03/:6:1","tags":["go grammar"],"title":"Go_base_03","uri":"/go_base_03/"},{"categories":["Go"],"content":"go递归函数 构成递归的两个条件： 子问题须与原始问题为同样的事，且更为简单。 不能无限制地调用本身，须有个出口，化简为非递归状况处理。 go的递归和其他语言基本无差别。 ","date":"2022-01-06 09:16:34","objectID":"/go_base_03/:6:2","tags":["go grammar"],"title":"Go_base_03","uri":"/go_base_03/"},{"categories":["Go"],"content":"延迟调用（defer） 特性： 关键字 defer 用于注册延迟调用。 这些调用直到 return 前才被执行。因此，可以用来做资源清理。 多个defer语句，按先进后出的方式执行。（因为后面的defer可能会用到前面的资源） defer语句中的变量，在defer声明时就决定了。 发生panic依然会执行defer，但不是任何情况都会执行，比如：os.Exit()不会调用defer。os.Exit()退出时不输出当前调用栈信息 Go1.13前的版本defer的开销还是非常大的，在后续团队优化后现在的开销比较小可以放心使用 用途： 关闭文件句柄、锁资源释放、数据库连接释放 使用 defer 可以跟踪函数的执行过程 // trace.go package main func Trace(name string) func() { println(\"enter:\", name) return func() { println(\"exit:\", name) } } func foo() { defer Trace(\"foo\")() bar() } func bar() { defer Trace(\"bar\")() } func main() { defer Trace(\"main\")() foo() } 不足：调用 Trace 时需手动显式传入要跟踪的函数名；如果是并发应用，不同 Goroutine 中函数链跟踪混在一起无法分辨；输出的跟踪结果缺少层次感，调用关系不易识别；对要跟踪的函数，需手动调用 Trace 函数。 实现一个自动注入跟踪代码，并输出有层次感的函数调用链跟踪命令行工具： 自动获取所跟踪函数的函数名 充当“断言”，提示潜在bug defer功能强大，对于资源管理非常方便，但是如果没用好，也会有陷阱。 defer碰上闭包： package main import \"fmt\" func main() { var whatever [5]struct{} for i := range whatever { defer func() { fmt.Println(i) }() } } output: 4 4 4 4 4 延迟引用，闭包里的i是原变量指针。 defer.f.Close: package main import \"fmt\" type Test struct { name string } func (t *Test) Close() { fmt.Println(t.name, \" closed\") } func main() { ts := []Test{\"a\", \"b\", \"c\"} for _, t := range ts { defer t.Close() } } output: c closed c closed c closed package main import \"fmt\" type Test struct { name string } func (t *Test) Close() { fmt.Println(t.name, \" closed\") } func Close(t Test) { t.Close() } func main() { ts := []Test\"a\", \"b\", \"c\"} for _, t := range ts { defer Close(t) } //或者for _, t := range ts { // t2 := t // defer t2.Close() // } } output: c closed b closed a closed 结论： defer后面的语句在执行的时候，函数调用的参数会被保存起来，但是不执行。也就是复制了一份。但是并没有说struct这里的this指针如何处理，通过这个例子可以看出go语言并没有把这个明确写出来的this指针当作参数来看待。 多个 defer 注册，按 FILO 次序执行 ( 先进后出 )。哪怕函数或某个延迟调用发生错误，比如发生panic，这些defer调用依旧会被执行。 package main func test(x int) { defer println(\"a\") defer println(\"b\") defer func() { println(100 / x) // div0 异常未被捕获，逐步往外传递，最终终止进程。 }() defer println(\"c\") } func main() { test(0) } output: c b a panic: runtime error: integer divide by zero 延迟调用参数在注册时求值或复制，可用指针或闭包 “延迟” 读取。 package main func test() { x, y := 10, 20 defer func(i int) { println(\"defer:\", i, y) // y 闭包引用 }(x) // x 被复制 x += 10 y += 100 println(\"x =\", x, \"y =\", y) } func main() { test() } output: x = 20 y = 120 defer: 10 120 滥用 defer 可能会导致性能问题，尤其是在一个 “大循环” 里。 package main import ( \"fmt\" \"sync\" \"time\" ) var lock sync.Mutex func test() { lock.Lock() lock.Unlock() } func testdefer() { lock.Lock() defer lock.Unlock() } func main() { func() { t1 := time.Now() for i := 0; i \u003c 10000; i++ { test() } elapsed := time.Since(t1) fmt.Println(\"test elapsed: \", elapsed) }() func() { t1 := time.Now() for i := 0; i \u003c 10000; i++ { testdefer() } elapsed := time.Since(t1) fmt.Println(\"testdefer elapsed: \", elapsed) }() } output: test elapsed: 223.162µs testdefer elapsed: 781.304µs ","date":"2022-01-06 09:16:34","objectID":"/go_base_03/:7:0","tags":["go grammar"],"title":"Go_base_03","uri":"/go_base_03/"},{"categories":["Go"],"content":"defer陷阱 defer 与 closure： package main import ( \"errors\" \"fmt\" ) func foo(a, b int) (i int, err error) { defer fmt.Printf(\"first defer err %v\\n\", err) defer func(err error) { fmt.Printf(\"second defer err %v\\n\", err) }(err) defer func() { fmt.Printf(\"third defer err %v\\n\", err) }() if b == 0 { err = errors.New(\"divided by zero!\") return } i = a / b return } func main() { foo(2, 0) } output： third defer err divided by zero! second defer err \u003cnil\u003e first defer err \u003cnil\u003e 解释：如果 defer 后面跟的不是一个 closure 最后执行的时候我们得到的并不是最新的值。 defer 与 return: package main import \"fmt\" func foo() (i int) { i = 0 defer func() { fmt.Println(i) }() return 2 } func main() { foo() } output： 2 解释：在有具名返回值的函数中（这里具名返回值为 i），执行 return 2 的时候实际上已经将 i 的值重新赋值为 2。所以defer closure 输出结果为 2 而不是 1。 defer nil 函数: package main import ( \"fmt\" ) func test() { var run func() = nil defer run() fmt.Println(\"runs\") } func main() { defer func() { if err := recover(); err != nil { fmt.Println(err) } }() test() } output： runs runtime error: invalid memory address or nil pointer dereference 解释：名为 test 的函数一直运行至结束，然后 defer 函数会被执行且会因为值为 nil 而产生 panic 异常。然而值得注意的是，run() 的声明是没有问题，因为在test函数运行完成后它才会被调用。 在错误的位置使用 defer: 当 http.Get 失败时会抛出异常。 package main import \"net/http\" func do() error { res, err := http.Get(\"http://www.google.com\") defer res.Body.Close() if err != nil { return err } // ..code... return nil } func main() { do() } output： panic: runtime error: invalid memory address or nil pointer dereference 因为在这里我们并没有检查我们的请求是否成功执行，当它失败的时候，我们访问了 Body 中的空变量 res ，因此会抛出异常 解决方案： 总是在一次成功的资源分配下面使用 defer ，对于这种情况来说意味着：当且仅当 http.Get 成功执行时才使用 defer package main import \"net/http\" func do() error { res, err := http.Get(\"http://xxxxxxxxxx\") if res != nil { defer res.Body.Close() } if err != nil { return err } // ..code... return nil } func main() { do() } 在上述的代码中，当有错误的时候，err 会被返回，否则当整个函数返回的时候，会关闭 res.Body 。 解释：在这里，你同样需要检查 res 的值是否为 nil ，这是 http.Get 中的一个警告。通常情况下，出错的时候，返回的内容应为空并且错误会被返回，可当你获得的是一个重定向 error 时， res 的值并不会为 nil ，但其又会将错误返回。上面的代码保证了无论如何 Body 都会被关闭，如果你没有打算使用其中的数据，那么你还需要丢弃已经接收的数据。 不检查错误: 在这里，f.Close() 可能会返回一个错误，可这个错误会被我们忽略掉。 package main import \"os\" func do() error { f, err := os.Open(\"book.txt\") if err != nil { return err } if f != nil { defer f.Close() } // ..code... return nil } func main() { do() } 改进一下： package main import \"os\" func do() error { f, err := os.Open(\"book.txt\") if err != nil { return err } if f != nil { defer func() { if err := f.Close(); err != nil { // log etc } }() } // ..code... return nil } func main() { do() } 再改进一下：通过命名的返回变量来返回defer内的错误。 package main import \"os\" func do() (err error) { f, err := os.Open(\"book.txt\") if err != nil { return err } if f != nil { defer func() { if ferr := f.Close(); ferr != nil { err = ferr } }() } // ..code... return nil } func main() { do() } 释放相同的资源 如果你尝试使用相同的变量释放不同的资源，那么这个操作可能无法正常执行。 package main import ( \"fmt\" \"os\" ) func do() error { f, err := os.Open(\"book.txt\") if err != nil { return err } if f != nil { defer func() { if err := f.Close(); err != nil { fmt.Printf(\"defer close book.txt err %v\\n\", err) } }() } // ..code... f, err = os.Open(\"another-book.txt\") if err != nil { return err } if f != nil { defer func() { if err := f.Close(); err != nil { fmt.Printf(\"defer close another-book.txt err %v\\n\", err) } }() } return nil } func main() { do() } 输出结果： defer close book.txt err close ./another-book.txt: file already closed 当延迟函数执行时，只有最后一个变量会被用到，因此，f 变量 会成为最后那个资源 (another-book.txt)。而且两个 defer 都会将这个资源作为最后的资源来关闭 解决方案： package main import ( \"fmt\" \"io\" \"os\" ) func do() error { f, err := os.Open(\"book.txt\") if err != nil { return err } if f != nil { defer func(f io.Closer) { if err := f.Close(); err != nil { fmt.Printf(\"defer close book.txt err %v\\n\", err) } }(f) } // ..code... f, err = os.Open(\"another-book.txt\") if err != nil { return err } if f != nil { defer func(f io.Closer) { if err := f.Close(); err != nil { fmt.Printf(\"defer close another-book.txt err %v\\n\", err) } }(f) } return nil } func main() { do() } ","date":"2022-01-06 09:16:34","objectID":"/go_base_03/:7:1","tags":["go grammar"],"title":"Go_base_03","uri":"/go_base_03/"},{"categories":["Go"],"content":"异常处理，错误处理 提倡及早失败，避免嵌套 Golang 没有结构化异常，使用 panic 抛出错误，recover 捕获错误。 异常的使用场景简单描述：Go中可以抛出一个panic的异常，然后在defer中通过recover捕获这个异常，然后正常处理。 panic: 内置函数 假如函数F中书写了panic语句，会终止其后要执行的代码，在panic所在函数F内如果存在要执行的defer函数列表，按照defer的逆序执行 返回函数F的调用者G，在G中，调用函数F语句之后的代码不会执行，假如函数G中存在要执行的defer函数列表，按照defer的逆序执行 直到goroutine整个退出，并报告错误 recover: 内置函数 用来控制一个goroutine的panicking行为，捕获panic，从而影响应用的行为 一般的调用建议 只能用在defer函数中，通过recever来终止一个goroutine的panicking过程，从而恢复正常代码的执行 可以获取通过panic传递的error 注意： 利用recover处理panic指令，defer 必须放在 panic 之前定义，另外 recover 只有在 defer 调用的函数中才有效。否则当panic时，recover无法捕获到panic，无法防止panic扩散。 recover 处理异常后，逻辑并不会恢复到 panic 那个点去，函数跑到 defer 之后的那个点。 多个 defer 会形成 defer 栈，后定义的 defer 语句会被最先调用。 当心recoer成为恶魔，因为recover并不会检测发生了什么错误。可能是系统的某些核心资源消耗完了，强制恢复之后系统依然无法正常工作的。还会导致一些健康检查程序无法检测出错误，health check无法检测出错误（很多health check程序只是检查这个进程在还是不在），形成僵尸服务进程（存在着但不能提供服务）。不如采用一种可恢复的设计模式，“Let it Crash”，干脆让进程crash掉，然后就会帮我们重新把服务进程提起来，如同重启。（重启是恢复不确定性最好的方法） package main func main() { test() } func test() { defer func() { if err := recover(); err != nil { println(err.(string)) // 将 interface{} 转型为具体类型。 } }() panic(\"panic error!\") } output: panic error! 由于 panic、recover 参数类型为 interface{}，因此可抛出任何类型对象。 func panic(v interface{}) func recover() interface{} 向已关闭的通道发送数据会引发panic package main import ( \"fmt\" ) func main() { defer func() { if err := recover(); err != nil { fmt.Println(err) } }() var ch chan int = make(chan int, 10) close(ch) ch \u003c- 1 } output: send on closed channel 延迟调用中引发的错误，可被后续延迟调用捕获，但仅最后一个错误可被捕获。 package main import \"fmt\" func test() { defer func() { fmt.Println(recover()) }() defer func() { panic(\"defer panic\") }() panic(\"test panic\") } func main() { test() } output: defer panic 捕获函数 recover 只有在延迟调用内直接调用才会终止错误，否则总是返回 nil。任何未捕获的错误都会沿调用堆栈向外传递。 package main import \"fmt\" func test() { defer func() { fmt.Println(recover()) //有效 }() defer recover() //无效！ defer fmt.Println(recover()) //无效！ defer func() { func() { println(\"defer inner\") recover() //无效！ }() }() panic(\"test panic\") } func main() { test() } output: defer inner \u003cnil\u003e test panic 使用延迟匿名函数或下面这样都是有效的。 package main import ( \"fmt\" ) func except() { fmt.Println(recover()) } func test() { defer except() panic(\"test panic\") } func main() { test() } output： test panic 如果需要保护代码段，可将代码块重构成匿名函数，如此可确保后续代码被执行。 package main import \"fmt\" func test(x, y int) { var z int func() { defer func() { if recover() != nil { z = 0 } }() panic(\"test panic\") z = x / y return }() fmt.Printf(\"x / y = %d\\n\", z) } func main() { test(2, 1) } output： x / y = 0 除用 panic 引发中断性错误外，还可返回 error 类型错误对象来表示函数调用状态。（error类型实现了error接口） type error interface { Error() string } 标准库 errors.New 和 fmt.Errorf 函数用于创建实现 error 接口的错误对象。通过判断错误对象实例来确定具体错误类型。 package main import ( \"errors\" \"fmt\" ) //定义预置错误 var ErrDivByZero = errors.New(\"division by zero\") func div(x, y int) (int, error) { if y == 0 { return 0, ErrDivByZero } return x / y, nil } func main() { defer func() { fmt.Println(recover()) }() switch z, err := div(10, 0); err { case nil: println(z) case ErrDivByZero: panic(err) } } output: division by zero Go实现类似 try catch 的异常处理。 package main import \"fmt\" func Try(fun func(), handler func(interface{})) { defer func() { if err := recover(); err != nil { handler(err) } }() fun() } func main() { Try(func() { panic(\"test panic\") }, func(err interface{}) { fmt.Println(err) }) } output： test panic 如何区别使用 panic 和 error 两种方式? 惯例是:导致关键流程出现不可修复性错误的使用 panic，其他使用 error。 几种错误处理策略： 透明错误处理策略 err := doSomething() if err != nil { // 不关心err变量底层错误值所携带的具体上下文信息 // 执行简单错误处理逻辑并返回 ... ... return err } “哨兵” 当错误处理方不能只根据“透明的错误值”就做出错误处理路径选取的情况下，错误处理方会尝试对返回的错误值进行检视，于是就有可能出现下面代码中的反模式： 反模式就是，错误处理方以透明错误值所能提供的唯一上下文信息（描述错误的字符串），作为错误处理路径选择的依据。但这种“反模式”会造成严重的隐式耦合。这也就意味着，错误值构造方不经意间的一次错误描述字符串的改动，都会造成错误处理方处理行为的变化，并且这种通过字符串比较的方式，对错误值进行检视的性能也很差。 data, err := b.Peek(1) if err != nil { switch err.Error() { case \"bufio: negative count\": // ... ... return case \"bufio: buffer full\": // ... ... return case \"bufio: invalid use of UnreadByte\": // ... ... return default: // ... ... return } } Go 标准库采用了定义导出的（Exported）“哨兵”错误值的方式，来辅助错误","date":"2022-01-06 09:16:34","objectID":"/go_base_03/:8:0","tags":["go grammar"],"title":"Go_base_03","uri":"/go_base_03/"},{"categories":["Go"],"content":"为啥说函数是Go语言的一等公民： 引用一下 wiki 发明人、C2 站点作者沃德·坎宁安 (Ward Cunningham)对“一等公民”的解释： 如果一门编程语言对某种语言元素的创建和使用没有限制，我们可以像对待值（value）一样对待这种语法元素，那么我们就称这种语法元素是这门编程语言的“一等公民”。拥有“一等公民”待遇的语法元素可以存储在变量中，可以作为参数传递给函数，可以在函数内部创建并可以作为返回值从函数返回 Go 函数可以存储在变量中。 支持在函数内创建并通过返回值返回。 作为参数传入函数。 拥有自己的类型。 应用go函数的这些灵活性： 函数类型的妙用 函数也可以被显式转型，见下面web server的例子。 func greeting(w http.ResponseWriter, r *http.Request) { fmt.Fprintf(w, \"Welcome, Gopher!\\n\") } func main() { //greeting这个函数被显示转化为HandleFunc类型，ListenAndServe的第二个参数是个需要实现ServeHTTP方法（即需要实现Handle接口）的类型 http.ListenAndServe(\":8080\", http.HandlerFunc(greeting)) } …… // $GOROOT/src/net/http/server.go type HandlerFunc func(ResponseWriter, *Request) // ServeHTTP calls f(w, r). func (f HandlerFunc) ServeHTTP(w ResponseWriter, r *Request) { f(w, r) } 利用闭包简化函数调用。 func partialTimes(x int) func(int) int { return func(y int) int { return times(x, y) } } func main() { timesTwo := partialTimes(2) // 以高频乘数2为固定乘数的乘法函数 timesThree := partialTimes(3) // 以高频乘数3为固定乘数的乘法函数 fmt.Println(timesTwo(5)) // 10，等价于times(2, 5) fmt.Println(timesTwo(6)) // 12，等价于times(2, 6) fmt.Println(timesThree(5)) // 15，等价于times(3, 5) fmt.Println(timesThree(6)) // 18，等价于times(3, 6) } ","date":"2022-01-06 09:16:34","objectID":"/go_base_03/:9:0","tags":["go grammar"],"title":"Go_base_03","uri":"/go_base_03/"},{"categories":["Go"],"content":"单元测试 单元测试还是挺重要的。 ","date":"2022-01-06 09:16:34","objectID":"/go_base_03/:10:0","tags":["go grammar"],"title":"Go_base_03","uri":"/go_base_03/"},{"categories":["Go"],"content":"go test工具 Go语言中的测试依赖go test命令。编写测试代码和编写普通的Go代码过程是类似的，并不需要学习新的语法、规则或工具。 go test命令是一个按照一定约定和组织的测试代码的驱动程序。在包目录内，所有以_test.go为后缀名的源代码文件都是go test测试的一部分，不会被go build编译到最终的可执行文件中。 在*_test.go文件中有三种类型的函数，单元测试函数、基准测试函数和示例函数。 类型 格式 作用 测试函数 函数名前缀为Test 测试程序的一些逻辑行为是否正确 基准函数 函数名前缀为Benchmark 测试函数的性能 示例函数 函数名前缀为Example 为文档提供示例文档 go test命令会遍历所有的*_test.go文件中符合上述命名规则的函数，然后生成一个临时的main包用于调用相应的测试函数，然后构建并运行、报告测试结果，最后清理测试中生成的临时文件。 Golang单元测试对文件名和方法名，参数都有很严格的要求。 文件名必须以xx_test.go命名 方法必须是Test[^a-z]开头 方法参数必须 t *testing.T 使用go test执行单元测试 go test的参数解读： go test是go语言自带的测试工具，其中包含的是两类，单元测试和性能测试 通过go help test可以看到go test的使用说明： 格式形如： go test [-c] [-i] [build flags] [packages] [flags for test binary] 参数解读： -c : 编译go test成为可执行的二进制文件，但是不运行测试。 -i : 安装测试包依赖的package，但是不运行测试。 关于build flags，调用go help build，这些是编译运行过程中需要使用到的参数，一般设置为空 关于packages，调用go help packages，这些是关于包的管理，一般设置为空 关于flags for test binary，调用go help testflag，这些是go test过程中经常使用到的参数 -test.v : 是否输出全部的单元测试用例（不管成功或者失败），默认没有加上，所以只输出失败的单元测试用例。 -test.run pattern: 只跑哪些单元测试用例 -test.bench patten: 只跑那些性能测试用例 -test.benchmem : 是否在性能测试的时候输出内存情况 -test.benchtime t : 性能测试运行的时间，默认是1s -test.cpuprofile cpu.out : 是否输出cpu性能分析文件 -test.memprofile mem.out : 是否输出内存性能分析文件 -test.blockprofile block.out : 是否输出内部goroutine阻塞的性能分析文件 -test.memprofilerate n : 内存性能分析的时候有一个分配了多少的时候才打点记录的问题。这个参数就是设置打点的内存分配间隔，也就是profile中一个sample代表的内存大小。默认是设置为512 * 1024的。如果你将它设置为1，则每分配一个内存块就会在profile中有个打点，那么生成的profile的sample就会非常多。如果你设置为0，那就是不做打点了。 你可以通过设置memprofilerate=1和GOGC=off来关闭内存回收，并且对每个内存块的分配进行观察。 -test.blockprofilerate n: 基本同上，控制的是goroutine阻塞时候打点的纳秒数。默认不设置就相当于-test.blockprofilerate=1，每一纳秒都打点记录一下 -test.parallel n : 性能测试的程序并行cpu数，默认等于GOMAXPROCS。 -test.timeout t : 如果测试用例运行时间超过t，则抛出panic -test.cpu 1,2,4 : 程序运行在哪些CPU上面，使用二进制的1所在位代表，和nginx的nginx_worker_cpu_affinity是一个道理 -test.short : 将那些运行时间较长的测试用例运行时间缩短 目录结构： test | —— calc.go | —— calc_test.go ","date":"2022-01-06 09:16:34","objectID":"/go_base_03/:10:1","tags":["go grammar"],"title":"Go_base_03","uri":"/go_base_03/"},{"categories":["Go"],"content":"性能分析工具 可以安装go-torch 通过⽂件⽅式输出 Profile - 灵活性⾼，适⽤于特定代码段的分析 - 通过⼿动调⽤ runtime/pprof 的 API - API 相关⽂档 https://studygolang.com/static/pkgdoc/pkg/runtime_pprof.htm - go tool pprof [binary] [binary.prof] 通过 HTTP ⽅式输出 Profile - 简单，适合于持续性运⾏的应⽤ - 在应⽤程序中导⼊ import _ \"net/http/pprof\"，并启动 http server 即可 - http://\u003chost\u003e:\u003cport\u003e/debug/pprof/ - go tool pprof http://\u003chost\u003e:\u003cport\u003e/debug/pprof/profile?seconds=10 （默认值为30秒） - go-torch -seconds 10 http://\u003chost\u003e:\u003cport\u003e/debug/pprof/profile Go ⽀持的多种 Profile go help testflag https://golang.org/src/runtime/pprof/pprof.go 性能调优 性能调优过程： 常⻅分析指标 Wall Time CPU Time Block Time Memory allocation GC times/time spent ch47 ","date":"2022-01-06 09:16:34","objectID":"/go_base_03/:10:2","tags":["go grammar"],"title":"Go_base_03","uri":"/go_base_03/"},{"categories":["Go"],"content":"测试函数 测试函数的格式 每个测试函数必须导入testing包，测试函数的基本格式（签名）如下： func TestName(t *testing.T){ // ... } 测试函数的名字必须以Test开头，可选的后缀名必须以大写字母开头，举几个例子：\\ func TestAdd(t *testing.T){ ... } func TestSum(t *testing.T){ ... } func TestLog(t *testing.T){ ... } 其中参数t用于报告测试失败和附加的日志信息。 testing.T的拥有的方法如下： func (c *T) Error(args ...interface{}) func (c *T) Errorf(format string, args ...interface{}) func (c *T) Fail() func (c *T) FailNow() func (c *T) Failed() bool func (c *T) Fatal(args ...interface{}) func (c *T) Fatalf(format string, args ...interface{}) func (c *T) Log(args ...interface{}) func (c *T) Logf(format string, args ...interface{}) func (c *T) Name() string func (t *T) Parallel() func (t *T) Run(name string, f func(t *T)) bool func (c *T) Skip(args ...interface{}) func (c *T) SkipNow() func (c *T) Skipf(format string, args ...interface{}) func (c *T) Skipped() bool 测试函数示例 就像细胞是构成我们身体的基本单位，一个软件程序也是由很多单元组件构成的。单元组件可以是函数、结构体、方法和最终用户可能依赖的任意东西。总之我们需要确保这些组件是能够正常运行的。单元测试是一些利用各种方法测试单元组件的程序，它会将结果与预期输出进行比较。 接下来，我们定义一个split的包，包中定义了一个Split函数，具体实现如下： // split/split.go package split import \"strings\" // split package with a single split function. // Split slices s into all substrings separated by sep and // returns a slice of the substrings between those separators. func Split(s, sep string) (result []string) { i := strings.Index(s, sep) for i \u003e -1 { result = append(result, s[:i]) s = s[i+1:] i = strings.Index(s, sep) } result = append(result, s) return } 在当前目录下，我们创建一个split_test.go的测试文件，并定义一个测试函数如下：（表格测试法） // split/split_test.go package split import ( \"reflect\" \"testing\" ) func TestSplit(t *testing.T) { // 测试函数名必须以Test开头，必须接收一个*testing.T类型参数 got := Split(\"a🅱c\", \":\") // 程序输出的结果 want := []string{\"a\", \"b\", \"c\"} // 期望的结果 if !reflect.DeepEqual(want, got) { // 因为slice不能比较直接，借助反射包中的方法比较 t.Errorf(\"excepted:%v, got:%v\", want, got) // 测试失败输出错误提示 } } 此时split这个包中的文件如下： split $ ls -l total 16 -rw-r--r-- 1 pprof staff 408 4 29 15:50 split.go -rw-r--r-- 1 pprof staff 466 4 29 16:04 split_test.go 在split包路径下，执行go test命令，可以看到输出结果如下： split $ go test PASS ok github.com/pprof/studygo/code_demo/test_demo/split 0.005s 一个测试用例有点单薄，我们再编写一个测试使用多个字符切割字符串的例子，在split_test.go中添加如下测试函数： func TestMoreSplit(t *testing.T) { got := Split(\"abcd\", \"bc\") want := []string{\"a\", \"d\"} if !reflect.DeepEqual(want, got) { t.Errorf(\"excepted:%v, got:%v\", want, got) } } 再次运行go test命令，输出结果如下： split $ go test --- FAIL: TestMultiSplit (0.00s) split_test.go:20: excepted:[a d], got:[a cd] FAIL exit status 1 FAIL github.com/pprof/studygo/code_demo/test_demo/split 0.006s 这一次，我们的测试失败了。我们可以为go test命令添加-v参数，查看测试函数名称和运行时间： split $ go test -v === RUN TestSplit --- PASS: TestSplit (0.00s) === RUN TestMoreSplit --- FAIL: TestMoreSplit (0.00s) split_test.go:21: excepted:[a d], got:[a cd] FAIL exit status 1 FAIL github.com/pprof/studygo/code_demo/test_demo/split 0.005s 这一次我们能清楚的看到是TestMoreSplit这个测试没有成功。 还可以在go test命令后添加-run参数，它对应一个正则表达式，只有函数名匹配上的测试函数才会被go test命令执行。 split $ go test -v -run=\"More\" === RUN TestMoreSplit --- FAIL: TestMoreSplit (0.00s) split_test.go:21: excepted:[a d], got:[a cd] FAIL exit status 1 FAIL github.com/pprof/studygo/code_demo/test_demo/split 0.006s 现在我们回过头来解决我们程序中的问题。很显然我们最初的split函数并没有考虑到sep为多个字符的情况，我们来修复下这个Bug： package split import \"strings\" // split package with a single split function. // Split slices s into all substrings separated by sep and // returns a slice of the substrings between those separators. func Split(s, sep string) (result []string) { i := strings.Index(s, sep) for i \u003e -1 { result = append(result, s[:i]) s = s[i+len(sep):] // 这里使用len(sep)获取sep的长度 i = strings.Index(s, sep) } result = append(result, s) return } 这一次我们再来测试一下，我们的程序。注意，当我们修改了我们的代码之后不要仅仅执行那些失败的测试函数，我们应该完整的运行所有的测试，保证不会因为修改代码而引入了新的问题。 split $ go test -v === RUN TestSplit --- PASS: TestSplit (0.00s) === RUN TestMoreSplit --- PASS: TestMoreSplit (0.00s) PASS ok github.com/pprof/studygo/code_demo/test_demo/split 0.006s 这一次我们的测试都通过了 ","date":"2022-01-06 09:16:34","objectID":"/go_base_03/:10:3","tags":["go grammar"],"title":"Go_base_03","uri":"/go_base_03/"},{"categories":["Go"],"content":"测试组 我们现在还想要测试一下split函数对中文字符串的支持，这个时候我们可以再编写一个TestChineseSplit测试函数，但是我们也可以使用如下更友好的一种方式来添加更多的测试用例。 func TestSplit(t *testing.T) { // 定义一个测试用例类型 type test struct { input string sep string want []string } // 定义一个存储测试用例的切片 tests := []test{ {input: \"a🅱c\", sep: \":\", want: []string{\"a\", \"b\", \"c\"} }, {input: \"a🅱c\", sep: \",\", want: []string{\"a🅱c\"} }, {input: \"abcd\", sep: \"bc\", want: []string{\"a\", \"d\"} }, {input: \"枯藤老树昏鸦\", sep: \"老\", want: []string{\"枯藤\", \"树昏鸦\"} }, } // 遍历切片，逐一执行测试用例 for _, tc := range tests { got := Split(tc.input, tc.sep) if !reflect.DeepEqual(got, tc.want) { t.Errorf(\"excepted:%v, got:%v\", tc.want, got) } } } 我们通过上面的代码把多个测试用例合到一起，再次执行go test命令。 split $ go test -v === RUN TestSplit --- FAIL: TestSplit (0.00s) split_test.go:42: excepted:[枯藤 树昏鸦], got:[ 枯藤 树昏鸦] FAIL exit status 1 FAIL github.com/pprof/studygo/code_demo/test_demo/split 0.006s 我们的测试出现了问题，仔细看打印的测试失败提示信息：excepted:[枯藤 树昏鸦], got:[ 枯藤 树昏鸦]，你会发现[ 枯藤 树昏鸦]中有个不明显的空串，这种情况下十分推荐使用%#v的格式化方式。 我们修改下测试用例的格式化输出错误提示部分： func TestSplit(t *testing.T) { ... for _, tc := range tests { got := Split(tc.input, tc.sep) if !reflect.DeepEqual(got, tc.want) { t.Errorf(\"excepted:%#v, got:%#v\", tc.want, got) } } } 此时运行go test命令后就能看到比较明显的提示信息了： split $ go test -v === RUN TestSplit --- FAIL: TestSplit (0.00s) split_test.go:42: excepted:[]string{\"枯藤\", \"树昏鸦\"}, got:[]string{\"\", \"枯藤\", \"树昏鸦\"} FAIL exit status 1 FAIL github.com/Q1mi/studygo/code_demo/test_demo/split 0.006s ","date":"2022-01-06 09:16:34","objectID":"/go_base_03/:10:4","tags":["go grammar"],"title":"Go_base_03","uri":"/go_base_03/"},{"categories":["Go"],"content":"子测试 看起来都挺不错的，但是如果测试用例比较多的时候，我们是没办法一眼看出来具体是哪个测试用例失败了。我们可能会想到下面的解决办法 func TestSplit(t *testing.T) { type test struct { // 定义test结构体 input string sep string want []string } tests := map[string]test{ // 测试用例使用map存储 \"simple\": {input: \"a🅱c\", sep: \":\", want: []string{\"a\", \"b\", \"c\"} }, \"wrong sep\": {input: \"a🅱c\", sep: \",\", want: []string{\"a🅱c\"} }, \"more sep\": {input: \"abcd\", sep: \"bc\", want: []string{\"a\", \"d\"} }, \"leading sep\": {input: \"枯藤老树昏鸦\", sep: \"老\", want: []string{\"枯藤\", \"树昏鸦\"} }, } for name, tc := range tests { got := Split(tc.input, tc.sep) if !reflect.DeepEqual(got, tc.want) { t.Errorf(\"name:%s excepted:%#v, got:%#v\", name, tc.want, got) // 将测试用例的name格式化输出 } } } 上面的做法是能够解决问题的。同时Go1.7+中新增了子测试，我们可以按照如下方式使用t.Run执行子测试： func TestSplit(t *testing.T) { type test struct { // 定义test结构体 input string sep string want []string } tests := map[string]test{ // 测试用例使用map存储 \"simple\": {input: \"a🅱c\", sep: \":\", want: []string{\"a\", \"b\", \"c\"} }, \"wrong sep\": {input: \"a🅱c\", sep: \",\", want: []string{\"a🅱c\"} }, \"more sep\": {input: \"abcd\", sep: \"bc\", want: []string{\"a\", \"d\"} }, \"leading sep\": {input: \"枯藤老树昏鸦\", sep: \"老\", want: []string{\"枯藤\", \"树昏鸦\"} }, } for name, tc := range tests { t.Run(name, func(t *testing.T) { // 使用t.Run()执行子测试 got := Split(tc.input, tc.sep) if !reflect.DeepEqual(got, tc.want) { t.Errorf(\"excepted:%#v, got:%#v\", tc.want, got) } }) } } 此时我们再执行go test命令就能够看到更清晰的输出内容了： split $ go test -v === RUN TestSplit === RUN TestSplit/leading_sep === RUN TestSplit/simple === RUN TestSplit/wrong_sep === RUN TestSplit/more_sep --- FAIL: TestSplit (0.00s) --- FAIL: TestSplit/leading_sep (0.00s) split_test.go:83: excepted:[]string{\"枯藤\", \"树昏鸦\"}, got:[]string{\"\", \"枯藤\", \"树昏鸦\"} --- PASS: TestSplit/simple (0.00s) --- PASS: TestSplit/wrong_sep (0.00s) --- PASS: TestSplit/more_sep (0.00s) FAIL exit status 1 FAIL github.com/pprof/studygo/code_demo/test_demo/split 0.006s 这个时候我们要把测试用例中的错误修改回来： func TestSplit(t *testing.T) { ... tests := map[string]test{ // 测试用例使用map存储 \"simple\": {input: \"a🅱c\", sep: \":\", want: []string{\"a\", \"b\", \"c\"} }, \"wrong sep\": {input: \"a🅱c\", sep: \",\", want: []string{\"a🅱c\"} }, \"more sep\": {input: \"abcd\", sep: \"bc\", want: []string{\"a\", \"d\"} }, \"leading sep\": {input: \"枯藤老树昏鸦\", sep: \"老\", want: []string{\"\", \"枯藤\", \"树昏鸦\"} }, } ... } 我们都知道可以通过-run=RegExp来指定运行的测试用例，还可以通过/来指定要运行的子测试用例，例如：go test -v -run=Split/simple只会运行simple对应的子测试用例。 ","date":"2022-01-06 09:16:34","objectID":"/go_base_03/:10:5","tags":["go grammar"],"title":"Go_base_03","uri":"/go_base_03/"},{"categories":["Go"],"content":"测试覆盖率 测试覆盖率是你的代码被测试套件覆盖的百分比。通常我们使用的都是语句的覆盖率，也就是在测试中至少被运行一次的代码占总代码的比例。 Go提供内置功能来检查你的代码覆盖率。我们可以使用go test -cover来查看测试覆盖率。例如： split $ go test -cover PASS coverage: 100.0% of statements ok github.com/pprof/studygo/code_demo/test_demo/split 0.005s 从上面的结果可以看到我们的测试用例覆盖了100%的代码。 Go还提供了一个额外的-coverprofile参数，用来将覆盖率相关的记录信息输出到一个文件。例如： split $ go test -cover -coverprofile=c.out PASS coverage: 100.0% of statements ok github.com/pprof/studygo/code_demo/test_demo/split 0.005s 上面的命令会将覆盖率相关的信息输出到当前文件夹下面的c.out文件中，然后我们执行go tool cover -html=c.out，使用cover工具来处理生成的记录信息，该命令会打开本地的浏览器窗口生成一个HTML报告。 ","date":"2022-01-06 09:16:34","objectID":"/go_base_03/:10:6","tags":["go grammar"],"title":"Go_base_03","uri":"/go_base_03/"},{"categories":["Go"],"content":"基准测试 基准测试函数格式 基准测试就是在一定的工作负载之下检测程序性能的一种方法。基准测试的基本格式如下： func BenchmarkName(b *testing.B){ // ... } 基准测试以Benchmark为前缀，需要一个*testing.B类型的参数b，基准测试必须要执行b.N次，这样的测试才有对照性，b.N的值是系统根据实际情况去调整的，从而保证测试的稳定性。 testing.B拥有的方法如下： func (c *B) Error(args ...interface{}) func (c *B) Errorf(format string, args ...interface{}) func (c *B) Fail() func (c *B) FailNow() func (c *B) Failed() bool func (c *B) Fatal(args ...interface{}) func (c *B) Fatalf(format string, args ...interface{}) func (c *B) Log(args ...interface{}) func (c *B) Logf(format string, args ...interface{}) func (c *B) Name() string func (b *B) ReportAllocs() func (b *B) ResetTimer() func (b *B) Run(name string, f func(b *B)) bool func (b *B) RunParallel(body func(*PB)) func (b *B) SetBytes(n int64) func (b *B) SetParallelism(p int) func (c *B) Skip(args ...interface{}) func (c *B) SkipNow() func (c *B) Skipf(format string, args ...interface{}) func (c *B) Skipped() bool func (b *B) StartTimer() func (b *B) StopTimer() 基准测试示例 我们为split包中的Split函数编写基准测试如下： func BenchmarkSplit(b *testing.B) { for i := 0; i \u003c b.N; i++ { Split(\"枯藤老树昏鸦\", \"老\") } } 基准测试并不会默认执行，需要增加-bench参数，所以我们通过执行go test -bench=Split命令执行基准测试，输出结果如下： split $ go test -bench=Split goos: darwin goarch: amd64 pkg: github.com/pprof/studygo/code_demo/test_demo/split BenchmarkSplit-8 10000000 203 ns/op PASS ok github.com/pprof/studygo/code_demo/test_demo/split 2.255s 其中BenchmarkSplit-8表示对Split函数进行基准测试，数字8表示GOMAXPROCS的值，这个对于并发基准测试很重要。10000000和203ns/op表示每次调用Split函数耗时203ns，这个结果是10000000次调用的平均值。 我们还可以为基准测试添加-benchmem参数，来获得内存分配的统计数据。 split $ go test -bench=Split -benchmem goos: darwin goarch: amd64 pkg: github.com/pprof/studygo/code_demo/test_demo/split BenchmarkSplit-8 10000000 215 ns/op 112 B/op 3 allocs/op PASS ok github.com/pprof/studygo/code_demo/test_demo/split 2.394s 其中，112 B/op表示每次操作内存分配了112字节，3 allocs/op则表示每次操作进行了3次内存分配。 我们将我们的Split函数优化如下： func Split(s, sep string) (result []string) { result = make([]string, 0, strings.Count(s, sep)+1) i := strings.Index(s, sep) for i \u003e -1 { result = append(result, s[:i]) s = s[i+len(sep):] // 这里使用len(sep)获取sep的长度 i = strings.Index(s, sep) } result = append(result, s) return } 这一次我们提前使用make函数将result初始化为一个容量足够大的切片，而不再像之前一样通过调用append函数来追加。我们来看一下这个改进会带来多大的性能提升： split $ go test -bench=Split -benchmem goos: darwin goarch: amd64 pkg: github.com/pprof/studygo/code_demo/test_demo/split BenchmarkSplit-8 10000000 127 ns/op 48 B/op 1 allocs/op PASS ok github.com/pprof/studygo/code_demo/test_demo/split 1.423s 这个使用make函数提前分配内存的改动，减少了2/3的内存分配次数，并且减少了一半的内存分配。 ","date":"2022-01-06 09:16:34","objectID":"/go_base_03/:10:7","tags":["go grammar"],"title":"Go_base_03","uri":"/go_base_03/"},{"categories":["Go"],"content":"x性能比较函数 上面的基准测试只能得到给定操作的绝对耗时，但是在很多性能问题是发生在两个不同操作之间的相对耗时，比如同一个函数处理1000个元素的耗时与处理1万甚至100万个元素的耗时的差别是多少？再或者对于同一个任务究竟使用哪种算法性能最佳？我们通常需要对两个不同算法的实现使用相同的输入来进行基准比较测试。 性能比较函数通常是一个带有参数的函数，被多个不同的Benchmark函数传入不同的值来调用。举个例子如下： func benchmark(b *testing.B, size int){/* ... */} func Benchmark10(b *testing.B){ benchmark(b, 10) } func Benchmark100(b *testing.B){ benchmark(b, 100) } func Benchmark1000(b *testing.B){ benchmark(b, 1000) } 例如我们编写了一个计算斐波那契数列的函数如下： // fib.go // Fib 是一个计算第n个斐波那契数的函数 func Fib(n int) int { if n \u003c 2 { return n } return Fib(n-1) + Fib(n-2) } 我们编写的性能比较函数如下： // fib_test.go func benchmarkFib(b *testing.B, n int) { for i := 0; i \u003c b.N; i++ { Fib(n) } } func BenchmarkFib1(b *testing.B) { benchmarkFib(b, 1) } func BenchmarkFib2(b *testing.B) { benchmarkFib(b, 2) } func BenchmarkFib3(b *testing.B) { benchmarkFib(b, 3) } func BenchmarkFib10(b *testing.B) { benchmarkFib(b, 10) } func BenchmarkFib20(b *testing.B) { benchmarkFib(b, 20) } func BenchmarkFib40(b *testing.B) { benchmarkFib(b, 40) } 运行基准测试： split $ go test -bench=. goos: darwin goarch: amd64 pkg: github.com/pprof/studygo/code_demo/test_demo/fib BenchmarkFib1-8 1000000000 2.03 ns/op BenchmarkFib2-8 300000000 5.39 ns/op BenchmarkFib3-8 200000000 9.71 ns/op BenchmarkFib10-8 5000000 325 ns/op BenchmarkFib20-8 30000 42460 ns/op BenchmarkFib40-8 2 638524980 ns/op PASS ok github.com/pprof/studygo/code_demo/test_demo/fib 12.944s 这里需要注意的是，默认情况下，每个基准测试至少运行1秒。如果在Benchmark函数返回时没有到1秒，则b.N的值会按1,2,5,10,20,50，…增加，并且函数再次运行。 最终的BenchmarkFib40只运行了两次，每次运行的平均值只有不到一秒。像这种情况下我们应该可以使用-benchtime标志增加最小基准时间，以产生更准确的结果。例如： split $ go test -bench=Fib40 -benchtime=20s goos: darwin goarch: amd64 pkg: github.com/pprof/studygo/code_demo/test_demo/fib BenchmarkFib40-8 50 663205114 ns/op PASS ok github.com/pprof/studygo/code_demo/test_demo/fib 33.849s 这一次BenchmarkFib40函数运行了50次，结果就会更准确一些了。 使用性能比较函数做测试的时候一个容易犯的错误就是把b.N作为输入的大小，例如以下两个例子都是错误的示范： // 错误示范1 func BenchmarkFibWrong(b *testing.B) { for n := 0; n \u003c b.N; n++ { Fib(n) } } // 错误示范2 func BenchmarkFibWrong2(b *testing.B) { Fib(b.N) } ","date":"2022-01-06 09:16:34","objectID":"/go_base_03/:10:8","tags":["go grammar"],"title":"Go_base_03","uri":"/go_base_03/"},{"categories":["Go"],"content":"重置时间 b.ResetTimer之前的处理不会放到执行时间里，也不会输出到报告中，所以可以在之前做一些不计划作为测试报告的操作。例如： func BenchmarkSplit(b *testing.B) { time.Sleep(5 * time.Second) // 假设需要做一些耗时的无关操作 b.ResetTimer() // 重置计时器 for i := 0; i \u003c b.N; i++ { Split(\"枯藤老树昏鸦\", \"老\") } } ","date":"2022-01-06 09:16:34","objectID":"/go_base_03/:10:9","tags":["go grammar"],"title":"Go_base_03","uri":"/go_base_03/"},{"categories":["Go"],"content":"并行测试 func (b B) RunParallel(body func(PB))会以并行的方式执行给定的基准测试。 RunParallel会创建出多个goroutine，并将b.N分配给这些goroutine执行， 其中goroutine数量的默认值为GOMAXPROCS。用户如果想要增加非CPU受限（non-CPU-bound）基准测试的并行性， 那么可以在RunParallel之前调用SetParallelism 。RunParallel通常会与-cpu标志一同使用。 func BenchmarkSplitParallel(b *testing.B) { // b.SetParallelism(1) // 设置使用的CPU数 b.RunParallel(func(pb *testing.PB) { for pb.Next() { Split(\"枯藤老树昏鸦\", \"老\") } }) } 执行一下基准测试： split $ go test -bench=. goos: darwin goarch: amd64 pkg: github.com/pprof/studygo/code_demo/test_demo/split BenchmarkSplit-8 10000000 131 ns/op BenchmarkSplitParallel-8 50000000 36.1 ns/op PASS ok github.com/pprof/studygo/code_demo/test_demo/split 3.308s 还可以通过在测试命令后添加-cpu参数如go test -bench=. -cpu 1来指定使用的CPU数量。 ","date":"2022-01-06 09:16:34","objectID":"/go_base_03/:10:10","tags":["go grammar"],"title":"Go_base_03","uri":"/go_base_03/"},{"categories":["Go"],"content":"Setup与TearDown 测试程序有时需要在测试之前进行额外的设置（setup）或在测试之后进行拆卸（teardown）。 TestMain 通过在*_test.go文件中定义TestMain函数来可以在测试之前进行额外的设置（setup）或在测试之后进行拆卸（teardown）操作。 如果测试文件包含函数:func TestMain(m *testing.M)那么生成的测试会先调用 TestMain(m)，然后再运行具体测试。TestMain运行在主goroutine中, 可以在调用 m.Run前后做任何设置（setup）和拆卸（teardown）。退出测试的时候应该使用m.Run的返回值作为参数调用os.Exit。 一个使用TestMain来设置Setup和TearDown的示例如下： func TestMain(m *testing.M) { fmt.Println(\"write setup code here...\") // 测试之前的做一些设置 // 如果 TestMain 使用了 flags，这里应该加上flag.Parse() retCode := m.Run() // 执行测试 fmt.Println(\"write teardown code here...\") // 测试之后做一些拆卸工作 os.Exit(retCode) // 退出测试 } 需要注意的是：在调用TestMain时, flag.Parse并没有被调用。所以如果TestMain 依赖于command-line标志 (包括 testing 包的标记), 则应该显示的调用flag.Parse。 子测试的Setup与Teardown 有时候我们可能需要为每个测试集设置Setup与Teardown，也有可能需要为每个子测试设置Setup与Teardown。下面我们定义两个函数工具函数如下： // 测试集的Setup与Teardown func setupTestCase(t *testing.T) func(t *testing.T) { t.Log(\"如有需要在此执行:测试之前的setup\") return func(t *testing.T) { t.Log(\"如有需要在此执行:测试之后的teardown\") } } // 子测试的Setup与Teardown func setupSubTest(t *testing.T) func(t *testing.T) { t.Log(\"如有需要在此执行:子测试之前的setup\") return func(t *testing.T) { t.Log(\"如有需要在此执行:子测试之后的teardown\") } } 使用方式如下： func TestSplit(t *testing.T) { type test struct { // 定义test结构体 input string sep string want []string } tests := map[string]test{ // 测试用例使用map存储 \"simple\": {input: \"a🅱c\", sep: \":\", want: []string{\"a\", \"b\", \"c\"} }, \"wrong sep\": {input: \"a🅱c\", sep: \",\", want: []string{\"a🅱c\"} }, \"more sep\": {input: \"abcd\", sep: \"bc\", want: []string{\"a\", \"d\"} }, \"leading sep\": {input: \"枯藤老树昏鸦\", sep: \"老\", want: []string{\"\", \"枯藤\", \"树昏鸦\"} }, } teardownTestCase := setupTestCase(t) // 测试之前执行setup操作 defer teardownTestCase(t) // 测试之后执行testdoen操作 for name, tc := range tests { t.Run(name, func(t *testing.T) { // 使用t.Run()执行子测试 teardownSubTest := setupSubTest(t) // 子测试之前执行setup操作 defer teardownSubTest(t) // 测试之后执行testdoen操作 got := Split(tc.input, tc.sep) if !reflect.DeepEqual(got, tc.want) { t.Errorf(\"excepted:%#v, got:%#v\", tc.want, got) } }) } } 测试结果如下： split $ go test -v === RUN TestSplit === RUN TestSplit/simple === RUN TestSplit/wrong_sep === RUN TestSplit/more_sep === RUN TestSplit/leading_sep --- PASS: TestSplit (0.00s) split_test.go:71: 如有需要在此执行:测试之前的setup --- PASS: TestSplit/simple (0.00s) split_test.go:79: 如有需要在此执行:子测试之前的setup split_test.go:81: 如有需要在此执行:子测试之后的teardown --- PASS: TestSplit/wrong_sep (0.00s) split_test.go:79: 如有需要在此执行:子测试之前的setup split_test.go:81: 如有需要在此执行:子测试之后的teardown --- PASS: TestSplit/more_sep (0.00s) split_test.go:79: 如有需要在此执行:子测试之前的setup split_test.go:81: 如有需要在此执行:子测试之后的teardown --- PASS: TestSplit/leading_sep (0.00s) split_test.go:79: 如有需要在此执行:子测试之前的setup split_test.go:81: 如有需要在此执行:子测试之后的teardown split_test.go:73: 如有需要在此执行:测试之后的teardown === RUN ExampleSplit --- PASS: ExampleSplit (0.00s) PASS ok github.com/Q1mi/studygo/code_demo/test_demo/split 0.006s ","date":"2022-01-06 09:16:34","objectID":"/go_base_03/:10:11","tags":["go grammar"],"title":"Go_base_03","uri":"/go_base_03/"},{"categories":["Go"],"content":"示例函数 示例函数的格式 被go test特殊对待的第三种函数就是示例函数，它们的函数名以Example为前缀。它们既没有参数也没有返回值。标准格式如下： func ExampleName() { // ... } 示例函数示例 下面的代码是我们为Split函数编写的一个示例函数： func ExampleSplit() { fmt.Println(split.Split(\"a🅱c\", \":\")) fmt.Println(split.Split(\"枯藤老树昏鸦\", \"老\")) // Output: // [a b c] // [ 枯藤 树昏鸦] } 为你的代码编写示例代码有如下三个用处： 示例函数能够作为文档直接使用，例如基于web的godoc中能把示例函数与对应的函数或包相关联。 示例函数只要包含了// Output:也是可以通过go test运行的可执行测试。 split $ go test -run Example PASS ok github.com/pprof/studygo/code_demo/test_demo/split 0.006s 示例函数提供了可以直接运行的示例代码，可以直接在golang.org的godoc文档服务器上使用Go Playground运行示例代码。下图为strings.ToUpper函数在Playground的示例函数效果 ","date":"2022-01-06 09:16:34","objectID":"/go_base_03/:10:12","tags":["go grammar"],"title":"Go_base_03","uri":"/go_base_03/"},{"categories":["Go"],"content":"func ToUpper func ToUpper(s string) string ToUpper returms a copy of the sring s with all Unicode ltters mapped to their upper case. Example: package main import ( \"fnt\" \"strings\" ) func main() { fmt. Println(strings . ToUpper(\"Gopher\")) } ","date":"2022-01-06 09:16:34","objectID":"/go_base_03/:10:13","tags":["go grammar"],"title":"Go_base_03","uri":"/go_base_03/"},{"categories":["Go"],"content":"压力测试 ","date":"2022-01-06 09:16:34","objectID":"/go_base_03/:11:0","tags":["go grammar"],"title":"Go_base_03","uri":"/go_base_03/"},{"categories":["Go"],"content":"Go怎么写测试用例 开发程序其中很重要的一点是测试，我们如何保证代码的质量，如何保证每个函数是可运行，运行结果是正确的，又如何保证写出来的代码性能是好的，我们知道单元测试的重点在于发现程序设计或实现的逻辑错误，使问题及早暴露，便于问题的定位解决，而性能测试的重点在于发现程序设计上的一些问题，让线上的程序能够在高并发的情况下还能保持稳定。本小节将带着这一连串的问题来讲解Go语言中如何来实现单元测试和性能测试。 Go语言中自带有一个轻量级的测试框架testing和自带的go test命令来实现单元测试和性能测试，testing框架和其他语言中的测试框架类似，你可以基于这个框架写针对相应函数的测试用例，也可以基于该框架写相应的压力测试用例，那么接下来让我们一一来看一下怎么写。 另外建议安装gotests插件自动生成测试代码: go get -u -v github.com/cweill/gotests/... ","date":"2022-01-06 09:16:34","objectID":"/go_base_03/:11:1","tags":["go grammar"],"title":"Go_base_03","uri":"/go_base_03/"},{"categories":["Go"],"content":"如何编写测试用例 由于go test命令只能在一个相应的目录下执行所有文件，所以我们接下来新建一个项目目录gotest,这样我们所有的代码和测试代码都在这个目录下。 接下来我们在该目录下面创建两个文件：gotest.go和gotest_test.go gotest.go:这个文件里面我们是创建了一个包，里面有一个函数实现了除法运算: package gotest import ( \"errors\" ) func Division(a, b float64) (float64, error) { if b == 0 { return 0, errors.New(\"除数不能为0\") } return a / b, nil } gotest_test.go:这是我们的单元测试文件，但是记住下面的这些原则： 文件名必须是_test.go结尾的，这样在执行go test的时候才会执行到相应的代码 你必须import testing这个包 所有的测试用例函数必须是Test开头 测试用例会按照源代码中写的顺序依次执行 测试函数TestXxx()的参数是testing.T，我们可以使用该类型来记录错误或者是测试状态 测试格式：func TestXxx (t *testing.T),Xxx部分可以为任意的字母数字的组合，但是首字母不能是小写字母[a-z]，例如Testintdiv是错误的函数名。 函数中通过调用testing.T的Error, Errorf, FailNow, Fatal, FatalIf方法，说明测试不通过，调用Log方法用来记录测试的信息。 下面是我们的测试用例的代码： package gotest import ( \"testing\" ) func Test_Division_1(t *testing.T) { if i, e := Division(6, 2); i != 3 || e != nil { //try a unit test on function t.Error(\"除法函数测试没通过\") // 如果不是如预期的那么就报错 } else { t.Log(\"第一个测试通过了\") //记录一些你期望记录的信息 } } func Test_Division_2(t *testing.T) { t.Error(\"就是不通过\") } 我们在项目目录下面执行go test,就会显示如下信息： --- FAIL: Test_Division_2 (0.00 seconds) gotest_test.go:16: 就是不通过 FAIL exit status 1 FAIL gotest 0.013s 从这个结果显示测试没有通过，因为在第二个测试函数中我们写死了测试不通过的代码t.Error，那么我们的第一个函数执行的情况怎么样呢？默认情况下执行go test是不会显示测试通过的信息的，我们需要带上参数go test -v，这样就会显示如下信息： === RUN Test_Division_1 --- PASS: Test_Division_1 (0.00 seconds) gotest_test.go:11: 第一个测试通过了 === RUN Test_Division_2 --- FAIL: Test_Division_2 (0.00 seconds) gotest_test.go:16: 就是不通过 FAIL exit status 1 FAIL gotest 0.012s 上面的输出详细的展示了这个测试的过程，我们看到测试函数1Test_Division_1测试通过，而测试函数2Test_Division_2测试失败了，最后得出结论测试不通过。接下来我们把测试函数2修改成如下代码： func Test_Division_2(t *testing.T) { if _, e := Division(6, 0); e == nil { //try a unit test on function t.Error(\"Division did not work as expected.\") // 如果不是如预期的那么就报错 } else { t.Log(\"one test passed.\", e) //记录一些你期望记录的信息 } } 然后我们执行go test -v，就显示如下信息，测试通过了： === RUN Test_Division_1 --- PASS: Test_Division_1 (0.00 seconds) gotest_test.go:11: 第一个测试通过了 === RUN Test_Division_2 --- PASS: Test_Division_2 (0.00 seconds) gotest_test.go:20: one test passed. 除数不能为0 PASS ok gotest 0.013s //一般 go test -v -run funcName fileName.go go test -v -cover -run funcName fileName.go ","date":"2022-01-06 09:16:34","objectID":"/go_base_03/:11:2","tags":["go grammar"],"title":"Go_base_03","uri":"/go_base_03/"},{"categories":["Go"],"content":"如何编写压力测试 压力测试用来检测函数(方法）的性能，和编写单元功能测试的方法类似,此处不再赘述，但需要注意以下几点： 压力测试用例必须遵循如下格式，其中XXX可以是任意字母数字的组合，但是首字母不能是小写字母 func BenchmarkXXX(b *testing.B) { ... } go test不会默认执行压力测试的函数，如果要执行压力测试需要带上参数-test.bench，语法:-test.bench=”test_name_regex”,例如go test -test.bench=”.*“表示测试全部的压力测试函数 在压力测试用例中,请记得在循环体内使用testing.B.N,以使测试可以正常的运行 文件名也必须以_test.go结尾 下面我们新建一个压力测试文件webbench_test.go，代码如下所示： import ( \"testing\" ) func Benchmark_Division(b *testing.B) { for i := 0; i \u003c b.N; i++ { //use b.N for looping Division(4, 5) } } func Benchmark_TimeConsumingFunction(b *testing.B) { b.StopTimer() //调用该函数停止压力测试的时间计数 //做一些初始化的工作,例如读取文件数据,数据库连接之类的, //这样这些时间不影响我们测试函数本身的性能 b.StartTimer() //重新开始时间 for i := 0; i \u003c b.N; i++ { Division(4, 5) } } 我们执行命令go test webbench_test.go -test.bench=”.*\"，可以看到如下结果： Benchmark_Division-4 500000000 7.76 ns/op 456 B/op 14 allocs/op Benchmark_TimeConsumingFunction-4 500000000 7.80 ns/op 224 B/op 4 allocs/op PASS ok gotest 9.364s 上面的结果显示我们没有执行任何TestXXX的单元测试函数，显示的结果只执行了压力测试函数，第一条显示了Benchmark_Division执行了500000000次，每次的执行平均时间是7.76纳秒，第二条显示了Benchmark_TimeConsumingFunction执行了500000000，每次的平均执行时间是7.80纳秒。最后一条显示总共的执行时间。 ","date":"2022-01-06 09:16:34","objectID":"/go_base_03/:11:3","tags":["go grammar"],"title":"Go_base_03","uri":"/go_base_03/"},{"categories":["Go"],"content":"BDD Behavior Driven Development行为驱动开发 行为驱动开发（Behavior-Driven Development）（简写BDD），在软件工程中，BDD是一种敏捷软件开发的技术。行为驱动开发(BDD)是测试驱动开发的延伸，开发使用简单的，特定于领域的脚本语言。这些DSL将结构化自然语言语句转换为可执行测试。结果是与给定功能的验收标准以及用于验证该功能的测试之间的关系更密切。因此，它一般是测试驱动开发(TDD)测试的自然延伸。（摘自百度百科） BDD in Go 一个测试框架 项⽬⽹站 https://github.com/smartystreets/goconvey 安装 go get -u github.com/smartystreets/goconvey/convey 启动 WEB UI $GOPATH/bin/goconvey package testing import ( \"testing\" . \"github.com/smartystreets/goconvey/convey\" ) func TestSpec(t *testing.T) { // Only pass t into top-level Convey calls Convey(\"Given 2 even numbers\", t, func() { a := 3 b := 4 Convey(\"When add the two numbers\", func() { c := a + b Convey(\"Then the result is still even\", func() { So(c%2, ShouldEqual, 0) }) }) }) } convey这个pkg还提供了一个web界面，在gopath下的bin目录里面有二进制程序goconvey。 ","date":"2022-01-06 09:16:34","objectID":"/go_base_03/:12:0","tags":["go grammar"],"title":"Go_base_03","uri":"/go_base_03/"},{"categories":["Go"],"content":" 参考学习go语言中文网、C语言中文网、golang官方文档等 流程控制 ","date":"2022-01-06 09:16:07","objectID":"/go_base_02/:0:0","tags":["go grammar"],"title":"Go_base_02","uri":"/go_base_02/"},{"categories":["Go"],"content":"if Go 编程语言中 if 语句的语法如下： 可省略条件表达式括号。 持初始化语句，可定义代码块局部变量，也叫做if语句的自用变量，而这自用变量作用域在其声明所在的代码块，但不能在该if语句块之外。 代码块左花括号必须在条件表达式尾部。 可嵌套 if 布尔表达式 { /* 在布尔表达式为 true 时执行 */ } if分支有三种：单分支、二分支、多分支，要减少多分支结构，甚至是二分支结构的使用。这样的代码更优雅、简洁、易读、易维护。单分支结构的使用被称为符合“快乐路径”原则。 Go不支持三元操作符(三目运算符) “a \u003e b ? a : b” ","date":"2022-01-06 09:16:07","objectID":"/go_base_02/:1:0","tags":["go grammar"],"title":"Go_base_02","uri":"/go_base_02/"},{"categories":["Go"],"content":"switch Golang switch 分支表达式可以是任意类型，不限于常量。可省略 break，默认自动终止。 可以同时测试多个可能符合条件的值，使用逗号分割它们，例如：case val1, val2, val3。 switch 语句支持声明临时变量。 如： switch { case grade == \"A\" : fmt.Printf(\"优秀!\\n\" ) case grade == \"B\", grade == \"C\" : fmt.Printf(\"良好\\n\" ) case grade == \"D\" : fmt.Printf(\"及格\\n\" ) case grade == \"F\": fmt.Printf(\"不及格\\n\" ) default: fmt.Printf(\"差\\n\" ) } ","date":"2022-01-06 09:16:07","objectID":"/go_base_02/:2:0","tags":["go grammar"],"title":"Go_base_02","uri":"/go_base_02/"},{"categories":["Go"],"content":"Type Switch switch 语句还可以被用于 type-switch 来判断某个 interface 变量中实际存储的变量类型。 switch x.(type){ case type1: statement(s) case type2: statement(s) /* 你可以定义任意个数的case */ default: /* 可选 */ statement(s) } ","date":"2022-01-06 09:16:07","objectID":"/go_base_02/:2:1","tags":["go grammar"],"title":"Go_base_02","uri":"/go_base_02/"},{"categories":["Go"],"content":"select select 语句类似于 switch 语句，但是select会随机执行一个可运行的case。如果没有case可运行，且没有default语句，它将阻塞，直到有case可运行。 每个case都必须是一个通信 所有channel表达式都会被求值 所有被发送的表达式都会被求值 如果任意某个通信可以进行，它就执行；其他被忽略。 如果有多个case都可以运行，Select会随机公平地选出一个执行。其他不会执行。 否则： 如果有default子句，则执行该语句。 如果没有default字句，select将阻塞，直到某个通信可以运行；Go不会重新对channel或值进行求值。 select可以监听channel的数据流动 select的用法与switch语法非常类似，由select开始的一个新的选择块，每个选择条件由case语句来描述 与switch语句可以选择任何使用相等比较的条件相比，select有比较多的限制，其中最大的一条限制就是每个case语句里必须是一个IO操作 select { //不停的在这里检测 case \u003c-chanl : //检测有没有数据可以读 //如果chanl成功读取到数据，则进行该case处理语句 case chan2 \u003c- 1 : //检测有没有可以写 //如果成功向chan2写入数据，则进行该case处理语句 //假如没有default，那么在以上两个条件都不成立的情况下，就会在此阻塞//一般default会不写在里面，select中的default子句总是可运行的，因为会很消耗CPU资源 default: //如果以上都没有符合条件，那么则进行default处理流程 } 在一个select语句中，Go会按顺序从头到尾评估每一个发送和接收的语句。 如果其中的任意一个语句可以继续执行（即没有被阻塞），那么就从那些可以执行的语句中任意选择一条来使用。 如果没有任意一条语句可以执行（即所有的通道都被阻塞），那么有两种可能的情况： ①如果给出了default语句，那么就会执行default的流程，同时程序的执行会从select语句后的语句中恢复。 ②如果没有default语句，那么select语句将被阻塞，直到至少有一个case可以进行下去。 ","date":"2022-01-06 09:16:07","objectID":"/go_base_02/:3:0","tags":["go grammar"],"title":"Go_base_02","uri":"/go_base_02/"},{"categories":["Go"],"content":"基本使用 select是Go中的一个控制结构，类似于switch语句，用于处理异步IO操作。select会监听case语句中channel的读写操作，当case中channel读写操作为非阻塞状态（即能读写）时，将会触发相应的动作。 select中的case语句必须是一个channel操作 select中的default子句总是可运行的。 如果有多个case都可以运行，select会随机公平地选出一个执行，其他不会执行。 如果没有可运行的case语句，且有default语句，那么就会执行default的动作。 如果没有可运行的case语句，且没有default语句，select将阻塞，直到某个case通信可以运行 例如： package main import \"fmt\" func main() { var c1, c2, c3 chan int var i1, i2 int select { case i1 = \u003c-c1: fmt.Printf(\"received \", i1, \" from c1\\n\") case c2 \u003c- i2: fmt.Printf(\"sent \", i2, \" to c2\\n\") case i3, ok := (\u003c-c3): // same as: i3, ok := \u003c-c3 if ok { fmt.Printf(\"received \", i3, \" from c3\\n\") } else { fmt.Printf(\"c3 is closed\\n\") } default: fmt.Printf(\"no communication\\n\") } } //输出：no communication ","date":"2022-01-06 09:16:07","objectID":"/go_base_02/:3:1","tags":["go grammar"],"title":"Go_base_02","uri":"/go_base_02/"},{"categories":["Go"],"content":"典型用法 超时判断： //比如在下面的场景中，使用全局resChan来接受response，如果时间超过3S,resChan中还没有数据返回，则第二条case将执行 var resChan = make(chan int) // do request func test() { select { case data := \u003c-resChan: doData(data) case \u003c-time.After(time.Second * 3): fmt.Println(\"request time out\") } } func doData(data int) { //... } 退出 //主线程（协程）中如下： var shouldQuit=make(chan struct{}) fun main(){ { //loop } //...out of the loop select { case \u003c-c.shouldQuit: cleanUp() return default: } //... } //再另外一个协程中，如果运行遇到非法操作或不可处理的错误，就向shouldQuit发送数据通知程序停止运行 close(shouldQuit) 判断channel是否阻塞 //在某些情况下是存在不希望channel缓存满了的需求的，可以用如下方法判断 ch := make (chan int, 5) //... data：=0 select { case ch \u003c- data: default: //做相应操作，比如丢弃data。视需求而定 } ","date":"2022-01-06 09:16:07","objectID":"/go_base_02/:3:2","tags":["go grammar"],"title":"Go_base_02","uri":"/go_base_02/"},{"categories":["Go"],"content":"for Go语言的For循环有3中形式，只有其中的一种使用分号。 for init; condition; post { } for condition { } for { } init： 一般为赋值表达式，给控制变量赋初值； condition： 关系表达式或逻辑表达式，循环控制条件； post： 一般为赋值表达式，给控制变量增量或减量。 for语句执行过程如下： ①先对表达式 init 赋初值； ②判别赋值表达式 init 是否满足给定 condition 条件，若其值为真，满足循环条件，则执行循环体内语句，然后执行 post，进入第二次循环，再判别 condition；否则判断 condition 的值为假，不满足条件，就终止for循环，执行循环体外语句。 可嵌套，可无限循环。 ","date":"2022-01-06 09:16:07","objectID":"/go_base_02/:4:0","tags":["go grammar"],"title":"Go_base_02","uri":"/go_base_02/"},{"categories":["Go"],"content":"range Golang range类似迭代器操作，返回 (索引, 值) 或 (键, 值)。 for 循环的 range 格式可以对 slice、map、数组、字符串等进行迭代循环。格式如下： for key, value := range oldMap { newMap[key] = value } ****注意range会复制对象： package main import \"fmt\" func main() { a := [3]int{0, 1, 2} for i, v := range a { // index、value 都是从复制品中取出。 if i == 0 { // 在修改前，我们先修改原数组。 a[1], a[2] = 999, 999 fmt.Println(a) // 确认修改有效，输出 [0, 999, 999]。 } a[i] = v + 100 // 使用复制品中取出的 value 修改原数组。 } fmt.Println(a) // 输出 [100, 101, 102]。 } output: [0 999 999] [100 101 102] 这说明开始执行range时就已经保存了当下的数组，所以在循环里修改数组也不会改动遍历时的使用的数组的值。 改用引用类型，其底层数据不会被复制： package main func main() { s := []int{1, 2, 3, 4, 5} for i, v := range s { // 复制 struct slice { pointer, len, cap }。 if i == 0 { s = s[:3] // 对 slice 的修改，不会影响 range。 s[2] = 100 // 对底层数据的修改。 } println(i, v) } } output: 0 1 1 2 2 100 3 4 4 5 另外两种引用类型 map、channel 也能应用于for range遍历上，是指针包装，而不像 slice 是 struct。 for 和 for range有什么区别? 主要是使用场景不同 for可以 遍历array和slice 遍历key为整型递增的map 遍历string for range可以完成所有for可以做的事情，却能做到for不能做的，包括 遍历key为string类型的map并同时获取key和value 遍历channel ","date":"2022-01-06 09:16:07","objectID":"/go_base_02/:5:0","tags":["go grammar"],"title":"Go_base_02","uri":"/go_base_02/"},{"categories":["Go"],"content":"Goto Break Continue 循环控制语句： Goto、Break、Continue 三个语句都可以配合标签(label)使用 标签名区分大小写，定以后若不使用会造成编译错误 continue、break配合标签(label)可用于多层循环跳出 goto是调整执行位置，与continue、break配合标签(label)的结果并不相同 ","date":"2022-01-06 09:16:07","objectID":"/go_base_02/:6:0","tags":["go grammar"],"title":"Go_base_02","uri":"/go_base_02/"},{"categories":["Coding"],"content":" 学习代码随想录笔记 动态规划 有很多重叠子问题，优先考虑使用动态规划。 与贪心的区别：贪心不会考虑之前的状态而只考虑局部最优。 ","date":"2022-01-06 08:27:39","objectID":"/algorithm_dynamicprogramming/:0:0","tags":["data structure"],"title":"Algorithm_dynamicProgramming","uri":"/algorithm_dynamicprogramming/"},{"categories":["Coding"],"content":"理论基础 dp步骤： 确定dp数组（dp table）以及下标的含义 确定递推公式 dp数组如何初始化 确定遍历顺序 举例推导dp数组 debug:把dp数组打印出来 ","date":"2022-01-06 08:27:39","objectID":"/algorithm_dynamicprogramming/:1:0","tags":["data structure"],"title":"Algorithm_dynamicProgramming","uri":"/algorithm_dynamicprogramming/"},{"categories":["Coding"],"content":"斐波那契数 func fib(n int) int { if n \u003c 2 { return n } a, b, c := 0, 1, 0 for i := 1; i \u003c n; i++ { c = a + b a, b = b, c } return c } ","date":"2022-01-06 08:27:39","objectID":"/algorithm_dynamicprogramming/:2:0","tags":["data structure"],"title":"Algorithm_dynamicProgramming","uri":"/algorithm_dynamicprogramming/"},{"categories":["Coding"],"content":"爬楼梯 假设你正在爬楼梯。需要 n 阶你才能到达楼顶。 每次你可以爬 1 或 2 个台阶。你有多少种不同的方法可以爬到楼顶呢？ 注意：给定 n 是一个正整数。 示例 1： 输入： 2 输出： 2 解释： 有两种方法可以爬到楼顶。 1 阶 + 1 阶 2 阶 func climbStairs(n int) int { if n==1{ return 1 } dp:=make([]int,n+1) dp[1]=1 dp[2]=2 for i:=3;i\u003c=n;i++{ dp[i]=dp[i-1]+dp[i-2] } return dp[n] } ","date":"2022-01-06 08:27:39","objectID":"/algorithm_dynamicprogramming/:3:0","tags":["data structure"],"title":"Algorithm_dynamicProgramming","uri":"/algorithm_dynamicprogramming/"},{"categories":["Coding"],"content":"使用最小花费爬楼梯 数组的每个下标作为一个阶梯，第 i 个阶梯对应着一个非负数的体力花费值 cost[i]（下标从 0 开始）。 每当你爬上一个阶梯你都要花费对应的体力值，一旦支付了相应的体力值，你就可以选择向上爬一个阶梯或者爬两个阶梯。 请你找出达到楼层顶部的最低花费。在开始时，你可以选择从下标为 0 或 1 的元素作为初始阶梯。 示例 1： 输入：cost = [10, 15, 20] 输出：15 解释：最低花费是从 cost[1] 开始，然后走两步即可到阶梯顶，一共花费 15 func minCostClimbingStairs(cost []int) int { dp := make([]int, len(cost)) dp[0], dp[1] = cost[0], cost[1] for i := 2; i \u003c len(cost); i++ { dp[i] = min(dp[i-1], dp[i-2]) + cost[i] } return min(dp[len(cost)-1], dp[len(cost)-2]) } func min(a, b int) int { if a \u003c b { return a } return b } ","date":"2022-01-06 08:27:39","objectID":"/algorithm_dynamicprogramming/:3:1","tags":["data structure"],"title":"Algorithm_dynamicProgramming","uri":"/algorithm_dynamicprogramming/"},{"categories":["Coding"],"content":"不同路径 ","date":"2022-01-06 08:27:39","objectID":"/algorithm_dynamicprogramming/:4:0","tags":["data structure"],"title":"Algorithm_dynamicProgramming","uri":"/algorithm_dynamicprogramming/"},{"categories":["Coding"],"content":"I 一个机器人位于一个 m x n 网格的左上角 （起始点在下图中标记为 “Start” ）。 机器人每次只能向下或者向右移动一步。机器人试图达到网格的右下角（在下图中标记为 “Finish” ）。 问总共有多少条不同的路径？ func uniquePaths(m int, n int) int { dp := make([][]int, m) for i := range dp { dp[i] = make([]int, n) dp[i][0] = 1 } for j := 0; j \u003c n; j++ { dp[0][j] = 1 } for i := 1; i \u003c m; i++ { for j := 1; j \u003c n; j++ { dp[i][j] = dp[i-1][j] + dp[i][j-1] } } return dp[m-1][n-1] } ","date":"2022-01-06 08:27:39","objectID":"/algorithm_dynamicprogramming/:4:1","tags":["data structure"],"title":"Algorithm_dynamicProgramming","uri":"/algorithm_dynamicprogramming/"},{"categories":["Coding"],"content":"II 一个机器人位于一个 m x n 网格的左上角 （起始点在下图中标记为“Start” ）。 机器人每次只能向下或者向右移动一步。机器人试图达到网格的右下角（在下图中标记为“Finish”）。 现在考虑网格中有障碍物。那么从左上角到右下角将会有多少条不同的路径？ func uniquePathsWithObstacles(obstacleGrid [][]int) int { m,n:= len(obstacleGrid),len(obstacleGrid[0]) // 定义一个dp数组 dp := make([][]int,m) for i,_ := range dp { dp[i] = make([]int,n) } // 初始化 for i:=0;i\u003cm;i++ { // 如果是障碍物, 后面的就都是0, 不用循环了 if obstacleGrid[i][0] == 1 { break } dp[i][0]=1 } for i:=0;i\u003cn;i++ { if obstacleGrid[0][i] == 1 { break } dp[0][i]=1 } // dp数组推导过程 for i:=1;i\u003cm;i++ { for j:=1;j\u003cn;j++ { // 如果obstacleGrid[i][j]这个点是障碍物, 那么我们的dp[i][j]保持为0 if obstacleGrid[i][j] != 1 { // 否则我们需要计算当前点可以到达的路径数 dp[i][j] = dp[i-1][j]+dp[i][j-1] } } } // debug遍历dp //for i,_ := range dp { // for j,_ := range dp[i] { // fmt.Printf(\"%.2v,\",dp[i][j]) // } // fmt.Println() //} return dp[m-1][n-1] } ","date":"2022-01-06 08:27:39","objectID":"/algorithm_dynamicprogramming/:4:2","tags":["data structure"],"title":"Algorithm_dynamicProgramming","uri":"/algorithm_dynamicprogramming/"},{"categories":["Coding"],"content":"整数拆分 给定一个正整数 n，将其拆分为至少两个正整数的和，并使这些整数的乘积最大化。 返回你可以获得的最大乘积。 func integerBreak(n int) int { /** 动态五部曲 1.确定dp下标及其含义 2.确定递推公式 3.确定dp初始化 4.确定遍历顺序 5.打印dp **/ dp:=make([]int,n+1) dp[1]=1 dp[2]=1 for i:=3;i\u003cn+1;i++{ for j:=1;j\u003ci-1;j++{ // i可以差分为i-j和j。由于需要最大值，故需要通过j遍历所有存在的值，取其中最大的值作为当前i的最大值，在求最大值的时候，一个是j与i-j相乘，一个是j与dp[i-j]. dp[i]=max(dp[i],max(j*(i-j),j*dp[i-j])) } } return dp[n] } func max(a,b int) int{ if a\u003eb{ return a } return b } ","date":"2022-01-06 08:27:39","objectID":"/algorithm_dynamicprogramming/:5:0","tags":["data structure"],"title":"Algorithm_dynamicProgramming","uri":"/algorithm_dynamicprogramming/"},{"categories":["Coding"],"content":"不同的二叉搜索树 给定一个整数 n，求以 1 … n 为节点组成的二叉搜索树有多少种？ func numTrees(n int)int{ dp:=make([]int,n+1) dp[0]=1 for i:=1;i\u003c=n;i++{ for j:=1;j\u003c=i;j++{ dp[i]+=dp[j-1]*dp[i-j] } } return dp[n] } ","date":"2022-01-06 08:27:39","objectID":"/algorithm_dynamicprogramming/:6:0","tags":["data structure"],"title":"Algorithm_dynamicProgramming","uri":"/algorithm_dynamicprogramming/"},{"categories":["Coding"],"content":"0-1背包理论基础 ","date":"2022-01-06 08:27:39","objectID":"/algorithm_dynamicprogramming/:7:0","tags":["data structure"],"title":"Algorithm_dynamicProgramming","uri":"/algorithm_dynamicprogramming/"},{"categories":["Coding"],"content":"I 暴力的解法是指数级别的时间复杂度。进而才需要动态规划的解法来进行优化 代码随想录详解 func test_2_wei_bag_problem1(weight, value []int, bagweight int) int { // 定义dp数组 dp := make([][]int, len(weight)) for i, _ := range dp { dp[i] = make([]int, bagweight+1) } // 初始化 for j := bagweight; j \u003e= weight[0]; j-- { dp[0][j] = dp[0][j-weight[0]] + value[0] } // 递推公式 for i := 1; i \u003c len(weight); i++ { //正序,也可以倒序 for j := weight[i];j\u003c= bagweight ; j++ { dp[i][j] = max(dp[i-1][j], dp[i-1][j-weight[i]]+value[i]) } } return dp[len(weight)-1][bagweight] } func max(a,b int) int { if a \u003e b { return a } return b } func main() { weight := []int{1,3,4} value := []int{15,20,30} test_2_wei_bag_problem1(weight,value,4) } ","date":"2022-01-06 08:27:39","objectID":"/algorithm_dynamicprogramming/:7:1","tags":["data structure"],"title":"Algorithm_dynamicProgramming","uri":"/algorithm_dynamicprogramming/"},{"categories":["Coding"],"content":"II 代码随想录详解 func test_1_wei_bag_problem(weight, value []int, bagWeight int) int { // 定义 and 初始化 dp := make([]int,bagWeight+1) // 递推顺序 for i := 0 ;i \u003c len(weight) ; i++ { // 这里必须倒序,区别二维,因为二维dp保存了i的状态 for j:= bagWeight; j \u003e= weight[i] ; j-- { // 递推公式 dp[j] = max(dp[j], dp[j-weight[i]]+value[i]) } } //fmt.Println(dp) return dp[bagWeight] } func max(a,b int) int { if a \u003e b { return a } return b } func main() { weight := []int{1,3,4} value := []int{15,20,30} test_1_wei_bag_problem(weight,value,4) } ","date":"2022-01-06 08:27:39","objectID":"/algorithm_dynamicprogramming/:7:2","tags":["data structure"],"title":"Algorithm_dynamicProgramming","uri":"/algorithm_dynamicprogramming/"},{"categories":["Coding"],"content":"分割等和子集 给定一个只包含正整数的非空数组。是否可以将这个数组分割成两个子集，使得两个子集的元素和相等。 注意: 每个数组中的元素不会超过 100 数组的大小不会超过 200 示例 1: 输入: [1, 5, 11, 5] 输出: true 解释: 数组可以分割成 [1, 5, 5] 和 [11]. 示例 2: 输入: [1, 2, 3, 5] 输出: false 解释: 数组不能分割成两个元素和相等的子集. // 分割等和子集 动态规划 // 时间复杂度O(n^2) 空间复杂度O(n) func canPartition(nums []int) bool { sum := 0 for _, num := range nums { sum += num } // 如果 nums 的总和为奇数则不可能平分成两个子集 if sum % 2 == 1 { return false } target := sum / 2 dp := make([]int, target + 1) for _, num := range nums { for j := target; j \u003e= num; j-- { if dp[j] \u003c dp[j - num] + num { dp[j] = dp[j - num] + num } } } return dp[target] == target } ","date":"2022-01-06 08:27:39","objectID":"/algorithm_dynamicprogramming/:8:0","tags":["data structure"],"title":"Algorithm_dynamicProgramming","uri":"/algorithm_dynamicprogramming/"},{"categories":["Coding"],"content":"最后一块石头的重量II 有一堆石头，每块石头的重量都是正整数。 每一回合，从中选出任意两块石头，然后将它们一起粉碎。假设石头的重量分别为 x 和 y，且 x \u003c= y。那么粉碎的可能结果如下： 如果 x == y，那么两块石头都会被完全粉碎； 如果 x != y，那么重量为 x 的石头将会完全粉碎，而重量为 y 的石头新重量为 y-x。 最后，最多只会剩下一块石头。返回此石头最小的可能重量。如果没有石头剩下，就返回 0。 示例： 输入：[2,7,4,1,8,1] 输出：1 解释： 组合 2 和 4，得到 2，所以数组转化为 [2,7,1,8,1]， 组合 7 和 8，得到 1，所以数组转化为 [2,1,1,1]， 组合 2 和 1，得到 1，所以数组转化为 [1,1,1]， 组合 1 和 1，得到 0，所以数组转化为 [1]，这就是最优值。 func lastStoneWeightII(stones []int) int { // 15001 = 30 * 1000 /2 +1 dp := make([]int, 15001) // 求target sum := 0 for _, v := range stones { sum += v } target := sum / 2 // 遍历顺序 for i := 0; i \u003c len(stones); i++ { for j := target; j \u003e= stones[i]; j-- { // 推导公式 dp[j] = max(dp[j], dp[j-stones[i]]+stones[i]) } } return sum - 2 * dp[target] } func max(a, b int) int { if a \u003e b { return a } return b } ","date":"2022-01-06 08:27:39","objectID":"/algorithm_dynamicprogramming/:9:0","tags":["data structure"],"title":"Algorithm_dynamicProgramming","uri":"/algorithm_dynamicprogramming/"},{"categories":["Coding"],"content":"目标和 给定一个非负整数数组，a1, a2, …, an, 和一个目标数，S。现在你有两个符号 + 和 -。对于数组中的任意一个整数，你都可以从 + 或 -中选择一个符号添加在前面。 返回可以使最终数组和为目标数 S 的所有添加符号的方法数。 示例： 输入：nums: [1, 1, 1, 1, 1], S: 3 输出：5 解释： -1+1+1+1+1 = 3 +1-1+1+1+1 = 3 +1+1-1+1+1 = 3 +1+1+1-1+1 = 3 +1+1+1+1-1 = 3 一共有5种方法让最终目标和为3。 可回溯可dp func findTargetSumWays(nums []int, target int) int { sum := 0 for _, v := range nums { sum += v } if target \u003e sum { return 0 } if (sum+target)%2 == 1 { return 0 } // 计算背包大小 bag := (sum + target) / 2 // 定义dp数组 dp := make([]int, bag+1) // 初始化 dp[0] = 1 // 遍历顺序 for i := 0; i \u003c len(nums); i++ { for j := bag; j \u003e= nums[i]; j-- { //推导公式 dp[j] += dp[j-nums[i]] //fmt.Println(dp) } } return dp[bag] } ","date":"2022-01-06 08:27:39","objectID":"/algorithm_dynamicprogramming/:10:0","tags":["data structure"],"title":"Algorithm_dynamicProgramming","uri":"/algorithm_dynamicprogramming/"},{"categories":["Coding"],"content":"一和零 给你一个二进制字符串数组 strs 和两个整数 m 和 n 。 请你找出并返回 strs 的最大子集的大小，该子集中 最多 有 m 个 0 和 n 个 1 。 如果 x 的所有元素也是 y 的元素，集合 x 是集合 y 的 子集 。 示例 1： 输入：strs = [“10”, “0001”, “111001”, “1”, “0”], m = 5, n = 3 输出：4 解释：最多有 5 个 0 和 3 个 1 的最大子集是 {“10”,“0001”,“1”,“0”} ，因此答案是 4 。 其他满足题意但较小的子集包括 {“0001”,“1”} 和 {“10”,“1”,“0”} 。{“111001”} 不满足题意，因为它含 4 个 1 ，大于 n 的值 3 。 func findMaxForm(strs []string, m int, n int) int { // 定义数组 dp := make([][]int, m+1) for i,_ := range dp { dp[i] = make([]int, n+1 ) } // 遍历 for i:=0;i\u003clen(strs);i++ { zeroNum,oneNum := 0 , 0 //计算0,1 个数 //或者直接strings.Count(strs[i],\"0\") for _,v := range strs[i] { if v == '0' { zeroNum++ } } oneNum = len(strs[i])-zeroNum // 从后往前 遍历背包容量 for j:= m ; j \u003e= zeroNum;j-- { for k:=n ; k \u003e= oneNum;k-- { // 推导公式 dp[j][k] = max(dp[j][k],dp[j-zeroNum][k-oneNum]+1) } } //fmt.Println(dp) } return dp[m][n] } func max(a,b int) int { if a \u003e b { return a } return b } ","date":"2022-01-06 08:27:39","objectID":"/algorithm_dynamicprogramming/:11:0","tags":["data structure"],"title":"Algorithm_dynamicProgramming","uri":"/algorithm_dynamicprogramming/"},{"categories":["Coding"],"content":"完全背包理论基础 每件物品都有无限个（也就是可以放入背包多次） // test_CompletePack1 先遍历物品, 在遍历背包 func test_CompletePack1(weight, value []int, bagWeight int) int { // 定义dp数组 和初始化 dp := make([]int, bagWeight+1) // 遍历顺序 for i := 0; i \u003c len(weight); i++ { // 正序会多次添加 value[i] for j := weight[i]; j \u003c= bagWeight; j++ { // 推导公式 dp[j] = max(dp[j], dp[j-weight[i]]+value[i]) // debug //fmt.Println(dp) } } return dp[bagWeight] } // test_CompletePack2 先遍历背包, 在遍历物品 func test_CompletePack2(weight, value []int, bagWeight int) int { // 定义dp数组 和初始化 dp := make([]int, bagWeight+1) // 遍历顺序 // j从0 开始 for j := 0; j \u003c= bagWeight; j++ { for i := 0; i \u003c len(weight); i++ { if j \u003e= weight[i] { // 推导公式 dp[j] = max(dp[j], dp[j-weight[i]]+value[i]) } // debug //fmt.Println(dp) } } return dp[bagWeight] } func max(a, b int) int { if a \u003e b { return a } return b } func main() { weight := []int{1, 3, 4} price := []int{15, 20, 30} fmt.Println(test_CompletePack1(weight, price, 4)) fmt.Println(test_CompletePack2(weight, price, 4)) } ","date":"2022-01-06 08:27:39","objectID":"/algorithm_dynamicprogramming/:12:0","tags":["data structure"],"title":"Algorithm_dynamicProgramming","uri":"/algorithm_dynamicprogramming/"},{"categories":["Coding"],"content":"零钱兑换II 给定不同面额的硬币和一个总金额。写出函数来计算可以凑成总金额的硬币组合数。假设每一种面额的硬币有无限个。 示例 1: 输入: amount = 5, coins = [1, 2, 5] 输出: 4 解释: 有四种方式可以凑成总金额: 5=5 5=2+2+1 5=2+1+1+1 5=1+1+1+1+1 func change(amount int, coins []int) int { // 定义dp数组 dp := make([]int, amount+1) // 初始化,0大小的背包, 当然是不装任何东西了, 就是1种方法 dp[0] = 1 // 遍历顺序 // 遍历物品 for i := 0 ;i \u003c len(coins);i++ { // 遍历背包 for j:= coins[i] ; j \u003c= amount ;j++ { // 推导公式 dp[j] += dp[j-coins[i]] } } return dp[amount] } ","date":"2022-01-06 08:27:39","objectID":"/algorithm_dynamicprogramming/:13:0","tags":["data structure"],"title":"Algorithm_dynamicProgramming","uri":"/algorithm_dynamicprogramming/"},{"categories":["Coding"],"content":"组合总和（IV） 给定一个由正整数组成且不存在重复数字的数组，找出和为给定目标正整数的组合的个数。 示例: nums = [1, 2, 3] target = 4 所有可能的组合为： (1, 1, 1, 1) (1, 1, 2) (1, 2, 1) (1, 3) (2, 1, 1) (2, 2) (3, 1) 请注意，顺序不同的序列被视作不同的组合。 因此输出为 7 func combinationSum4(nums []int, target int) int { //定义dp数组 dp := make([]int, target+1) // 初始化 dp[0] = 1 // 遍历顺序, 先遍历背包,再循环遍历物品 for j:=0;j\u003c=target;j++ { for i:=0 ;i \u003c len(nums);i++ { if j \u003e= nums[i] { dp[j] += dp[j-nums[i]] } } } return dp[target] } ","date":"2022-01-06 08:27:39","objectID":"/algorithm_dynamicprogramming/:14:0","tags":["data structure"],"title":"Algorithm_dynamicProgramming","uri":"/algorithm_dynamicprogramming/"},{"categories":["Coding"],"content":"爬楼梯（进阶） 假设你正在爬楼梯。需要 n 阶你才能到达楼顶。 每次你可以爬 1 或 2 个台阶。你有多少种不同的方法可以爬到楼顶呢？ 注意：给定 n 是一个正整数。 示例 1： 输入： 2 输出： 2 解释： 有两种方法可以爬到楼顶。 1 阶 + 1 阶 2 阶 func climbStairs(n int) int { //定义 dp := make([]int, n+1) //初始化 dp[0] = 1 // 本题物品只有两个1,2 m := 2 // 遍历顺序 for j := 1; j \u003c= n; j++ { //先遍历背包 for i := 1; i \u003c= m; i++ { //再遍历物品 if j \u003e= i { dp[j] += dp[j-i] } //fmt.Println(dp) } } return dp[n] } ","date":"2022-01-06 08:27:39","objectID":"/algorithm_dynamicprogramming/:15:0","tags":["data structure"],"title":"Algorithm_dynamicProgramming","uri":"/algorithm_dynamicprogramming/"},{"categories":["Coding"],"content":"零钱兑换 给定不同面额的硬币 coins 和一个总金额 amount。编写一个函数来计算可以凑成总金额所需的最少的硬币个数。如果没有任何一种硬币组合能组成总金额，返回 -1。 你可以认为每种硬币的数量是无限的。 示例 1： 输入：coins = [1, 2, 5], amount = 11 输出：3 解释：11 = 5 + 5 + 1 // 版本一, 先遍历物品,再遍历背包 func coinChange1(coins []int, amount int) int { dp := make([]int, amount+1) // 初始化dp[0] dp[0] = 0 // 初始化为math.MaxInt32 for j := 1; j \u003c= amount; j++ { dp[j] = math.MaxInt32 } // 遍历物品 for i := 0; i \u003c len(coins); i++ { // 遍历背包 for j := coins[i]; j \u003c= amount; j++ { if dp[j-coins[i]] != math.MaxInt32 { // 推导公式 dp[j] = min(dp[j], dp[j-coins[i]]+1) //fmt.Println(dp,j,i) } } } // 没找到能装满背包的, 就返回-1 if dp[amount] == math.MaxInt32 { return -1 } return dp[amount] } // 版本二,先遍历背包,再遍历物品 func coinChange2(coins []int, amount int) int { dp := make([]int, amount+1) // 初始化dp[0] dp[0] = 0 // 遍历背包,从1开始 for j := 1; j \u003c= amount; j++ { // 初始化为math.MaxInt32 dp[j] = math.MaxInt32 // 遍历物品 for i := 0; i \u003c len(coins); i++ { if j \u003e= coins[i] \u0026\u0026 dp[j-coins[i]] != math.MaxInt32 { // 推导公式 dp[j] = min(dp[j], dp[j-coins[i]]+1) //fmt.Println(dp) } } } // 没找到能装满背包的, 就返回-1 if dp[amount] == math.MaxInt32 { return -1 } return dp[amount] } func min(a, b int) int { if a \u003c b { return a } return b } ","date":"2022-01-06 08:27:39","objectID":"/algorithm_dynamicprogramming/:16:0","tags":["data structure"],"title":"Algorithm_dynamicProgramming","uri":"/algorithm_dynamicprogramming/"},{"categories":["Coding"],"content":"完全平方数 给定正整数 n，找到若干个完全平方数（比如 1, 4, 9, 16, …）使得它们的和等于 n。你需要让组成和的完全平方数的个数最少。 给你一个整数 n ，返回和为 n 的完全平方数的 最少数量 。 完全平方数 是一个整数，其值等于另一个整数的平方；换句话说，其值等于一个整数自乘的积。例如，1、4、9 和 16 都是完全平方数，而 3 和 11 不是。 示例 1： 输入：n = 12 输出：3 解释：12 = 4 + 4 + 4 // 版本一,先遍历物品, 再遍历背包 func numSquares1(n int) int { //定义 dp := make([]int, n+1) // 初始化 dp[0] = 0 for i := 1; i \u003c= n; i++ { dp[i] = math.MaxInt32 } // 遍历物品 for i := 1; i \u003c= n; i++ { // 遍历背包 for j := i*i; j \u003c= n; j++ { dp[j] = min(dp[j], dp[j-i*i]+1) } } return dp[n] } // 版本二,先遍历背包, 再遍历物品 func numSquares2(n int) int { //定义 dp := make([]int, n+1) // 初始化 dp[0] = 0 // 遍历背包 for j := 1; j \u003c= n; j++ { //初始化 dp[j] = math.MaxInt32 // 遍历物品 for i := 1; i \u003c= n; i++ { if j \u003e= i*i { dp[j] = min(dp[j], dp[j-i*i]+1) } } } return dp[n] } func min(a, b int) int { if a \u003c b { return a } return b } ","date":"2022-01-06 08:27:39","objectID":"/algorithm_dynamicprogramming/:17:0","tags":["data structure"],"title":"Algorithm_dynamicProgramming","uri":"/algorithm_dynamicprogramming/"},{"categories":["Coding"],"content":"单词拆分 给定一个非空字符串 s 和一个包含非空单词的列表 wordDict，判定 s 是否可以被空格拆分为一个或多个在字典中出现的单词。 说明： 拆分时可以重复使用字典中的单词。 你可以假设字典中没有重复的单词。 示例 1： 输入: s = “leetcode”, wordDict = [“leet”, “code”] 输出: true 解释: 返回 true 因为 “leetcode” 可以被拆分成 “leet code”。 func wordBreak(s string,wordDict []string) bool { wordDictSet:=make(map[string]bool) for _,w:=range wordDict{ wordDictSet[w]=true } dp:=make([]bool,len(s)+1) dp[0]=true for i:=1;i\u003c=len(s);i++{ for j:=0;j\u003ci;j++{ if dp[j]\u0026\u0026 wordDictSet[s[j:i]]{ dp[i]=true break } } } return dp[len(s)] } ","date":"2022-01-06 08:27:39","objectID":"/algorithm_dynamicprogramming/:18:0","tags":["data structure"],"title":"Algorithm_dynamicProgramming","uri":"/algorithm_dynamicprogramming/"},{"categories":["Coding"],"content":"打家劫舍 ","date":"2022-01-06 08:27:39","objectID":"/algorithm_dynamicprogramming/:19:0","tags":["data structure"],"title":"Algorithm_dynamicProgramming","uri":"/algorithm_dynamicprogramming/"},{"categories":["Coding"],"content":"I 你是一个专业的小偷，计划偷窃沿街的房屋。每间房内都藏有一定的现金，影响你偷窃的唯一制约因素就是相邻的房屋装有相互连通的防盗系统，如果两间相邻的房屋在同一晚上被小偷闯入，系统会自动报警。 给定一个代表每个房屋存放金额的非负整数数组，计算你 不触动警报装置的情况下 ，一夜之内能够偷窃到的最高金额。 示例 1： 输入：[1,2,3,1] 输出：4 解释：偷窃 1 号房屋 (金额 = 1) ，然后偷窃 3 号房屋 (金额 = 3)。 偷窃到的最高金额 = 1 + 3 = 4 。 func rob(nums []int) int { if len(nums)\u003c1{ return 0 } if len(nums)==1{ return nums[0] } if len(nums)==2{ return max(nums[0],nums[1]) } dp :=make([]int,len(nums)) dp[0]=nums[0] dp[1]=max(nums[0],nums[1]) for i:=2;i\u003clen(nums);i++{ dp[i]=max(dp[i-2]+nums[i],dp[i-1]) } return dp[len(dp)-1] } func max(a, b int) int { if a\u003eb{ return a } return b } ","date":"2022-01-06 08:27:39","objectID":"/algorithm_dynamicprogramming/:19:1","tags":["data structure"],"title":"Algorithm_dynamicProgramming","uri":"/algorithm_dynamicprogramming/"},{"categories":["Coding"],"content":"II 你是一个专业的小偷，计划偷窃沿街的房屋，每间房内都藏有一定的现金。这个地方所有的房屋都 围成一圈 ，这意味着第一个房屋和最后一个房屋是紧挨着的。同时，相邻的房屋装有相互连通的防盗系统，如果两间相邻的房屋在同一晚上被小偷闯入，系统会自动报警 。 给定一个代表每个房屋存放金额的非负整数数组，计算你 在不触动警报装置的情况下 ，能够偷窃到的最高金额。 示例 1： 输入：nums = [2,3,2] 输出：3 解释：你不能先偷窃 1 号房屋（金额 = 2），然后偷窃 3 号房屋（金额 = 2）, 因为他们是相邻的。 // 打家劫舍Ⅱ 动态规划 // 时间复杂度O(n) 空间复杂度O(n) func rob(nums []int) int { if len(nums) == 1 { return nums[0] } if len(nums) == 2 { return max(nums[0], nums[1]) } result1 := robRange(nums, 0) result2 := robRange(nums, 1) return max(result1, result2) } // 偷盗指定的范围 func robRange(nums []int, start int) int { dp := make([]int, len(nums)) dp[1] = nums[start] for i := 2; i \u003c len(nums); i++ { dp[i] = max(dp[i - 2] + nums[i - 1 + start], dp[i - 1]) } return dp[len(nums) - 1] } func max(a, b int) int { if a \u003e b { return a } return b } ","date":"2022-01-06 08:27:39","objectID":"/algorithm_dynamicprogramming/:19:2","tags":["data structure"],"title":"Algorithm_dynamicProgramming","uri":"/algorithm_dynamicprogramming/"},{"categories":["Coding"],"content":"III 在上次打劫完一条街道之后和一圈房屋后，小偷又发现了一个新的可行窃的地区。这个地区只有一个入口，我们称之为“根”。 除了“根”之外，每栋房子有且只有一个“父“房子与之相连。一番侦察之后，聪明的小偷意识到“这个地方的所有房屋的排列类似于一棵二叉树”。 如果两个直接相连的房子在同一天晚上被打劫，房屋将自动报警。 计算在不触动警报的情况下，小偷一晚能够盗取的最高金额。 动态规划 func rob(root *TreeNode) int { res := robTree(root) return max(res[0], res[1]) } func max(a, b int) int { if a \u003e b { return a } return b } func robTree(cur *TreeNode) []int { if cur == nil { return []int{0, 0} } // 后序遍历 left := robTree(cur.Left) right := robTree(cur.Right) // 考虑去偷当前的屋子 robCur := cur.Val + left[0] + right[0] // 考虑不去偷当前的屋子 notRobCur := max(left[0], left[1]) + max(right[0], right[1]) // **注意**顺序：0:不偷，1:去偷 return []int{notRobCur, robCur} } ","date":"2022-01-06 08:27:39","objectID":"/algorithm_dynamicprogramming/:19:3","tags":["data structure"],"title":"Algorithm_dynamicProgramming","uri":"/algorithm_dynamicprogramming/"},{"categories":["Coding"],"content":"买卖股票的最佳时机 ","date":"2022-01-06 08:27:39","objectID":"/algorithm_dynamicprogramming/:20:0","tags":["data structure"],"title":"Algorithm_dynamicProgramming","uri":"/algorithm_dynamicprogramming/"},{"categories":["Coding"],"content":"I 给定一个数组 prices ，它的第 i 个元素 prices[i] 表示一支给定股票第 i 天的价格。 你只能选择 某一天 买入这只股票，并选择在 未来的某一个不同的日子 卖出该股票。设计一个算法来计算你所能获取的最大利润。 返回你可以从这笔交易中获取的最大利润。如果你不能获取任何利润，返回 0 。 示例 1： 输入：[7,1,5,3,6,4] 输出：5 解释：在第 2 天（股票价格 = 1）的时候买入，在第 5 天（股票价格 = 6）的时候卖出，最大利润 = 6-1 = 5 。**注意**利润不能是 7-1 = 6, 因为卖出价格需要大于买入价格；同时，你不能在买入前卖出股票。 func maxProfit(prices []int) int { length:=len(prices) if length==0{return 0} dp:=make([][]int,length) for i:=0;i\u003clength;i++{ dp[i]=make([]int,2) } dp[0][0]=-prices[0] dp[0][1]=0 for i:=1;i\u003clength;i++{ dp[i][0]=max(dp[i-1][0],-prices[i]) dp[i][1]=max(dp[i-1][1],dp[i-1][0]+prices[i]) } return dp[length-1][1] } func max(a,b int)int { if a\u003eb{ return a } return b } ","date":"2022-01-06 08:27:39","objectID":"/algorithm_dynamicprogramming/:20:1","tags":["data structure"],"title":"Algorithm_dynamicProgramming","uri":"/algorithm_dynamicprogramming/"},{"categories":["Coding"],"content":"II 给定一个数组，它的第 i 个元素是一支给定股票第 i 天的价格。 设计一个算法来计算你所能获取的最大利润。你可以尽可能地完成更多的交易（多次买卖一支股票）。 注意：你不能同时参与多笔交易（你必须在再次购买前出售掉之前的股票）。 示例 1: 输入: [7,1,5,3,6,4] 输出: 7 解释: 在第 2 天（股票价格 = 1）的时候买入，在第 3 天（股票价格 = 5）的时候卖出, 这笔交易所能获得利润 = 5-1 = 4。随后，在第 4 天（股票价格 = 3）的时候买入，在第 5 天（股票价格 = 6）的时候卖出, 这笔交易所能获得利润 = 6-3 = 3 。 // 买卖股票的最佳时机Ⅱ 动态规划 // 时间复杂度：O(n) 空间复杂度：O(n) func maxProfit(prices []int) int { dp := make([][]int, len(prices)) status := make([]int, len(prices) * 2) for i := range dp { dp[i] = status[:2] status = status[2:] } dp[0][0] = -prices[0] for i := 1; i \u003c len(prices); i++ { dp[i][0] = max(dp[i - 1][0], dp[i - 1][1] - prices[i]) dp[i][1] = max(dp[i - 1][1], dp[i - 1][0] + prices[i]) } return dp[len(prices) - 1][1] } func max(a, b int) int { if a \u003e b { return a } return b } func maxProfit(prices []int) int { //创建数组 dp:=make([][]int,len(prices)) for i:=0;i\u003clen(prices);i++{ dp[i]=make([]int,2) } dp[0][0]=-prices[0] dp[0][1]=0 for i:=1;i\u003clen(prices);i++{ dp[i][0]=max(dp[i-1][0],dp[i-1][1]-prices[i]) dp[i][1]=max(dp[i-1][1],dp[i-1][0]+prices[i]) } return dp[len(prices)-1][1] } func max(a,b int)int{ if a\u003cb{ return b } return a } ","date":"2022-01-06 08:27:39","objectID":"/algorithm_dynamicprogramming/:20:2","tags":["data structure"],"title":"Algorithm_dynamicProgramming","uri":"/algorithm_dynamicprogramming/"},{"categories":["Coding"],"content":"III 给定一个数组，它的第 i 个元素是一支给定的股票在第 i 天的价格。 设计一个算法来计算你所能获取的最大利润。你最多可以完成 两笔 交易。 注意：你不能同时参与多笔交易（你必须在再次购买前出售掉之前的股票）。 示例 1: 输入：prices = [3,3,5,0,0,3,1,4] 输出：6 解释：在第 4 天（股票价格 = 0）的时候买入，在第 6 天（股票价格 = 3）的时候卖出，这笔交易所能获得利润 = 3-0 = 3 。随后，在第 7 天（股票价格 = 1）的时候买入，在第 8 天 （股票价格 = 4）的时候卖出，这笔交易所能获得利润 = 4-1 = 3。 func maxProfit(prices []int) int { dp:=make([][]int,len(prices)) for i:=0;i\u003clen(prices);i++{ dp[i]=make([]int,5) } dp[0][0]=0 dp[0][1]=-prices[0] dp[0][2]=0 dp[0][3]=-prices[0] dp[0][4]=0 for i:=1;i\u003clen(prices);i++{ dp[i][0]=dp[i-1][0] dp[i][1]=max(dp[i-1][1],dp[i-1][0]-prices[i]) dp[i][2]=max(dp[i-1][2],dp[i-1][1]+prices[i]) dp[i][3]=max(dp[i-1][3],dp[i-1][2]-prices[i]) dp[i][4]=max(dp[i-1][4],dp[i-1][3]+prices[i]) } return dp[len(prices)-1][4] } func max(a,b int)int{ if a\u003eb{ return a } return b } ","date":"2022-01-06 08:27:39","objectID":"/algorithm_dynamicprogramming/:20:3","tags":["data structure"],"title":"Algorithm_dynamicProgramming","uri":"/algorithm_dynamicprogramming/"},{"categories":["Coding"],"content":"IV 给定一个整数数组 prices ，它的第 i 个元素 prices[i] 是一支给定的股票在第 i 天的价格。 设计一个算法来计算你所能获取的最大利润。你最多可以完成 k 笔交易。 注意：你不能同时参与多笔交易（你必须在再次购买前出售掉之前的股票）。 示例 1： 输入：k = 2, prices = [2,4,1] 输出：2 解释：在第 1 天 (股票价格 = 2) 的时候买入，在第 2 天 (股票价格 = 4) 的时候卖出，这笔交易所能获得利润 = 4-2 = 2。 版本一： // 买卖股票的最佳时机IV 动态规划 // 时间复杂度O(kn) 空间复杂度O(kn) func maxProfit(k int, prices []int) int { if k == 0 || len(prices) == 0 { return 0 } dp := make([][]int, len(prices)) status := make([]int, (2 * k + 1) * len(prices)) for i := range dp { dp[i] = status[:2 * k + 1] status = status[2 * k + 1:] } for j := 1; j \u003c 2 * k; j += 2 { dp[0][j] = -prices[0] } for i := 1; i \u003c len(prices); i++ { for j := 0; j \u003c 2 * k; j += 2 { dp[i][j + 1] = max(dp[i - 1][j + 1], dp[i - 1][j] - prices[i]) dp[i][j + 2] = max(dp[i - 1][j + 2], dp[i - 1][j + 1] + prices[i]) } } return dp[len(prices) - 1][2 * k] } func max(a, b int) int { if a \u003e b { return a } return b } func maxProfit(k int, prices []int) int { if len(prices)==0{ return 0 } dp:=make([][]int,len(prices)) for i:=0;i\u003clen(prices);i++{ dp[i]=make([]int,2*k+1) } for i:=1;i\u003clen(dp[0]);i++{ if i%2!=0{ dp[0][i]=-prices[0] } } for i:=1;i\u003clen(prices);i++{ dp[i][0]=dp[i-1][0] for j:=1;j\u003clen(dp[0]);j++{ if j%2!=0{ dp[i][j]=max(dp[i-1][j],dp[i-1][j-1]-prices[i]) }else { dp[i][j]=max(dp[i-1][j],dp[i-1][j-1]+prices[i]) } } } return dp[len(prices)-1][2*k] } func max(a,b int)int{ if a\u003eb{ return a } return b } ","date":"2022-01-06 08:27:39","objectID":"/algorithm_dynamicprogramming/:20:4","tags":["data structure"],"title":"Algorithm_dynamicProgramming","uri":"/algorithm_dynamicprogramming/"},{"categories":["Coding"],"content":"最佳买卖股票时机含冷冻期 给定一个整数数组，其中第 i 个元素代表了第 i 天的股票价格 。 设计一个算法计算出最大利润。在满足以下约束条件下，你可以尽可能地完成更多的交易（多次买卖一支股票）: 你不能同时参与多笔交易（你必须在再次购买前出售掉之前的股票）。 卖出股票后，你无法在第二天买入股票 (即冷冻期为 1 天)。 示例: 输入: [1,2,3,0,2] 输出: 3 解释: 对应的交易状态为: [买入, 卖出, 冷冻期, 买入, 卖出] // 最佳买卖股票时机含冷冻期 动态规划 // 时间复杂度O(n) 空间复杂度O(n) func maxProfit(prices []int) int { n := len(prices) if n \u003c 2 { return 0 } dp := make([][]int, n) status := make([]int, n * 4) for i := range dp { dp[i] = status[:4] status = status[4:] } dp[0][0] = -prices[0] for i := 1; i \u003c n; i++ { dp[i][0] = max(dp[i - 1][0], max(dp[i - 1][1] - prices[i], dp[i - 1][3] - prices[i])) dp[i][1] = max(dp[i - 1][1], dp[i - 1][3]) dp[i][2] = dp[i - 1][0] + prices[i] dp[i][3] = dp[i - 1][2] } return max(dp[n - 1][1], max(dp[n - 1][2], dp[n - 1][3])) } func max(a, b int) int { if a \u003e b { return a } return b } ","date":"2022-01-06 08:27:39","objectID":"/algorithm_dynamicprogramming/:20:5","tags":["data structure"],"title":"Algorithm_dynamicProgramming","uri":"/algorithm_dynamicprogramming/"},{"categories":["Coding"],"content":"最佳买卖股票时机含手续费 给定一个整数数组 prices，其中第 i 个元素代表了第 i 天的股票价格 ；非负整数 fee 代表了交易股票的手续费用。 你可以无限次地完成交易，但是你每笔交易都需要付手续费。如果你已经购买了一个股票，在卖出它之前你就不能再继续购买股票了。 返回获得利润的最大值。 注意：这里的一笔交易指买入持有并卖出股票的整个过程，每笔交易你只需要为支付一次手续费。 示例 1: 输入: prices = [1, 3, 2, 8, 4, 9], fee = 2 输出: 8 解释: 能够达到的最大利润: 在此处买入 prices[0] = 1 在此处卖出 prices[3] = 8 在此处买入 prices[4] = 4 在此处卖出 prices[5] = 9 总利润: ((8 - 1) - 2) + ((9 - 4) - 2) = 8. // 买卖股票的最佳时机含手续费 动态规划 // 时间复杂度O(n) 空间复杂度O(n) func maxProfit(prices []int, fee int) int { n := len(prices) dp := make([][2]int, n) dp[0][0] = -prices[0] for i := 1; i \u003c n; i++ { dp[i][1] = max(dp[i-1][1], dp[i-1][0] + prices[i] - fee) dp[i][0] = max(dp[i-1][0], dp[i-1][1] - prices[i]) } return dp[n-1][1] } func max(a, b int) int { if a \u003e b { return a } return b } ","date":"2022-01-06 08:27:39","objectID":"/algorithm_dynamicprogramming/:20:6","tags":["data structure"],"title":"Algorithm_dynamicProgramming","uri":"/algorithm_dynamicprogramming/"},{"categories":["Coding"],"content":"股票问题总结 代码随想录 ","date":"2022-01-06 08:27:39","objectID":"/algorithm_dynamicprogramming/:21:0","tags":["data structure"],"title":"Algorithm_dynamicProgramming","uri":"/algorithm_dynamicprogramming/"},{"categories":["Coding"],"content":"最长上升子序列 给你一个整数数组 nums ，找到其中最长严格递增子序列的长度。 子序列是由数组派生而来的序列，删除（或不删除）数组中的元素而不改变其余元素的顺序。例如，[3,6,2,7] 是数组 [0,3,1,6,2,2,7] 的子序列。 示例 1： 输入：nums = [10,9,2,5,3,7,101,18] 输出：4 解释：最长递增子序列是 [2,3,7,101]，因此长度为 4 。 func lengthOfLIS(nums []int ) int { dp := []int{} for _, num := range nums { if len(dp) ==0 || dp[len(dp) - 1] \u003c num { dp = append(dp, num) } else { l, r := 0, len(dp) - 1 pos := r for l \u003c= r { mid := (l + r) \u003e\u003e 1 if dp[mid] \u003e= num { pos = mid; r = mid - 1 } else { l = mid + 1 } } dp[pos] = num }//二分查找 } return len(dp) } ","date":"2022-01-06 08:27:39","objectID":"/algorithm_dynamicprogramming/:22:0","tags":["data structure"],"title":"Algorithm_dynamicProgramming","uri":"/algorithm_dynamicprogramming/"},{"categories":["Coding"],"content":"最长连续递增序列 给定一个未经排序的整数数组，找到最长且 连续递增的子序列，并返回该序列的长度。 连续递增的子序列 可以由两个下标 l 和 r（l \u003c r）确定，如果对于每个 l \u003c= i \u003c r，都有 nums[i] \u003c nums[i + 1] ，那么子序列 [nums[l], nums[l + 1], …, nums[r - 1], nums[r]] 就是连续递增子序列。 示例 1： 输入：nums = [1,3,5,4,7] 输出：3 解释：最长连续递增序列是 [1,3,5], 长度为3。 尽管 [1,3,5,7] 也是升序的子序列, 但它不是连续的，因为 5 和 7 在原数组里被 4 隔开。 动态规划： class Solution: def findLengthOfLCIS(self, nums: List[int]) -\u003e int: if len(nums) == 0: return 0 result = 1 dp = [1] * len(nums) for i in range(len(nums)-1): if nums[i+1] \u003e nums[i]: #连续记录 dp[i+1] = dp[i] + 1 result = max(result, dp[i+1]) return result 贪心法： class Solution: def findLengthOfLCIS(self, nums: List[int]) -\u003e int: if len(nums) == 0: return 0 result = 1 #连续子序列最少也是1 count = 1 for i in range(len(nums)-1): if nums[i+1] \u003e nums[i]: #连续记录 count += 1 else: #不连续，count从头开始 count = 1 result = max(result, count) return result ","date":"2022-01-06 08:27:39","objectID":"/algorithm_dynamicprogramming/:23:0","tags":["data structure"],"title":"Algorithm_dynamicProgramming","uri":"/algorithm_dynamicprogramming/"},{"categories":["Coding"],"content":"最长重复子数组 给两个整数数组 A 和 B ，返回两个数组中公共的、长度最长的子数组的长度。 示例： 输入： A: [1,2,3,2,1] B: [3,2,1,4,7] 输出：3 解释： 长度最长的公共子数组是 [3, 2, 1] 。 func findLength(A []int, B []int) int { m, n := len(A), len(B) res := 0 dp := make([][]int, m+1) for i := 0; i \u003c= m; i++ { dp[i] = make([]int, n+1) } for i := 1; i \u003c= m; i++ { for j := 1; j \u003c= n; j++ { if A[i-1] == B[j-1] { dp[i][j] = dp[i-1][j-1] + 1 } if dp[i][j] \u003e res { res = dp[i][j] } } } return res } ","date":"2022-01-06 08:27:39","objectID":"/algorithm_dynamicprogramming/:24:0","tags":["data structure"],"title":"Algorithm_dynamicProgramming","uri":"/algorithm_dynamicprogramming/"},{"categories":["Coding"],"content":"最长公共子序列 给定两个字符串 text1 和 text2，返回这两个字符串的最长公共子序列的长度。 一个字符串的 子序列 是指这样一个新的字符串：它是由原字符串在不改变字符的相对顺序的情况下删除某些字符（也可以不删除任何字符）后组成的新字符串。 例如，“ace” 是 “abcde” 的子序列，但 “aec” 不是 “abcde” 的子序列。两个字符串的「公共子序列」是这两个字符串所共同拥有的子序列。 若这两个字符串没有公共子序列，则返回 0。 示例 1: 输入：text1 = \"abcde\", text2 = \"ace\" 输出：3 解释：最长公共子序列是 \"ace\"，它的长度为 3。 func longestCommonSubsequence(text1 string, text2 string) int { t1 := len(text1) t2 := len(text2) dp:=make([][]int,t1+1) for i:=range dp{ dp[i]=make([]int,t2+1) } for i := 1; i \u003c= t1; i++ { for j := 1; j \u003c=t2; j++ { if text1[i-1]==text2[j-1]{ dp[i][j]=dp[i-1][j-1]+1 }else{ dp[i][j]=max(dp[i-1][j],dp[i][j-1]) } } } return dp[t1][t2] } func max(a,b int)int { if a\u003eb{ return a } return b } ","date":"2022-01-06 08:27:39","objectID":"/algorithm_dynamicprogramming/:25:0","tags":["data structure"],"title":"Algorithm_dynamicProgramming","uri":"/algorithm_dynamicprogramming/"},{"categories":["Coding"],"content":"不相交的线 我们在两条独立的水平线上按给定的顺序写下 A 和 B 中的整数。 现在，我们可以绘制一些连接两个数字 A[i] 和 B[j] 的直线，只要 A[i] == B[j]，且我们绘制的直线不与任何其他连线（非水平线）相交。 以这种方法绘制线条，并返回我们可以绘制的最大连线数。 func maxUncrossedLines(A []int, B []int) int { m, n := len(A), len(B) dp := make([][]int, m+1) for i := range dp { dp[i] = make([]int, n+1) } for i := 1; i \u003c= len(A); i++ { for j := 1; j \u003c= len(B); j++ { if (A[i - 1] == B[j - 1]) { dp[i][j] = dp[i - 1][j - 1] + 1 } else { dp[i][j] = max(dp[i - 1][j], dp[i][j - 1]) } } } return dp[m][n] } func max(a, b int) int { if a \u003e b { return a } return b } ","date":"2022-01-06 08:27:39","objectID":"/algorithm_dynamicprogramming/:26:0","tags":["data structure"],"title":"Algorithm_dynamicProgramming","uri":"/algorithm_dynamicprogramming/"},{"categories":["Coding"],"content":"最大子序和 给定一个整数数组 nums ，找到一个具有最大和的连续子数组（子数组最少包含一个元素），返回其最大和。 示例: 输入: [-2,1,-3,4,-1,2,1,-5,4] 输出: 6 解释: 连续子数组 [4,-1,2,1] 的和最大，为 6 // solution // 1, dp // 2, 贪心 func maxSubArray(nums []int) int { n := len(nums) // 这里的dp[i] 表示，最大的连续子数组和，包含num[i] 元素 dp := make([]int,n) // 初始化，由于dp 状态转移方程依赖dp[0] dp[0] = nums[0] // 初始化最大的和 mx := nums[0] for i:=1;i\u003cn;i++ { // 这里的状态转移方程就是：求最大和 // 会面临2种情况，一个是带前面的和，一个是不带前面的和 dp[i] = max(dp[i-1]+nums[i],nums[i]) mx = max(mx,dp[i]) } return mx } func max(a,b int) int{ if a\u003eb { return a } return b } ","date":"2022-01-06 08:27:39","objectID":"/algorithm_dynamicprogramming/:27:0","tags":["data structure"],"title":"Algorithm_dynamicProgramming","uri":"/algorithm_dynamicprogramming/"},{"categories":["Coding"],"content":"判断子序列 给定字符串 s 和 t ，判断 s 是否为 t 的子序列。 字符串的一个子序列是原始字符串删除一些（也可以不删除）字符而不改变剩余字符相对位置形成的新字符串。（例如，“ace\"是\"abcde\"的一个子序列，而\"aec\"不是）。 示例 1： 输入：s = “abc”, t = “ahbgdc” 输出：true 示例 2： 输入：s = “axc”, t = “ahbgdc” 输出：false func isSubsequence(s string, t string) bool { dp := make([][]int,len(s)+1) for i:=0;i\u003clen(dp);i++{ dp[i] = make([]int,len(t)+1) } for i:=1;i\u003clen(dp);i++{ for j:=1;j\u003clen(dp[i]);j++{ if s[i-1] == t[j-1]{ dp[i][j] = dp[i-1][j-1] +1 }else{ dp[i][j] = dp[i][j-1] } } } return dp[len(s)][len(t)]==len(s) } ","date":"2022-01-06 08:27:39","objectID":"/algorithm_dynamicprogramming/:28:0","tags":["data structure"],"title":"Algorithm_dynamicProgramming","uri":"/algorithm_dynamicprogramming/"},{"categories":["Coding"],"content":"不同的子序列 给定一个字符串 s 和一个字符串 t ，计算在 s 的子序列中 t 出现的个数。 字符串的一个 子序列 是指，通过删除一些（也可以不删除）字符且不干扰剩余字符相对位置所组成的新字符串。（例如，“ACE” 是 “ABCDE” 的一个子序列，而 “AEC” 不是） 题目数据保证答案符合 32 位带符号整数范围。 func numDistinct(s string, t string) int { dp:= make([][]int,len(s)+1) for i:=0;i\u003clen(dp);i++{ dp[i] = make([]int,len(t)+1) } // 初始化 for i:=0;i\u003clen(dp);i++{ dp[i][0] = 1 } // dp[0][j] 为 0，默认值，因此不需要初始化 for i:=1;i\u003clen(dp);i++{ for j:=1;j\u003clen(dp[i]);j++{ if s[i-1] == t[j-1]{ dp[i][j] = dp[i-1][j-1] + dp[i-1][j] }else{ dp[i][j] = dp[i-1][j] } } } return dp[len(dp)-1][len(dp[0])-1] } ","date":"2022-01-06 08:27:39","objectID":"/algorithm_dynamicprogramming/:29:0","tags":["data structure"],"title":"Algorithm_dynamicProgramming","uri":"/algorithm_dynamicprogramming/"},{"categories":["Coding"],"content":"两个字符串的删除操作 给定两个单词 word1 和 word2，找到使得 word1 和 word2 相同所需的最小步数，每步可以删除任意一个字符串中的一个字符。 示例： 输入: \"sea\", \"eat\" 输出: 2 解释: 第一步将\"sea\"变为\"ea\"，第二步将\"eat\"变为\"ea\" ","date":"2022-01-06 08:27:39","objectID":"/algorithm_dynamicprogramming/:30:0","tags":["data structure"],"title":"Algorithm_dynamicProgramming","uri":"/algorithm_dynamicprogramming/"},{"categories":["Coding"],"content":"编辑距离 给你两个单词 word1 和 word2，请你计算出将 word1 转换成 word2 所使用的最少操作数 。 你可以对一个单词进行如下三种操作： 插入一个字符 删除一个字符 替换一个字符 示例 1： 输入：word1 = “horse”, word2 = “ros” 输出：3 解释： horse -\u003e rorse (将 ‘h’ 替换为 ‘r’) rorse -\u003e rose (删除 ‘r’) rose -\u003e ros (删除 ‘e’) func minDistance(word1 string, word2 string) int { m, n := len(word1), len(word2) dp := make([][]int, m+1) for i := range dp { dp[i] = make([]int, n+1) } for i := 0; i \u003c m+1; i++ { dp[i][0] = i // word1[i] 变成 word2[0], 删掉 word1[i], 需要 i 部操作 } for j := 0; j \u003c n+1; j++ { dp[0][j] = j // word1[0] 变成 word2[j], 插入 word1[j]，需要 j 部操作 } for i := 1; i \u003c m+1; i++ { for j := 1; j \u003c n+1; j++ { if word1[i-1] == word2[j-1] { dp[i][j] = dp[i-1][j-1] } else { // Min(插入，删除，替换) dp[i][j] = Min(dp[i][j-1], dp[i-1][j], dp[i-1][j-1]) + 1 } } } return dp[m][n] } func Min(args ...int) int { min := args[0] for _, item := range args { if item \u003c min { min = item } } return min } ","date":"2022-01-06 08:27:39","objectID":"/algorithm_dynamicprogramming/:31:0","tags":["data structure"],"title":"Algorithm_dynamicProgramming","uri":"/algorithm_dynamicprogramming/"},{"categories":["Coding"],"content":"编辑距离总结 代码随想录 ","date":"2022-01-06 08:27:39","objectID":"/algorithm_dynamicprogramming/:31:1","tags":["data structure"],"title":"Algorithm_dynamicProgramming","uri":"/algorithm_dynamicprogramming/"},{"categories":["Coding"],"content":"回文子串 给定一个字符串，你的任务是计算这个字符串中有多少个回文子串。 具有不同开始位置或结束位置的子串，即使是由相同的字符组成，也会被视作不同的子串。 示例 1： 输入：“abc” 输出：3 解释：三个回文子串: “a”, “b”, “c” func countSubstrings(s string) int { res:=0 dp:=make([][]bool,len(s)) for i:=0;i\u003clen(s);i++{ dp[i]=make([]bool,len(s)) } for i:=len(s)-1;i\u003e=0;i--{ for j:=i;j\u003clen(s);j++{ if s[i]==s[j]{ if j-i\u003c=1{ res++ dp[i][j]=true }else if dp[i+1][j-1]{ res++ dp[i][j]=true } } } } return res } ","date":"2022-01-06 08:27:39","objectID":"/algorithm_dynamicprogramming/:32:0","tags":["data structure"],"title":"Algorithm_dynamicProgramming","uri":"/algorithm_dynamicprogramming/"},{"categories":["Coding"],"content":"最长回文子序列 给定一个字符串 s ，找到其中最长的回文子序列，并返回该序列的长度。可以假设 s 的最大长度为 1000 。 示例 1: 输入: “bbbab” 输出: 4 一个可能的最长回文子序列为 “bbbb”。 示例 2: 输入:“cbbd” 输出: 2 一个可能的最长回文子序列为 “bb”。 func longestPalindromeSubseq(s string) int { lenth:=len(s) dp:=make([][]int,lenth) for i:=0;i\u003clenth;i++{ for j:=0;j\u003clenth;j++{ if dp[i]==nil{ dp[i]=make([]int,lenth) } if i==j{ dp[i][j]=1 } } } for i:=lenth-1;i\u003e=0;i--{ for j:=i+1;j\u003clenth;j++{ if s[i]==s[j]{ dp[i][j]=dp[i+1][j-1]+2 }else { dp[i][j]=max(dp[i+1][j],dp[i][j-1]) } } } return dp[0][lenth-1] } ","date":"2022-01-06 08:27:39","objectID":"/algorithm_dynamicprogramming/:33:0","tags":["data structure"],"title":"Algorithm_dynamicProgramming","uri":"/algorithm_dynamicprogramming/"},{"categories":["Coding"],"content":"DP总结 代码随想录 ","date":"2022-01-06 08:27:39","objectID":"/algorithm_dynamicprogramming/:34:0","tags":["data structure"],"title":"Algorithm_dynamicProgramming","uri":"/algorithm_dynamicprogramming/"},{"categories":["Coding"],"content":"开篇词｜为什么大厂都爱考动态规划？ 你好，我是卢誉声，很高兴能在这个专栏与你见面，和你一起搞定动态规划。开门见山，我先做一个自我介绍。最开始，我在思科系统（Cisco Systems）工作，曾参与设计和开发了下一代视频会议系统的核心数据交换服务。我的工作涵盖了协议栈开发、微服务设计、分布式系统编配以及弹性算法设计。这段经历让我形成了一个认知：算法对设计关键服务来说十分重要，它决定了系统的稳定性、弹性以及可扩展性。后来，我加入了 Autodesk，成为了一款三维设计旗舰软件的框架和平台软件工程师。负责开发了基于大规模结构化数据的高性能搜索引擎，首次将灵活的多线程和异步框架带入产品框架层面，在原有的底层内存模型上采用了改进后的检索引擎，相较于原有的搜索功能，实现了超过 300 倍的性能提升。除此之外，我还改进并维护了用于改进用户体验的数据处理系统，在平台框架层面的工作，让我积累了大量的工程实践经验。现在，我在 Autodesk 数据平台就职，负责设计和开发大规模数据的分析、丰富化以及流化分布式服务。我发现自己的职业发展一直围绕着数据在不断前进。基于此，我常说的一句话是：“数据即是正义”。那直到今天，我的态度依然没有变。数据为媒，算法为介，而在极其重要的算法中，动态规划其实占了很大的比重。 事实上，如果你平常关注大厂面试的话，你会发现，但凡是研发岗位，无论是招聘初级还是高级工程师，大厂都倾向于安排一轮或多轮专门的算法面试环节，而且在面试环节提出动态规划相关问题的这种趋势已经愈发明显。这是为什么呢？我来谈谈我的看法。 先说算法这件事吧。我想请你回想一下，当处理数据结构相关的问题时，你有没有这样的经历？你本能地到工具函数或者库函数中寻找有没有现成的工具。如果问题得到快速解决，它是不是迅速就成了过眼云烟？如果这个问题看起来比较棘手，它不是一个典型的算法问题，那么就寻求搜索引擎的帮助，或者干脆访问 Stack Overflow 这样的“智库”寻找前人留下的解决方案？虽然平时工作中表现优异，但当你想换工作参加大厂面试时，又发现自己难以解决面试官提出的算法问题，无从下手，面对白板“望洋兴叹”？ 相信我，你不是一个人！这种现象很普遍。其实，对于开发人员来说，算法和数据结构就是我们的基本功。我们常常自嘲软件研发人员的工作就是复制粘贴，搬砖就是日常工作的全部。但当公司或部门要求你去研究一个全新的技术，或者快速阅读一份开发多年且成熟的开源项目代码，并对其改造来服务于自己的产品功能时，你的压力会让你明白基本功到底有多重要！关于基本功这事儿，我要插个故事进来，再多说几句。我曾有幸与 C++ 之父 Bjarne Stroustrup 先生进行过面对面的交流。我问了他一个问题：“如今新生代技术人员倾向于学习 Java、Go 或 Python 这些更容易上手的编程语言，您是如何看待这个现象的？”Stroustrup 先生的回答大概是这样的：“如果一个人只了解一种编程语言，那么他不能称自己是专业人士，而从我的角度上看，将 C++ 作为基础，能让你深入洞察各种各样编程语言背后的思想和设计思路。” 我作为面试官曾接触过许多优秀的候选人，他们有着各种各样的背景，既有潜力又非常努力，但在面对算法问题和解决问题时没有太多思路，始终无法更上一层楼，十分遗憾。而动态规划恰恰是解决问题的重要方法论，面对很多数据处理的应用场景，它在降低时间复杂度上极具优势，因此成为了考察重点。不仅如此，动态规划问题还能很好地考察面试者的数学模型抽象能力和逻辑思维能力，可以反应个人在算法上的综合能力。所以我觉得，大厂之所以如此看中一个面试者的算法基础，特别是动态规划问题的解决能力，是因为他们更加看中一位面试者解决问题的思路与逻辑思维能力，而不只是工具与技能的熟练程度。 不同于普通算法，如排序或递归，动态规划从名字上看就显得很特别，有些“高端、大气、上档次”的味道在里面。但其实它离我们很近。我举个例子你就明白了，在云计算平台上一个解决方案的计算能力（容量）肯定是有限的，那么为了高效服务那些重要程度或优先级最高的客户，同时又不想浪费计算资源（说白了为了省钱），我们该怎么办？这个问题其实可以通过队列这样的分发方式来进行一个简单的编配。但是这不够好，如果我们能够事先知道一个计算任务的重要程度和所需的计算时长，就可以通过动态规划算法来进行预演算，从数学角度推导出一个严谨的编排结果，实现有限资源的最大化利用。 你看，似乎遥不可及的动态规划问题，其实就是求最优解问题，它无时无刻都在我们身边，总是戏剧般地提高了最优化问题的性能！这再一次凸显出大厂为何青睐于动态规划问题，而且成为了区别面试者的一个隐形门槛。甚至可以说，掌握动态规划思想，在工作面试、技术等级晋升上都扮演了核心角色。总之一句话，动归必学。 模块一：初识动态规划我会为你讲解复杂面试题的思考和解决方式。从贪心算法开始，一步步阐述动态规划的由来，并通过一个贯穿全篇的例子来展现动态规划的强大之处。学习和掌握这些经典的处理方法，能够为你后续掌握动态规划打下一个坚实基础。通过这部分内容，你会系统了解到动态规划问题的特点和解题经验。模块二：动态规划的套路我会为你讲解动态规划问题的解题框架和套路，你可以把这个套路理解成是解决动归问题的模板。在此模板的基础上，我会向你讲解面试真题，有针对性地套用解题框架。而应对面试题的纷繁复杂，我会为你进行有效的分类，并针对每一种动态规划问题进行深入而全面的讲解。通过这部分内容，你会快速掌握常见面试题的解题套路。模块三：举一反三，突破套路我会针对几种特别易考的动态规划面试题进行总结，帮助你攻破套路。并在这些高级话题的基础上，提出设计动态规划算法的关键问题。另外，还有刷题指南，所谓孰能生巧，必要的练习我们还是要的。通过这部分内容，你会快速掌握动态规划面试题的进阶法门。 ","date":"2022-01-06 08:27:39","objectID":"/algorithm_dynamicprogramming/:35:0","tags":["data structure"],"title":"Algorithm_dynamicProgramming","uri":"/algorithm_dynamicprogramming/"},{"categories":["Coding"],"content":"什么样的问题应该使用动态规划？ 动态规划问题的典型特点：求“最”优解问题（最大值和最小值）、求可行性（True 或 False）、求方案总数、数据结构不可排序（Unsortable）、算法不可使用交换（Non-swappable）。 数据不可排序（Unsortable） 假设我们有一个无序数列，希望求出这个数列中最大的两个数字之和。很多初学者刚刚学完动态规划会走火入魔到看到最优化问题就想用动态规划来求解，嗯，那么这样应该也是可以的吧……不，等等，这个问题不是简单做一个排序或者做一个遍历就可以求解出来了吗？ 数据不可交换（Non-swapable） 还有一类问题，可以归类到我们总结的几类问题里去，但是不存在动态规划要求的重叠子问题（比如经典的八皇后问题），那么这类问题就无法通过动态规划求解。这种情况需要避免被套进去。 ","date":"2022-01-06 08:27:39","objectID":"/algorithm_dynamicprogramming/:36:0","tags":["data structure"],"title":"Algorithm_dynamicProgramming","uri":"/algorithm_dynamicprogramming/"},{"categories":["Coding"],"content":"常见的动态规划面试题串烧 简单的路径规划、带障碍的路径规划、跳跃游戏（题目：给出一个非负整数数组 A，你最初定位在数组的第一个位置。数组中的每个元素代表你在那个位置可以跳跃的最大长度。判断你是否能到达数组的最后一个位置。） ","date":"2022-01-06 08:27:39","objectID":"/algorithm_dynamicprogramming/:37:0","tags":["data structure"],"title":"Algorithm_dynamicProgramming","uri":"/algorithm_dynamicprogramming/"},{"categories":["Coding"],"content":" 学习代码随想录笔记 贪心算法 ","date":"2022-01-06 08:23:51","objectID":"/algorithm_greedy/:0:0","tags":["data structure"],"title":"Algorithm_greedy","uri":"/algorithm_greedy/"},{"categories":["Coding"],"content":"理论基础 贪心算法一般分为如下四步： 将问题分解为若干个子问题 找出适合的贪心策略 求解每一个子问题的最优解 将局部最优解堆叠成全局最优解 ","date":"2022-01-06 08:23:51","objectID":"/algorithm_greedy/:1:0","tags":["data structure"],"title":"Algorithm_greedy","uri":"/algorithm_greedy/"},{"categories":["Coding"],"content":"分发饼干 假设你是一位很棒的家长，想要给你的孩子们一些小饼干。但是，每个孩子最多只能给一块饼干。 对每个孩子 i，都有一个胃口值 g[i]，这是能让孩子们满足胃口的饼干的最小尺寸；并且每块饼干 j，都有一个尺寸 s[j] 。如果 s[j] \u003e= g[i]，我们可以将这个饼干 j 分配给孩子 i ，这个孩子会得到满足。你的目标是尽可能满足越多数量的孩子，并输出这个最大数值。 示例 1: 输入: g = [1,2,3], s = [1,1] 输出: 1 解释:你有三个孩子和两块小饼干，3个孩子的胃口值分别是：1,2,3。虽然你有两块小饼干，由于他们的尺寸都是1，你只能让胃口值是1的孩子满足。所以你应该输出1。 //排序后，局部最优 func findContentChildren(g []int, s []int) int { sort.Ints(g) sort.Ints(s) // 从小到大 child := 0 for sIdx := 0; child \u003c len(g) \u0026\u0026 sIdx \u003c len(s); sIdx++ { if s[sIdx] \u003e= g[child] {//如果饼干的大小大于或等于孩子的为空则给与，否则不给予，继续寻找选一个饼干是否符合 child++ } } return child } ","date":"2022-01-06 08:23:51","objectID":"/algorithm_greedy/:2:0","tags":["data structure"],"title":"Algorithm_greedy","uri":"/algorithm_greedy/"},{"categories":["Coding"],"content":"摆动序列 如果连续数字之间的差严格地在正数和负数之间交替，则数字序列称为摆动序列。第一个差（如果存在的话）可能是正数或负数。少于两个元素的序列也是摆动序列。 例如， [1,7,4,9,2,5] 是一个摆动序列，因为差值 (6,-3,5,-7,3) 是正负交替出现的。相反, [1,4,7,2,5] 和 [1,7,4,5,5] 不是摆动序列，第一个序列是因为它的前两个差值都是正数，第二个序列是因为它的最后一个差值为零。 给定一个整数序列，返回作为摆动序列的最长子序列的长度。 通过从原始序列中删除一些（也可以不删除）元素来获得子序列，剩下的元素保持其原始顺序。 示例 1: 输入: [1,7,4,9,2,5] 输出: 6 解释: 整个序列均为摆动序列。 示例 2: 输入: [1,17,5,10,13,15,10,5,16,8] 输出: 7 解释: 这个序列包含几个长度为 7 摆动序列，其中一个可为[1,17,10,13,10,16,8]。 贪心或者dp func wiggleMaxLength(nums []int) int { var count,preDiff,curDiff int count=1 if len(nums)\u003c2{ return count } for i:=0;i\u003clen(nums)-1;i++{ curDiff=nums[i+1]-nums[i] //如果有正有负则更新下标值||或者只有前一个元素为0（针对两个不等元素的序列也视作摆动序列，且摆动长度为2） if (curDiff \u003e 0 \u0026\u0026 preDiff \u003c= 0) || (preDiff \u003e= 0 \u0026\u0026 curDiff \u003c 0){ preDiff=curDiff count++ } } return count } ","date":"2022-01-06 08:23:51","objectID":"/algorithm_greedy/:3:0","tags":["data structure"],"title":"Algorithm_greedy","uri":"/algorithm_greedy/"},{"categories":["Coding"],"content":"最大子序和 给定一个整数数组 nums ，找到一个具有最大和的连续子数组（子数组最少包含一个元素），返回其最大和。 示例: 输入: [-2,1,-3,4,-1,2,1,-5,4] 输出: 6 解释: 连续子数组 [4,-1,2,1] 的和最大，为 6 贪心或者dp func maxSubArray(nums []int) int { maxSum := nums[0] for i := 1; i \u003c len(nums); i++ { if nums[i] + nums[i-1] \u003e nums[i] { nums[i] += nums[i-1] } if nums[i] \u003e maxSum { maxSum = nums[i] } } return maxSum } ","date":"2022-01-06 08:23:51","objectID":"/algorithm_greedy/:4:0","tags":["data structure"],"title":"Algorithm_greedy","uri":"/algorithm_greedy/"},{"categories":["Coding"],"content":"买卖股票的最佳时机II 给定一个数组，它的第 i 个元素是一支给定股票第 i 天的价格。 设计一个算法来计算你所能获取的最大利润。你可以尽可能地完成更多的交易（多次买卖一支股票）。 注意：你不能同时参与多笔交易（你必须在再次购买前出售掉之前的股票）。 示例 1: 输入: [7,1,5,3,6,4] 输出: 7 解释: 在第 2 天（股票价格 = 1）的时候买入，在第 3 天（股票价格 = 5）的时候卖出, 这笔交易所能获得利润 = 5-1 = 4。随后，在第 4 天（股票价格 = 3）的时候买入，在第 5 天（股票价格 = 6）的时候卖出, 这笔交易所能获得利润 = 6-3 = 3 。 示例 2: 输入: [1,2,3,4,5] 输出: 4 解释: 在第 1 天（股票价格 = 1）的时候买入，在第 5 天 （股票价格 = 5）的时候卖出, 这笔交易所能获得利润 = 5-1 = 4 。**注意**你不能在第 1 天和第 2 天接连购买股票，之后再将它们卖出。因为这样属于同时参与了多笔交易，你必须在再次购买前出售掉之前的股票。 示例 3: 输入: [7,6,4,3,1] 输出: 0 解释: 在这种情况下, 没有交易完成, 所以最大利润为 0 贪心或者dp //贪心算法 func maxProfit(prices []int) int { var sum int for i := 1; i \u003c len(prices); i++ { // 累加每次大于0的交易 if prices[i]-prices[i-1] \u003e 0 { sum += prices[i]-prices[i-1] } } return sum } //确定售卖点 func maxProfit(prices []int) int { var result,buy int prices=append(prices,0)//在price末尾加个0，防止price一直递增 /** 思路：检查后一个元素是否大于当前元素，如果小于，则表明这是一个售卖点，当前元素的值减去购买时候的值 如果不小于，说明后面有更好的售卖点， **/ for i:=0;i\u003clen(prices)-1;i++{ if prices[i]\u003eprices[i+1]{ result+=prices[i]-prices[buy] buy=i+1 }else if prices[buy]\u003eprices[i]{//更改最低购买点 buy=i } } return result } ","date":"2022-01-06 08:23:51","objectID":"/algorithm_greedy/:5:0","tags":["data structure"],"title":"Algorithm_greedy","uri":"/algorithm_greedy/"},{"categories":["Coding"],"content":"跳跃游戏 ","date":"2022-01-06 08:23:51","objectID":"/algorithm_greedy/:6:0","tags":["data structure"],"title":"Algorithm_greedy","uri":"/algorithm_greedy/"},{"categories":["Coding"],"content":"I 给定一个非负整数数组，你最初位于数组的第一个位置。 数组中的每个元素代表你在该位置可以跳跃的最大长度。 判断你是否能够到达最后一个位置。 func canJUmp(nums []int) bool { if len(nums)\u003c=1{ return true } dp:=make([]bool,len(nums)) dp[0]=true for i:=1;i\u003clen(nums);i++{ for j:=i-1;j\u003e=0;j--{ if dp[j]\u0026\u0026nums[j]+j\u003e=i{ dp[i]=true break } } } return dp[len(nums)-1] } ","date":"2022-01-06 08:23:51","objectID":"/algorithm_greedy/:6:1","tags":["data structure"],"title":"Algorithm_greedy","uri":"/algorithm_greedy/"},{"categories":["Coding"],"content":"II 给定一个非负整数数组，你最初位于数组的第一个位置。 数组中的每个元素代表你在该位置可以跳跃的最大长度。 你的目标是使用最少的跳跃次数到达数组的最后一个位置。 func jump(nums []int) int { dp:=make([]int ,len(nums)) dp[0]=0 for i:=1;i\u003clen(nums);i++{ dp[i]=i for j:=0;j\u003ci;j++{ if nums[j]+j\u003ei{ dp[i]=min(dp[j]+1,dp[i]) } } } return dp[len(nums)-1] } ","date":"2022-01-06 08:23:51","objectID":"/algorithm_greedy/:6:2","tags":["data structure"],"title":"Algorithm_greedy","uri":"/algorithm_greedy/"},{"categories":["Coding"],"content":"K次取反后最大化的数组和 给定一个整数数组 A，我们只能用以下方法修改该数组：我们选择某个索引 i 并将 A[i] 替换为 -A[i]，然后总共重复这个过程 K 次。（我们可以多次选择同一个索引 i。） 以这种方式修改数组后，返回数组可能的最大和。 func largestSumAfterKNegations(nums []int, K int) int { sort.Slice(nums, func(i, j int) bool { return math.Abs(float64(nums[i])) \u003e math.Abs(float64(nums[j])) }) for i := 0; i \u003c len(nums); i++ { if K \u003e 0 \u0026\u0026 nums[i] \u003c 0 { nums[i] = -nums[i] K-- } } if K%2 == 1 { nums[len(nums)-1] = -nums[len(nums)-1] } result := 0 for i := 0; i \u003c len(nums); i++ { result += nums[i] } return result } ","date":"2022-01-06 08:23:51","objectID":"/algorithm_greedy/:7:0","tags":["data structure"],"title":"Algorithm_greedy","uri":"/algorithm_greedy/"},{"categories":["Coding"],"content":"加油站 在一条环路上有 N 个加油站，其中第 i 个加油站有汽油 gas[i] 升。 你有一辆油箱容量无限的的汽车，从第 i 个加油站开往第 i+1 个加油站需要消耗汽油 cost[i] 升。你从其中的一个加油站出发，开始时油箱为空。 如果你可以绕环路行驶一周，则返回出发时加油站的编号，否则返回 -1。 如果题目有解，该答案即为唯一答案。 输入数组均为非空数组，且长度相同。 输入数组中的元素均为非负数。 func canCompleteCircuit(gas []int, cost []int) int { curSum := 0 totalSum := 0 start := 0 for i := 0; i \u003c len(gas); i++ { curSum += gas[i] - cost[i] totalSum += gas[i] - cost[i] if curSum \u003c 0 { start = i+1 curSum = 0 } } if totalSum \u003c 0 { return -1 } return start } ","date":"2022-01-06 08:23:51","objectID":"/algorithm_greedy/:8:0","tags":["data structure"],"title":"Algorithm_greedy","uri":"/algorithm_greedy/"},{"categories":["Coding"],"content":"分发糖果 老师想给孩子们分发糖果，有 N 个孩子站成了一条直线，老师会根据每个孩子的表现，预先给他们评分。 你需要按照以下要求，帮助老师给这些孩子分发糖果： 每个孩子至少分配到 1 个糖果。 相邻的孩子中，评分高的孩子必须获得更多的糖果。 那么这样下来，老师至少需要准备多少颗糖果呢？ 示例 1: 输入: [1,0,2] 输出: 5 解释: 你可以分别给这三个孩子分发 2、1、2 颗糖果。 示例 2: 输入: [1,2,2] 输出: 4 解释: 你可以分别给这三个孩子分发 1、2、1 颗糖果。第三个孩子只得到 1 颗糖果，这已满足上述两个条件。 func candy(ratings []int) int { /**先确定一边，再确定另外一边 1.先从左到右，当右边的大于左边的就加1 2.再从右到左，当左边的大于右边的就再加1 **/ need:=make([]int,len(ratings)) sum:=0 //初始化(每个人至少一个糖果) for i:=0;i\u003clen(ratings);i++{ need[i]=1 } //1.先从左到右，当右边的大于左边的就加1 for i:=0;i\u003clen(ratings)-1;i++{ if ratings[i]\u003cratings[i+1]{ need[i+1]=need[i]+1 } } //2.再从右到左，当左边的大于右边的就右边加1，但要花费糖果最少，所以需要做下判断 for i:=len(ratings)-1;i\u003e0;i--{ if ratings[i-1]\u003eratings[i]{ need[i-1]=findMax(need[i-1],need[i]+1) } } //计算总共糖果 for i:=0;i\u003clen(ratings);i++{ sum+=need[i] } return sum } func findMax(num1 int ,num2 int) int{ if num1\u003enum2{ return num1 } return num2 } ","date":"2022-01-06 08:23:51","objectID":"/algorithm_greedy/:9:0","tags":["data structure"],"title":"Algorithm_greedy","uri":"/algorithm_greedy/"},{"categories":["Coding"],"content":"柠檬水找零 在柠檬水摊上，每一杯柠檬水的售价为 5 美元。 顾客排队购买你的产品，（按账单 bills 支付的顺序）一次购买一杯。 每位顾客只买一杯柠檬水，然后向你付 5 美元、10 美元或 20 美元。你必须给每个顾客正确找零，也就是说净交易是每位顾客向你支付 5 美元。 注意，一开始你手头没有任何零钱。 如果你能给每位顾客正确找零，返回 true ，否则返回 false 。 示例 1： 输入：[5,5,5,10,20] 输出：true 解释： 前 3 位顾客那里，我们按顺序收取 3 张 5 美元的钞票。 第 4 位顾客那里，我们收取一张 10 美元的钞票，并返还 5 美元。 第 5 位顾客那里，我们找还一张 10 美元的钞票和一张 5 美元的钞票。 由于所有客户都得到了正确的找零，所以我们输出 true。 示例 2： 输入：[5,5,10] 输出：true func lemonadeChange(bills []int) bool { //left表示还剩多少 下标0位5元的个数 ，下标1为10元的个数 left:=[2]int{0,0} //第一个元素不为5，直接退出 if bills[0]!=5{ return false } for i:=0;i\u003clen(bills);i++{ //先统计5元和10元的个数 if bills[i]==5{ left[0]+=1 } if bills[i]==10{ left[1]+=1 } //接着处理找零的 tmp:=bills[i]-5 if tmp==5{ if left[0]\u003e0{ left[0]-=1 }else { return false } } if tmp==15{ if left[1]\u003e0\u0026\u0026left[0]\u003e0{ left[0]-=1 left[1]-=1 }else if left[1]==0\u0026\u0026left[0]\u003e2{ left[0]-=3 }else{ return false } } } return true } ","date":"2022-01-06 08:23:51","objectID":"/algorithm_greedy/:10:0","tags":["data structure"],"title":"Algorithm_greedy","uri":"/algorithm_greedy/"},{"categories":["Coding"],"content":"根据身高重建队列 假设有打乱顺序的一群人站成一个队列，数组 people 表示队列中一些人的属性（不一定按顺序）。每个 people[i] = [hi, ki] 表示第 i 个人的身高为 hi ，前面 正好 有 ki 个身高大于或等于 hi 的人。 请你重新构造并返回输入数组 people 所表示的队列。返回的队列应该格式化为数组 queue ，其中 queue[j] = [hj, kj] 是队列中第 j 个人的属性（queue[0] 是排在队列前面的人）。 示例 1： 输入：people = [[7,0],[4,4],[7,1],[5,0],[6,1],[5,2]] 输出：[[5,0],[7,0],[5,2],[6,1],[4,4],[7,1]] 解释： 编号为 0 的人身高为 5 ，没有身高更高或者相同的人排在他前面。 编号为 1 的人身高为 7 ，没有身高更高或者相同的人排在他前面。 编号为 2 的人身高为 5 ，有 2 个身高更高或者相同的人排在他前面，即编号为 0 和 1 的人。 编号为 3 的人身高为 6 ，有 1 个身高更高或者相同的人排在他前面，即编号为 1 的人。 编号为 4 的人身高为 4 ，有 4 个身高更高或者相同的人排在他前面，即编号为 0、1、2、3 的人。 编号为 5 的人身高为 7 ，有 1 个身高更高或者相同的人排在他前面，即编号为 1 的人。 因此 [[5,0],[7,0],[5,2],[6,1],[4,4],[7,1]] 是重新构造后的队列。 func reconstructQueue(people [][]int) [][]int { //先将身高从大到小排序，确定最大个子的相对位置 sort.Slice(people,func(i,j int)bool{ if people[i][0]==people[j][0]{ return people[i][1]\u003cpeople[j][1]//这个才是当身高相同时，将K按照从小到大排序 } return people[i][0]\u003epeople[j][0]//这个只是确保身高按照由大到小的顺序来排，并不确定K是按照从小到大排序的 }) //再按照K进行插入排序，优先插入K小的 result := make([][]int, 0) for _, info := range people { result = append(result, info) copy(result[info[1] +1:], result[info[1]:])//将插入位置之后的元素后移动一位（意思是腾出空间） result[info[1]] = info//将插入元素位置插入元素 } return result } //链表法 func reconstructQueue(people [][]int) [][]int { sort.Slice(people,func (i,j int) bool { if people[i][0]==people[j][0]{ return people[i][1]\u003cpeople[j][1]//当身高相同时，将K按照从小到大排序 } //先将身高从大到小排序，确定最大个子的相对位置 return people[i][0]\u003epeople[j][0] }) l:=list.New()//创建链表 for i:=0;i\u003clen(people);i++{ position:=people[i][1] mark:=l.PushBack(people[i])//插入元素 e:=l.Front() for position!=0{//获取相对位置 position-- e=e.Next() } l.MoveBefore(mark,e)//移动位置 } res:=[][]int{} for e:=l.Front();e!=nil;e=e.Next(){ res=append(res,e.Value.([]int)) } return res } ","date":"2022-01-06 08:23:51","objectID":"/algorithm_greedy/:11:0","tags":["data structure"],"title":"Algorithm_greedy","uri":"/algorithm_greedy/"},{"categories":["Coding"],"content":"用最少数量的箭引爆气球 在二维空间中有许多球形的气球。对于每个气球，提供的输入是水平方向上，气球直径的开始和结束坐标。由于它是水平的，所以纵坐标并不重要，因此只要知道开始和结束的横坐标就足够了。开始坐标总是小于结束坐标。 一支弓箭可以沿着 x 轴从不同点完全垂直地射出。在坐标 x 处射出一支箭，若有一个气球的直径的开始和结束坐标为 xstart，xend， 且满足 xstart ≤ x ≤ xend，则该气球会被引爆。可以射出的弓箭的数量没有限制。 弓箭一旦被射出之后，可以无限地前进。我们想找到使得所有气球全部被引爆，所需的弓箭的最小数量。 给你一个数组 points ，其中 points [i] = [xstart,xend] ，返回引爆所有气球所必须射出的最小弓箭数。 示例 1： 输入：points = [[10,16],[2,8],[1,6],[7,12]] 输出：2 解释：对于该样例，x = 6 可以射爆 [2,8],[1,6] 两个气球，以及 x = 11 射爆另外两个气球 示例 2： 输入：points = [[1,2],[3,4],[5,6],[7,8]] 输出：4 示例 3： 输入：points = [[1,2],[2,3],[3,4],[4,5]] 输出：2 示例 4： 输入：points = [[1,2]] 输出：1 示例 5： 输入：points = [[2,3],[2,3]] 输出：1 提示： 0 \u003c= points.length \u003c= 10^4 points[i].length == 2 -2^31 \u003c= xstart \u003c xend \u003c= 2^31 - 1 func findMinArrowShots(points [][]int) int { var res int =1//弓箭数 //先按照第一位排序 sort.Slice(points,func (i,j int) bool{ return points[i][0]\u003cpoints[j][0] }) for i:=1;i\u003clen(points);i++{ if points[i-1][1]\u003cpoints[i][0]{//如果前一位的右边界小于后一位的左边界，则一定不重合 res++ }else{ points[i][1] = min(points[i - 1][1], points[i][1]); // 更新重叠气球最小右边界,覆盖该位置的值，留到下一步使用 } } return res } func min(a,b int) int{ if a\u003eb{ return b } return a } ","date":"2022-01-06 08:23:51","objectID":"/algorithm_greedy/:12:0","tags":["data structure"],"title":"Algorithm_greedy","uri":"/algorithm_greedy/"},{"categories":["Coding"],"content":"无重叠区间 给定一个区间的集合，找到需要移除区间的最小数量，使剩余区间互不重叠。 注意: 可以认为区间的终点总是大于它的起点。 区间 [1,2] 和 [2,3] 的边界相互“接触”，但没有相互重叠。 示例 1: 输入: [ [1,2], [2,3], [3,4], [1,3] ] 输出: 1 解释: 移除 [1,3] 后，剩下的区间没有重叠。 func eraseOverlapIntervals(intervals [][]int) int { var flag int //先排序 sort.Slice(intervals,func(i,j int)bool{ return intervals[i][0]\u003cintervals[j][0] }) fmt.Println(intervals) for i:=1;i\u003clen(intervals);i++{ if intervals[i-1][1]\u003eintervals[i][0]{ flag++ intervals[i][1]=min(intervals[i-1][1],intervals[i][1])//由于是先排序的，所以，第一位是递增顺序，故只需要将临近两个元素的第二个值最小值更新到该元素的第二个值即可作之后的判断 } } return flag } func min(a,b int)int{ if a\u003eb{ return b } return a } ","date":"2022-01-06 08:23:51","objectID":"/algorithm_greedy/:13:0","tags":["data structure"],"title":"Algorithm_greedy","uri":"/algorithm_greedy/"},{"categories":["Coding"],"content":"划分字母区间 字符串 S 由小写字母组成。我们要把这个字符串划分为尽可能多的片段，同一字母最多出现在一个片段中。返回一个表示每个字符串片段的长度的列表。 示例： 输入：S = “ababcbacadefegdehijhklij” 输出：[9,7,8] 解释： 划分结果为 “ababcbaca”, “defegde”, “hijhklij”。 每个字母最多出现在一个片段中。 像 “ababcbacadefegde”, “hijhklij” 的划分是错误的，因为划分的片段数较少。 func partitionLabels(s string) []int { var res []int; var marks [26]int; size, left, right := len(s), 0, 0; for i := 0; i \u003c size; i++ { marks[s[i] - 'a'] = i; } for i := 0; i \u003c size; i++ { right = max(right, marks[s[i] - 'a']); if i == right { res = append(res, right - left + 1); left = i + 1; } } return res; } func max(a, b int) int { if a \u003c b { a = b; } return a; } ","date":"2022-01-06 08:23:51","objectID":"/algorithm_greedy/:14:0","tags":["data structure"],"title":"Algorithm_greedy","uri":"/algorithm_greedy/"},{"categories":["Coding"],"content":"合并区间 给出一个区间的集合，请合并所有重叠的区间。 func merge(intervals [][]int) [][]int { //先从小到大排序 sort.Slice(intervals,func(i,j int)bool{ return intervals[i][0]\u003cintervals[j][0] }) //再弄重复的 for i:=0;i\u003clen(intervals)-1;i++{ if intervals[i][1]\u003e=intervals[i+1][0]{ intervals[i][1]=max(intervals[i][1],intervals[i+1][1])//赋值最大值 intervals=append(intervals[:i+1],intervals[i+2:]...) i-- } } return intervals } func max(a,b int)int{ if a\u003eb{ return a } return b } ","date":"2022-01-06 08:23:51","objectID":"/algorithm_greedy/:15:0","tags":["data structure"],"title":"Algorithm_greedy","uri":"/algorithm_greedy/"},{"categories":["Coding"],"content":"单调递增的数字 给定一个非负整数 N，找出小于或等于 N 的最大的整数，同时这个整数需要满足其各个位数上的数字是单调递增 示例： 输入: N = 332 输出: 299 func monotoneIncreasingDigits(N int) int { s := strconv.Itoa(N)//将数字转为字符串，方便使用下标 ss := []byte(s)//将字符串转为byte数组，方便更改。 n := len(ss) if n \u003c= 1 { return N } for i:=n-1 ; i\u003e0; i-- { if ss[i-1] \u003e ss[i] {//前一个大于后一位,前一位减1，后面的全部置为9 ss[i-1] -= 1 for j := i ; j \u003c n; j++ {//后面的全部置为9 ss[j] = '9' } } } res, _ := strconv.Atoi(string(ss)) return res } ","date":"2022-01-06 08:23:51","objectID":"/algorithm_greedy/:16:0","tags":["data structure"],"title":"Algorithm_greedy","uri":"/algorithm_greedy/"},{"categories":["Coding"],"content":"买卖股票的最佳时机含手续费 给定一个整数数组 prices，其中第 i 个元素代表了第 i 天的股票价格 ；非负整数 fee 代表了交易股票的手续费用。 你可以无限次地完成交易，但是你每笔交易都需要付手续费。如果你已经购买了一个股票，在卖出它之前你就不能再继续购买股票了。 返回获得利润的最大值。 注意：这里的一笔交易指买入持有并卖出股票的整个过程，每笔交易你只需要为支付一次手续费。 示例 1: 输入: prices = [1, 3, 2, 8, 4, 9], fee = 2 输出: 8 解释: 能够达到的最大利润: 在此处买入 prices[0] = 1 在此处卖出 prices[3] = 8 在此处买入 prices[4] = 4 在此处卖出 prices[5] = 9 总利润: ((8 - 1) - 2) + ((9 - 4) - 2) = 8. 贪心或者dp func maxProfit(prices []int, fee int) int { var minBuy int = prices[0] //第一天买入 var res int for i:=0;i\u003clen(prices);i++{ //如果当前价格小于最低价，则在此处买入 if prices[i]\u003cminBuy{ minBuy=prices[i] } //如果以当前价格卖出亏本，则不卖，继续找下一个可卖点 if prices[i]\u003e=minBuy\u0026\u0026prices[i]-fee-minBuy\u003c=0{ continue } //可以售卖了 if prices[i]\u003eminBuy+fee{ //累加每天的收益 res+=prices[i]-minBuy-fee //更新最小值（如果还在收获利润的区间里，表示并不是真正的卖出，而计算利润每次都要减去手续费，所以要让minBuy = prices[i] - fee;，这样在明天收获利润的时候，才不会多减一次手续费！） minBuy=prices[i]-fee } } return res } ","date":"2022-01-06 08:23:51","objectID":"/algorithm_greedy/:17:0","tags":["data structure"],"title":"Algorithm_greedy","uri":"/algorithm_greedy/"},{"categories":["Coding"],"content":"监控二叉树 给定一个二叉树，我们在树的节点上安装摄像头。 节点上的每个摄影头都可以监视其父对象、自身及其直接子对象。 计算监控树的所有节点所需的最小摄像头数量。 const inf = math.MaxInt64 / 2 func minCameraCover(root *TreeNode) int { var dfs func(*TreeNode) (a, b, c int) dfs = func(node *TreeNode) (a, b, c int) { if node == nil { return inf, 0, 0 } lefta, leftb, leftc := dfs(node.Left) righta, rightb, rightc := dfs(node.Right) a = leftc + rightc + 1 b = min(a, min(lefta+rightb, righta+leftb)) c = min(a, leftb+rightb) return } _, ans, _ := dfs(root) return ans } func min(a, b int) int { if a \u003c= b { return a } return b } ","date":"2022-01-06 08:23:51","objectID":"/algorithm_greedy/:18:0","tags":["data structure"],"title":"Algorithm_greedy","uri":"/algorithm_greedy/"},{"categories":["Coding"],"content":" 学习代码随想录笔记 回溯算法 ","date":"2022-01-06 08:22:27","objectID":"/algorithm_backtracking/:0:0","tags":["data structure"],"title":"Algorithm_backTracking","uri":"/algorithm_backtracking/"},{"categories":["Coding"],"content":"理论基础 也叫回溯搜索算法。 回溯是递归的副产品，只要有递归就会有回溯 回溯的本质是穷举，穷举所有可能，然后选出我们想要的答案，并不算高效。加一些剪枝操作或许会高效一点。 一般用来解决除了暴力搜索无可奈何的情况。 回溯法，一般可以解决如下几种问题： 组合问题：N个数里面按一定规则找出k个数的集合 切割问题：一个字符串按一定规则有几种切割方式 子集问题：一个N个数的集合里有多少符合条件的子集 排列问题：N个数按一定规则全排列，有几种排列方式 棋盘问题：N皇后，解数独等等 回溯法解决的问题都可以抽象为树形结构 因为回溯法解决的都是在集合中递归查找子集，集合的大小就构成了树的宽度，递归的深度，都构成的树的深度。 回溯模板： void backtracking(参数) { if (终止条件) { 存放结果; return; } for (选择：本层集合中元素（树中节点孩子的数量就是集合的大小）) { 处理节点; backtracking(路径，选择列表); // 递归 回溯，撤销处理结果 } } ","date":"2022-01-06 08:22:27","objectID":"/algorithm_backtracking/:1:0","tags":["data structure"],"title":"Algorithm_backTracking","uri":"/algorithm_backtracking/"},{"categories":["Coding"],"content":"*组合问题及其优化 给定两个整数 n 和 k，返回 1 … n 中所有可能的 k 个数的组合。 回溯法三部曲：函数参数、终止条件和单层搜索 剪枝优化： 可以剪枝的地方就在递归中每一层的for循环所选择的起始位置。 如果for循环选择的起始位置之后的元素个数已经不足我们需要的元素个数了，那么就没有必要搜索了。 var res [][]int func combine(n int, k int) [][]int { res=[][]int{} if n \u003c= 0 || k \u003c= 0 || k \u003e n { return res } backtrack(n, k, 1, []int{}) return res } func backtrack(n,k,start int,track []int){ if len(track)==k{ temp:=make([]int,k) copy(temp,track) res=append(res,temp) } if len(track)+n-start+1 \u003c k { return } for i:=start;i\u003c=n;i++{ track=append(track,i) backtrack(n,k,i+1,track) track=track[:len(track)-1] } } 剪枝：go语言的剪枝优化会爆内存溢出，不知道是为啥…… var res [][]int func combine(n int, k int) [][]int { res=[][]int{} if n \u003c= 0 || k \u003c= 0 || k \u003e n { return res } backtrack(n, k, 1, []int{}) return res } func backtrack(n,k,start int,track []int){ if len(track)==k{ temp:=make([]int,k) copy(temp,track) res=append(res,temp) } if len(track)+n-start+1 \u003c k { return } for i:=start;i\u003c=(n-k+len(track)+1);i++{ track=append(track,i) backtrack(n,k,i+1,track) track=track[:len(track)-1] } } ","date":"2022-01-06 08:22:27","objectID":"/algorithm_backtracking/:2:0","tags":["data structure"],"title":"Algorithm_backTracking","uri":"/algorithm_backtracking/"},{"categories":["Coding"],"content":"组合总和III 找出所有相加之和为 n 的 k 个数的组合。组合中只允许含有 1 - 9 的正整数，并且每种组合中不存在重复的数字。 说明： 所有数字都是正整数。 解集不能包含重复的组合。 回溯+减枝 func combinationSum3(k int, n int) [][]int { var track []int// 遍历路径 var result [][]int// 存放结果集 backTree(n,k,1,\u0026track,\u0026result) return result } func backTree(n,k,startIndex int,track *[]int,result *[][]int){ if len(*track)==k{ var sum int tmp:=make([]int,k) for k,v:=range *track{ sum+=v tmp[k]=v } if sum==n{ *result=append(*result,tmp) } return } for i:=startIndex;i\u003c=9-(k-len(*track))+1;i++{//减枝（k-len(*track)表示还剩多少个可填充的元素） *track=append(*track,i)//记录路径 backTree(n,k,i+1,track,result)//递归 *track=(*track)[:len(*track)-1]//回溯 } } ","date":"2022-01-06 08:22:27","objectID":"/algorithm_backtracking/:3:0","tags":["data structure"],"title":"Algorithm_backTracking","uri":"/algorithm_backtracking/"},{"categories":["Coding"],"content":"电话号码的字母组合 给定一个仅包含数字 2-9 的字符串，返回所有它能表示的字母组合。 给出数字到字母的映射如电话按键。注意 1 不对应任何字母。 主要在于递归中传递下一个数字 func letterCombinations(digits string) []string { lenth:=len(digits) if lenth==0 ||lenth\u003e4{ return nil } digitsMap:= [10]string{ \"\", // 0 \"\", // 1 \"abc\", // 2 \"def\", // 3 \"ghi\", // 4 \"jkl\", // 5 \"mno\", // 6 \"pqrs\", // 7 \"tuv\", // 8 \"wxyz\", // 9 } res:=make([]string,0) recursion(\"\",digits,0,digitsMap,\u0026res) return res } func recursion(tempString ,digits string, Index int,digitsMap [10]string, res *[]string) {//index表示第几个数字 if len(tempString)==len(digits){//终止条件，字符串长度等于digits的长度 *res=append(*res,tempString) return } tmpK:=digits[Index]-'0' // 将index指向的数字转为int（确定下一个数字） letter:=digitsMap[tmpK]// 取数字对应的字符集 for i:=0;i\u003clen(letter);i++{ tempString=tempString+string(letter[i])//拼接结果 recursion(tempString,digits,Index+1,digitsMap,res) tempString=tempString[:len(tempString)-1]//回溯 } } ","date":"2022-01-06 08:22:27","objectID":"/algorithm_backtracking/:4:0","tags":["data structure"],"title":"Algorithm_backTracking","uri":"/algorithm_backtracking/"},{"categories":["Coding"],"content":"组合总和 给定一个无重复元素的数组 candidates 和一个目标数 target ，找出 candidates 中所有可以使数字和为 target 的组合。candidates 中的数字可以无限制重复被选取。 主要在于递归中传递下一个数字 func combinationSum(candidates []int, target int) [][]int { var trcak []int var res [][]int backtracking(0,0,target,candidates,trcak,\u0026res) return res } func backtracking(startIndex,sum,target int,candidates,trcak []int,res *[][]int){ //终止条件 if sum==target{ tmp:=make([]int,len(trcak)) copy(tmp,trcak)//拷贝 *res=append(*res,tmp)//放入结果集 return } if sum\u003etarget{return} //回溯 for i:=startIndex;i\u003clen(candidates);i++{ //更新路径集合和sum trcak=append(trcak,candidates[i]) sum+=candidates[i] //递归 backtracking(i,sum,target,candidates,trcak,res) //回溯 trcak=trcak[:len(trcak)-1] sum-=candidates[i] } } ","date":"2022-01-06 08:22:27","objectID":"/algorithm_backtracking/:5:0","tags":["data structure"],"title":"Algorithm_backTracking","uri":"/algorithm_backtracking/"},{"categories":["Coding"],"content":"组合总和II 给定一个数组 candidates 和一个目标数 target ，找出 candidates 中所有可以使数字和为 target 的组合。 candidates 中的每个数字在每个组合中只能使用一次。 主要在于如何在回溯中去重 使用used数组 func combinationSum2(candidates []int, target int) [][]int { var trcak []int var res [][]int var history map[int]bool history=make(map[int]bool) sort.Ints(candidates) backtracking(0,0,target,candidates,trcak,\u0026res,history) return res } func backtracking(startIndex,sum,target int,candidates,trcak []int,res *[][]int,history map[int]bool){ //终止条件 if sum==target{ tmp:=make([]int,len(trcak)) copy(tmp,trcak)//拷贝 *res=append(*res,tmp)//放入结果集 return } if sum\u003etarget{return} //回溯 // used[i - 1] == true，说明同一树枝candidates[i - 1]使用过 // used[i - 1] == false，说明同一树层candidates[i - 1]使用过 for i:=startIndex;i\u003clen(candidates);i++{ if i\u003e0\u0026\u0026candidates[i]==candidates[i-1]\u0026\u0026history[i-1]==false{ continue } //更新路径集合和sum trcak=append(trcak,candidates[i]) sum+=candidates[i] history[i]=true //递归 backtracking(i+1,sum,target,candidates,trcak,res,history) //回溯 trcak=trcak[:len(trcak)-1] sum-=candidates[i] history[i]=false } } 不使用used数组 func combinationSum2(candidates []int, target int) [][]int { var trcak []int var res [][]int sort.Ints(candidates) backtracking(0,0,target,candidates,trcak,\u0026res) return res } func backtracking(startIndex,sum,target int,candidates,trcak []int,res *[][]int){ //终止条件 if sum==target{ tmp:=make([]int,len(trcak)) //拷贝 copy(tmp,trcak) //放入结果集 *res=append(*res,tmp) return } //回溯 for i:=startIndex;i\u003clen(candidates) \u0026\u0026 sum+candidates[i]\u003c=target;i++{ // 若当前树层有使用过相同的元素，则跳过 if i\u003estartIndex\u0026\u0026candidates[i]==candidates[i-1]{ continue } //更新路径集合和sum trcak=append(trcak,candidates[i]) sum+=candidates[i] backtracking(i+1,sum,target,candidates,trcak,res) //回溯 trcak=trcak[:len(trcak)-1] sum-=candidates[i] } } ","date":"2022-01-06 08:22:27","objectID":"/algorithm_backtracking/:6:0","tags":["data structure"],"title":"Algorithm_backTracking","uri":"/algorithm_backtracking/"},{"categories":["Coding"],"content":"分割回文串 给定一个字符串 s，将 s 分割成一些子串，使每个子串都是回文串。 返回 s 所有可能的分割方案。 示例: 输入: \"aab\" 输出: [ [\"aa\",\"b\"], [\"a\",\"a\",\"b\"] ] 注意切片（go切片是披着值类型外衣的引用类型） func partition(s string) [][]string { var tmpString []string//切割字符串集合 var res [][]string//结果集合 backTracking(s,tmpString,0,\u0026res) return res } func backTracking(s string,tmpString []string,startIndex int,res *[][]string){ if startIndex==len(s){//到达字符串末尾了 //进行一次切片拷贝，怕之后的操作影响tmpString切片内的值 t := make([]string, len(tmpString)) copy(t, tmpString) *res=append(*res,t) } for i:=startIndex;i\u003clen(s);i++{ //处理（首先通过startIndex和i判断切割的区间，进而判断该区间的字符串是否为回文，若为回文，则加入到tmpString，否则继续后移，找到回文区间）（这里为一层处理） if isPartition(s,startIndex,i){ tmpString=append(tmpString,s[startIndex:i+1]) }else{ continue } //递归 backTracking(s,tmpString,i+1,res) //回溯 tmpString=tmpString[:len(tmpString)-1] } } //判断是否为回文 func isPartition(s string,startIndex,end int)bool{ left:=startIndex right:=end for ;left\u003cright;{ if s[left]!=s[right]{ return false } //移动左右指针 left++ right-- } return true } ","date":"2022-01-06 08:22:27","objectID":"/algorithm_backtracking/:7:0","tags":["data structure"],"title":"Algorithm_backTracking","uri":"/algorithm_backtracking/"},{"categories":["Coding"],"content":"复原IP地址 给定一个只包含数字的字符串，复原它并返回所有可能的 IP 地址格式。 有效的 IP 地址 正好由四个整数（每个整数位于 0 到 255 之间组成，且不能含有前导 0），整数之间用 ‘.’ 分隔。 例如：“0.1.2.201” 和 “192.168.1.1” 是 有效的 IP 地址，但是 “0.011.255.245”、“192.168.1.312” 和 “192.168@1.1” 是 无效的 IP 地址。 示例 1： 输入：s = \"25525511135\" 输出：[\"255.255.11.135\",\"255.255.111.35\"] 示例 2： 输入：s = \"0000\" 输出：[\"0.0.0.0\"] 回溯（对于前导 0的IP（特别注意s[startIndex]=='0'的判断，不应该写成s[startIndex]==0，因为s截取出来不是数字）） func restoreIpAddresses(s string) []string { var res,path []string backTracking(s,path,0,\u0026res) return res } func backTracking(s string,path []string,startIndex int,res *[]string){ //终止条件 if startIndex==len(s)\u0026\u0026len(path)==4{ tmpIpString:=path[0]+\".\"+path[1]+\".\"+path[2]+\".\"+path[3] *res=append(*res,tmpIpString) } for i:=startIndex;i\u003clen(s);i++{ //处理 path:=append(path,s[startIndex:i+1]) if i-startIndex+1\u003c=3\u0026\u0026len(path)\u003c=4\u0026\u0026isNormalIp(s,startIndex,i){ //递归 backTracking(s,path,i+1,res) }else {//如果首尾超过了3个，或路径多余4个，或前导为0，或大于255，直接回退 return } //回溯 path=path[:len(path)-1] } } func isNormalIp(s string,startIndex,end int)bool{ checkInt,_:=strconv.Atoi(s[startIndex:end+1]) if end-startIndex+1\u003e1\u0026\u0026s[startIndex]=='0'{//对于前导 0的IP（特别**注意**s[startIndex]=='0'的判断，不应该写成s[startIndex]==0，因为s截取出来不是数字） return false } if checkInt\u003e255{ return false } return true } ","date":"2022-01-06 08:22:27","objectID":"/algorithm_backtracking/:8:0","tags":["data structure"],"title":"Algorithm_backTracking","uri":"/algorithm_backtracking/"},{"categories":["Coding"],"content":"子集问题 给定一组不含重复元素的整数数组 nums，返回该数组所有可能的子集（幂集）。 var res [][]int func subset(nums []int) [][]int { res = make([][]int, 0) sort.Ints(nums) Dfs([]int{}, nums, 0) return res } func Dfs(temp, nums []int, start int){ tmp := make([]int, len(temp)) copy(tmp, temp) res = append(res, tmp) for i := start; i \u003c len(nums); i++{ //if i\u003estart\u0026\u0026nums[i]==nums[i-1]{ // continue //} temp = append(temp, nums[i]) Dfs(temp, nums, i+1) temp = temp[:len(temp)-1] } } ","date":"2022-01-06 08:22:27","objectID":"/algorithm_backtracking/:9:0","tags":["data structure"],"title":"Algorithm_backTracking","uri":"/algorithm_backtracking/"},{"categories":["Coding"],"content":"子集II 给定一个可能包含重复元素的整数数组 nums，返回该数组所有可能的子集（幂集）。 var res[][]int func subsetsWithDup(nums []int)[][]int { res=make([][]int,0) sort.Ints(nums) dfs([]int{},nums,0) return res } func dfs(temp, num []int, start int) { tmp:=make([]int,len(temp)) copy(tmp,temp) res=append(res,tmp) for i:=start;i\u003clen(num);i++{ if i\u003estart\u0026\u0026num[i]==num[i-1]{ continue } temp=append(temp,num[i]) dfs(temp,num,i+1) temp=temp[:len(temp)-1] } } ","date":"2022-01-06 08:22:27","objectID":"/algorithm_backtracking/:10:0","tags":["data structure"],"title":"Algorithm_backTracking","uri":"/algorithm_backtracking/"},{"categories":["Coding"],"content":"递增子序列 给定一个整型数组, 你的任务是找到所有该数组的递增子序列，递增子序列的长度至少是2。给定数组中可能包含重复数字，相等的数字应该被视为递增的一种情况 func findSubsequences(nums []int) [][]int { var subRes []int var res [][]int backTring(0,nums,subRes,\u0026res) return res } func backTring(startIndex int,nums,subRes []int,res *[][]int){ if len(subRes)\u003e1{ tmp:=make([]int,len(subRes)) copy(tmp,subRes) *res=append(*res,tmp) } history:=[201]int{}//记录本层元素使用记录 for i:=startIndex;i\u003clen(nums);i++{ //分两种情况判断：一，当前取的元素小于子集的最后一个元素，则继续寻找下一个适合的元素 // 或者二，当前取的元素在本层已经出现过了，所以跳过该元素，继续寻找 if len(subRes)\u003e0\u0026\u0026nums[i]\u003csubRes[len(subRes)-1]||history[nums[i] + 100]==1{ continue } history[nums[i] + 100]=1//表示本层该元素使用过了 subRes=append(subRes,nums[i]) backTring(i+1,nums,subRes,res) subRes=subRes[:len(subRes)-1] } } ","date":"2022-01-06 08:22:27","objectID":"/algorithm_backtracking/:11:0","tags":["data structure"],"title":"Algorithm_backTracking","uri":"/algorithm_backtracking/"},{"categories":["Coding"],"content":"全排列 给定一个 没有重复 数字的序列，返回其所有可能的全排列。 var res [][]int func permute(nums []int) [][]int { res = [][]int{} backTrack(nums,len(nums),[]int{}) return res } func backTrack(nums []int,numsLen int,path []int) { if len(nums)==0{ p:=make([]int,len(path)) copy(p,path) res = append(res,p) } for i:=0;i\u003cnumsLen;i++{ cur:=nums[i] path = append(path,cur) nums = append(nums[:i],nums[i+1:]...)//直接使用切片 backTrack(nums,len(nums),path) nums = append(nums[:i],append([]int{cur},nums[i:]...)...)//回溯的时候切片也要复原，元素位置不能变 path = path[:len(path)-1] } } ","date":"2022-01-06 08:22:27","objectID":"/algorithm_backtracking/:12:0","tags":["data structure"],"title":"Algorithm_backTracking","uri":"/algorithm_backtracking/"},{"categories":["Coding"],"content":"全排列II 给定一个可包含重复数字的序列 nums ，按任意顺序 返回所有不重复的全排列。 var res [][]int func permute(nums []int) [][]int { res = [][]int{} backTrack(nums,len(nums),[]int{}) return res } func backTrack(nums []int,numsLen int,path []int) { if len(nums)==0{ p:=make([]int,len(path)) copy(p,path) res = append(res,p) } used := [21]int{}//跟前一题唯一的区别，同一层不使用重复的数。关于used的思想carl在递增子序列那一题中提到过 for i:=0;i\u003cnumsLen;i++{ if used[nums[i]+10]==1{ continue } cur:=nums[i] path = append(path,cur) used[nums[i]+10]=1 nums = append(nums[:i],nums[i+1:]...) backTrack(nums,len(nums),path) nums = append(nums[:i],append([]int{cur},nums[i:]...)...) path = path[:len(path)-1] } } ","date":"2022-01-06 08:22:27","objectID":"/algorithm_backtracking/:13:0","tags":["data structure"],"title":"Algorithm_backTracking","uri":"/algorithm_backtracking/"},{"categories":["Coding"],"content":"回溯算法去重问题的另一种写法 https://programmercarl.com/%E5%9B%9E%E6%BA%AF%E7%AE%97%E6%B3%95%E5%8E%BB%E9%87%8D%E9%97%AE%E9%A2%98%E7%9A%84%E5%8F%A6%E4%B8%80%E7%A7%8D%E5%86%99%E6%B3%95.html#_90-%E5%AD%90%E9%9B%86ii 看c++版的 ","date":"2022-01-06 08:22:27","objectID":"/algorithm_backtracking/:14:0","tags":["data structure"],"title":"Algorithm_backTracking","uri":"/algorithm_backtracking/"},{"categories":["Coding"],"content":"重新安排行程 深搜和回溯也是相辅相成的，都用了递归。 给定一个机票的字符串二维数组 [from, to]，子数组中的两个成员分别表示飞机出发和降落的机场地点，对该行程进行重新规划排序。所有这些机票都属于一个从 JFK（肯尼迪国际机场）出发的先生，所以该行程必须从 JFK 开始。 提示： 如果存在多种有效的行程，请你按字符自然排序返回最小的行程组合。例如，行程 [“JFK”, “LGA”] 与 [“JFK”, “LGB”] 相比就更小，排序更靠前 所有的机场都用三个大写字母表示（机场代码）。 假定所有机票至少存在一种合理的行程。 所有的机票必须都用一次 且 只能用一次。 示例 1： 输入：[[\"MUC\", \"LHR\"], [\"JFK\", \"MUC\"], [\"SFO\", \"SJC\"], [\"LHR\", \"SFO\"]] 输出：[\"JFK\", \"MUC\", \"LHR\", \"SFO\", \"SJC\"] 示例 2： 输入：[[\"JFK\",\"SFO\"],[\"JFK\",\"ATL\"],[\"SFO\",\"ATL\"],[\"ATL\",\"JFK\"],[\"ATL\",\"SFO\"]] 输出：[\"JFK\",\"ATL\",\"JFK\",\"SFO\",\"ATL\",\"SFO\"] 解释：另一种有效的行程是 [\"JFK\",\"SFO\",\"ATL\",\"JFK\",\"ATL\",\"SFO\"]。但是它自然排序更大更靠后 class Solution { private: // unordered_map\u003c出发机场, map\u003c到达机场, 航班次数\u003e\u003e targets unordered_map\u003cstring, map\u003cstring, int\u003e\u003e targets; bool backtracking(int ticketNum, vector\u003cstring\u003e\u0026 result) { if (result.size() == ticketNum + 1) { return true; } for (pair\u003cconst string, int\u003e\u0026 target : targets[result[result.size() - 1]]) { if (target.second \u003e 0 ) { // 记录到达机场是否飞过了 result.push_back(target.first); target.second--; if (backtracking(ticketNum, result)) return true; result.pop_back(); target.second++; } } return false; } public: vector\u003cstring\u003e findItinerary(vector\u003cvector\u003cstring\u003e\u003e\u0026 tickets) { targets.clear(); vector\u003cstring\u003e result; for (const vector\u003cstring\u003e\u0026 vec : tickets) { targets[vec[0]][vec[1]]++; // 记录映射关系 } result.push_back(\"JFK\"); // 起始机场 backtracking(tickets.size(), result); return result; } }; ","date":"2022-01-06 08:22:27","objectID":"/algorithm_backtracking/:15:0","tags":["data structure"],"title":"Algorithm_backTracking","uri":"/algorithm_backtracking/"},{"categories":["Coding"],"content":"N皇后 n 皇后问题 研究的是如何将 n 个皇后放置在 n×n 的棋盘上，并且使皇后彼此之间不能相互攻击。 给你一个整数 n ，返回所有不同的 n 皇后问题 的解决方案。 每一种解法包含一个不同的 n 皇后问题 的棋子放置方案，该方案中 ‘Q’ 和 ‘.’ 分别代表了皇后和空位。 import \"strings\" var res [][]string func isValid(board [][]string, row, col int) (res bool){ n := len(board) for i:=0; i \u003c row; i++ { if board[i][col] == \"Q\" { return false } } for i := 0; i \u003c n; i++{ if board[row][i] == \"Q\" { return false } } for i ,j := row, col; i \u003e= 0 \u0026\u0026 j \u003e=0 ; i, j = i - 1, j- 1{ if board[i][j] == \"Q\"{ return false } } for i, j := row, col; i \u003e=0 \u0026\u0026 j \u003c n; i,j = i-1, j+1 { if board[i][j] == \"Q\" { return false } } return true } func backtrack(board [][]string, row int) { size := len(board) if row == size{ temp := make([]string, size) for i := 0; i\u003csize;i++{ temp[i] = strings.Join(board[i],\"\") } res =append(res,temp) return } for col := 0; col \u003c size; col++ { if !isValid(board, row, col){ continue } board[row][col] = \"Q\" backtrack(board, row+1) board[row][col] = \".\" } } func solveNQueens(n int) [][]string { res = [][]string{} board := make([][]string, n) for i := 0; i \u003c n; i++{ board[i] = make([]string, n) } for i := 0; i \u003c n; i++{ for j := 0; j\u003cn;j++{ board[i][j] = \".\" } } backtrack(board, 0) return res } ","date":"2022-01-06 08:22:27","objectID":"/algorithm_backtracking/:16:0","tags":["data structure"],"title":"Algorithm_backTracking","uri":"/algorithm_backtracking/"},{"categories":["Coding"],"content":"解数独 编写一个程序，通过填充空格来解决数独问题。 func solveSudoku(board [][]byte) { var backtracking func(board [][]byte) bool backtracking=func(board [][]byte) bool{ for i:=0;i\u003c9;i++{ for j:=0;j\u003c9;j++{ //判断此位置是否适合填数字 if board[i][j]!='.'{ continue } //尝试填1-9 for k:='1';k\u003c='9';k++{ if isvalid(i,j,byte(k),board)==true{//如果满足要求就填 board[i][j]=byte(k) if backtracking(board)==true{ return true } board[i][j]='.' } } return false } } return true } backtracking(board) } //判断填入数字是否满足要求 func isvalid(row,col int,k byte,board [][]byte)bool{ for i:=0;i\u003c9;i++{//行 if board[row][i]==k{ return false } } for i:=0;i\u003c9;i++{//列 if board[i][col]==k{ return false } } //方格 startrow:=(row/3)*3 startcol:=(col/3)*3 for i:=startrow;i\u003cstartrow+3;i++{ for j:=startcol;j\u003cstartcol+3;j++{ if board[i][j]==k{ return false } } } return true } ","date":"2022-01-06 08:22:27","objectID":"/algorithm_backtracking/:17:0","tags":["data structure"],"title":"Algorithm_backTracking","uri":"/algorithm_backtracking/"},{"categories":["Coding"],"content":" 学习代码随想录笔记 二叉树 ","date":"2022-01-06 08:21:45","objectID":"/algorithm_binarytree/:0:0","tags":["data structure"],"title":"Algorithm_binaryTree","uri":"/algorithm_binarytree/"},{"categories":["Coding"],"content":"理论基础 一般主要会碰到满二叉树以及完全二叉树。 完全二叉树的定义如下：在完全二叉树中，除了最底层节点可能没填满外，其余每层节点数都达到最大值，并且最下面一层的节点都集中在该层最左边的若干位置。若最底层为第 h 层，则该层包含 1~ 2^h -1 个节点。 优先级队列其实是一个堆，堆就是一棵完全二叉树，同时保证父子节点的顺序关系。 二叉搜索树： 与前面两个树不同，该树有节点权值。 有序树，左节点 \u003c 中节点 \u003c 右节点 平衡二叉搜索树：又被称为AVL（Adelson-Velsky and Landis）树，且具有以下性质：它是一棵空树或它的左右两个子树的高度差的绝对值不超过1，并且左右两个子树都是一棵平衡二叉树。 二叉树可以链式存储，也可以顺序存储。 深度优先遍历 前序遍历（递归法，迭代法） 中序遍历（递归法，迭代法） 后序遍历（递归法，迭代法） 广度优先遍历 层次遍历（迭代法） type TreeNode struct { Val int Left *TreeNode Right *TreeNode } ","date":"2022-01-06 08:21:45","objectID":"/algorithm_binarytree/:1:0","tags":["data structure"],"title":"Algorithm_binaryTree","uri":"/algorithm_binarytree/"},{"categories":["Coding"],"content":"递归遍历 递归的实现就是：每一次递归调用都会把函数的局部变量、参数值和返回地址等压入调用栈中，然后递归返回的时候，从栈顶弹出上一次递归的各项参数，所以这就是递归为什么可以返回上一层位置的原因。 写递归还是得有方法论。 确定递归函数的参数和返回值： 确定哪些参数是递归的过程中需要处理的，那么就在递归函数里加上这个参数， 并且还要明确每次递归的返回值是什么进而确定递归函数的返回类型。 确定终止条件： 写完了递归算法, 运行的时候，经常会遇到栈溢出的错误，就是没写终止条件或者终止条件写的不对，操作系统也是用一个栈的结构来保存每一层递归的信息，如果递归没有终止，操作系统的内存栈必然就会溢出。 确定单层递归的逻辑： 确定每一层递归需要处理的信息。在这里也就会重复调用自己来实现递归的过程。 前序遍历： func preorderTraversal(root *TreeNode) (res []int) { var traversal func(node *TreeNode) traversal = func(node *TreeNode) { if node == nil { return } res = append(res,node.Val) traversal(node.Left) traversal(node.Right) } traversal(root) return res } func pre(r *TreeNode)(res []int){ } 中序遍历： func inorderTraversal(root *TreeNode) (res []int) { var traversal func(node *TreeNode) traversal = func(node *TreeNode) { if node == nil { return } traversal(node.Left) res = append(res,node.Val) traversal(node.Right) } traversal(root) return res } 后序遍历: func postorderTraversal(root *TreeNode) (res []int) { var traversal func(node *TreeNode) traversal = func(node *TreeNode) { if node == nil { return } traversal(node.Left) traversal(node.Right) res = append(res,node.Val) } traversal(root) return res } ","date":"2022-01-06 08:21:45","objectID":"/algorithm_binarytree/:2:0","tags":["data structure"],"title":"Algorithm_binaryTree","uri":"/algorithm_binarytree/"},{"categories":["Coding"],"content":"迭代遍历 迭代法前序遍历 func preorderTraversal(root *TreeNode) []int { ans := []int{} if root == nil { return ans } st := list.New() st.PushBack(root) for st.Len() \u003e 0 { node := st.Remove(st.Back()).(*TreeNode) ans = append(ans, node.Val) if node.Right != nil { st.PushBack(node.Right) } if node.Left != nil { st.PushBack(node.Left) } } return ans } 迭代法后序遍历 func postorderTraversal(root *TreeNode) []int { ans := []int{} if root == nil { return ans } st := list.New() st.PushBack(root) for st.Len() \u003e 0 { node := st.Remove(st.Back()).(*TreeNode) ans = append(ans, node.Val) if node.Left != nil { st.PushBack(node.Left) } if node.Right != nil { st.PushBack(node.Right) } } reverse(ans) return ans } func reverse(a []int) { l, r := 0, len(a) - 1 for l \u003c r { a[l], a[r] = a[r], a[l] l, r = l+1, r-1 } } 迭代法中序遍历 func inorderTraversal(root *TreeNode) []int { ans := []int{} if root == nil { return ans } st := list.New() cur := root for cur != nil || st.Len() \u003e 0 { if cur != nil { st.PushBack(cur) cur = cur.Left } else { cur = st.Remove(st.Back()).(*TreeNode) ans = append(ans, cur.Val) cur = cur.Right } } return ans } ","date":"2022-01-06 08:21:45","objectID":"/algorithm_binarytree/:3:0","tags":["data structure"],"title":"Algorithm_binaryTree","uri":"/algorithm_binarytree/"},{"categories":["Coding"],"content":"统一迭代 迭代法实现的先中后序，其实风格也不是那么统一，除了先序和后序，有关联，中序完全就是另一个风格了，一会用栈遍历，一会又用指针来遍历。 使用迭代法实现先中后序遍历，很难写出统一的代码，不像是递归法，实现了其中的一种遍历方式，其他两种只要稍稍改一下节点顺序就可以了。 要处理的节点放入栈之后，紧接着放入一个空指针作为标记。 这种方法也可以叫做标记法 前序遍历统一迭代法 /** type Element struct { // 元素保管的值 Value interface{} // 内含隐藏或非导出字段 } func (l *List) Back() *Element 前序遍历：中左右 压栈顺序：右左中 **/ func preorderTraversal(root *TreeNode) []int { if root == nil { return nil } var stack = list.New()//栈 res:=[]int{}//结果集 stack.PushBack(root) var node *TreeNode for stack.Len()\u003e0{ e := stack.Back() stack.Remove(e)//弹出元素 if e.Value==nil{// 如果为空，则表明是需要处理中间节点 e=stack.Back()//弹出元素（即中间节点） stack.Remove(e)//删除中间节点 node=e.Value.(*TreeNode) res=append(res,node.Val)//将中间节点加入到结果集中 continue//继续弹出栈中下一个节点 } node = e.Value.(*TreeNode) //压栈顺序：右左中 if node.Right!=nil{ stack.PushBack(node.Right) } if node.Left!=nil{ stack.PushBack(node.Left) } stack.PushBack(node)//中间节点压栈后再压入nil作为中间节点的标志符 stack.PushBack(nil) } return res } 中序遍历统一迭代法 /** * Definition for a binary tree node. * type TreeNode struct { * Val int * Left *TreeNode * Right *TreeNode * } */ //中序遍历：左中右 //压栈顺序：右中左 func inorderTraversal(root *TreeNode) []int { if root==nil{ return nil } stack:=list.New()//栈 res:=[]int{}//结果集 stack.PushBack(root) var node *TreeNode for stack.Len()\u003e0{ e := stack.Back() stack.Remove(e) if e.Value==nil{// 如果为空，则表明是需要处理中间节点 e=stack.Back()//弹出元素（即中间节点） stack.Remove(e)//删除中间节点 node=e.Value.(*TreeNode) res=append(res,node.Val)//将中间节点加入到结果集中 continue//继续弹出栈中下一个节点 } node = e.Value.(*TreeNode) //压栈顺序：右中左 if node.Right!=nil{ stack.PushBack(node.Right) } stack.PushBack(node)//中间节点压栈后再压入nil作为中间节点的标志符 stack.PushBack(nil) if node.Left!=nil{ stack.PushBack(node.Left) } } return res } 后序遍历统一迭代法 //后续遍历：左右中 //压栈顺序：中右左 func postorderTraversal(root *TreeNode) []int { if root == nil { return nil } var stack = list.New()//栈 res:=[]int{}//结果集 stack.PushBack(root) var node *TreeNode for stack.Len()\u003e0{ e := stack.Back() stack.Remove(e) if e.Value==nil{// 如果为空，则表明是需要处理中间节点 e=stack.Back()//弹出元素（即中间节点） stack.Remove(e)//删除中间节点 node=e.Value.(*TreeNode) res=append(res,node.Val)//将中间节点加入到结果集中 continue//继续弹出栈中下一个节点 } node = e.Value.(*TreeNode) //压栈顺序：中右左 stack.PushBack(node)//中间节点压栈后再压入nil作为中间节点的标志符 stack.PushBack(nil) if node.Right!=nil{ stack.PushBack(node.Right) } if node.Left!=nil{ stack.PushBack(node.Left) } } return res } ","date":"2022-01-06 08:21:45","objectID":"/algorithm_binarytree/:4:0","tags":["data structure"],"title":"Algorithm_binaryTree","uri":"/algorithm_binarytree/"},{"categories":["Coding"],"content":"层序遍历 ","date":"2022-01-06 08:21:45","objectID":"/algorithm_binarytree/:5:0","tags":["data structure"],"title":"Algorithm_binaryTree","uri":"/algorithm_binarytree/"},{"categories":["Coding"],"content":"102.二叉树的层序遍历 https://leetcode-cn.com/problems/binary-tree-level-order-traversal/ /** 102. 二叉树的层序遍历 */ func levelOrder(root *TreeNode) [][]int { res:=[][]int{} if root==nil{//防止为空 return res } queue:=list.New() queue.PushBack(root) var tmpArr []int for queue.Len()\u003e0 { length:=queue.Len()//保存当前层的长度，然后处理当前层（十分重要，防止添加下层元素影响判断层中元素的个数） for i:=0;i\u003clength;i++{ node:=queue.Remove(queue.Front()).(*TreeNode)//出队列 if node.Left!=nil{ queue.PushBack(node.Left) } if node.Right!=nil{ queue.PushBack(node.Right) } tmpArr=append(tmpArr,node.Val)//将值加入本层切片中 } res=append(res,tmpArr)//放入结果集 tmpArr=[]int{}//清空层的数据 } return res } ","date":"2022-01-06 08:21:45","objectID":"/algorithm_binarytree/:5:1","tags":["data structure"],"title":"Algorithm_binaryTree","uri":"/algorithm_binarytree/"},{"categories":["Coding"],"content":"107.二叉树的层次遍历II https://leetcode-cn.com/problems/binary-tree-level-order-traversal-ii/ /** 107. 二叉树的层序遍历 II */ func levelOrderBottom(root *TreeNode) [][]int { queue:=list.New() res:=[][]int{} if root==nil{ return res } queue.PushBack(root) for queue.Len()\u003e0{ length:=queue.Len() tmp:=[]int{} for i:=0;i\u003clength;i++{ node:=queue.Remove(queue.Front()).(*TreeNode) if node.Left!=nil{ queue.PushBack(node.Left) } if node.Right!=nil{ queue.PushBack(node.Right) } tmp=append(tmp,node.Val) } res=append(res,tmp) } //反转结果集 for i:=0;i\u003clen(res)/2;i++{ res[i],res[len(res)-i-1]=res[len(res)-i-1],res[i] } return res } ","date":"2022-01-06 08:21:45","objectID":"/algorithm_binarytree/:5:2","tags":["data structure"],"title":"Algorithm_binaryTree","uri":"/algorithm_binarytree/"},{"categories":["Coding"],"content":"199.二叉树的右视图 https://leetcode-cn.com/problems/binary-tree-right-side-view/ func rightSideView(root *TreeNode) []int { queue:=list.New() res:=[][]int{} var finaRes []int if root==nil{ return finaRes } queue.PushBack(root) for queue.Len()\u003e0{ length:=queue.Len() tmp:=[]int{} for i:=0;i\u003clength;i++{ node:=queue.Remove(queue.Front()).(*TreeNode) if node.Left!=nil{ queue.PushBack(node.Left) } if node.Right!=nil{ queue.PushBack(node.Right) } tmp=append(tmp,node.Val) } res=append(res,tmp) } //取每一层的最后一个元素 for i:=0;i\u003clen(res);i++{ finaRes=append(finaRes,res[i][len(res[i])-1]) } return finaRes } ","date":"2022-01-06 08:21:45","objectID":"/algorithm_binarytree/:5:3","tags":["data structure"],"title":"Algorithm_binaryTree","uri":"/algorithm_binarytree/"},{"categories":["Coding"],"content":"637.二叉树的层平均值 https://leetcode-cn.com/problems/average-of-levels-in-binary-tree/ /** 637. 二叉树的层平均值 */ func averageOfLevels(root *TreeNode) []float64 { res:=[][]int{} var finRes []float64 if root==nil{//防止为空 return finRes } queue:=list.New() queue.PushBack(root) var tmpArr []int for queue.Len()\u003e0 { length:=queue.Len()//保存当前层的长度，然后处理当前层（十分重要，防止添加下层元素影响判断层中元素的个数） for i:=0;i\u003clength;i++{ node:=queue.Remove(queue.Front()).(*TreeNode)//出队列 if node.Left!=nil{ queue.PushBack(node.Left) } if node.Right!=nil{ queue.PushBack(node.Right) } tmpArr=append(tmpArr,node.Val)//将值加入本层切片中 } res=append(res,tmpArr)//放入结果集 tmpArr=[]int{}//清空层的数据 } //计算每层的平均值 length:=len(res) for i:=0;i\u003clength;i++{ var sum int for j:=0;j\u003clen(res[i]);j++{ sum+=res[i][j] } tmp:=float64(sum)/float64(len(res[i])) finRes=append(finRes,tmp)//将平均值放入结果集合 } return finRes } ","date":"2022-01-06 08:21:45","objectID":"/algorithm_binarytree/:5:4","tags":["data structure"],"title":"Algorithm_binaryTree","uri":"/algorithm_binarytree/"},{"categories":["Coding"],"content":"429.N叉树的层序遍历 https://leetcode-cn.com/problems/n-ary-tree-level-order-traversal/ func levelOrder(root *Node) [][]int { queue:=list.New() res:=[][]int{}//结果集 if root==nil{ return res } queue.PushBack(root) for queue.Len()\u003e0{ length:=queue.Len()//记录当前层的数量 var tmp []int for T:=0;T\u003clength;T++{//该层的每个元素：一添加到该层的结果集中；二找到该元素的下层元素加入到队列中，方便下次使用 myNode:=queue.Remove(queue.Front()).(*Node) tmp=append(tmp,myNode.Val) for i:=0;i\u003clen(myNode.Children);i++{ queue.PushBack(myNode.Children[i]) } } res=append(res,tmp) } return res } ","date":"2022-01-06 08:21:45","objectID":"/algorithm_binarytree/:5:5","tags":["data structure"],"title":"Algorithm_binaryTree","uri":"/algorithm_binarytree/"},{"categories":["Coding"],"content":"515.在每个树行中找最大值 https://leetcode-cn.com/problems/find-largest-value-in-each-tree-row/ /** 515. 在每个树行中找最大值 */ func largestValues(root *TreeNode) []int { res:=[][]int{} var finRes []int if root==nil{//防止为空 return finRes } queue:=list.New() queue.PushBack(root) var tmpArr []int //层次遍历 for queue.Len()\u003e0 { length:=queue.Len()//保存当前层的长度，然后处理当前层（十分重要，防止添加下层元素影响判断层中元素的个数） for i:=0;i\u003clength;i++{ node:=queue.Remove(queue.Front()).(*TreeNode)//出队列 if node.Left!=nil{ queue.PushBack(node.Left) } if node.Right!=nil{ queue.PushBack(node.Right) } tmpArr=append(tmpArr,node.Val)//将值加入本层切片中 } res=append(res,tmpArr)//放入结果集 tmpArr=[]int{}//清空层的数据 } //找到每层的最大值 for i:=0;i\u003clen(res);i++{ finRes=append(finRes,max(res[i]...)) } return finRes } func max(vals...int) int { max:=int(math.Inf(-1))//负无穷 for _, val := range vals { if val \u003e max { max = val } } return max } ","date":"2022-01-06 08:21:45","objectID":"/algorithm_binarytree/:5:6","tags":["data structure"],"title":"Algorithm_binaryTree","uri":"/algorithm_binarytree/"},{"categories":["Coding"],"content":"116.填充每个节点的下一个右侧节点指针 https://leetcode-cn.com/problems/populating-next-right-pointers-in-each-node/ /** 116. 填充每个节点的下一个右侧节点指针 117. 填充每个节点的下一个右侧节点指针 II */ func connect(root *Node) *Node { res:=[][]*Node{} if root==nil{//防止为空 return root } queue:=list.New() queue.PushBack(root) var tmpArr []*Node for queue.Len()\u003e0 { length:=queue.Len()//保存当前层的长度，然后处理当前层（十分重要，防止添加下层元素影响判断层中元素的个数） for i:=0;i\u003clength;i++{ node:=queue.Remove(queue.Front()).(*Node)//出队列 if node.Left!=nil{ queue.PushBack(node.Left) } if node.Right!=nil{ queue.PushBack(node.Right) } tmpArr=append(tmpArr,node)//将值加入本层切片中 } res=append(res,tmpArr)//放入结果集 tmpArr=[]*Node{}//清空层的数据 } //遍历每层元素,指定next for i:=0;i\u003clen(res);i++{ for j:=0;j\u003clen(res[i])-1;j++{ res[i][j].Next=res[i][j+1] } } return root } ","date":"2022-01-06 08:21:45","objectID":"/algorithm_binarytree/:5:7","tags":["data structure"],"title":"Algorithm_binaryTree","uri":"/algorithm_binarytree/"},{"categories":["Coding"],"content":"117.填充每个节点的下一个右侧节点指针II https://leetcode-cn.com/problems/populating-next-right-pointers-in-each-node-ii/ /** 116. 填充每个节点的下一个右侧节点指针 117. 填充每个节点的下一个右侧节点指针 II */ func connect(root *Node) *Node { res:=[][]*Node{} if root==nil{//防止为空 return root } queue:=list.New() queue.PushBack(root) var tmpArr []*Node for queue.Len()\u003e0 { length:=queue.Len()//保存当前层的长度，然后处理当前层（十分重要，防止添加下层元素影响判断层中元素的个数） for i:=0;i\u003clength;i++{ node:=queue.Remove(queue.Front()).(*Node)//出队列 if node.Left!=nil{ queue.PushBack(node.Left) } if node.Right!=nil{ queue.PushBack(node.Right) } tmpArr=append(tmpArr,node)//将值加入本层切片中 } res=append(res,tmpArr)//放入结果集 tmpArr=[]*Node{}//清空层的数据 } //遍历每层元素,指定next for i:=0;i\u003clen(res);i++{ for j:=0;j\u003clen(res[i])-1;j++{ res[i][j].Next=res[i][j+1] } } return root } ","date":"2022-01-06 08:21:45","objectID":"/algorithm_binarytree/:5:8","tags":["data structure"],"title":"Algorithm_binaryTree","uri":"/algorithm_binarytree/"},{"categories":["Coding"],"content":"104.二叉树的最大深度 https://leetcode-cn.com/problems/maximum-depth-of-binary-tree/ /** * Definition for a binary tree node. * type TreeNode struct { * Val int * Left *TreeNode * Right *TreeNode * } */ func maxDepth(root *TreeNode) int { ans:=0 if root==nil{ return 0 } queue:=list.New() queue.PushBack(root) for queue.Len()\u003e0{ length:=queue.Len() for i:=0;i\u003clength;i++{ node:=queue.Remove(queue.Front()).(*TreeNode) if node.Left!=nil{ queue.PushBack(node.Left) } if node.Right!=nil{ queue.PushBack(node.Right) } } ans++//记录深度，其他的是层序遍历的板子 } return ans } ","date":"2022-01-06 08:21:45","objectID":"/algorithm_binarytree/:5:9","tags":["data structure"],"title":"Algorithm_binaryTree","uri":"/algorithm_binarytree/"},{"categories":["Coding"],"content":"111.二叉树的最小深度 https://leetcode-cn.com/problems/minimum-depth-of-binary-tree/ /** * Definition for a binary tree node. * type TreeNode struct { * Val int * Left *TreeNode * Right *TreeNode * } */ func minDepth(root *TreeNode) int { ans:=0 if root==nil{ return 0 } queue:=list.New() queue.PushBack(root) for queue.Len()\u003e0{ length:=queue.Len() for i:=0;i\u003clength;i++{ node:=queue.Remove(queue.Front()).(*TreeNode) if node.Left==nil\u0026\u0026node.Right==nil{//当前节点没有左右节点，则代表此层是最小层 return ans+1//返回当前层 ans代表是上一层 } if node.Left!=nil{ queue.PushBack(node.Left) } if node.Right!=nil{ queue.PushBack(node.Right) } } ans++//记录层数 } return ans+1 } ","date":"2022-01-06 08:21:45","objectID":"/algorithm_binarytree/:5:10","tags":["data structure"],"title":"Algorithm_binaryTree","uri":"/algorithm_binarytree/"},{"categories":["Coding"],"content":"翻转二叉树 二叉树，当然是左右翻转。 递归版本的前序遍历 func invertTree(root *TreeNode) *TreeNode { if root ==nil{ return nil } temp:=root.Left root.Left=root.Right root.Right=temp invertTree(root.Left) invertTree(root.Right) return root } 递归版本的后序遍历 func invertTree(root *TreeNode) *TreeNode { if root==nil{ return root } invertTree(root.Left)//遍历左节点 invertTree(root.Right)//遍历右节点 root.Left,root.Right=root.Right,root.Left//交换 return root } 迭代版本的前序遍历 func invertTree(root *TreeNode) *TreeNode { stack:=[]*TreeNode{} node:=root for node!=nil||len(stack)\u003e0{ for node!=nil{ node.Left,node.Right=node.Right,node.Left//交换 stack=append(stack,node) node=node.Left } node=stack[len(stack)-1] stack=stack[:len(stack)-1] node=node.Right } return root } 迭代版本的后序遍历 func invertTree(root *TreeNode) *TreeNode { stack:=[]*TreeNode{} node:=root var prev *TreeNode for node!=nil||len(stack)\u003e0{ for node!=nil{ stack=append(stack,node) node=node.Left } node=stack[len(stack)-1] stack=stack[:len(stack)-1] if node.Right==nil||node.Right==prev{ node.Left,node.Right=node.Right,node.Left//交换 prev=node node=nil }else { stack=append(stack,node) node=node.Right } } return root } 层序遍历 func invertTree(root *TreeNode) *TreeNode { if root==nil{ return root } queue:=list.New() node:=root queue.PushBack(node) for queue.Len()\u003e0{ length:=queue.Len() for i:=0;i\u003clength;i++{ e:=queue.Remove(queue.Front()).(*TreeNode) e.Left,e.Right=e.Right,e.Left//交换 if e.Left!=nil{ queue.PushBack(e.Left) } if e.Right!=nil{ queue.PushBack(e.Right) } } } return root } ","date":"2022-01-06 08:21:45","objectID":"/algorithm_binarytree/:6:0","tags":["data structure"],"title":"Algorithm_binaryTree","uri":"/algorithm_binarytree/"},{"categories":["Coding"],"content":"对称二叉树 检查二叉树是否镜像对称。 /** * Definition for a binary tree node. * type TreeNode struct { * Val int * Left *TreeNode * Right *TreeNode * } */ // 递归 func defs(left *TreeNode, right *TreeNode) bool { if left == nil \u0026\u0026 right == nil { return true; }; if left == nil || right == nil { return false; }; if left.Val != right.Val { return false; } return defs(left.Left, right.Right) \u0026\u0026 defs(right.Left, left.Right); } func isSymmetric(root *TreeNode) bool { return defs(root.Left, root.Right); } // 迭代 func isSymmetric(root *TreeNode) bool { var queue []*TreeNode; if root != nil { queue = append(queue, root.Left, root.Right); } for len(queue) \u003e 0 { left := queue[0]; right := queue[1]; queue = queue[2:]; if left == nil \u0026\u0026 right == nil { continue; } if left == nil || right == nil || left.Val != right.Val { return false; }; queue = append(queue, left.Left, right.Right, right.Left, left.Right); } return true; } ","date":"2022-01-06 08:21:45","objectID":"/algorithm_binarytree/:7:0","tags":["data structure"],"title":"Algorithm_binaryTree","uri":"/algorithm_binarytree/"},{"categories":["Coding"],"content":"最大深度 /** * definition for a binary tree node. * type treenode struct { * val int * left *treenode * right *treenode * } */ func max (a, b int) int { if a \u003e b { return a; } return b; } // 递归 func maxdepth(root *treenode) int { if root == nil { return 0; } return max(maxdepth(root.left), maxdepth(root.right)) + 1; } // 遍历 func maxdepth(root *treenode) int { levl := 0; queue := make([]*treenode, 0); if root != nil { queue = append(queue, root); } for l := len(queue); l \u003e 0; { for ;l \u003e 0;l-- { node := queue[0]; if node.left != nil { queue = append(queue, node.left); } if node.right != nil { queue = append(queue, node.right); } queue = queue[1:]; } levl++; l = len(queue); } return levl; } ","date":"2022-01-06 08:21:45","objectID":"/algorithm_binarytree/:8:0","tags":["data structure"],"title":"Algorithm_binaryTree","uri":"/algorithm_binarytree/"},{"categories":["Coding"],"content":"最小深度 /** * Definition for a binary tree node. * type TreeNode struct { * Val int * Left *TreeNode * Right *TreeNode * } */ func min(a, b int) int { if a \u003c b { return a; } return b; } // 递归 func minDepth(root *TreeNode) int { if root == nil { return 0; } if root.Left == nil \u0026\u0026 root.Right != nil { return 1 + minDepth(root.Right); } if root.Right == nil \u0026\u0026 root.Left != nil { return 1 + minDepth(root.Left); } return min(minDepth(root.Left), minDepth(root.Right)) + 1; } // 迭代 func minDepth(root *TreeNode) int { dep := 0; queue := make([]*TreeNode, 0); if root != nil { queue = append(queue, root); } for l := len(queue); l \u003e 0; { dep++; for ; l \u003e 0; l-- { node := queue[0]; if node.Left == nil \u0026\u0026 node.Right == nil { return dep; } if node.Left != nil { queue = append(queue, node.Left); } if node.Right != nil { queue = append(queue, node.Right); } queue = queue[1:]; } l = len(queue); } return dep; } ","date":"2022-01-06 08:21:45","objectID":"/algorithm_binarytree/:9:0","tags":["data structure"],"title":"Algorithm_binaryTree","uri":"/algorithm_binarytree/"},{"categories":["Coding"],"content":"完全二叉树的节点个数 递归版本 /** * Definition for a binary tree node. * type TreeNode struct { * Val int * Left *TreeNode * Right *TreeNode * } */ //本题直接就是求有多少个节点，无脑存进数组算长度就行了。 func countNodes(root *TreeNode) int { if root == nil { return 0 } res := 1 if root.Right != nil { res += countNodes(root.Right) } if root.Left != nil { res += countNodes(root.Left) } return res } 利用完全二叉树特性的递归解法 func countNodes(root *TreeNode) int { if root == nil { return 0 } leftH, rightH := 0, 0 leftNode := root.Left rightNode := root.Right for leftNode != nil { leftNode = leftNode.Left leftH++ } for rightNode != nil { rightNode = rightNode.Right rightH++ } if leftH == rightH { return (2 \u003c\u003c leftH) - 1 } return countNodes(root.Left) + countNodes(root.Right) + 1 } ","date":"2022-01-06 08:21:45","objectID":"/algorithm_binarytree/:10:0","tags":["data structure"],"title":"Algorithm_binaryTree","uri":"/algorithm_binarytree/"},{"categories":["Coding"],"content":"平衡二叉树 给定一个二叉树，判断它是否是高度平衡的二叉树。 本题中，一棵高度平衡二叉树定义为：一个二叉树每个节点 的左右两个子树的高度差的绝对值不超过1。 func isBalanced(root *TreeNode) bool { if root==nil{ return true } if !isBalanced(root.Left) || !isBalanced(root.Right){ return false } LeftH:=maxdepth(root.Left)+1 RightH:=maxdepth(root.Right)+1 if abs(LeftH-RightH)\u003e1{ return false } return true } func maxdepth(root *TreeNode)int{ if root==nil{ return 0 } return max(maxdepth(root.Left),maxdepth(root.Right))+1 } func max(a,b int)int{ if a\u003eb{ return a } return b } func abs(a int)int{ if a\u003c0{ return -a } return a } ","date":"2022-01-06 08:21:45","objectID":"/algorithm_binarytree/:11:0","tags":["data structure"],"title":"Algorithm_binaryTree","uri":"/algorithm_binarytree/"},{"categories":["Coding"],"content":"二叉树的所有路径 递归法： func binaryTreePaths(root *TreeNode) []string { res := make([]string, 0) var travel func(node *TreeNode, s string) travel = func(node *TreeNode, s string) { if node.Left == nil \u0026\u0026 node.Right == nil { v := s + strconv.Itoa(node.Val) res = append(res, v) return } s = s + strconv.Itoa(node.Val) + \"-\u003e\" if node.Left != nil { travel(node.Left, s) } if node.Right != nil { travel(node.Right, s) } } travel(root, \"\") return res } 迭代法： func binaryTreePaths(root *TreeNode) []string { stack := []*TreeNode{} paths := make([]string, 0) res := make([]string, 0) if root != nil { stack = append(stack, root) paths = append(paths, \"\") } for len(stack) \u003e 0 { l := len(stack) node := stack[l-1] path := paths[l-1] stack = stack[:l-1] paths = paths[:l-1] if node.Left == nil \u0026\u0026 node.Right == nil { res = append(res, path+strconv.Itoa(node.Val)) continue } if node.Right != nil { stack = append(stack, node.Right) paths = append(paths, path+strconv.Itoa(node.Val)+\"-\u003e\") } if node.Left != nil { stack = append(stack, node.Left) paths = append(paths, path+strconv.Itoa(node.Val)+\"-\u003e\") } } return res } ","date":"2022-01-06 08:21:45","objectID":"/algorithm_binarytree/:12:0","tags":["data structure"],"title":"Algorithm_binaryTree","uri":"/algorithm_binarytree/"},{"categories":["Coding"],"content":"二叉树的递归+回溯 100.相同的树 /** * Definition for a binary tree node. * type TreeNode struct { * Val int * Left *TreeNode * Right *TreeNode * } */ func isSameTree(p *TreeNode, q *TreeNode) bool { switch { case p == nil \u0026\u0026 q == nil: return true case p == nil || q == nil: fallthrough case p.Val != q.Val: return false } return isSameTree(p.Left, q.Left) \u0026\u0026 isSameTree(p.Right, q.Right) } 257.二叉的所有路径 递归法 /** * Definition for a binary tree node. * type TreeNode struct { * Val int * Left *TreeNode * Right *TreeNode * } */ func binaryTreePaths(root *TreeNode) []string { var result []string traversal(root,\u0026result,\"\") return result } func traversal(root *TreeNode,result *[]string,pathStr string){ //判断是否为第一个元素 if len(pathStr)!=0{ pathStr=pathStr+\"-\u003e\"+strconv.Itoa(root.Val) }else{ pathStr=strconv.Itoa(root.Val) } //判断是否为叶子节点 if root.Left==nil\u0026\u0026root.Right==nil{ *result=append(*result,pathStr) return } //左右 if root.Left!=nil{ traversal(root.Left,result,pathStr) } if root.Right!=nil{ traversal(root.Right,result,pathStr) } } 回溯法 /** * Definition for a binary tree node. * type TreeNode struct { * Val int * Left *TreeNode * Right *TreeNode * } */ func binaryTreePaths(root *TreeNode) []string { var result []string var path []int traversal(root,\u0026result,\u0026path) return result } func traversal(root *TreeNode,result *[]string,path *[]int){ *path=append(*path,root.Val) //判断是否为叶子节点 if root.Left==nil\u0026\u0026root.Right==nil{ pathStr:=strconv.Itoa((*path)[0]) for i:=1;i\u003clen(*path);i++{ pathStr=pathStr+\"-\u003e\"+strconv.Itoa((*path)[i]) } *result=append(*result,pathStr) return } //左右 if root.Left!=nil{ traversal(root.Left,result,path) *path=(*path)[:len(*path)-1]//回溯到上一个节点（因为traversal会加下一个节点值到path中） } if root.Right!=nil{ traversal(root.Right,result,path) *path=(*path)[:len(*path)-1]//回溯 } } ","date":"2022-01-06 08:21:45","objectID":"/algorithm_binarytree/:13:0","tags":["data structure"],"title":"Algorithm_binaryTree","uri":"/algorithm_binarytree/"},{"categories":["Coding"],"content":"左叶子之和 递归法 func sumOfLeftLeaves(root *TreeNode) int { var res int findLeft(root,\u0026res) return res } func findLeft(root *TreeNode,res *int){ //左节点 if root.Left!=nil\u0026\u0026root.Left.Left==nil\u0026\u0026root.Left.Right==nil{ *res=*res+root.Left.Val } if root.Left!=nil{ findLeft(root.Left,res) } if root.Right!=nil{ findLeft(root.Right,res) } } 迭代法 func sumOfLeftLeaves(root *TreeNode) int { var res int queue:=list.New() queue.PushBack(root) for queue.Len()\u003e0{ length:=queue.Len() for i:=0;i\u003clength;i++{ node:=queue.Remove(queue.Front()).(*TreeNode) if node.Left!=nil\u0026\u0026node.Left.Left==nil\u0026\u0026node.Left.Right==nil{ res=res+node.Left.Val } if node.Left!=nil{ queue.PushBack(node.Left) } if node.Right!=nil{ queue.PushBack(node.Right) } } } return res } ","date":"2022-01-06 08:21:45","objectID":"/algorithm_binarytree/:14:0","tags":["data structure"],"title":"Algorithm_binaryTree","uri":"/algorithm_binarytree/"},{"categories":["Coding"],"content":"找树左下角的值 给定一个二叉树，在树的最后一行找到最左边的值。 递归法： var maxDeep int // 全局变量 深度 var value int //全局变量 最终值 func findBottomLeftValue(root *TreeNode) int { if root.Left==nil\u0026\u0026root.Right==nil{//需要提前判断一下（不要这个if的话提交结果会出错，但执行代码不会。防止这种情况出现，故先判断是否只有一个节点） return root.Val } findLeftValue (root,maxDeep) return value } func findLeftValue (root *TreeNode,deep int){ //最左边的值在左边 if root.Left==nil\u0026\u0026root.Right==nil{ if deep\u003emaxDeep{ value=root.Val maxDeep=deep } } //递归 if root.Left!=nil{ deep++ findLeftValue(root.Left,deep) deep--//回溯 } if root.Right!=nil{ deep++ findLeftValue(root.Right,deep) deep--//回溯 } } 迭代法： func findBottomLeftValue(root *TreeNode) int { queue:=list.New() var gradation int queue.PushBack(root) for queue.Len()\u003e0{ length:=queue.Len() for i:=0;i\u003clength;i++{ node:=queue.Remove(queue.Front()).(*TreeNode) if i==0{gradation=node.Val} if node.Left!=nil{ queue.PushBack(node.Left) } if node.Right!=nil{ queue.PushBack(node.Right) } } } return gradation } ","date":"2022-01-06 08:21:45","objectID":"/algorithm_binarytree/:15:0","tags":["data structure"],"title":"Algorithm_binaryTree","uri":"/algorithm_binarytree/"},{"categories":["Coding"],"content":"路径总和 给定一个二叉树和一个目标和，判断该树中是否存在根节点到叶子节点的路径，这条路径上所有节点值相加等于目标和 路径总和 //递归法 /** * definition for a binary tree node. * type treenode struct { * val int * left *treenode * right *treenode * } */ func haspathsum(root *treenode, targetsum int) bool { var flage bool //找没找到的标志 if root==nil{ return flage } pathsum(root,0,targetsum,\u0026flage) return flage } func pathsum(root *treenode, sum int,targetsum int,flage *bool){ sum+=root.val if root.left==nil\u0026\u0026root.right==nil\u0026\u0026sum==targetsum{ *flage=true return } if root.left!=nil\u0026\u0026!(*flage){//左节点不为空且还没找到 pathsum(root.left,sum,targetsum,flage) } if root.right!=nil\u0026\u0026!(*flage){//右节点不为空且没找到 pathsum(root.right,sum,targetsum,flage) } } 113 递归法 /** * definition for a binary tree node. * type treenode struct { * val int * left *treenode * right *treenode * } */ func pathsum(root *treenode, targetsum int) [][]int { var result [][]int//最终结果 if root==nil{ return result } var sumnodes []int//经过路径的节点集合 haspathsum(root,\u0026sumnodes,targetsum,\u0026result) return result } func haspathsum(root *treenode,sumnodes *[]int,targetsum int,result *[][]int){ *sumnodes=append(*sumnodes,root.val) if root.left==nil\u0026\u0026root.right==nil{//叶子节点 fmt.println(*sumnodes) var sum int var number int for k,v:=range *sumnodes{//求该路径节点的和 sum+=v number=k } tempnodes:=make([]int,number+1)//新的nodes接受指针里的值，防止最终指针里的值发生变动，导致最后的结果都是最后一个sumnodes的值 for k,v:=range *sumnodes{ tempnodes[k]=v } if sum==targetsum{ *result=append(*result,tempnodes) } } if root.left!=nil{ haspathsum(root.left,sumnodes,targetsum,result) *sumnodes=(*sumnodes)[:len(*sumnodes)-1]//回溯 } if root.right!=nil{ haspathsum(root.right,sumnodes,targetsum,result) *sumnodes=(*sumnodes)[:len(*sumnodes)-1]//回溯 } } ","date":"2022-01-06 08:21:45","objectID":"/algorithm_binarytree/:16:0","tags":["data structure"],"title":"Algorithm_binaryTree","uri":"/algorithm_binarytree/"},{"categories":["Coding"],"content":"从中序、后序遍历序列构造二叉树 106 从中序与后序遍历序列构造二叉树 /** * Definition for a binary tree node. * type TreeNode struct { * Val int * Left *TreeNode * Right *TreeNode * } */ func buildTree(inorder []int, postorder []int) *TreeNode { if len(inorder)\u003c1||len(postorder)\u003c1{return nil} //先找到根节点（后续遍历的最后一个就是根节点） nodeValue:=postorder[len(postorder)-1] //从中序遍历中找到一分为二的点，左边为左子树，右边为右子树 left:=findRootIndex(inorder,nodeValue) //构造root root:=\u0026TreeNode{Val: nodeValue, Left: buildTree(inorder[:left],postorder[:left]),//将后续遍历一分为二，左边为左子树，右边为右子树 Right: buildTree(inorder[left+1:],postorder[left:len(postorder)-1])} return root } func findRootIndex(inorder []int,target int) (index int){ for i:=0;i\u003clen(inorder);i++{ if target==inorder[i]{ return i } } return -1 } 105 从前序与中序遍历序列构造二叉树 /** * Definition for a binary tree node. * type TreeNode struct { * Val int * Left *TreeNode * Right *TreeNode * } */ func buildTree(preorder []int, inorder []int) *TreeNode { if len(preorder)\u003c1||len(inorder)\u003c1{return nil} left:=findRootIndex(preorder[0],inorder) root:=\u0026TreeNode{ Val: preorder[0], Left: buildTree(preorder[1:left+1],inorder[:left]), Right: buildTree(preorder[left+1:],inorder[left+1:])} return root } func findRootIndex(target int,inorder []int) int{ for i:=0;i\u003clen(inorder);i++{ if target==inorder[i]{ return i } } return -1 } ","date":"2022-01-06 08:21:45","objectID":"/algorithm_binarytree/:17:0","tags":["data structure"],"title":"Algorithm_binaryTree","uri":"/algorithm_binarytree/"},{"categories":["Coding"],"content":"最大二叉树 给定一个不含重复元素的整数数组。一个以此数组构建的最大二叉树定义如下： 二叉树的根是数组中的最大元素。 左子树是通过数组中最大值左边部分构造出的最大二叉树。 右子树是通过数组中最大值右边部分构造出的最大二叉树。 /** * Definition for a binary tree node. * type TreeNode struct { * Val int * Left *TreeNode * Right *TreeNode * } */ func constructMaximumBinaryTree(nums []int) *TreeNode { if len(nums)\u003c1{return nil} //首选找到最大值 index:=findMax(nums) //其次构造二叉树 root:=\u0026TreeNode{ Val: nums[index], Left:constructMaximumBinaryTree(nums[:index]),//左半边 Right:constructMaximumBinaryTree(nums[index+1:]),//右半边 } return root } func findMax(nums []int) (index int){ for i:=0;i\u003clen(nums);i++{ if nums[i]\u003enums[index]{ index=i } } return } ","date":"2022-01-06 08:21:45","objectID":"/algorithm_binarytree/:18:0","tags":["data structure"],"title":"Algorithm_binaryTree","uri":"/algorithm_binarytree/"},{"categories":["Coding"],"content":"合并二叉树 /** * Definition for a binary tree node. * type TreeNode struct { * Val int * Left *TreeNode * Right *TreeNode * } */ //前序遍历（递归遍历，跟105 106差不多的思路） func mergeTrees(t1 *TreeNode, t2 *TreeNode) *TreeNode { var value int var nullNode *TreeNode//空node，便于遍历 nullNode=\u0026TreeNode{ Val:0, Left:nil, Right:nil} switch { case t1==nil\u0026\u0026t2==nil: return nil//终止条件 default : //如果其中一个节点为空，则将该节点置为nullNode，方便下次遍历 if t1==nil{ value=t2.Val t1=nullNode }else if t2==nil{ value=t1.Val t2=nullNode }else { value=t1.Val+t2.Val } } root:=\u0026TreeNode{//构造新的二叉树 Val: value, Left: mergeTrees(t1.Left,t2.Left), Right: mergeTrees(t1.Right,t2.Right)} return root } // 前序遍历简洁版 func mergeTrees(root1 *TreeNode, root2 *TreeNode) *TreeNode { if root1 == nil { return root2 } if root2 == nil { return root1 } root1.Val += root2.Val root1.Left = mergeTrees(root1.Left, root2.Left) root1.Right = mergeTrees(root1.Right, root2.Right) return root1 } // 迭代版本 func mergeTrees(root1 *TreeNode, root2 *TreeNode) *TreeNode { queue := make([]*TreeNode,0) if root1 == nil{ return root2 } if root2 == nil{ return root1 } queue = append(queue,root1) queue = append(queue,root2) for size:=len(queue);size\u003e0;size=len(queue){ node1 := queue[0] queue = queue[1:] node2 := queue[0] queue = queue[1:] node1.Val += node2.Val // 左子树都不为空 if node1.Left != nil \u0026\u0026 node2.Left != nil{ queue = append(queue,node1.Left) queue = append(queue,node2.Left) } // 右子树都不为空 if node1.Right !=nil \u0026\u0026 node2.Right !=nil{ queue = append(queue,node1.Right) queue = append(queue,node2.Right) } // 树 1 的左子树为 nil，直接接上树 2 的左子树 if node1.Left == nil{ node1.Left = node2.Left } // 树 1 的右子树为 nil，直接接上树 2 的右子树 if node1.Right == nil{ node1.Right = node2.Right } } return root1 } ","date":"2022-01-06 08:21:45","objectID":"/algorithm_binarytree/:19:0","tags":["data structure"],"title":"Algorithm_binaryTree","uri":"/algorithm_binarytree/"},{"categories":["Coding"],"content":"二叉搜索树 ","date":"2022-01-06 08:21:45","objectID":"/algorithm_binarytree/:20:0","tags":["data structure"],"title":"Algorithm_binaryTree","uri":"/algorithm_binarytree/"},{"categories":["Coding"],"content":"搜索 给定二叉搜索树（BST）的根节点和一个值。 你需要在BST中找到节点值等于给定值的节点。 返回以该节点为根的子树。 如果节点不存在，则返回 NULL。 递归法： //递归法 func searchBST(root *TreeNode, val int) *TreeNode { if root==nil||root.Val==val{ return root } if root.Val\u003eval{ return searchBST(root.Left,val) } return searchBST(root.Right,val) } 迭代法： //迭代法 func searchBST(root *TreeNode, val int) *TreeNode { for root!=nil{ if root.Val\u003eval{ root=root.Left }else if root.Val\u003cval{ root=root.Right }else{ break } } return root } ","date":"2022-01-06 08:21:45","objectID":"/algorithm_binarytree/:20:1","tags":["data structure"],"title":"Algorithm_binaryTree","uri":"/algorithm_binarytree/"},{"categories":["Coding"],"content":"验证 给定一个二叉树，判断其是否是一个有效的二叉搜索树。 假设一个二叉搜索树具有如下特征： 节点的左子树只包含小于当前节点的数。 节点的右子树只包含大于当前节点的数。 所有左子树和右子树自身必须也是二叉搜索树。 import \"math\" func isValidBST(root *TreeNode) bool { // 二叉搜索树也可以是空树 if root == nil { return true } // 由题目中的数据限制可以得出min和max return check(root,math.MinInt64,math.MaxInt64) } func check(node *TreeNode,min,max int64) bool { if node == nil { return true } if min \u003e= int64(node.Val) || max \u003c= int64(node.Val) { return false } // 分别对左子树和右子树递归判断，如果左子树和右子树都符合则返回true return check(node.Right,int64(node.Val),max) \u0026\u0026 check(node.Left,min,int64(node.Val)) } // 中序遍历解法 func isValidBST(root *TreeNode) bool { // 保存上一个指针 var prev *TreeNode var travel func(node *TreeNode) bool travel = func(node *TreeNode) bool { if node == nil { return true } leftRes := travel(node.Left) // 当前值小于等于前一个节点的值，返回false if prev != nil \u0026\u0026 node.Val \u003c= prev.Val { return false } prev = node rightRes := travel(node.Right) return leftRes \u0026\u0026 rightRes } return travel(root) } ","date":"2022-01-06 08:21:45","objectID":"/algorithm_binarytree/:20:2","tags":["data structure"],"title":"Algorithm_binaryTree","uri":"/algorithm_binarytree/"},{"categories":["Coding"],"content":"最小绝对差 给你一棵所有节点为非负值的二叉搜索树，请你计算树中任意两节点的差的绝对值的最小值。 中序遍历，然后计算最小差值 func getMinimumDifference(root *TreeNode) int { var res []int findMIn(root,\u0026res) min:=1000000//一个比较大的值 for i:=1;i\u003clen(res);i++{ tempValue:=res[i]-res[i-1] if tempValue\u003cmin{ min=tempValue } } return min } //中序遍历 func findMIn(root *TreeNode,res *[]int){ if root==nil{return} findMIn(root.Left,res) *res=append(*res,root.Val) findMIn(root.Right,res) } // 中序遍历的同时计算最小值 func getMinimumDifference(root *TreeNode) int { // 保留前一个节点的指针 var prev *TreeNode // 定义一个比较大的值 min := math.MaxInt64 var travel func(node *TreeNode) travel = func(node *TreeNode) { if node == nil { return } travel(node.Left) if prev != nil \u0026\u0026 node.Val - prev.Val \u003c min { min = node.Val - prev.Val } prev = node travel(node.Right) } travel(root) return min } ","date":"2022-01-06 08:21:45","objectID":"/algorithm_binarytree/:20:3","tags":["data structure"],"title":"Algorithm_binaryTree","uri":"/algorithm_binarytree/"},{"categories":["Coding"],"content":"众数 给定一个有相同值的二叉搜索树（BST），找出 BST 中的所有众数（出现频率最高的元素）。 暴力法 func findMode(root *TreeNode) []int { var history map[int]int var maxValue int var maxIndex int var result []int history=make(map[int]int) traversal(root,history) for k,value:=range history{ if value\u003emaxValue{ maxValue=value maxIndex=k } } for k,value:=range history{ if value==history[maxIndex]{ result=append(result,k) } } return result } func traversal(root *TreeNode,history map[int]int){ if root.Left!=nil{ traversal(root.Left,history) } if value,ok:=history[root.Val];ok{ history[root.Val]=value+1 }else{ history[root.Val]=1 } if root.Right!=nil{ traversal(root.Right,history) } } 计数法，不使用额外空间，利用二叉树性质，中序遍历 func findMode(root *TreeNode) []int { res := make([]int, 0) count := 1 max := 1 var prev *TreeNode var travel func(node *TreeNode) travel = func(node *TreeNode) { if node == nil { return } travel(node.Left) if prev != nil \u0026\u0026 prev.Val == node.Val { count++ } else { count = 1 } if count \u003e= max { if count \u003e max \u0026\u0026 len(res) \u003e 0 { res = []int{node.Val} } else { res = append(res, node.Val) } max = count } prev = node travel(node.Right) } travel(root) return res } ","date":"2022-01-06 08:21:45","objectID":"/algorithm_binarytree/:20:4","tags":["data structure"],"title":"Algorithm_binaryTree","uri":"/algorithm_binarytree/"},{"categories":["Coding"],"content":"二叉树最近公共祖先 给定一个二叉树, 找到该树中两个指定节点的最近公共祖先（可以是自己） func lowestCommonAncestor(root, p, q *TreeNode) *TreeNode { // check if root == nil { return root } // 相等 直接返回root节点即可 if root == p || root == q { return root } // Divide left := lowestCommonAncestor(root.Left, p, q) right := lowestCommonAncestor(root.Right, p, q) // Conquer // 左右两边都不为空，则根节点为祖先 if left != nil \u0026\u0026 right != nil { return root } if left != nil { return left } if right != nil { return right } return nil } ","date":"2022-01-06 08:21:45","objectID":"/algorithm_binarytree/:20:5","tags":["data structure"],"title":"Algorithm_binaryTree","uri":"/algorithm_binarytree/"},{"categories":["Coding"],"content":"二叉搜索树的最近公共祖先 递归法： //利用BSL的性质（前序遍历有序） func lowestCommonAncestor(root, p, q *TreeNode) *TreeNode { if root==nil{return nil} if root.Val\u003ep.Val\u0026\u0026root.Val\u003eq.Val{//当前节点的值大于给定的值，则说明满足条件的在左边 return lowestCommonAncestor(root.Left,p,q) }else if root.Val\u003cp.Val\u0026\u0026root.Val\u003cq.Val{//当前节点的值小于各点的值，则说明满足条件的在右边 return lowestCommonAncestor(root.Right,p,q) }else {return root}//当前节点的值在给定值的中间（或者等于），即为最深的祖先 } ","date":"2022-01-06 08:21:45","objectID":"/algorithm_binarytree/:20:6","tags":["data structure"],"title":"Algorithm_binaryTree","uri":"/algorithm_binarytree/"},{"categories":["Coding"],"content":"插入操作 给定二叉搜索树（BST）的根节点和要插入树中的值，将值插入二叉搜索树。 返回插入后二叉搜索树的根节点。 输入数据保证，新值和原始二叉搜索树中的任意节点值都不同。 递归法 func insertIntoBST(root *TreeNode, val int) *TreeNode { if root == nil { root = \u0026TreeNode{Val: val} return root } if root.Val \u003e val { root.Left = insertIntoBST(root.Left, val) } else { root.Right = insertIntoBST(root.Right, val) } return root } 迭代法 func insertIntoBST(root *TreeNode, val int) *TreeNode { if root == nil { return \u0026TreeNode{Val:val} } node := root var pnode *TreeNode for node != nil { if val \u003e node.Val { pnode = node node = node.Right } else { pnode = node node = node.Left } } if val \u003e pnode.Val { pnode.Right = \u0026TreeNode{Val: val} } else { pnode.Left = \u0026TreeNode{Val: val} } return root } ","date":"2022-01-06 08:21:45","objectID":"/algorithm_binarytree/:20:7","tags":["data structure"],"title":"Algorithm_binaryTree","uri":"/algorithm_binarytree/"},{"categories":["Coding"],"content":"删除节点 搜索树的节点删除要比节点增加复杂的多。 // 递归版本 func deleteNode(root *TreeNode, key int) *TreeNode { if root==nil{ return nil } if key\u003croot.Val{ root.Left=deleteNode(root.Left,key) return root } if key\u003eroot.Val{ root.Right=deleteNode(root.Right,key) return root } if root.Right==nil{ return root.Left } if root.Left==nil{ return root.Right } minnode:=root.Right for minnode.Left!=nil{ minnode=minnode.Left } root.Val=minnode.Val root.Right=deleteNode1(root.Right) return root } func deleteNode1(root *TreeNode)*TreeNode{ if root.Left==nil{ pRight:=root.Right root.Right=nil return pRight } root.Left=deleteNode1(root.Left) return root } // 迭代版本 func deleteOneNode(target *TreeNode) *TreeNode { if target == nil { return target } if target.Right == nil { return target.Left } cur := target.Right for cur.Left != nil { cur = cur.Left } cur.Left = target.Left return target.Right } func deleteNode(root *TreeNode, key int) *TreeNode { // 特殊情况处理 if root == nil { return root } cur := root var pre *TreeNode for cur != nil { if cur.Val == key { break } pre = cur if cur.Val \u003e key { cur = cur.Left } else { cur = cur.Right } } if pre == nil { return deleteOneNode(cur) } // pre 要知道是删除左孩子还有右孩子 if pre.Left != nil \u0026\u0026 pre.Left.Val == key { pre.Left = deleteOneNode(cur) } if pre.Right != nil \u0026\u0026 pre.Right.Val == key { pre.Right = deleteOneNode(cur) } return root } ","date":"2022-01-06 08:21:45","objectID":"/algorithm_binarytree/:20:8","tags":["data structure"],"title":"Algorithm_binaryTree","uri":"/algorithm_binarytree/"},{"categories":["Coding"],"content":"修剪 给定一个二叉搜索树，同时给定最小边界L 和最大边界 R。通过修剪二叉搜索树，使得所有节点的值在[L, R]中 (R\u003e=L) 。你可能需要改变树的根节点，所以结果应当返回修剪好的二叉搜索树的新的根节点。 // 递归 func trimBST(root *TreeNode, low int, high int) *TreeNode { if root==nil{ return nil } if root.Val\u003clow{//如果该节点值小于最小值，则该节点更换为该节点的右节点值，继续遍历 right:=trimBST(root.Right,low,high) return right } if root.Val\u003ehigh{//如果该节点的值大于最大值，则该节点更换为该节点的左节点值，继续遍历 left:=trimBST(root.Left,low,high) return left } root.Left=trimBST(root.Left,low,high) root.Right=trimBST(root.Right,low,high) return root } // 迭代 func trimBST(root *TreeNode, low int, high int) *TreeNode { if root == nil { return nil } // 处理 root，让 root 移动到[low, high] 范围内，**注意**是左闭右闭 for root != nil \u0026\u0026 (root.Val\u003clow||root.Val\u003ehigh){ if root.Val \u003c low{ root = root.Right }else{ root = root.Left } } cur := root // 此时 root 已经在[low, high] 范围内，处理左孩子元素小于 low 的情况（左节点是一定小于 root.Val，因此天然小于 high） for cur != nil{ for cur.Left!=nil \u0026\u0026 cur.Left.Val \u003c low{ cur.Left = cur.Left.Right } cur = cur.Left } cur = root // 此时 root 已经在[low, high] 范围内，处理右孩子大于 high 的情况 for cur != nil{ for cur.Right!=nil \u0026\u0026 cur.Right.Val \u003e high{ cur.Right = cur.Right.Left } cur = cur.Right } return root } ","date":"2022-01-06 08:21:45","objectID":"/algorithm_binarytree/:20:9","tags":["data structure"],"title":"Algorithm_binaryTree","uri":"/algorithm_binarytree/"},{"categories":["Coding"],"content":"将有序数组转化为二叉搜索树 将一个按照升序排列的有序数组，转换为一棵高度平衡二叉搜索树。 本题中，一个高度平衡二叉树是指一个二叉树每个节点 的左右两个子树的高度差的绝对值不超过 1。 递归（隐含回溯） func sortedArrayToBST(nums []int) *TreeNode { if len(nums)==0{return nil}//终止条件，最后数组为空则可以返回 root:=\u0026TreeNode{nums[len(nums)/2],nil,nil}//按照BSL的特点，从中间构造节点 root.Left=sortedArrayToBST(nums[:len(nums)/2])//数组的左边为左子树 root.Right=sortedArrayToBST(nums[len(nums)/2+1:])//数字的右边为右子树 return root } ","date":"2022-01-06 08:21:45","objectID":"/algorithm_binarytree/:20:10","tags":["data structure"],"title":"Algorithm_binaryTree","uri":"/algorithm_binarytree/"},{"categories":["Coding"],"content":"把二叉搜索树转化为累加树 给出二叉 搜索 树的根节点，该树的节点值各不相同，请你将其转换为累加树（Greater Sum Tree），使每个节点 node 的新值等于原树中大于或等于 node.val 的值之和。 弄一个sum暂存其和值 //右中左 func bstToGst(root *TreeNode) *TreeNode { var sum int RightMLeft(root,\u0026sum) return root } func RightMLeft(root *TreeNode,sum *int) *TreeNode { if root==nil{return nil}//终止条件，遇到空节点就返回 RightMLeft(root.Right,sum)//先遍历右边 temp:=*sum//暂存总和值 *sum+=root.Val//将总和值变更 root.Val+=temp//更新节点值 RightMLeft(root.Left,sum)//遍历左节点 return root } ","date":"2022-01-06 08:21:45","objectID":"/algorithm_binarytree/:20:11","tags":["data structure"],"title":"Algorithm_binaryTree","uri":"/algorithm_binarytree/"},{"categories":["Coding"],"content":" 学习代码随想录笔记 栈和队列 需要知道栈和队列的底层实现，不同编程语言不同STL的实现原理都是不尽相同的。 ","date":"2022-01-06 08:20:54","objectID":"/algorithm_stackandqueue/:0:0","tags":["data structure"],"title":"Algorithm_stackAndQueue","uri":"/algorithm_stackandqueue/"},{"categories":["Coding"],"content":"理论基础(c++) 栈其实就是递归的一种实现结构。 栈（本身可以说是一个容器适配器）是以底层容器（数组或者链表）完成其所有的工作，对外提供统一的接口，底层容器是可插拔的（也就是说我们可以控制使用哪种容器来实现栈的功能）。 SGI -- Silicon Graphics [Computer System] Inc. 硅图[计算机系统] 公司. STL -- Standard Template Library 标准模板库。 SGI STL -- SGI的标准模板库。 SGI的全称 -- 硅图[计算机系统] 公司。 我们常用的SGI STL，如果没有指定底层实现的话，默认是以deque为缺省情况下栈的低层结构。 deque是一个双向队列，只要封住一段，只开通另一端就可以实现栈的逻辑了。 SGI STL中 队列底层实现缺省情况下一样使用deque实现的。 我们也可以指定vector为栈的底层实现，初始化语句如下： std::stack\u003cint, std::vector\u003cint\u003e \u003e third; // 使用vector为底层容器的栈 对应的队列的情况是一样的。 队列中先进先出的数据结构，同样不允许有遍历行为，不提供迭代器, SGI STL中队列一样是以deque为缺省情况下的底部结构。 也可以指定list 为起底层实现，初始化queue的语句如下： std::queue\u003cint, std::list\u003cint\u003e\u003e third; // 定义以list为底层容器的队列 所以STL 队列也不被归类为容器，而被归类为container adapter（ 容器适配器）。 ","date":"2022-01-06 08:20:54","objectID":"/algorithm_stackandqueue/:1:0","tags":["data structure"],"title":"Algorithm_stackAndQueue","uri":"/algorithm_stackandqueue/"},{"categories":["Coding"],"content":"用栈实现队列 使用栈实现队列的下列操作： push(x) – 将一个元素放入队列的尾部。 pop() – 从队列首部移除元素。 peek() – 返回队列首部的元素。 empty() – 返回队列是否为空。 示例: MyQueue queue = new MyQueue(); queue.push(1); queue.push(2); queue.peek(); // 返回 1 queue.pop(); // 返回 1 queue.empty(); // 返回 false 说明: 你只能使用标准的栈操作 – 也就是只有 push to top, peek/pop from top, size, 和 is empty 操作是合法的。 你所使用的语言也许不支持栈。你可以使用 list 或者 deque（双端队列）来模拟一个栈，只要是标准的栈操作即可。 假设所有操作都是有效的 （例如，一个空的队列不会调用 pop 或者 peek 操作） type MyQueue struct { stack []int back []int } /** Initialize your data structure here. */ func Constructor() MyQueue { return MyQueue{ stack: make([]int, 0), back: make([]int, 0), } } /** Push element x to the back of queue. */ func (this *MyQueue) Push(x int) { for len(this.back) != 0 { val := this.back[len(this.back)-1] this.back = this.back[:len(this.back)-1] this.stack = append(this.stack, val) } this.stack = append(this.stack, x) } /** Removes the element from in front of queue and returns that element. */ func (this *MyQueue) Pop() int { for len(this.stack) != 0 { val := this.stack[len(this.stack)-1] this.stack = this.stack[:len(this.stack)-1] this.back = append(this.back, val) } if len(this.back) == 0 { return 0 } val := this.back[len(this.back)-1] this.back = this.back[:len(this.back)-1] return val } /** Get the front element. */ func (this *MyQueue) Peek() int { for len(this.stack) != 0 { val := this.stack[len(this.stack)-1] this.stack = this.stack[:len(this.stack)-1] this.back = append(this.back, val) } if len(this.back) == 0 { return 0 } val := this.back[len(this.back)-1] return val } /** Returns whether the queue is empty. */ func (this *MyQueue) Empty() bool { return len(this.stack) == 0 \u0026\u0026 len(this.back) == 0 } ","date":"2022-01-06 08:20:54","objectID":"/algorithm_stackandqueue/:2:0","tags":["data structure"],"title":"Algorithm_stackAndQueue","uri":"/algorithm_stackandqueue/"},{"categories":["Coding"],"content":"用队列实现栈 使用队列实现栈的下列操作： push(x) -- 元素 x 入栈 pop() -- 移除栈顶元素 top() -- 获取栈顶元素 empty() -- 返回栈是否为空 注意: 你只能使用队列的基本操作– 也就是 push to back, peek/pop from front, size, 和 is empty 这些操作是合法的。 你所使用的语言也许不支持队列。 你可以使用 list 或者 deque（双端队列）来模拟一个队列 , 只要是标准的队列操作即可。 你可以假设所有操作都是有效的（例如, 对一个空的栈不会调用 pop 或者 top 操作） 用两个队列实现： type MyStack struct { //创建两个队列 queue1 []int queue2 []int } func Constructor() MyStack { return MyStack{ //初始化 queue1:make([]int,0), queue2:make([]int,0), } } func (this *MyStack) Push(x int) { //先将数据存在queue2中 this.queue2 = append(this.queue2,x) //将queue1中所有元素移到queue2中，再将两个队列进行交换 this.Move() } func (this *MyStack) Move(){ if len(this.queue1) == 0{ //交换，queue1置为queue2,queue2置为空 this.queue1,this.queue2 = this.queue2,this.queue1 }else{ //queue1元素从头开始一个一个追加到queue2中 this.queue2 = append(this.queue2,this.queue1[0]) this.queue1 = this.queue1[1:] //去除第一个元素 this.Move() //重复 } } func (this *MyStack) Pop() int { val := this.queue1[0] this.queue1 = this.queue1[1:] //去除第一个元素 return val } func (this *MyStack) Top() int { return this.queue1[0] //直接返回 } func (this *MyStack) Empty() bool { return len(this.queue1) == 0 } /** * Your MyStack object will be instantiated and called as such: * obj := Constructor(); * obj.Push(x); * param_2 := obj.Pop(); * param_3 := obj.Top(); * param_4 := obj.Empty(); */ 用一个队列实现： type MyStack struct { queue []int//创建一个队列 } /** Initialize your data structure here. */ func Constructor() MyStack { return MyStack{ //初始化 queue:make([]int,0), } } /** Push element x onto stack. */ func (this *MyStack) Push(x int) { //添加元素 this.queue=append(this.queue,x) } /** Removes the element on top of the stack and returns that element. */ func (this *MyStack) Pop() int { n:=len(this.queue)-1//判断长度 for n!=0{ //除了最后一个，其余的都重新添加到队列里 val:=this.queue[0] this.queue=this.queue[1:] this.queue=append(this.queue,val) n-- } //弹出元素 val:=this.queue[0] this.queue=this.queue[1:] return val } /** Get the top element. */ func (this *MyStack) Top() int { //利用Pop函数，弹出来的元素重新添加 val:=this.Pop() this.queue=append(this.queue,val) return val } /** Returns whether the stack is empty. */ func (this *MyStack) Empty() bool { return len(this.queue)==0 } /** * Your MyStack object will be instantiated and called as such: * obj := Constructor(); * obj.Push(x); * param_2 := obj.Pop(); * param_3 := obj.Top(); * param_4 := obj.Empty(); */ ","date":"2022-01-06 08:20:54","objectID":"/algorithm_stackandqueue/:3:0","tags":["data structure"],"title":"Algorithm_stackAndQueue","uri":"/algorithm_stackandqueue/"},{"categories":["Coding"],"content":"有效的括号 给定一个只包括 ‘('，')'，'{'，'}'，'['，']’ 的字符串，判断字符串是否有效。 有效字符串需满足： 左括号必须用相同类型的右括号闭合。 左括号必须以正确的顺序闭合。 注意空字符串可被认为是有效字符串。 栈的经典问题。 func isValid(s string) bool { hash := map[byte]byte{')':'(', ']':'[', '}':'{'} stack := make([]byte, 0) if s == \"\" { return true } for i := 0; i \u003c len(s); i++ { if s[i] == '(' || s[i] == '[' || s[i] == '{' { stack = append(stack, s[i]) } else if len(stack) \u003e 0 \u0026\u0026 stack[len(stack)-1] == hash[s[i]] { stack = stack[:len(stack)-1] } else { return false } } return len(stack) == 0 } ","date":"2022-01-06 08:20:54","objectID":"/algorithm_stackandqueue/:4:0","tags":["data structure"],"title":"Algorithm_stackAndQueue","uri":"/algorithm_stackandqueue/"},{"categories":["Coding"],"content":"删除字符串中的所有相邻重复项 给出由小写字母组成的字符串 S，重复项删除操作会选择两个相邻且相同的字母，并删除它们。 在 S 上反复执行重复项删除操作，直到无法继续删除。 在完成所有重复项删除操作后返回最终的字符串。答案保证唯一。 示例： 输入：\"abbaca\" 输出：\"ca\" 解释：例如，在 \"abbaca\" 中，我们可以删除 \"bb\" 由于两字母相邻且相同，这是此时唯一可以执行删除操作的重复项。之后我们得到字符串 \"aaca\"，其中又只有 \"aa\" 可以执行重复项删除操作，所以最后的字符串为 \"ca\"。 提示： 1 \u003c= S.length \u003c= 20000 S 仅由小写英文字母组成 func removeDuplicates(s string) string { var stack []byte for i := 0; i \u003c len(s);i++ { // 栈不空 且 与栈顶元素不等 if len(stack) \u003e 0 \u0026\u0026 stack[len(stack)-1] == s[i] { // 弹出栈顶元素 并 忽略当前元素(s[i]) stack = stack[:len(stack)-1] }else{ // 入栈 stack = append(stack, s[i]) } } return string(stack) } ","date":"2022-01-06 08:20:54","objectID":"/algorithm_stackandqueue/:5:0","tags":["data structure"],"title":"Algorithm_stackAndQueue","uri":"/algorithm_stackandqueue/"},{"categories":["Coding"],"content":"逆波兰表达式求值 根据 逆波兰表示法，求表达式的值。 有效的运算符包括 + , - , * , / 。每个运算对象可以是整数，也可以是另一个逆波兰表达式。 说明： 整数除法只保留整数部分。 给定逆波兰表达式总是有效的。换句话说，表达式总会得出有效数值且不存在除数为 0 的情况。 示例 1： 输入: [\"2\", \"1\", \"+\", \"3\", \" * \"] 输出: 9 解释: 该算式转化为常见的中缀算术表达式为：((2 + 1) * 3) = 9 逆波兰表达式：是一种后缀表达式，所谓后缀就是指算符写在后面。 平常使用的算式则是一种中缀表达式，如 ( 1 + 2 ) * ( 3 + 4 ) 。 该算式的逆波兰表达式写法为 ( ( 1 2 + ) ( 3 4 + ) * ) 。 逆波兰表达式主要有以下两个优点： 去掉括号后表达式无歧义，上式即便写成 1 2 + 3 4 + * 也可以依据次序计算出正确结果。 适合用栈操作运算：遇到数字则入栈；遇到算符则取出栈顶两个数字进行计算，并将结果压入栈中 func evalRPN(tokens []string) int { stack := []int{} for _, token := range tokens { val, err := strconv.Atoi(token) if err == nil { stack = append(stack, val) } else { num1, num2 := stack[len(stack)-2], stack[(len(stack))-1] stack = stack[:len(stack)-2] switch token { case \"+\": stack = append(stack, num1+num2) case \"-\": stack = append(stack, num1-num2) case \"*\": stack = append(stack, num1*num2) case \"/\": stack = append(stack, num1/num2) } } } return stack[0] } ","date":"2022-01-06 08:20:54","objectID":"/algorithm_stackandqueue/:6:0","tags":["data structure"],"title":"Algorithm_stackAndQueue","uri":"/algorithm_stackandqueue/"},{"categories":["Coding"],"content":"滑动窗口最大值 给定一个数组 nums，有一个大小为 k 的滑动窗口从数组的最左侧移动到数组的最右侧。你只可以看到在滑动窗口内的 k 个数字。滑动窗口每次只向右移动一位。 每个窗口都有一个最大值，返回滑动窗口中的最大值数组。 进阶：你能在线性时间复杂度内解决此题吗？ // 封装单调队列的方式解题 type MyQueue struct { queue []int } func NewMyQueue() *MyQueue { return \u0026MyQueue{ queue: make([]int, 0), } } func (m *MyQueue) Front() int { return m.queue[0] } func (m *MyQueue) Back() int { return m.queue[len(m.queue)-1] } func (m *MyQueue) Empty() bool { return len(m.queue) == 0 } func (m *MyQueue) Push(val int) { for !m.Empty() \u0026\u0026 val \u003e m.Back() { m.queue = m.queue[:len(m.queue)-1] } m.queue = append(m.queue, val) } func (m *MyQueue) Pop(val int) { if !m.Empty() \u0026\u0026 val == m.Front() { m.queue = m.queue[1:] } } func maxSlidingWindow(nums []int, k int) []int { queue := NewMyQueue() length := len(nums) res := make([]int, 0) // 先将前k个元素放入队列 for i := 0; i \u003c k; i++ { queue.Push(nums[i]) } // 记录前k个元素的最大值 res = append(res, queue.Front()) for i := k; i \u003c length; i++ { // 滑动窗口移除最前面的元素 queue.Pop(nums[i-k]) // 滑动窗口添加最后面的元素 queue.Push(nums[i]) // 记录最大值 res = append(res, queue.Front()) } return res } ","date":"2022-01-06 08:20:54","objectID":"/algorithm_stackandqueue/:7:0","tags":["data structure"],"title":"Algorithm_stackAndQueue","uri":"/algorithm_stackandqueue/"},{"categories":["Coding"],"content":"前K个高频元素 给定一个非空的整数数组，返回其中出现频率前 k 高的元素。 示例 1: 输入: nums = [1,1,1,2,2,3], k = 2 输出: [1,2] //方法一：小顶堆 func topKFrequent(nums []int, k int) []int { map_num:=map[int]int{} //记录每个元素出现的次数 for _,item:=range nums{ map_num[item]++ } h:=\u0026IHeap{} heap.Init(h) //所有元素入堆，堆的长度为k for key,value:=range map_num{ heap.Push(h,[2]int{key,value}) if h.Len()\u003ek{ heap.Pop(h) } } res:=make([]int,k) //按顺序返回堆中的元素 for i:=0;i\u003ck;i++{ res[k-i-1]=heap.Pop(h).([2]int)[0] } return res } //构建小顶堆 type IHeap [][2]int func (h IHeap) Len()int { return len(h) } func (h IHeap) Less (i,j int) bool { return h[i][1]\u003ch[j][1] } func (h IHeap) Swap(i,j int) { h[i],h[j]=h[j],h[i] } func (h *IHeap) Push(x interface{}){ *h=append(*h,x.([2]int)) } func (h *IHeap) Pop() interface{}{ old:=*h n:=len(old) x:=old[n-1] *h=old[0:n-1] return x } //方法二:利用O(logn)排序 func topKFrequent(nums []int, k int) []int { ans:=[]int{} map_num:=map[int]int{} for _,item:=range nums { map_num[item]++ } for key,_:=range map_num{ ans=append(ans,key) } //核心思想：排序 //可以不用包函数，自己实现快排 sort.Slice(ans,func (a,b int)bool{ return map_num[ans[a]]\u003emap_num[ans[b]] }) return ans[:k] } ","date":"2022-01-06 08:20:54","objectID":"/algorithm_stackandqueue/:8:0","tags":["data structure"],"title":"Algorithm_stackAndQueue","uri":"/algorithm_stackandqueue/"},{"categories":["Coding"],"content":" 学习代码随想录笔记 双指针法 双指针法（快慢指针法）在数组和链表的操作中是非常常见的，很多考察数组、链表、字符串等操作的面试题，都使用双指针法。 ","date":"2022-01-06 08:19:19","objectID":"/algorithm_doublepointer/:0:0","tags":["data structure"],"title":"Algorithm_doublePointer","uri":"/algorithm_doublepointer/"},{"categories":["Coding"],"content":"移除元素 给你一个数组 nums 和一个值 val，你需要原地移除所有数值等于 val 的元素，并返回移除后数组的新长度。 不要使用额外的数组空间，你必须仅使用 $O(1)$ 额外空间并原地修改输入数组。 元素的顺序可以改变。你不需要考虑数组中超出新长度后面的元素。 示例 1: 给定 nums = [3,2,2,3], val = 3, 函数应该返回新的长度 2, 并且 nums 中的前两个元素均为 2。 你不需要考虑数组中超出新长度后面的元素。 示例 2: 给定 nums = [0,1,2,2,3,0,4,2], val = 2, 函数应该返回新的长度 5, 并且 nums 中的前五个元素为 0, 1, 3, 0, 4。 你不需要考虑数组中超出新长度后面的元素。 func removeElement(nums []int, val int) int { length:=len(nums) res:=0 for i:=0;i\u003clength;i++{ if nums[i]!=val { nums[res]=nums[i] res++ } } return res } ","date":"2022-01-06 08:19:19","objectID":"/algorithm_doublepointer/:1:0","tags":["data structure"],"title":"Algorithm_doublePointer","uri":"/algorithm_doublepointer/"},{"categories":["Coding"],"content":"反转字符串 func reverseString(s []byte) { left:=0 right:=len(s)-1 for left\u003cright{ s[left],s[right]=s[right],s[left] left++ right-- } } ","date":"2022-01-06 08:19:19","objectID":"/algorithm_doublepointer/:2:0","tags":["data structure"],"title":"Algorithm_doublePointer","uri":"/algorithm_doublepointer/"},{"categories":["Coding"],"content":"替换空格 // 遍历添加 func replaceSpace(s string) string { b := []byte(s) result := make([]byte, 0) for i := 0; i \u003c len(b); i++ { if b[i] == ' ' { result = append(result, []byte(\"%20\")...) } else { result = append(result, b[i]) } } return string(result) } // 原地修改 func replaceSpace(s string) string { b := []byte(s) length := len(b) spaceCount := 0 // 计算空格数量 for _, v := range b { if v == ' ' { spaceCount++ } } // 扩展原有切片 resizeCount := spaceCount * 2 tmp := make([]byte, resizeCount) b = append(b, tmp...) i := length - 1 j := len(b) - 1 for i \u003e= 0 { if b[i] != ' ' { b[j] = b[i] i-- j-- } else { b[j] = '0' b[j-1] = '2' b[j-2] = '%' i-- j = j - 3 } } return string(b) } ","date":"2022-01-06 08:19:19","objectID":"/algorithm_doublepointer/:3:0","tags":["data structure"],"title":"Algorithm_doublePointer","uri":"/algorithm_doublepointer/"},{"categories":["Coding"],"content":"翻转字符串里的单词 import ( \"fmt\" ) func reverseWords(s string) string { //1.使用双指针删除冗余的空格 slowIndex, fastIndex := 0, 0 b := []byte(s) //删除头部冗余空格 for len(b) \u003e 0 \u0026\u0026 fastIndex \u003c len(b) \u0026\u0026 b[fastIndex] == ' ' { fastIndex++ } //删除单词间冗余空格 for ; fastIndex \u003c len(b); fastIndex++ { if fastIndex-1 \u003e 0 \u0026\u0026 b[fastIndex-1] == b[fastIndex] \u0026\u0026 b[fastIndex] == ' ' { continue } b[slowIndex] = b[fastIndex] slowIndex++ } //删除尾部冗余空格 if slowIndex-1 \u003e 0 \u0026\u0026 b[slowIndex-1] == ' ' { b = b[:slowIndex-1] } else { b = b[:slowIndex] } //2.反转整个字符串 reverse(\u0026b, 0, len(b)-1) //3.反转单个单词 i单词开始位置，j单词结束位置 i := 0 for i \u003c len(b) { j := i for ; j \u003c len(b) \u0026\u0026 b[j] != ' '; j++ { } reverse(\u0026b, i, j-1) i = j i++ } return string(b) } func reverse(b *[]byte, left, right int) { for left \u003c right { (*b)[left], (*b)[right] = (*b)[right], (*b)[left] left++ right-- } } ","date":"2022-01-06 08:19:19","objectID":"/algorithm_doublepointer/:4:0","tags":["data structure"],"title":"Algorithm_doublePointer","uri":"/algorithm_doublepointer/"},{"categories":["Coding"],"content":"翻转链表 //双指针 func reverseList(head *ListNode) *ListNode { var pre *ListNode cur := head for cur != nil { next := cur.Next cur.Next = pre pre = cur cur = next } return pre } //递归 func reverseList(head *ListNode) *ListNode { return help(nil, head) } func help(pre, head *ListNode)*ListNode{ if head == nil { return pre } next := head.Next head.Next = pre return help(head, next) } ","date":"2022-01-06 08:19:19","objectID":"/algorithm_doublepointer/:5:0","tags":["data structure"],"title":"Algorithm_doublePointer","uri":"/algorithm_doublepointer/"},{"categories":["Coding"],"content":"删除链表的倒数第N个节点 /** * Definition for singly-linked list. * type ListNode struct { * Val int * Next *ListNode * } */ func removeNthFromEnd(head *ListNode, n int) *ListNode { dummyHead := \u0026ListNode{} dummyHead.Next = head cur := head prev := dummyHead i := 1 for cur != nil { cur = cur.Next if i \u003e n { prev = prev.Next } i++ } prev.Next = prev.Next.Next return dummyHead.Next } ","date":"2022-01-06 08:19:19","objectID":"/algorithm_doublepointer/:6:0","tags":["data structure"],"title":"Algorithm_doublePointer","uri":"/algorithm_doublepointer/"},{"categories":["Coding"],"content":"链表相交 给你两个单链表的头节点 headA 和 headB ，请你找出并返回两个单链表相交的起始节点。如果两个链表没有交点，返回 null 。 func getIntersectionNode(headA, headB *ListNode) *ListNode { curA := headA curB := headB lenA, lenB := 0, 0 // 求A，B的长度 for curA != nil { curA = curA.Next lenA++ } for curB != nil { curB = curB.Next lenB++ } var step int var fast, slow *ListNode // 请求长度差，并且让更长的链表先走相差的长度 if lenA \u003e lenB { step = lenA - lenB fast, slow = headA, headB } else { step = lenB - lenA fast, slow = headB, headA } for i:=0; i \u003c step; i++ { fast = fast.Next } // 遍历两个链表遇到相同则跳出遍历 for fast != slow { fast = fast.Next slow = slow.Next } return fast } ","date":"2022-01-06 08:19:19","objectID":"/algorithm_doublepointer/:7:0","tags":["data structure"],"title":"Algorithm_doublePointer","uri":"/algorithm_doublepointer/"},{"categories":["Coding"],"content":"环形链表II 题意： 给定一个链表，返回链表开始入环的第一个节点。 如果链表无环，则返回 null。 func detectCycle(head *ListNode) *ListNode { slow, fast := head, head for fast != nil \u0026\u0026 fast.Next != nil { slow = slow.Next fast = fast.Next.Next if slow == fast { for slow != head { slow = slow.Next head = head.Next } return head } } return nil } ","date":"2022-01-06 08:19:19","objectID":"/algorithm_doublepointer/:8:0","tags":["data structure"],"title":"Algorithm_doublePointer","uri":"/algorithm_doublepointer/"},{"categories":["Coding"],"content":"三数之和 给你一个包含 n 个整数的数组 nums，判断 nums 中是否存在三个元素 a，b，c ，使得 a + b + c = 0 ？请你找出所有满足条件且不重复的三元组。 注意： 答案中不可以包含重复的三元组。 示例： 给定数组 nums = [-1, 0, 1, 2, -1, -4]， 满足要求的三元组集合为： [ [-1, 0, 1], [-1, -1, 2] ] func threeSum(nums []int)[][]int{ sort.Ints(nums) res:=[][]int{} for i:=0;i\u003clen(nums)-2;i++{ n1:=nums[i] if n1\u003e0{ break } if i\u003e0\u0026\u0026n1==nums[i-1]{ continue } l,r:=i+1,len(nums)-1 for l\u003cr{ n2,n3:=nums[l],nums[r] if n1+n2+n3==0{ res=append(res,[]int{n1,n2,n3}) for l\u003cr\u0026\u0026nums[l]==n2{ l++ } for l\u003cr\u0026\u0026nums[r]==n3{ r-- } }else if n1+n2+n3\u003c0{ l++ }else { r-- } } } return res } ","date":"2022-01-06 08:19:19","objectID":"/algorithm_doublepointer/:9:0","tags":["data structure"],"title":"Algorithm_doublePointer","uri":"/algorithm_doublepointer/"},{"categories":["Coding"],"content":"四数之和 题意：给定一个包含 n 个整数的数组 nums 和一个目标值 target，判断 nums 中是否存在四个元素 a，b，c 和 d ，使得 a + b + c + d 的值与 target 相等？找出所有满足条件且不重复的四元组。 注意： 答案中不可以包含重复的四元组。 示例： 给定数组 nums = [1, 0, -1, 0, -2, 2]，和 target = 0。 满足要求的四元组集合为： [ [-1, 0, 0, 1], [-2, -1, 1, 2], [-2, 0, 0, 2] ] func fourSum(nums []int, target int) [][]int { if len(nums) \u003c 4 { return nil } sort.Ints(nums) var res [][]int for i := 0; i \u003c len(nums)-3; i++ { n1 := nums[i] // if n1 \u003e target { // 不能这样写,因为可能是负数 // break // } if i \u003e 0 \u0026\u0026 n1 == nums[i-1] { continue } for j := i + 1; j \u003c len(nums)-2; j++ { n2 := nums[j] if j \u003e i+1 \u0026\u0026 n2 == nums[j-1] { continue } l := j + 1 r := len(nums) - 1 for l \u003c r { n3 := nums[l] n4 := nums[r] sum := n1 + n2 + n3 + n4 if sum \u003c target { l++ } else if sum \u003e target { r-- } else { res = append(res, []int{n1, n2, n3, n4}) for l \u003c r \u0026\u0026 n3 == nums[l+1] { // 去重 l++ } for l \u003c r \u0026\u0026 n4 == nums[r-1] { // 去重 r-- } // 找到答案时,双指针同时靠近 r-- l++ } } } } return res } ","date":"2022-01-06 08:19:19","objectID":"/algorithm_doublepointer/:10:0","tags":["data structure"],"title":"Algorithm_doublePointer","uri":"/algorithm_doublepointer/"},{"categories":["Coding"],"content":"双指针总结 总结 ","date":"2022-01-06 08:19:19","objectID":"/algorithm_doublepointer/:11:0","tags":["data structure"],"title":"Algorithm_doublePointer","uri":"/algorithm_doublepointer/"},{"categories":["Coding"],"content":" 学习代码随想录笔记 字符串 库函数是工具，基础更重要。 ","date":"2022-01-06 08:18:50","objectID":"/algorithm_string/:0:0","tags":["data structure"],"title":"Algorithm_string","uri":"/algorithm_string/"},{"categories":["Coding"],"content":"反转字符串 ","date":"2022-01-06 08:18:50","objectID":"/algorithm_string/:1:0","tags":["data structure"],"title":"Algorithm_string","uri":"/algorithm_string/"},{"categories":["Coding"],"content":"I 和反转链表相同，都用双指针法。 func reverseString(s []byte) { left:=0 right:=len(s)-1 for left\u003cright{ s[left],s[right]=s[right],s[left] left++ right-- } } ","date":"2022-01-06 08:18:50","objectID":"/algorithm_string/:1:1","tags":["data structure"],"title":"Algorithm_string","uri":"/algorithm_string/"},{"categories":["Coding"],"content":"II 给定一个字符串 s 和一个整数 k，你需要对从字符串开头算起的每隔 2k 个字符的前 k 个字符进行反转。 如果剩余字符少于 k 个，则将剩余字符全部反转。 如果剩余字符小于 2k 但大于或等于 k 个，则反转前 k 个字符，其余字符保持原样。 示例: 输入: s = \"abcdefg\", k = 2 输出: \"bacdfeg\" func reverseStr(s string, k int) string { ss := []byte(s) length := len(s) for i := 0; i \u003c length; i += 2 * k { if i + k \u003c= length { reverse(ss[i:i+k]) } else { reverse(ss[i:length]) } } return string(ss) } func reverse(b []byte) { left := 0 right := len(b) - 1 for left \u003c right { b[left], b[right] = b[right], b[left] left++ right-- } } ","date":"2022-01-06 08:18:50","objectID":"/algorithm_string/:1:2","tags":["data structure"],"title":"Algorithm_string","uri":"/algorithm_string/"},{"categories":["Coding"],"content":"替换空格 // 遍历添加 func replaceSpace(s string) string { b := []byte(s) result := make([]byte, 0) for i := 0; i \u003c len(b); i++ { if b[i] == ' ' { result = append(result, []byte(\"%20\")...) } else { result = append(result, b[i]) } } return string(result) } // 原地修改 func replaceSpace(s string) string { b := []byte(s) length := len(b) spaceCount := 0 // 计算空格数量 for _, v := range b { if v == ' ' { spaceCount++ } } // 扩展原有切片 resizeCount := spaceCount * 2 tmp := make([]byte, resizeCount) b = append(b, tmp...) i := length - 1 j := len(b) - 1 for i \u003e= 0 { if b[i] != ' ' { b[j] = b[i] i-- j-- } else { b[j] = '0' b[j-1] = '2' b[j-2] = '%' i-- j = j - 3 } } return string(b) } ","date":"2022-01-06 08:18:50","objectID":"/algorithm_string/:2:0","tags":["data structure"],"title":"Algorithm_string","uri":"/algorithm_string/"},{"categories":["Coding"],"content":"翻转字符串里的单词 给定一个字符串，逐个翻转字符串中的每个单词。 输入: \"the sky is blue\" 输出: \"blue is sky the\" import ( \"fmt\" ) func reverseWords(s string) string { //1.使用双指针删除冗余的空格 slowIndex, fastIndex := 0, 0 b := []byte(s) //删除头部冗余空格 for len(b) \u003e 0 \u0026\u0026 fastIndex \u003c len(b) \u0026\u0026 b[fastIndex] == ' ' { fastIndex++ } //删除单词间冗余空格 for ; fastIndex \u003c len(b); fastIndex++ { if fastIndex-1 \u003e 0 \u0026\u0026 b[fastIndex-1] == b[fastIndex] \u0026\u0026 b[fastIndex] == ' ' { continue } b[slowIndex] = b[fastIndex] slowIndex++ } //删除尾部冗余空格 if slowIndex-1 \u003e 0 \u0026\u0026 b[slowIndex-1] == ' ' { b = b[:slowIndex-1] } else { b = b[:slowIndex] } //2.反转整个字符串 reverse(\u0026b, 0, len(b)-1) //3.反转单个单词 i单词开始位置，j单词结束位置 i := 0 for i \u003c len(b) { j := i for ; j \u003c len(b) \u0026\u0026 b[j] != ' '; j++ { } reverse(\u0026b, i, j-1) i = j i++ } return string(b) } func reverse(b *[]byte, left, right int) { for left \u003c right { (*b)[left], (*b)[right] = (*b)[right], (*b)[left] left++ right-- } } ","date":"2022-01-06 08:18:50","objectID":"/algorithm_string/:3:0","tags":["data structure"],"title":"Algorithm_string","uri":"/algorithm_string/"},{"categories":["Coding"],"content":"左旋转字符串 输入: s = \"abcdefg\", k = 2 输出: \"cdefgab\" func reverseLeftWords(s string, n int) string { b := []byte(s) // 1. 反转前n个字符 // 2. 反转第n到end字符 // 3. 反转整个字符 reverse(b, 0, n-1) reverse(b, n, len(b)-1) reverse(b, 0, len(b)-1) return string(b) } // 切片是引用传递 func reverse(b []byte, left, right int){ for left \u003c right{ b[left], b[right] = b[right],b[left] left++ right-- } } ","date":"2022-01-06 08:18:50","objectID":"/algorithm_string/:4:0","tags":["data structure"],"title":"Algorithm_string","uri":"/algorithm_string/"},{"categories":["Coding"],"content":"实现strStr() 给定一个 haystack 字符串和一个 needle 字符串，在 haystack 字符串中找出 needle 字符串出现的第一个位置 (从0开始)。如果不存在，则返回 -1。 示例 1: 输入: haystack = “hello”, needle = “ll” 输出: 2 示例 2: 输入: haystack = “aaaaa”, needle = “bba” 输出: -1 说明: 当 needle 是空字符串时，我们应当返回什么值呢？这是一个在面试中很好的问题。 对于本题而言，当 needle 是空字符串时我们应当返回 0 。这与C语言的 strstr() 以及 Java的 indexOf() 定义相符。 ","date":"2022-01-06 08:18:50","objectID":"/algorithm_string/:5:0","tags":["data structure"],"title":"Algorithm_string","uri":"/algorithm_string/"},{"categories":["Coding"],"content":"什么是KMP KMP算法是一种改进的字符串匹配算法，由D.E.Knuth，J.H.Morris和V.R.Pratt提出的，因此人们称它为克努特—莫里斯—普拉特操作（简称KMP算法）。KMP算法的核心是利用匹配失败后的信息，尽量减少模式串与主串的匹配次数以达到快速匹配的目的。具体实现就是通过一个next()函数实现，函数本身包含了模式串的局部匹配信息。KMP算法的时间复杂度O(m+n) [1] 。（来自百度百科） KMP的经典思想就是:当出现字符串不匹配时，可以记录一部分之前已经匹配的文本内容，利用这些信息避免从头再去做匹配。 如何记录已经匹配的文本内容，是KMP的重点，也是next数组的任务。 ","date":"2022-01-06 08:18:50","objectID":"/algorithm_string/:5:1","tags":["data structure"],"title":"Algorithm_string","uri":"/algorithm_string/"},{"categories":["Coding"],"content":"什么是前缀表与next数组 next数组就是一个前缀表。 前缀表是用来回退的，它记录了模式串与主串(文本串)不匹配的时候，模式串应该从哪里开始重新匹配。 什么是前缀表：记录下标i之前（包括i）的字符串中，有多大长度的相同前缀后缀。 最长公共前后缀：字符串aa的最长相等前后缀为1。 字符串aaa的最长相等前后缀为2。 等等….. 前缀表要求的就是最长相同前后缀的长度。它能告诉我们上次匹配的位置。 下标5之前这部分的字符串（也就是字符串aabaa）的最长相等的前缀和后缀字符串是子字符串aa ，因为找到了最长相等的前缀和后缀，匹配失败的位置是后缀子串的后面，那么我们找到与其相同的前缀的后面从新匹配就可以了。 前缀表与next数组有什么关系： next数组即可以就是前缀表，也可以是前缀表统一减一（右移一位，初始位置为-1）。 ","date":"2022-01-06 08:18:50","objectID":"/algorithm_string/:5:2","tags":["data structure"],"title":"Algorithm_string","uri":"/algorithm_string/"},{"categories":["Coding"],"content":"前缀表统一减一之后的next数组： 时间复杂度分析： 其中n为文本串长度，m为模式串长度，因为在匹配的过程中，根据前缀表不断调整匹配的位置，可以看出匹配的过程是$O(n)$，之前还要单独生成next数组，时间复杂度是$O(m)$。所以整个KMP算法的时间复杂度是$O(n+m)$的。 暴力的解法显而易见是$O(n × m)$，所以KMP在字符串匹配中极大的提高的搜索的效率。 为了和力扣题目28.实现strStr保持一致，方便大家理解，以下文章统称haystack为文本串, needle为模式串。 都知道使用KMP算法，一定要构造next数组 构造next数组： 我们定义一个函数getNext来构建next数组，函数参数为指向next数组的指针，和一个字符串。 代码如下： void getNext(int* next, const string\u0026 s) 构造next数组其实就是计算模式串s，前缀表的过程。 主要有如下三步： 初始化 定义两个指针i和j，j指向前缀起始位置，i指向后缀起始位置。 然后还要对next数组进行初始化赋值，如下： int j = -1; next[0] = j; j 为什么要初始化为 -1呢，因为之前说过 前缀表要统一减一的操作仅仅是其中的一种实现，我们这里选择j初始化为-1，下文我还会给出j不初始化为-1的实现代码。 next[i] 表示 i（包括i）之前最长相等的前后缀长度（其实就是j） 所以初始化next[0] = j 。 处理前后缀不相同的情况 因为j初始化为-1，那么i就从1开始，进行s[i] 与 s[j+1]的比较。 所以遍历模式串s的循环下标i 要从 1开始，代码如下： for(int i = 1; i \u003c s.size(); i++) { 如果 s[i] 与 s[j+1]不相同，也就是遇到 前后缀末尾不相同的情况，就要向前回退。 怎么回退呢？ next[j]就是记录着j（包括j）之前的子串的相同前后缀的长度。 那么 s[i] 与 s[j+1] 不相同，就要找 j+1前一个元素在next数组里的值（就是next[j]）。 所以，处理前后缀不相同的情况代码如下： while (j \u003e= 0 \u0026\u0026 s[i] != s[j + 1]) { // 前后缀不相同了 j = next[j]; // 向前回退 } 处理前后缀相同的情况 如果s[i] 与 s[j + 1] 相同，那么就同时向后移动i 和j 说明找到了相同的前后缀，同时还要将j（前缀的长度）赋给next[i], 因为next[i]要记录相同前后缀的长度。 代码如下： if (s[i] == s[j + 1]) { // 找到相同的前后缀 j++; } next[i] = j; 最后整体构建next数组的函数代码如下： void getNext(int* next, const string\u0026 s){ int j = -1; next[0] = j; for(int i = 1; i \u003c s.size(); i++) { // **注意**i从1开始 while (j \u003e= 0 \u0026\u0026 s[i] != s[j + 1]) { // 前后缀不相同了 j = next[j]; // 向前回退 } if (s[i] == s[j + 1]) { // 找到相同的前后缀 j++; } next[i] = j; // 将j（前缀的长度）赋给next[i] } } 使用next数组进行匹配 在文本串s里 找是否出现过模式串t。 定义两个下标j 指向模式串起始位置，i指向文本串起始位置。 那么j初始值依然为-1，为什么呢？ 依然因为next数组里记录的起始位置为-1。 i就从0开始，遍历文本串，代码如下： for (int i = 0; i \u003c s.size(); i++) 接下来就是 s[i] 与 t[j + 1] （因为j从-1开始的） 进行比较。 如果 s[i] 与 t[j + 1] 不相同，j就要从next数组里寻找下一个匹配的位置。 代码如下： while(j \u003e= 0 \u0026\u0026 s[i] != t[j + 1]) { j = next[j]; } 如果 s[i] 与 t[j + 1] 相同，那么i 和 j 同时向后移动， 代码如下： if (s[i] == t[j + 1]) { j++; // i的增加在for循环里 } 如何判断在文本串s里出现了模式串t呢，如果j指向了模式串t的末尾，那么就说明模式串t完全匹配文本串s里的某个子串了。 本题要在文本串字符串中找出模式串出现的第一个位置 (从0开始)，所以返回当前在文本串匹配模式串的位置i 减去 模式串的长度，就是文本串字符串中出现模式串的第一个位置。 代码如下： if (j == (t.size() - 1) ) { return (i - t.size() + 1); } 那么使用next数组，用模式串匹配文本串的整体代码如下： int j = -1; // 因为next数组里记录的起始位置为-1 for (int i = 0; i \u003c s.size(); i++) { // **注意**i就从0开始 while(j \u003e= 0 \u0026\u0026 s[i] != t[j + 1]) { // 不匹配 j = next[j]; // j 寻找之前匹配的位置 } if (s[i] == t[j + 1]) { // 匹配，j和i同时向后移动 j++; // i的增加在for循环里 } if (j == (t.size() - 1) ) { // 文本串s里出现了模式串t return (i - t.size() + 1); } } 此时所有逻辑的代码都已经写出来了，力扣 28.实现strStr 题目的整体代码如下： class Solution { public: void getNext(int* next, const string\u0026 s) { int j = -1; next[0] = j; for(int i = 1; i \u003c s.size(); i++) { // **注意**i从1开始 while (j \u003e= 0 \u0026\u0026 s[i] != s[j + 1]) { // 前后缀不相同了 j = next[j]; // 向前回退 } if (s[i] == s[j + 1]) { // 找到相同的前后缀 j++; } next[i] = j; // 将j（前缀的长度）赋给next[i] } } int strStr(string haystack, string needle) { if (needle.size() == 0) { return 0; } int next[needle.size()]; getNext(next, needle); int j = -1; // // 因为next数组里记录的起始位置为-1 for (int i = 0; i \u003c haystack.size(); i++) { // **注意**i就从0开始 while(j \u003e= 0 \u0026\u0026 haystack[i] != needle[j + 1]) { // 不匹配 j = next[j]; // j 寻找之前匹配的位置 } if (haystack[i] == needle[j + 1]) { // 匹配，j和i同时向后移动 j++; // i的增加在for循环里 } if (j == (needle.size() - 1) ) { // 文本串s里出现了模式串t return (i - needle.size() + 1); } } return -1; } }; ","date":"2022-01-06 08:18:50","objectID":"/algorithm_string/:5:3","tags":["data structure"],"title":"Algorithm_string","uri":"/algorithm_string/"},{"categories":["Coding"],"content":"前缀表不减一的next数组： 直接使用前缀表可以换一种回退方式，找j=next[j-1] 来进行回退。 主要就是j=next[x]这一步最为关键！ 构建next数组： void getNext(int* next, const string\u0026 s) { int j = 0; next[0] = 0; for(int i = 1; i \u003c s.size(); i++) { while (j \u003e 0 \u0026\u0026 s[i] != s[j]) { // j要保证大于0，因为下面有取j-1作为数组下标的操作 j = next[j - 1]; // **注意**这里，是要找前一位的对应的回退位置了 } if (s[i] == s[j]) { j++; } next[i] = j; } } 使用next数组进行匹配： class Solution { public: void getNext(int* next, const string\u0026 s) { int j = 0; next[0] = 0; for(int i = 1; i \u003c s.size(); i++) { while (j \u003e 0 \u0026\u0026 s[i] != s[j]) { j = next[j - 1]; } if (s[i] == s[j]) { j++; } next[i] = j; } } int strStr(string haystack, string needle) { if (needle.size() == 0) { return 0; } int next[needle.size()]; getNext(next, needle); int j = 0; for (int i = 0; i \u003c haystack.size(); i++) { while(j \u003e 0 \u0026\u0026 haystack[i] != needle[j]) { j = next[j - 1]; } if (haystack[i] == needle[j]) { j++; } if (j == needle.size() ) { return (i - needle.size() + 1); } } return -1; } }; ","date":"2022-01-06 08:18:50","objectID":"/algorithm_string/:5:4","tags":["data structure"],"title":"Algorithm_string","uri":"/algorithm_string/"},{"categories":["Coding"],"content":"kmp总结 介绍了什么是KMP，KMP可以解决什么问题，然后分析KMP算法里的next数组，知道了next数组就是前缀表，再分析为什么要是前缀表而不是什么其他表。 接着从给出的模式串中，我们一步一步的推导出了前缀表，得出前缀表无论是统一减一还是不减一得到的next数组仅仅是kmp的实现方式的不同。 其中还分析了KMP算法的时间复杂度，并且和暴力方法做了对比。 然后先用前缀表统一减一得到的next数组，求得文本串s里是否出现过模式串t，并给出了具体分析代码。 又给出了直接用前缀表作为next数组，来做匹配的实现代码。 可以说把KMP的每一个细微的细节都扣了出来，毫无遮掩的展示给大家了 go实现： // 方法一:前缀表使用减1实现 // getNext 构造前缀表next // params: // next 前缀表数组 // s 模式串 func getNext(next []int, s string) { j := -1 // j表示 最长相等前后缀长度 next[0] = j for i := 1; i \u003c len(s); i++ { for j \u003e= 0 \u0026\u0026 s[i] != s[j+1] { j = next[j] // 回退前一位 } if s[i] == s[j+1] { j++ } next[i] = j // next[i]是i（包括i）之前的最长相等前后缀长度 } } func strStr(haystack string, needle string) int { if len(needle) == 0 { return 0 } next := make([]int, len(needle)) getNext(next, needle) j := -1 // 模式串的起始位置 next为-1 因此也为-1 for i := 0; i \u003c len(haystack); i++ { for j \u003e= 0 \u0026\u0026 haystack[i] != needle[j+1] { j = next[j] // 寻找下一个匹配点 } if haystack[i] == needle[j+1] { j++ } if j == len(needle)-1 { // j指向了模式串的末尾 return i - len(needle) + 1 } } return -1 } // 方法二: 前缀表无减一或者右移 // getNext 构造前缀表next // params: // next 前缀表数组 // s 模式串 func getNext(next []int, s string) { j := 0 next[0] = j for i := 1; i \u003c len(s); i++ { for j \u003e 0 \u0026\u0026 s[i] != s[j] { j = next[j-1] } if s[i] == s[j] { j++ } next[i] = j } } func strStr(haystack string, needle string) int { n := len(needle) if n == 0 { return 0 } j := 0 next := make([]int, n) getNext(next, needle) for i := 0; i \u003c len(haystack); i++ { for j \u003e 0 \u0026\u0026 haystack[i] != needle[j] { j = next[j-1] // 回退到j的前一位 } if haystack[i] == needle[j] { j++ } if j == n { return i - n + 1 } } return -1 } ","date":"2022-01-06 08:18:50","objectID":"/algorithm_string/:5:5","tags":["data structure"],"title":"Algorithm_string","uri":"/algorithm_string/"},{"categories":["Coding"],"content":"重复的子字符串 给定一个非空的字符串，判断它是否可以由它的一个子串重复多次构成。给定的字符串只含有小写英文字母，并且长度不超过10000。 示例 1: 输入: \"abab\" 输出: True 解释: 可由子字符串 \"ab\" 重复两次构成。 示例 2: 输入: \"aba\" 输出: False 示例 3: 输入: \"abcabcabcabc\" 输出: True 解释: 可由子字符串 \"abc\" 重复四次构成。 (或者子字符串 \"abcabc\" 重复两次构成。) 标准的KMP题目~ 数组长度减去最长相同前后缀的长度相当于是第一个周期的长度，也就是一个周期的长度，如果这个周期可以被整除，就说明整个数组就是这个周期的循环。 强烈建议大家把next数组打印出来，看看next数组里的规律，有助于理解KMP算法 代码实现： //前缀表统一减一的实现 func repeatedSubstringPattern(s string) bool { n := len(s) if n == 0 { return false } next := make([]int, n) j := -1 next[0] = j for i := 1; i \u003c n; i++ { for j \u003e= 0 \u0026\u0026 s[i] != s[j+1] { j = next[j] } if s[i] == s[j+1] { j++ } next[i] = j } // next[n-1]+1 最长相同前后缀的长度 if next[n-1] != -1 \u0026\u0026 n%(n-(next[n-1]+1)) == 0 { return true } return false } //前缀表不减一 func repeatedSubstringPattern(s string) bool { n := len(s) if n == 0 { return false } j := 0 next := make([]int, n) next[0] = j for i := 1; i \u003c n; i++ { for j \u003e 0 \u0026\u0026 s[i] != s[j] { j = next[j-1] } if s[i] == s[j] { j++ } next[i] = j } // next[n-1] 最长相同前后缀的长度 if next[n-1] != 0 \u0026\u0026 n%(n-next[n-1]) == 0 { return true } return false } ","date":"2022-01-06 08:18:50","objectID":"/algorithm_string/:6:0","tags":["data structure"],"title":"Algorithm_string","uri":"/algorithm_string/"},{"categories":["Coding"],"content":" 学习代码随想录笔记 哈希表 ","date":"2022-01-06 08:18:30","objectID":"/algorithm_hashtable/:0:0","tags":["data structure"],"title":"Algorithm_hashTable","uri":"/algorithm_hashtable/"},{"categories":["Coding"],"content":"哈希表理论基础 哈希表是根据关键码的值而直接进行访问的数据结构，比如数组、map（映射）、set（集合）。 一般用来快速判断一个元素是否出现在集合里。 比如把字符串映射为索引的例子： 通过哈希函数/hashCode将字符串转化为数值 如果得到的数值大于哈希表的大小了，取模，保证所有字符串映射到哈希表上。 如果字符串数量都大于哈希表的大小了，会出现同一索引不同字符串的情况。也称，哈希碰撞。 解决哈希碰撞： 拉链法 发生冲突的元素用链表存储 要选择适当的哈希表的大小，这样既不会因为数组空值而浪费大量内存，也不会因为链表太长而在查找上浪费太多时间。 线性探测法 保证tableSize大于dataSize，避免碰撞。 ","date":"2022-01-06 08:18:30","objectID":"/algorithm_hashtable/:1:0","tags":["data structure"],"title":"Algorithm_hashTable","uri":"/algorithm_hashtable/"},{"categories":["Coding"],"content":"有效的字母异位词 给定两个字符串 s 和 t ，编写一个函数来判断 t 是否是 s 的字母异位词。 示例 1: 输入: s = “anagram”, t = “nagaram” 输出: true 示例 2: 输入: s = “rat”, t = “car” 输出: false 说明: 你可以假设字符串只包含小写字母。 func isAnagram(s string, t string) bool { if len(s)!=len(t){ return false } exists := make(map[byte]int) for i:=0;i\u003clen(s);i++{ if v,ok:=exists[s[i]];v\u003e=0\u0026\u0026ok{ exists[s[i]]=v+1 }else{ exists[s[i]]=1 } } for i:=0;i\u003clen(t);i++{ if v,ok:=exists[t[i]];v\u003e=1\u0026\u0026ok{ exists[t[i]]=v-1 }else{ return false } } return true } ","date":"2022-01-06 08:18:30","objectID":"/algorithm_hashtable/:2:0","tags":["data structure"],"title":"Algorithm_hashTable","uri":"/algorithm_hashtable/"},{"categories":["Coding"],"content":"两个数组的交集 使用数组来做哈希的题目，是因为题目都限制了数值的大小。 而这道题目没有限制数值的大小，就无法使用数组来做哈希表了。 func intersection(nums1 []int, nums2 []int) []int { m := make(map[int]int) for _, v := range nums1 { m[v] = 1 } var res []int // 利用count\u003e0，实现重复值只拿一次放入返回结果中 for _, v := range nums2 { if count, ok := m[v]; ok \u0026\u0026 count \u003e 0 { res = append(res, v) m[v]-- } } return res } //优化版，利用set，减少count统计 func intersection(nums1 []int, nums2 []int) []int { set:=make(map[int]struct{},0) res:=make([]int,0) for _,v:=range nums1{ if _,ok:=set[v];!ok{ set[v]=struct{}{} } } for _,v:=range nums2{ //如果存在于上一个数组中，则加入结果集，并清空该set值 if _,ok:=set[v];ok{ res=append(res,v) delete(set, v) } } return res } ","date":"2022-01-06 08:18:30","objectID":"/algorithm_hashtable/:3:0","tags":["data structure"],"title":"Algorithm_hashTable","uri":"/algorithm_hashtable/"},{"categories":["Coding"],"content":"快乐数 编写一个算法来判断一个数 n 是不是快乐数。 「快乐数」定义为：对于一个正整数，每一次将该数替换为它每个位置上的数字的平方和，然后重复这个过程直到这个数变为 1，也可能是 无限循环 但始终变不到 1。如果 可以变为 1，那么这个数就是快乐数。 如果 n 是快乐数就返回 True ；不是，则返回 False 。 示例： 输入：19 输出：true 解释： 1^2 + 9^2 = 82 8^2 + 2^2 = 68 6^2 + 8^2 = 100 1^2 + 0^2 + 0^2 = 1 func isHappy(n int) bool { m := make(map[int]bool) for n != 1 \u0026\u0026 !m[n] { n, m[n] = getSum(n), true } return n == 1 } func getSum(n int) int { sum := 0 for n \u003e 0 { sum += (n % 10) * (n % 10) n = n / 10 } return sum } ","date":"2022-01-06 08:18:30","objectID":"/algorithm_hashtable/:4:0","tags":["data structure"],"title":"Algorithm_hashTable","uri":"/algorithm_hashtable/"},{"categories":["Coding"],"content":"两数之和 给定一个整数数组 nums 和一个目标值 target，请你在该数组中找出和为目标值的那 两个 整数，并返回他们的数组下标。 你可以假设每种输入只会对应一个答案。但是，数组中同一个元素不能使用两遍。 示例: 给定 nums = [2, 7, 11, 15], target = 9 因为 nums[0] + nums[1] = 2 + 7 = 9 所以返回 [0, 1] func twoSum(nums []int, target int) []int { for k1, _ := range nums { for k2 := k1 + 1; k2 \u003c len(nums); k2++ { if target == nums[k1] + nums[k2] { return []int{k1, k2} } } } return []int{} } // 使用map方式解题，降低时间复杂度 func twoSum(nums []int, target int) []int { m := make(map[int]int) for index, val := range nums { if preIndex, ok := m[target-val]; ok { return []int{preIndex, index} } else { m[val] = index } } return []int{} } ","date":"2022-01-06 08:18:30","objectID":"/algorithm_hashtable/:5:0","tags":["data structure"],"title":"Algorithm_hashTable","uri":"/algorithm_hashtable/"},{"categories":["Coding"],"content":"四数相加II 给定四个包含整数的数组列表 A , B , C , D ,计算有多少个元组 (i, j, k, l) ，使得 A[i] + B[j] + C[k] + D[l] = 0。 为了使问题简单化，所有的 A, B, C, D 具有相同的长度 N，且 0 ≤ N ≤ 500 。所有整数的范围在 -2^28 到 2^28 - 1 之间，最终结果不会超过 2^31 - 1 。 例如: 输入: A = [ 1, 2] B = [-2,-1] C = [-1, 2] D = [ 0, 2] 输出: 2 解释: 两个元组如下: (0, 0, 0, 1) -\u003e A[0] + B[0] + C[0] + D[1] = 1 + (-2) + (-1) + 2 = 0 (1, 1, 0, 0) -\u003e A[1] + B[1] + C[0] + D[0] = 2 + (-1) + (-1) + 0 = 0 func fourSumCount(nums1 []int, nums2 []int, nums3 []int, nums4 []int) int { m := make(map[int]int) count := 0 for _, v1 := range nums1 { for _, v2 := range nums2 { m[v1+v2]++ } } for _, v3 := range nums3 { for _, v4 := range nums4 { count += m[-v3-v4] } } return count } ","date":"2022-01-06 08:18:30","objectID":"/algorithm_hashtable/:6:0","tags":["data structure"],"title":"Algorithm_hashTable","uri":"/algorithm_hashtable/"},{"categories":["Coding"],"content":"赎金信 给定一个赎金信 (ransom) 字符串和一个杂志(magazine)字符串，判断第一个字符串 ransom 能不能由第二个字符串 magazines 里面的字符构成。如果可以构成，返回 true ；否则返回 false。 (题目说明：为了不暴露赎金信字迹，要从杂志上搜索各个需要的字母，组成单词来表达意思。杂志字符串中的每个字符只能在赎金信字符串中使用一次。) 注意： 你可以假设两个字符串均只含有小写字母。 canConstruct(“a”, “b”) -\u003e false canConstruct(“aa”, “ab”) -\u003e false canConstruct(“aa”, “aab”) -\u003e true func canConstruct(ransomNote string, magazine string) bool { record := make([]int, 26) for _, v := range magazine { record[v-'a']++ } for _, v := range ransomNote { record[v-'a']-- if record[v-'a'] \u003c 0 { return false } } return true } ","date":"2022-01-06 08:18:30","objectID":"/algorithm_hashtable/:7:0","tags":["data structure"],"title":"Algorithm_hashTable","uri":"/algorithm_hashtable/"},{"categories":["Coding"],"content":"三数之和 给你一个包含 n 个整数的数组 nums，判断 nums 中是否存在三个元素 a，b，c ，使得 a + b + c = 0 ？请你找出所有满足条件且不重复的三元组。 注意： 答案中不可以包含重复的三元组。 示例： 给定数组 nums = [-1, 0, 1, 2, -1, -4]， 满足要求的三元组集合为： [ [-1, 0, 1], [-1, -1, 2] ] 解法：哈希、双指针（更高效） func threeSum(nums []int)[][]int{ sort.Ints(nums) res:=[][]int{} for i:=0;i\u003clen(nums)-2;i++{ n1:=nums[i] if n1\u003e0{ break } if i\u003e0\u0026\u0026n1==nums[i-1]{ continue } l,r:=i+1,len(nums)-1 for l\u003cr{ n2,n3:=nums[l],nums[r] if n1+n2+n3==0{ res=append(res,[]int{n1,n2,n3}) for l\u003cr\u0026\u0026nums[l]==n2{ l++ } for l\u003cr\u0026\u0026nums[r]==n3{ r-- } }else if n1+n2+n3\u003c0{ l++ }else { r-- } } } return res } ","date":"2022-01-06 08:18:30","objectID":"/algorithm_hashtable/:8:0","tags":["data structure"],"title":"Algorithm_hashTable","uri":"/algorithm_hashtable/"},{"categories":["Coding"],"content":"四数之和 题意：给定一个包含 n 个整数的数组 nums 和一个目标值 target，判断 nums 中是否存在四个元素 a，b，c 和 d ，使得 a + b + c + d 的值与 target 相等？找出所有满足条件且不重复的四元组。 注意： 答案中不可以包含重复的四元组。 示例： 给定数组 nums = [1, 0, -1, 0, -2, 2]，和 target = 0。 满足要求的四元组集合为： [ [-1, 0, 0, 1], [-2, -1, 1, 2], [-2, 0, 0, 2] ] func fourSum(nums []int, target int) [][]int { if len(nums) \u003c 4 { return nil } sort.Ints(nums) var res [][]int for i := 0; i \u003c len(nums)-3; i++ { n1 := nums[i] // if n1 \u003e target { // 不能这样写,因为可能是负数 // break // } if i \u003e 0 \u0026\u0026 n1 == nums[i-1] { continue } for j := i + 1; j \u003c len(nums)-2; j++ { n2 := nums[j] if j \u003e i+1 \u0026\u0026 n2 == nums[j-1] { continue } l := j + 1 r := len(nums) - 1 for l \u003c r { n3 := nums[l] n4 := nums[r] sum := n1 + n2 + n3 + n4 if sum \u003c target { l++ } else if sum \u003e target { r-- } else { res = append(res, []int{n1, n2, n3, n4}) for l \u003c r \u0026\u0026 n3 == nums[l+1] { // 去重 l++ } for l \u003c r \u0026\u0026 n4 == nums[r-1] { // 去重 r-- } // 找到答案时,双指针同时靠近 r-- l++ } } } } return res } ","date":"2022-01-06 08:18:30","objectID":"/algorithm_hashtable/:9:0","tags":["data structure"],"title":"Algorithm_hashTable","uri":"/algorithm_hashtable/"},{"categories":["Coding"],"content":" 学习代码随想录笔记 链表 ","date":"2022-01-06 08:18:06","objectID":"/algorithm_linkedlist/:0:0","tags":["data structure"],"title":"Algorithm_linkedList","uri":"/algorithm_linkedlist/"},{"categories":["Coding"],"content":"链表理论基础 循环链表可以用来解决约瑟夫环问题。 链表和数组对比： 数组 插入删除时间复杂度：$O(n)$ 查询时间复杂度：$O(1)$ 适用场景：数据量固定，频繁查询，较少增删 链表 插入删除时间复杂度：$O(1)$ 查询时间复杂度：$O(n)$ 适用场景：数据量不固定，频繁增删，较少查询 ","date":"2022-01-06 08:18:06","objectID":"/algorithm_linkedlist/:1:0","tags":["data structure"],"title":"Algorithm_linkedList","uri":"/algorithm_linkedlist/"},{"categories":["Coding"],"content":"移除链表元素 /** * Definition for singly-linked list. * type ListNode struct { * Val int * Next *ListNode * } */ func removeElements(head *ListNode, val int) *ListNode { dummyHead := \u0026ListNode{} dummyHead.Next = head cur := dummyHead for cur != nil \u0026\u0026 cur.Next != nil { if cur.Next.Val == val { cur.Next = cur.Next.Next } else { cur = cur.Next } } return dummyHead.Next } //如果是c或者c++要在删除后free或者delete掉节点释放内存，java,python,go都会自动回收 另外，移除头节点的话，如果没有虚拟头节点，将头节点往后移一个节点，有虚拟头节点就一般删除节点即可。 ","date":"2022-01-06 08:18:06","objectID":"/algorithm_linkedlist/:2:0","tags":["data structure"],"title":"Algorithm_linkedList","uri":"/algorithm_linkedlist/"},{"categories":["Coding"],"content":"设计链表 设计五个接口： 获取链表第index个节点的数值 在链表的最前面插入一个节点 在链表的最后面插入一个节点 在链表第index个节点前面插入一个节点 删除链表的第index个节点 链表操作的两种方式： 直接使用原来的链表来进行操作。 设置一个虚拟头结点在进行操作。（更方便一点） //循环双链表 type MyLinkedList struct { dummy *Node } type Node struct { Val int Next *Node Pre *Node } //仅保存哑节点，pre-\u003e rear, next-\u003e head /** Initialize your data structure here. */ func Constructor() MyLinkedList { rear := \u0026Node{ Val: -1, Next: nil, Pre: nil, } rear.Next = rear rear.Pre = rear return MyLinkedList{rear} } /** Get the value of the index-th node in the linked list. If the index is invalid, return -1. */ func (this *MyLinkedList) Get(index int) int { head := this.dummy.Next //head == this, 遍历完全 for head != this.dummy \u0026\u0026 index \u003e 0 { index-- head = head.Next } //否则, head == this, 索引无效 if 0 != index { return -1 } return head.Val } /** Add a node of value val before the first element of the linked list. After the insertion, the new node will be the first node of the linked list. */ func (this *MyLinkedList) AddAtHead(val int) { dummy := this.dummy node := \u0026Node{ Val: val, //head.Next指向原头节点 Next: dummy.Next, //head.Pre 指向哑节点 Pre: dummy, } //更新原头节点 dummy.Next.Pre = node //更新哑节点 dummy.Next = node //以上两步不能反 } /** Append a node of value val to the last element of the linked list. */ func (this *MyLinkedList) AddAtTail(val int) { dummy := this.dummy rear := \u0026Node{ Val: val, //rear.Next = dummy(哑节点) Next: dummy, //rear.Pre = ori_rear Pre: dummy.Pre, } //ori_rear.Next = rear dummy.Pre.Next = rear //update dummy dummy.Pre = rear //以上两步不能反 } /** Add a node of value val before the index-th node in the linked list. If index equals to the length of linked list, the node will be appended to the end of linked list. If index is greater than the length, the node will not be inserted. */ func (this *MyLinkedList) AddAtIndex(index int, val int) { head := this.dummy.Next //head = MyLinkedList[index] for head != this.dummy \u0026\u0026 index \u003e 0 { head = head.Next index-- } if index \u003e 0 { return } node := \u0026Node{ Val: val, //node.Next = MyLinkedList[index] Next: head, //node.Pre = MyLinkedList[index-1] Pre: head.Pre, } //MyLinkedList[index-1].Next = node head.Pre.Next = node //MyLinkedList[index].Pre = node head.Pre = node //以上两步不能反 } /** Delete the index-th node in the linked list, if the index is valid. */ func (this *MyLinkedList) DeleteAtIndex(index int) { //链表为空 if this.dummy.Next == this.dummy { return } head := this.dummy.Next //head = MyLinkedList[index] for head.Next != this.dummy \u0026\u0026 index \u003e 0 { head = head.Next index-- } //验证index有效 if index == 0 { //MyLinkedList[index].Pre = index[index-2] head.Next.Pre = head.Pre //MyLinedList[index-2].Next = index[index] head.Pre.Next = head.Next //以上两步顺序无所谓 } } ","date":"2022-01-06 08:18:06","objectID":"/algorithm_linkedlist/:3:0","tags":["data structure"],"title":"Algorithm_linkedList","uri":"/algorithm_linkedlist/"},{"categories":["Coding"],"content":"翻转链表 //双指针 func reverseList(head *ListNode) *ListNode { var pre *ListNode cur := head for cur != nil { next := cur.Next cur.Next = pre pre = cur cur = next } return pre } //递归 func reverseList(head *ListNode) *ListNode { return help(nil, head) } func help(pre, head *ListNode)*ListNode{ if head == nil { return pre } next := head.Next head.Next = pre return help(head, next) } ","date":"2022-01-06 08:18:06","objectID":"/algorithm_linkedlist/:4:0","tags":["data structure"],"title":"Algorithm_linkedList","uri":"/algorithm_linkedlist/"},{"categories":["Coding"],"content":"两两交换链表中的节点 正常模拟，使用虚拟头节点。 func swapPairs(head *ListNode) *ListNode { dummy := \u0026ListNode{ Next: head, } //head=list[i] //pre=list[i-1] pre := dummy for head != nil \u0026\u0026 head.Next != nil { pre.Next = head.Next next := head.Next.Next head.Next.Next = head head.Next = next //pre=list[(i+2)-1] pre = head //head=list[(i+2)] head = next } return dummy.Next } // 递归版本 func swapPairs(head *ListNode) *ListNode { if head == nil || head.Next == nil { return head } next := head.Next head.Next = swapPairs(next.Next) next.Next = head return next } ","date":"2022-01-06 08:18:06","objectID":"/algorithm_linkedlist/:5:0","tags":["data structure"],"title":"Algorithm_linkedList","uri":"/algorithm_linkedlist/"},{"categories":["Coding"],"content":"删除链表的倒数第N个节点 双指针很好做： /** * Definition for singly-linked list. * type ListNode struct { * Val int * Next *ListNode * } */ func removeNthFromEnd(head *ListNode, n int) *ListNode { dummyHead := \u0026ListNode{} dummyHead.Next = head cur := head prev := dummyHead i := 1 for cur != nil { cur = cur.Next if i \u003e n { prev = prev.Next } i++ } prev.Next = prev.Next.Next return dummyHead.Next } ","date":"2022-01-06 08:18:06","objectID":"/algorithm_linkedlist/:6:0","tags":["data structure"],"title":"Algorithm_linkedList","uri":"/algorithm_linkedlist/"},{"categories":["Coding"],"content":"链表相交 func getIntersectionNode(headA, headB *ListNode) *ListNode { curA := headA curB := headB lenA, lenB := 0, 0 // 求A，B的长度 for curA != nil { curA = curA.Next lenA++ } for curB != nil { curB = curB.Next lenB++ } var step int var fast, slow *ListNode // 请求长度差，并且让更长的链表先走相差的长度 if lenA \u003e lenB { step = lenA - lenB fast, slow = headA, headB } else { step = lenB - lenA fast, slow = headB, headA } for i:=0; i \u003c step; i++ { fast = fast.Next } // 遍历两个链表遇到相同则跳出遍历 for fast != slow { fast = fast.Next slow = slow.Next } return fast } ","date":"2022-01-06 08:18:06","objectID":"/algorithm_linkedlist/:7:0","tags":["data structure"],"title":"Algorithm_linkedList","uri":"/algorithm_linkedlist/"},{"categories":["Coding"],"content":"环形链表II 第一次做有点难~ 判断是否有环：快慢指针法 如何确定环的入口：一个简单的数学题 func detectCycle(head *ListNode) *ListNode { slow, fast := head, head for fast != nil \u0026\u0026 fast.Next != nil { slow = slow.Next fast = fast.Next.Next if slow == fast { for slow != head { slow = slow.Next head = head.Next } return head } } return nil } ","date":"2022-01-06 08:18:06","objectID":"/algorithm_linkedlist/:8:0","tags":["data structure"],"title":"Algorithm_linkedList","uri":"/algorithm_linkedlist/"},{"categories":["Coding"],"content":"如何实现LRU缓存淘汰算法? 缓存是一种提高数据读取性能的技术，在硬件设计、软件开发中都有着非常广泛的应用，比如常见的 CPU 缓存、数据库缓存、浏览器缓存等等。 缓存的大小有限，当缓存被用满时，哪些数据应该被清理出去，哪些数据应该被保留？这就需要缓存淘汰策略来决定。常见的策略有三种：先进先出策略 FIFO（First In，First Out）、最少使用策略 LFU（Least Frequently Used）、最近最少使用策略 LRU（Least Recently Used）。 ","date":"2022-01-06 08:18:06","objectID":"/algorithm_linkedlist/:9:0","tags":["data structure"],"title":"Algorithm_linkedList","uri":"/algorithm_linkedlist/"},{"categories":["Coding"],"content":"链表结构 我们先从底层的存储结构上来看一看。从图中我们看到，数组需要一块连续的内存空间来存储，对内存的要求比较高。如果我们申请一个 100MB 大小的数组，当内存中没有连续的、足够大的存储空间时，即便内存的剩余总可用空间大于 100MB，仍然会申请失败。而链表恰恰相反，它并不需要一块连续的内存空间，它通过“指针”将一组零散的内存块串联起来使用，所以如果我们申请的是 100MB 大小的链表，根本不会有问题。 链表结构五花八门，今天我重点给你介绍三种最常见的链表结构，它们分别是：单链表、双向链表和循环链表。我们首先来看最简单、最常用的单链表。 其中有两个结点是比较特殊的，它们分别是第一个结点和最后一个结点。我们习惯性地把第一个结点叫作头结点，把最后一个结点叫作尾结点。其中，头结点用来记录链表的基地址。有了它，我们就可以遍历得到整条链表。而尾结点特殊的地方是：指针不是指向下一个结点，而是指向一个空地址 NULL，表示这是链表上最后一个结点。 循环链表是一种特殊的单链表。实际上，循环链表也很简单。它跟单链表唯一的区别就在尾结点。我们知道，单链表的尾结点指针指向空地址，表示这就是最后的结点了。而循环链表的尾结点指针是指向链表的头结点。从我画的循环链表图中，你应该可以看出来，它像一个环一样首尾相连，所以叫作“循环”链表。 接下来我们再来看一个稍微复杂的，在实际的软件开发中，也更加常用的链表结构：双向链表。 相比单链表，双向链表适合解决哪种问题呢？ 从结构上来看，双向链表可以支持 O(1) 时间复杂度的情况下找到前驱结点，正是这样的特点，也使双向链表在某些情况下的插入、删除等操作都要比单链表简单、高效。 对于执行较慢的程序，可以通过消耗更多的内存（空间换时间）来进行优化；而消耗过多内存的程序，可以通过消耗更多的时间（时间换空间）来降低内存的消耗 如何基于链表实现 LRU 缓存淘汰算法？我的思路是这样的：我们维护一个有序单链表，越靠近链表尾部的结点是越早之前访问的。当有一个新的数据被访问时，我们从链表头开始顺序遍历链表。 如果此数据之前已经被缓存在链表中了，我们遍历得到这个数据对应的结点，并将其从原来的位置删除，然后再插入到链表的头部。 如果此数据没有在缓存链表中，又可以分为两种情况： 如果此时缓存未满，则将此结点直接插入到链表的头部；如果此时缓存已满，则链表尾结点删除，将新的数据结点插入链表的头部。 现在我们来看下缓存访问的时间复杂度是多少。因为不管缓存有没有满，我们都需要遍历一遍链表，所以这种基于链表的实现思路，缓存访问的时间复杂度为 O(n)。实际上，我们可以继续优化这个实现思路，比如引入散列表（Hash table）来记录每个数据的位置，将缓存访问的时间复杂度降到 O(1)。因为要涉及我们还没有讲到的数据结构，所以这个优化方案，我现在就不详细说了，等讲到散列表的时候，我会再拿出来讲。 除了基于链表的实现思路，实际上还可以用数组来实现 LRU 缓存淘汰策略。如何利用数组实现 LRU 缓存淘汰策略呢？ ","date":"2022-01-06 08:18:06","objectID":"/algorithm_linkedlist/:10:0","tags":["data structure"],"title":"Algorithm_linkedList","uri":"/algorithm_linkedlist/"},{"categories":["Coding"],"content":" 学习代码随想录笔记 数组 ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:0:0","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"数组基础 数组是存放在连续内存空间上的相同类型数据的集合。 因为数组的在内存空间的地址是连续的，所以我们在删除或者增添元素的时候，就难免要移动其他元素的地址。 数组元素无法删除，只能覆盖。 ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:1:0","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"二分查找 有序数组、无重复元素便可想到用二分法查找。注意区间左闭右闭还是左闭右开。 简单实现： func search(nums []int, target int) int { left := 0 right := len(nums) - 1 for left \u003c= right { middle := left + (right - left)/2 if nums[middle] \u003e target { right = middle - 1 }else if nums[middle] \u003c target { left = middle + 1 }else{ return middle; } } return -1 } func search(nums []int,t int)int{ left,middle:=0 right:=len(nums) for left\u003c=right{ num=(left+right)/2 if nums[num]==t{ return num } if nums[num]\u003et{ right=num-1 }else{ left=num+1 } } } func search(nums []int,t int)int{ if middle:=len(nums)/2;nums[middle]==t{ return middle } if len(nums)==1{ return -1 } if nums[middle]\u003et{ return search(nums[:middle],t) }else{ return search(nums[middle+1:]) } } func recureionSearch(nums []int,t int) ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:2:0","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"移除元素 双指针法（快慢指针法）： 通过一个快指针和慢指针在一个for循环下完成两个for循环的工作。 func removeElement(nums []int, val int) int { length:=len(nums) res:=0 for i:=0;i\u003clength;i++{ if nums[i]!=val { nums[res]=nums[i] res++ } } return res } func removeElement(nums []int, val int) int { num := 0 for i, v := range nums { if v == val { num++ } else { nums[i-num] = v } } return len(nums)-num } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:3:0","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"有序数组的平方 双指针法： func sortedSquares(nums []int) []int { n := len(nums) i, j, k := 0, n-1, n-1 ans := make([]int, n) for i \u003c= j { lm, rm := nums[i]*nums[i], nums[j]*nums[j] if lm \u003e rm { ans[k] = lm i++ } else { ans[k] = rm j-- } k-- } return ans } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:4:0","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"长度最小的子数组 给定一个含有 n 个正整数的数组和一个正整数 s ，找出该数组中满足其和 ≥ s 的长度最小的 连续 子数组，并返回其长度。如果不存在符合条件的子数组，返回 0。 滑动窗口 func minSubArrayLen(target int, nums []int) int { i := 0 l := len(nums) // 数组长度 sum := 0 // 子数组之和 result := l + 1 // 初始化返回长度为l+1，目的是为了判断“不存在符合条件的子数组，返回0”的情况 for j := 0; j \u003c l; j++ { sum += nums[j] for sum \u003e= target { subLength := j - i + 1 if subLength \u003c result { result = subLength } sum -= nums[i] i++ } } if result == l+1 { return 0 } else { return result } } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:5:0","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"螺旋矩阵II 模拟行为： func generateMatrix(n int) [][]int { top, bottom := 0, n-1 left, right := 0, n-1 num := 1 tar := n * n matrix := make([][]int, n) for i := 0; i \u003c n; i++ { matrix[i] = make([]int, n) } for num \u003c= tar { for i := left; i \u003c= right; i++ { matrix[top][i] = num num++ } top++ for i := top; i \u003c= bottom; i++ { matrix[i][right] = num num++ } right-- for i := right; i \u003e= left; i-- { matrix[bottom][i] = num num++ } bottom-- for i := bottom; i \u003e= top; i-- { matrix[i][left] = num num++ } left++ } return matrix } ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:6:0","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"总结 ","date":"2022-01-06 08:17:33","objectID":"/algorithm_array/:7:0","tags":["data structure"],"title":"Algorithm_array","uri":"/algorithm_array/"},{"categories":["Coding"],"content":"算法性能分析 ","date":"2022-01-02 19:28:23","objectID":"/performance_analysis/:0:0","tags":["data structure"],"title":"Performance_analysis","uri":"/performance_analysis/"},{"categories":["Coding"],"content":"03 | 复杂度分析（上）：如何分析、统计算法的执行效率和资源消耗？ 复杂度分析是整个算法学习的精髓，只要掌握了它，数据结构和算法的内容基本上就掌握了一半。 ","date":"2022-01-02 19:28:23","objectID":"/performance_analysis/:1:0","tags":["data structure"],"title":"Performance_analysis","uri":"/performance_analysis/"},{"categories":["Coding"],"content":"为什么需要复杂度分析？ 你可能会有些疑惑，我把代码跑一遍，通过统计、监控，就能得到算法执行的时间和占用的内存大小。为什么还要做时间、空间复杂度分析呢？这种分析方法能比我实实在在跑一遍得到的数据更准确吗？首先，我可以肯定地说，你这种评估算法执行效率的方法是正确的。很多数据结构和算法书籍还给这种方法起了一个名字，叫事后统计法。但是，这种统计方法有非常大的局限性。 测试结果非常依赖测试环境 测试结果受数据规模的影响很大 所以，我们需要一个不用具体的测试数据来测试，就可以粗略地估计算法的执行效率的方法。这就是我们今天要讲的时间、空间复杂度分析方法。 ","date":"2022-01-02 19:28:23","objectID":"/performance_analysis/:1:1","tags":["data structure"],"title":"Performance_analysis","uri":"/performance_analysis/"},{"categories":["Coding"],"content":"时间复杂度分析 只关注循环执行次数最多的一段代码 加法法则：总复杂度等于量级最大的那段代码的复杂度 乘法法则：嵌套代码的复杂度等于嵌套内外代码复杂度的乘积 ","date":"2022-01-02 19:28:23","objectID":"/performance_analysis/:1:2","tags":["data structure"],"title":"Performance_analysis","uri":"/performance_analysis/"},{"categories":["Coding"],"content":"空间复杂度分析 时间复杂度的全称是渐进时间复杂度，表示算法的执行时间与数据规模之间的增长关系。类比一下，空间复杂度全称就是渐进空间复杂度（asymptotic space complexity），表示算法的存储空间与数据规模之间的增长关系。 ","date":"2022-01-02 19:28:23","objectID":"/performance_analysis/:1:3","tags":["data structure"],"title":"Performance_analysis","uri":"/performance_analysis/"},{"categories":["Coding"],"content":"时间复杂度 时间复杂度是定性描述算法运行时间的函数。 实际情况中会因为数据用例、数据规模不同而变化，一般讨论一般情况。 时间复杂度$O(log(n))$并不以某一个确定的数为底数，因为可以通过乘以某个对数常数达到换底数的效果。 递归算法的时间复杂度：递归次数*每次递归的时间复杂度（操作的次数） ","date":"2022-01-02 19:28:23","objectID":"/performance_analysis/:2:0","tags":["data structure"],"title":"Performance_analysis","uri":"/performance_analysis/"},{"categories":["Coding"],"content":"空间复杂度分析 空间复杂度：程序运行时占用内存的大小，受很多因素的影响，比如编译器的内存对齐，编程语言容器的底层实现等 递归算法的空间复杂度：递归深度*每次递归的空间复杂度 递归所需的空间都被压到调用栈里，一次递归结束，这个栈就是就是把本次递归的数据弹出去。所以这个栈最大的长度就是递归的深度。 ","date":"2022-01-02 19:28:23","objectID":"/performance_analysis/:3:0","tags":["data structure"],"title":"Performance_analysis","uri":"/performance_analysis/"},{"categories":["Coding"],"content":"算法复杂度主方法 主方法亦可称为主定理。适用于求那些用分治法以及有递推关系式的算法的复杂度。 假设有递推关系式：$T(n)=aT(n/b)+f(n)$ n是问题规模 a为递推的子问题数量 n/b是每个子问题的规模，假设每个子问题规模一致 f(n)为递推以外进行的计算工作 T(n)为非负整数 分类讨论： 若$f(n)=O(n^{log_b(a-e)}),e\u003e0$ 则$T(n)=Θ(n^{log_b(a)})$ 若$f(n)=Θ(n^{log_b(a)})$ 则$T(n)=Θ(n^{log_b(a)}log(n))$ 若$f(n)=Ω(n^{log_b(a+e)}),e\u003e0$，且对于某个常数c \u003c 1和所有充分大的n有$af(n/b)\u003c=cf(n)$， 则$T(n)=Θ(f(n))$ 不是很容易记忆。下面有一种简化的版本： 若算法运行时间$T(n)\u003c=aT(n/b)+O(n^d)$ a\u003e=1是子问题的个数，b\u003e=1是输入规模减小的倍数，d\u003e=0是递归过程之外的步骤的时间复杂度指数，则： ","date":"2022-01-02 19:28:23","objectID":"/performance_analysis/:4:0","tags":["data structure"],"title":"Performance_analysis","uri":"/performance_analysis/"},{"categories":["Coding"],"content":"代码的内存消耗 每种语言都有着自己的内存管理方式。 ","date":"2022-01-02 19:28:23","objectID":"/performance_analysis/:5:0","tags":["data structure"],"title":"Performance_analysis","uri":"/performance_analysis/"},{"categories":["Coding"],"content":"内存对齐 对于各种基本数据类型来说，它的变量的内存地址值必须是其类型本身大小的整数倍。在go里面，对于结构体而言，它的变量的内存地址，只要是它最长字段长度与系统对齐系数两者之间较小的那个的整数倍就可以了。但对于结构体类型来说，我们还要让它每个字段的内存地址都严格满足内存对齐要求。 举个例子： type T struct { b byte i int64 u uint16 } 64bit平台系统对齐系数是8 计算过程：sum=1+7+8+2+6 第一个阶段是对齐结构体的各个字段: 首先，我们看第一个字段 b 是长度 1 个字节的 byte 类型变量，这样字段 b 放在任意地址上都可以被 1 整除，所以我们说它是天生对齐的。我们用一个 sum 来表示当前已经对齐的内存空间的大小，这个时候 sum=1； 接下来，我们看第二个字段 i，它是一个长度为 8 个字节的 int64 类型变量。按照内存对齐要求，它应该被放在可以被 8 整除的地址上。但是，如果把 i 紧邻 b 进行分配，当 i 的地址可以被 8 整除时，b 的地址就无法被 8 整除。这个时候，我们需要在 b 与 i 之间做一些填充，使得 i 的地址可以被 8 整除时，b 的地址也始终可以被 8 整除，于是我们在 i 与 b 之间填充了 7 个字节，此时此刻 sum=1+7+8； 再下来，我们看第三个字段 u，它是一个长度为 2 个字节的 uint16 类型变量，按照内存对其要求，它应该被放在可以被 2 整除的地址上。有了对其的 i 作为基础，我们现在知道将 u 与 i 相邻而放，是可以满足其地址的对齐要求的。i 之后的那个字节的地址肯定可以被 8 整除，也一定可以被 2 整除。于是我们把 u 直接放在 i 的后面，中间不需要填充，此时此刻，sum=1+7+8+2。 结构体 T 的所有字段都已经对齐了，开始第二个阶段，也就是对齐整个结构体: 结构体的内存地址为 min（结构体最长字段的长度，系统内存对齐系数）的整数倍，那么这里结构体 T 最长字段为 i，它的长度为 8，而 64bit 系统上的系统内存对齐系数一般为 8，两者相同，我们取 8 就可以了。那么整个结构体的对齐系数就是 8。 在尾部填充6字节原因： 结构体 T 的对齐系数是 8，那么我们就要保证每个结构体 T 的变量的内存地址，都能被 8 整除。如果我们只分配一个 T 类型变量，不再继续填充，也可能保证其内存地址为 8 的倍数。但如果考虑我们分配的是一个元素为 T 类型的数组，数组是元素连续存储的一种类型，元素 T[1]的地址为 T[0]地址 +T 的大小 (18)，显然无法被 8 整除，这将导致 T[1]及后续元素的地址都无法对齐，这显然不能满足内存对齐的要求。 所以，定义结构体时，一定要注意结构体中字段顺序，尽量合理排序，降低结构体对内存空间的占用。 前面例子中的内存填充部分，是由编译器自动完成的。不过，有些时候，为了保证某个字段的内存地址有更为严格的约束，我们也会做主动填充。比如 runtime 包中的 mstats 结构体定义就采用了主动填充： // $GOROOT/src/runtime/mstats.go type mstats struct { ... ... // Add an uint32 for even number of size classes to align below fields // to 64 bits for atomic operations on 32 bit platforms. _ [1 - _NumSizeClasses%2]uint32 // 这里做了主动填充,通常我们会通过空标识符来进行主动填充 last_gc_nanotime uint64 // last gc (monotonic time) last_heap_inuse uint64 // heap_inuse at mark termination of the previous GC ... ... } 为什么会有内存对齐？ 平台原因：不是所有的硬件平台都能访问任意内存地址上的任意数据，某些硬件平台只能在某些地址处取某些特定类型的数据，否则抛出硬件异常。为了同一个程序可以在多平台运行，需要内存对齐。 硬件原因：经过内存对齐后，CPU访问内存的速度大大提升 CPU读取内存不是一次读取单个字节，而是一块一块的来读取内存，块的大小可以是2，4，8，16个字节，具体取多少个字节取决于硬件。 只要可以跨平台的编程语言都需要做内存对齐，不做内存对齐会使运行速度下降，因为寻址访存次数多了。现在的编译器一般都会做内存对齐的优化操作，也就是说当考虑程序真正占用的内存大小的时候，也需要认识到内存对齐的影响。 ","date":"2022-01-02 19:28:23","objectID":"/performance_analysis/:5:1","tags":["data structure"],"title":"Performance_analysis","uri":"/performance_analysis/"},{"categories":["Coding"],"content":"go语言的内存管理 显然，go的内存管理内部机制建立于操作系统以及机器硬件如何管理内存之上的。尽可能扬长避短。 介绍一下和开发者关系较大的操作系统内存管理机制。 暂停 ","date":"2022-01-02 19:28:23","objectID":"/performance_analysis/:5:2","tags":["data structure"],"title":"Performance_analysis","uri":"/performance_analysis/"},{"categories":["Coding"],"content":"编程相关 ","date":"2022-01-02 18:57:57","objectID":"/programming_literacy/:0:0","tags":["data structure"],"title":"Programming_literacy","uri":"/programming_literacy/"},{"categories":["Coding"],"content":"代码风格与规范 go其实没有什么要说的。。 ","date":"2022-01-02 18:57:57","objectID":"/programming_literacy/:1:0","tags":["data structure"],"title":"Programming_literacy","uri":"/programming_literacy/"},{"categories":["Coding"],"content":"变量命名 主要以团队风格为主； 主流有如下三种变量规则： 小驼峰、大驼峰(帕斯卡命名法)命名法（java,go） 下划线命名法(python,linux下的c/c++编程) 匈牙利命名法 该命名规范，要求前缀字母用变量类型的缩写，其余部分用变量的英文或英文的缩写，单词第一个字母大写。（很少用，在windows下的c/c++编程有时会用,没有IDE的时代挺好） int iMyAge; // \"i\": int char cMyName[10]; // \"c\": char float fManHeight; // \"f\": float ","date":"2022-01-02 18:57:57","objectID":"/programming_literacy/:1:1","tags":["data structure"],"title":"Programming_literacy","uri":"/programming_literacy/"},{"categories":["Coding"],"content":"核心代码模式和ACM模式 核心代码模式： 把要处理的数据都已经放入容器里，可以直接写逻辑 ACM输入模式呢： 自己构造输入数据格式，把要需要处理的容器填充好，不会给你任何代码，包括include哪些函数都要自己写，最后也要自己控制返回数据的格式。 ","date":"2022-01-02 18:57:57","objectID":"/programming_literacy/:1:2","tags":["data structure"],"title":"Programming_literacy","uri":"/programming_literacy/"},{"categories":["Advanced learning"],"content":" 极客时间学习笔记 MySQL基础 ","date":"2021-12-01 21:31:10","objectID":"/mysql_advanced_01/:0:0","tags":["mysql"],"title":"Mysql_advanced_01","uri":"/mysql_advanced_01/"},{"categories":["Advanced learning"],"content":"一条sql查询语句 mysql\u003e select * from T where ID=10； 一条简单的查询语句，MYSQL内部发生了什么？ 大体来说，MySQL 可以分为 Server 层和存储引擎层两部分。 Server 层包括连接器、查询缓存、分析器、优化器、执行器等，涵盖 MySQL 的大多数核心服务功能，以及所有的内置函数（如日期、时间、数学和加密函数等），所有跨存储引擎的功能都在这一层实现，比如存储过程、触发器、视图等。 存储引擎层负责数据的存储和提取。其架构模式是插件式的，支持 InnoDB、MyISAM、Memory 等多个存储引擎。现在最常用的存储引擎是 InnoDB，它从 MySQL 5.5.5 版本开始成为了默认存储引擎。可以在 create table 语句中使用 engine=memory, 来指定使用内存引擎创建表。 不同的存储引擎共用一个 Server 层，也就是从连接器到执行器的部分。 ","date":"2021-12-01 21:31:10","objectID":"/mysql_advanced_01/:1:0","tags":["mysql"],"title":"Mysql_advanced_01","uri":"/mysql_advanced_01/"},{"categories":["Advanced learning"],"content":"连接器 连接器负责跟客户端建立连接、获取权限、维持和管理连接。 mysql -h$ip -P$port -u$user -p 如果把密码直接写在-p后面容易导致密码泄露 连接命令中的 mysql 是客户端工具，用来跟服务端建立连接。在完成经典的 TCP 握手后，连接器就要开始认证你的身份，这个时候用的就是你输入的用户名和密码： 如果用户名或密码不对，你就会收到一个\"Access denied for user\"的错误，然后客户端程序结束执行。 如果用户名密码认证通过，连接器会到权限表里面查出你拥有的权限（注意修改权限后下次登录才生效）。之后，这个连接里面的权限判断逻辑，都将依赖于此时读到的权限。 连接完成后，如果你没有后续的动作，这个连接就处于空闲状态，你可以在 show processlist 命令中看到该连接的command是sleep的。show processlist 是显示用户正在运行的线程，需要注意的是，除了 root 用户能看到所有正在运行的线程外，其他用户都只能看到自己正在运行的线程，看不到其它用户正在运行的线程。除非单独个这个用户赋予了PROCESS 权限。 客户端如果太长时间没动静，连接器就会自动将它断开。这个时间是由参数 wait_timeout 控制的，默认值是 8 小时。 mysql\u003e show variables like 'wait_timeout'; 长连接和短连接： 长连接是指连接成功后，如果客户端持续有请求，则一直使用同一个连接。（推荐，因为建立连接的过程通常是比较复杂的） 短连接则是指每次执行完很少的几次查询就断开连接，下次查询再重新建立一个。 但是全部使用长连接后，有些时候 MySQL 占用内存涨得特别快，这是因为 **MySQL 在执行过程中临时使用的内存是管理在连接对象里面的。这些资源会在连接断开的时候才释放。**所以如果长连接累积下来，可能导致内存占用太大，被系统强行杀掉（OOM），从现象看就是 MySQL 异常重启了。 解决方法： 定期断开长连接。使用一段时间，或者程序里面判断执行过一个占用内存的大查询后，断开连接，之后要查询再重连。 如果你用的是 MySQL 5.7 或更新版本，可以在每次执行一个比较大的操作后，通过执行 mysql_reset_connection 来重新初始化连接资源。这个过程不需要重连和重新做权限验证，但是会将连接恢复到刚刚创建完时的状态。 mysql_reset_connection：重置连接以清除会话状态。（将句柄都给置空，并且清理掉了预处理语句、结果集） mysql_reset_connection()的作用类似于mysql_change_user()或自动重新连接，除了未关闭并重新打开连接且未完成重新身份验证外。 与连接有关的状态受到以下影响： 回滚所有活动的事务，并重置自动提交模式。 所有 table 锁均已释放。 所有TEMPORARYtable 均已关闭(并删除)。 会话系统变量将重新初始化为相应的全局系统变量的值，包括由诸如SET NAMES之类的语句隐式设置的系统变量。 用户变量设置丢失。 准备好的语句被释放。 HANDLER个变量已关闭。 LAST_INSERT_ID()的值重置为 0. 用GET_LOCK()获取的锁被释放。 Return Values 零成功。如果发生错误，则为非零值。 查询缓存 MySQL 拿到一个查询请求后，会先到查询缓存看看，之前是不是执行过这条语句。之前执行过的语句及其结果可能会以 key-value 对的形式，被直接缓存在内存中。key 是查询的语句，value 是查询的结果。如果你的查询能够直接在这个缓存中找到 key，那么这个 value 就会被直接返回给客户端。 如果语句不在查询缓存中，就会继续后面的执行阶段。执行完成后，执行结果会被存入查询缓存中。 查询缓存的弊端： 查询缓存的失效非常频繁，只要有对一个表的更新，这个表上所有的查询缓存都会被清空。因此很可能你费劲地把结果存起来，还没使用呢，就被一个更新全清空了。对于更新压力大的数据库来说，查询缓存的命中率会非常低。除非你的业务就是有一张静态表，很长时间才会更新一次。比如，一个系统配置表，那这张表上的查询才适合使用查询缓存。 可以将参数 query_cache_type 设置成 DEMAND，这样对于默认的 SQL 语句都不使用查询缓存。而对于确定要使用查询缓存的语句，可以用 SQL_CACHE 显式指定： mysql\u003e select SQL_CACHE * from T where ID=10； MySQL 8.0 版本直接将查询缓存的整块功能删掉了。 分析器 分析器对 SQL 语句做解析让mysql知道你要做什么： 词法分析 你输入的是由多个字符串和空格组成的一条 SQL 语句，MySQL 需要识别出里面的字符串分别是什么，代表什么。 MySQL 从你输入的\"select\"这个关键字识别出来，这是一个查询语句。它也要把字符串“T”识别成“表名 T”，把字符串“ID”识别成“列 ID” 语法分析 根据词法分析的结果，语法分析器会根据语法规则，判断你输入的这个 SQL 语句是否满足 MySQL 语法。 如果你的语句不对，就会收到“You have an error in your SQL syntax”的错误提醒，比如你的查询语句 select 少打了开头的字母“s”。 优化器 执行前，需经过优化器的处理： 优化器是在表里面有多个索引的时候，决定使用哪个索引；或者在一个语句有多表关联（join）的时候，决定各个表的连接顺序。比如你执行下面这样的语句，这个语句是执行两个表的 join： mysql\u003e select * from t1 join t2 using(ID) where t1.c=10 and t2.d=20; 既可以先从表 t1 里面取出 c=10 的记录的 ID 值，再根据 ID 值关联到表 t2，再判断 t2 里面 d 的值是否等于 20。 也可以先从表 t2 里面取出 d=20 的记录的 ID 值，再根据 ID 值关联到 t1，再判断 t1 里面 c 的值是否等于 10。 逻辑结果是一样的，但是执行的效率会有不同，而优化器的作用就是决定选择使用哪一个方案。 执行器 执行语句： 先判断一下你对这个表 T 有没有执行查询的权限，如果没有，就会返回没有权限的错误，如下所示 (在工程实现上，如果命中查询缓存，会在查询缓存返回结果的时候，做权限验证。查询也会在优化器之前调用 precheck 验证权限)。 mysql\u003e select * from T where ID=10; ERROR 1142 (42000): SELECT command denied to user 'b'@'localhost' for table 'T' 如果有权限，就打开表继续执行。打开表的时候，执行器就会根据表的引擎定义，去使用这个引擎提供的接口。 比如我们这个例子中的表 T 中，ID 字段没有索引，那么执行器的执行流程是这样的： 调用 InnoDB 引擎接口取这个表的第一行，判断 ID 值是不是 10，如果不是则跳过，如果是则将这行存在结果集中； 调用引擎接口取“下一行”，重复相同的判断逻辑，直到取到这个表的最后一行。 执行器将上述遍历过程中所有满足条件的行组成的记录集作为结果集返回给客户端。 对于有索引的表，执行的逻辑也差不多。第一次调用的是“取满足条件的第一行”这个接口，之后循环取“满足条件的下一行”这个接口，这些接口都是引擎中已经定义好的。 你会在数据库的慢查询日志中看到一个 rows_examined 的字段，表示这个语句执行过程中扫描了多少行。这个值就是在执行器每次调用引擎获取数据行的时候累加的。 在有些场景下，执行器调用一次，在引擎内部则扫描了多行，因此引擎扫描行数跟 rows_examined 并不是完全相同的。 ","date":"2021-12-01 21:31:10","objectID":"/mysql_advanced_01/:1:1","tags":["mysql"],"title":"Mysql_advanced_01","uri":"/mysql_advanced_01/"},{"categories":["Advanced learning"],"content":"一条sql更新语句 日志系统：一条SQL更新语句是如何执行的? mysql\u003e create table T(ID int primary key, c int); mysql\u003e update T set c=c+1 where ID=2; 查询语句的那一套流程，更新语句也是同样会走一遍。 执行语句前要先连接数据库，这是连接器的工作。 在一个表上有更新的时候，跟这个表有关的查询缓存会失效，所以这条语句就会把表 T 上所有缓存结果都清空。 分析器会通过词法和语法解析知道这是一条更新语句。优化器决定要使用 ID 这个索引。然后，执行器负责具体执行，找到这一行，然后更新。 与查询流程不一样的是，更新流程还涉及两个重要的日志模块，redo log（重做日志）和 binlog（归档日志）。redo log 和 binlog 在设计上有很多有意思的地方。 ","date":"2021-12-01 21:31:10","objectID":"/mysql_advanced_01/:2:0","tags":["mysql"],"title":"Mysql_advanced_01","uri":"/mysql_advanced_01/"},{"categories":["Advanced learning"],"content":"重要的日志模块：redo log 如果每一次的更新操作都需要写进磁盘，然后磁盘也要找到对应的那条记录，然后再更新，整个过程 IO 成本、查找成本都很高。 WAL 的全称是 Write-Ahead Logging，它的关键点就是先写日志，再写磁盘 具体来说，当有一条记录需要更新的时候，InnoDB 引擎就会先把记录写到 redo log里面，并更新内存，这个时候更新就算完成了。同时，InnoDB 引擎会在适当的时候，将这个操作记录更新到磁盘里面，而这个更新往往是在系统比较空闲的时候做。 InnoDB 的 redo log 是固定大小的，比如可以配置为一组 4 个文件，每个文件的大小是 1GB。从头开始写，写到末尾就又回到开头循环写。 write pos 是当前记录的位置，一边写一边后移，写到第 3 号文件末尾后就回到 0 号文件开头。checkpoint 是当前要擦除的位置，也是往后推移并且循环的，擦除记录前要把记录更新到数据文件。 write pos 和 checkpoint 之间的还空着的部分，可以用来记录新的操作。如果 write pos 追上 checkpoint，表示日志满了，这时候不能再执行新的更新，得停下来先擦掉一些记录，把 checkpoint 推进一下。 有了 redo log，InnoDB 就可以保证即使数据库发生异常重启，之前提交的记录都不会丢失，这个能力称为 crash-safe。 redo log 用于保证 crash-safe 能力。innodb_flush_log_at_trx_commit 这个参数设置成 1 的时候，表示每次事务的 redo log 都直接持久化到磁盘。这个参数我建议你设置成 1，这样可以保证 MySQL 异常重启之后数据不丢失。 ","date":"2021-12-01 21:31:10","objectID":"/mysql_advanced_01/:2:1","tags":["mysql"],"title":"Mysql_advanced_01","uri":"/mysql_advanced_01/"},{"categories":["Advanced learning"],"content":"重要的日志模块 MySQL 整体来看，其实就有两块：一块是 Server 层，它主要做的是 MySQL 功能层面的事情；还有一块是引擎层，负责存储相关的具体事宜。上面的 redo log 是 InnoDB 引擎特有的日志，而 Server 层也有自己的日志，称为 binlog（归档日志）。 最开始 MySQL 里并没有 InnoDB 引擎。MySQL 自带的引擎是 MyISAM，但是 MyISAM 没有 crash-safe 的能力，binlog 日志只能用于归档。而 InnoDB 是另一个公司以插件形式引入 MySQL 的，既然只依靠 binlog 是没有 crash-safe 能力的，所以 InnoDB 使用另外一套日志系统——也就是 redo log 来实现 crash-safe 能力。 两种日志的不同： redo log 是 InnoDB 引擎特有的；binlog 是 MySQL 的 Server 层实现的，所有引擎都可以使用。 redo log 是物理日志，记录的是“在某个数据页上做了什么修改”；binlog 是逻辑日志，记录的是这个语句的原始逻辑，比如“给 ID=2 这一行的 c 字段加 1”。 redo log 是循环写的，空间固定会用完；binlog 是可以追加写入的。“追加写”是指 binlog 文件写到一定大小后会切换到下一个，并不会覆盖以前的日志。 执行器和 InnoDB 引擎在执行这个简单的 update 语句时的内部流程： 执行器先找引擎取 ID=2 这一行。ID 是主键，引擎直接用树搜索找到这一行。如果 ID=2 这一行所在的数据页本来就在内存中，就直接返回给执行器；否则，需要先从磁盘读入内存，然后再返回。 执行器拿到引擎给的行数据，把这个值加上 1，比如原来是 N，现在就是 N+1，得到新的一行数据，再调用引擎接口写入这行新数据。 引擎将这行新数据更新到内存中，同时将这个更新操作记录到 redo log 里面，此时 redo log 处于 prepare 状态。然后告知执行器执行完成了，随时可以提交事务。 执行器生成这个操作的 binlog，并把 binlog 写入磁盘。 执行器调用引擎的提交事务接口，引擎把刚刚写入的 redo log 改成提交（commit）状态，更新完成。 注：redo log 的写入拆成了两个步骤：prepare 和 commit，这就是\"两阶段提交”。 sync_binlog 这个参数设置成 1 的时候，表示每次事务的 binlog 都持久化到磁盘。这个参数我也建议你设置成 1，这样可以保证 MySQL 异常重启之后 binlog 不丢失。 ","date":"2021-12-01 21:31:10","objectID":"/mysql_advanced_01/:2:2","tags":["mysql"],"title":"Mysql_advanced_01","uri":"/mysql_advanced_01/"},{"categories":["Advanced learning"],"content":"两阶段提交 两阶段提交是为了让两份日志之间的逻辑一致。 binlog 会记录所有的逻辑操作，并且是采用“追加写”的形式。如果你的 DBA 承诺说半个月内可以恢复，那么备份系统中一定会保存最近半个月的所有 binlog，同时系统会定期做整库备份。这里的“定期”取决于系统的重要性，可以是一天一备，也可以是一周一备。 当需要恢复到指定的某一秒时，比如某天下午两点发现中午十二点有一次误删表，需要找回数据，那你可以这么做： 首先，找到最近的一次全量备份，如果你运气好，可能就是昨天晚上的一个备份，从这个备份恢复到临时库； 然后，从备份的时间点开始，将备份的 binlog 依次取出来，重放到中午误删表之前的那个时刻。 为什么日志需要“两阶段提交”？ 由于 redo log 和 binlog 是两个独立的逻辑，如果不用两阶段提交，要么就是先写完 redo log 再写 binlog，或者采用反过来的顺序。这两种方式的问题： 先写 redo log 后写 binlog。假设在 redo log 写完，binlog 还没有写完的时候，MySQL 进程异常重启。由于我们前面说过的，redo log 写完之后，系统即使崩溃，仍然能够把数据恢复回来，所以恢复后这一行 c 的值是 1。但是由于 binlog 没写完就 crash 了，这时候 binlog 里面就没有记录这个语句。因此，之后备份日志的时候，存起来的 binlog 里面就没有这条语句。然后你会发现，如果需要用这个 binlog 来恢复临时库的话，由于这个语句的 binlog 丢失，这个临时库就会少了这一次更新，恢复出来的这一行 c 的值就是 0，与原库的值不同。 先写 binlog 后写 redo log。如果在 binlog 写完之后 crash，由于 redo log 还没写，崩溃恢复以后这个事务无效，所以这一行 c 的值是 0。但是 binlog 里面已经记录了“把 c 从 0 改成 1”这个日志。所以，在之后用 binlog 来恢复的时候就多了一个事务出来，恢复出来的这一行 c 的值就是 1，与原库的值不同。 即，如果不使用“两阶段提交”，那么数据库的状态就有可能和用它的日志恢复出来的库的状态不一致。 不只是误操作后需要用这个过程来恢复数据。当你需要扩容的时候，也就是需要再多搭建一些备库来增加系统的读能力的时候，现在常见的做法也是用全量备份加上应用 binlog来实现的，这个“不一致”就会导致你的线上出现主从数据库不一致的情况。 ","date":"2021-12-01 21:31:10","objectID":"/mysql_advanced_01/:2:3","tags":["mysql"],"title":"Mysql_advanced_01","uri":"/mysql_advanced_01/"},{"categories":["Advanced learning"],"content":"一天一备份和一周一备份的区别 一天一备份“最长恢复时间”更短。 频繁全量备份需要消耗更多存储空间，所以这个 RTO 是成本换来的，就需要你根据业务重要性来评估了。 ","date":"2021-12-01 21:31:10","objectID":"/mysql_advanced_01/:2:4","tags":["mysql"],"title":"Mysql_advanced_01","uri":"/mysql_advanced_01/"},{"categories":["Advanced learning"],"content":"事务隔离 转账过程具体到程序里会有一系列的操作，比如查询余额、做加减法、更新余额等，这些操作必须保证是一体的，不然等程序查完之后，还没做减法之前，你这 100 块钱，完全可以借着这个时间差再查一次，然后再给另外一个朋友转账，如果银行这么整，不就乱了么？这时就要用到“事务”这个概念了。 简单来说，事务就是要保证一组数据库操作，要么全部成功，要么全部失败。在 MySQL 中，事务支持是在引擎层实现的。你现在知道，MySQL 是一个支持多引擎的系统，但并不是所有的引擎都支持事务。比如 MySQL 原生的 MyISAM 引擎就不支持事务，这也是 MyISAM 被 InnoDB 取代的重要原因之一。 以下会以 InnoDB 为例，剖析 MySQL 在事务支持方面的特定实现。 ","date":"2021-12-01 21:31:10","objectID":"/mysql_advanced_01/:3:0","tags":["mysql"],"title":"Mysql_advanced_01","uri":"/mysql_advanced_01/"},{"categories":["Advanced learning"],"content":"隔离性与隔离级别 事务：ACID（Atomicity、Consistency、Isolation、Durability，即原子性、一致性、隔离性、持久性） 当数据库上有多个事务同时执行的时候，就可能出现脏读（dirty read）、不可重复读（non-repeatable read）、幻读（phantom read）的问题，为了解决这些问题，就有了“隔离级别”的概念。 隔离做得越好，往往效率越低，所以需要互相平衡。 SQL 标准的事务隔离级别包括：读未提交（read uncommitted）、读提交（read committed）、可重复读（repeatable read）和串行化（serializable ）。 读未提交是指，一个事务还没提交时，它做的变更就能被别的事务看到。 读提交是指，一个事务提交之后，它做的变更才会被其他事务看到。 可重复读是指，一个事务执行过程中看到的数据，总是跟这个事务在启动时看到的数据是一致的。当然在可重复读隔离级别下，未提交变更对其他事务也是不可见的。(事务在执行期间看到的数据前后必须是一致的。) 串行化，顾名思义是对于同一行记录，“写”会加“写锁”，“读”会加“读锁”。当出现读写锁冲突的时候，后访问的事务必须等前一个事务执行完成，才能继续执行。 Oracle 数据库的默认隔离级别其实就是“读提交”，因此对于一些从 Oracle 迁移到 MySQL 的应用，为保证数据库隔离级别的一致，一定要记得将 MySQL 的隔离级别设置为“读提交”。 配置的方式是，将启动参数 transaction-isolation 的值设置成 READ-COMMITTED。你可以用 show variables 来查看当前的值: mysql\u003e show variables like 'transaction_isolation'; +-----------------------+----------------+ | Variable_name | Value | +-----------------------+----------------+ | transaction_isolation | READ-COMMITTED | +-----------------------+----------------+ 应用案例： 假设你在管理一个个人银行账户表。一个表存了账户余额，一个表存了账单明细。到了月底你要做数据校对，也就是判断上个月的余额和当前余额的差额，是否与本月的账单明细一致。你一定希望在校对过程中，即使有用户发生了一笔新的交易，也不影响你的校对结果。 使用“可重复读”隔离级别就很方便。事务启动时的视图可以认为是静态的，不受其他事务更新的影响。 ","date":"2021-12-01 21:31:10","objectID":"/mysql_advanced_01/:3:1","tags":["mysql"],"title":"Mysql_advanced_01","uri":"/mysql_advanced_01/"},{"categories":["Advanced learning"],"content":"事务隔离的实现 可重复读： 在 MySQL 中，实际上每条记录在更新的时候都会同时记录一条回滚操作。记录上的最新值，通过回滚操作，都可以得到前一个状态的值。 假设一个值从 1 被按顺序改成了 2、3、4，在回滚日志里面就会保存相应记录。 当前值是 4，但是在查询这条记录的时候，不同时刻启动的事务会有不同的 read-view(eg:将2改成1)。 同一条记录在系统中可以存在多个版本，就是数据库的多版本并发控制（MVCC） 系统会判断，当没有事务再需要用到这些回滚日志时，回滚日志会被删除。 什么时候才不需要了呢？就是当系统里没有比这个回滚日志更早的 read-view 的时候。 为什么建议你尽量不要使用长事务？ 长事务意味着系统里面会存在很老的事务视图。由于这些事务随时可能访问数据库里面的任何数据，所以这个事务提交之前，数据库里面它可能用到的回滚记录都必须保留，这就会导致大量占用存储空间。 在 MySQL 5.5 及以前的版本，回滚日志是跟数据字典一起放在 ibdata 文件里的，即使长事务最终提交，回滚段被清理，文件也不会变小。数据只有 20GB，但回滚段可能有 200GB 的库。最终只好为了清理回滚段，重建整个库。 除了对回滚段的影响，长事务还占用锁资源，也可能拖垮整个库。 很多时候业务开发同学并不是有意使用长事务，通常是由于误用所致。 ","date":"2021-12-01 21:31:10","objectID":"/mysql_advanced_01/:3:2","tags":["mysql"],"title":"Mysql_advanced_01","uri":"/mysql_advanced_01/"},{"categories":["Advanced learning"],"content":"事务的启动方式 MySQL 的事务启动方式有以下几种： 显式启动事务语句， begin 或 start transaction。配套的提交语句是 commit，回滚语句是 rollback。 set autocommit=0，这个命令会将这个线程的自动提交关掉。意味着如果你只执行一个 select 语句，这个事务就启动了，而且并不会自动提交。这个事务持续存在直到你主动执行 commit 或 rollback 语句，或者断开连接。 有些客户端连接框架会默认连接成功后先执行一个 set autocommit=0 的命令。这就导致接下来的查询都在事务中，如果是长连接，就导致了意外的长事务。 因此，建议总是使用 set autocommit=1, 通过显式语句的方式来启动事务。 若考虑到交互次数的增加，建议使用 commit work and chain 语法。 在 autocommit 为 1 的情况下，用 begin 显式启动的事务，如果执行 commit 则提交事务。如果执行 commit work and chain，则是提交事务并自动启动下一个事务，这样也省去了再次执行 begin 语句的开销。同时带来的好处是从程序开发的角度明确地知道每个语句是否处于事务中。 你可以在 information_schema 库的 innodb_trx 这个表中查询长事务，比如下面这个语句，用于查找持续时间超过 60s 的事务： select * from information_schema.innodb_trx where TIME_TO_SEC(timediff(now(),trx_started))\u003e60 ","date":"2021-12-01 21:31:10","objectID":"/mysql_advanced_01/:3:3","tags":["mysql"],"title":"Mysql_advanced_01","uri":"/mysql_advanced_01/"},{"categories":["Advanced learning"],"content":"索引 索引的出现就是为了提高数据查询的效率，就像书的目录一样。 索引常见模型（数据结构）： 哈希表 一般用拉链法解决哈希等值数据的存储，适用于只有等值查询的场景。范围查询则需要全部扫描一遍。 比如 Memcached 及其他一些 NoSQL 引擎。 有序数组 有序数组在等值查询和范围查询场景中的性能就都非常优秀。 查询效率最好的数据结构，更新数据效率低下。 只适用于静态存储引擎 搜索树 查询的时候当然要控制尽量少地读磁盘，查询的时候尽量少地访问数据块（树根的数据块一般都在内存中，而其余节点的数据块大概率都需要访问磁盘），二叉搜索树理论上搜索效率虽高，但实际应用时都是用 N 叉树，N 取决于数据块的大小 N 叉树由于在读写上的性能优点，以及适配磁盘的访问模式，已经被广泛应用在数据库引擎中了。 在 MySQL 中，索引是在存储引擎层实现的，所以并没有统一的索引标准，即不同存储引擎的索引的工作方式并不一样。而即使多个存储引擎支持同一种类型的索引，其底层的实现也可能不同 ","date":"2021-12-01 21:31:10","objectID":"/mysql_advanced_01/:4:0","tags":["mysql"],"title":"Mysql_advanced_01","uri":"/mysql_advanced_01/"},{"categories":["Advanced learning"],"content":"InnoDB的索引模型 在 InnoDB 中，表都是根据主键顺序以索引的形式存放的，这种存储方式的表称为索引组织表。 InnoDB 使用了 B+ 树索引模型，所以数据都是存储在 B+ 树中的。 主键索引的叶子节点存的是整行数据。在 InnoDB 里，主键索引也被称为聚簇索引（clustered index）。非主键索引的叶子节点内容是主键的值。在 InnoDB 里，非主键索引也被称为二级索引（secondary index）。 mysql\u003e create table T( id int primary key, k int not null, name varchar(16), index (k))engine=InnoDB; 表中 R1~R5 的 (ID,k) 值分别为 (100,1)、(200,2)、(300,3)、(500,5) 和 (600,6)，两棵树的示例示意图如下。 如果语句是 select * from T where ID=500（ID是主键），即主键查询方式，则只需要搜索 ID 这棵 B+ 树；如果语句是 select * from T where k=5，即普通索引查询方式，则需要先搜索 k 索引树，得到 ID 的值为 500，再到 ID 索引树搜索一次。这个过程称为回表。也就是说，基于非主键索引的查询需要多扫描一棵索引树。因此，我们在应用中应该尽量使用主键查询。 索引维护 B+ 树为了维护索引有序性，在插入新值的时候需要做必要的维护。否则需要挪动插入索引位置后的数据甚至导致页分裂，性能下降，空间利用率也下降。 哪些场景下应该使用自增主键，而哪些场景下不应该？ 自增主键是指自增列上定义的主键，在建表语句中一般是这么定义的： NOT NULL PRIMARY KEY AUTO_INCREMENT。 有业务逻辑的字段做主键，则往往不容易保证有序插入，这样写数据成本相对较高 主键长度越小，普通索引的叶子节点就越小，普通索引占用的空间也就越小 所以，从性能和存储空间方面考量，自增主键往往是更合理的选择。 用业务字段直接做主键：比如，有些业务的场景需求是这样的：（典型的KV场景） 只有一个索引； 该索引必须是唯一索引。 由于没有其他索引，所以也就不用考虑其他索引的叶子节点大小的问题。这时候我们就要优先考虑上一段提到的“尽量使用主键查询”原则，直接将这个索引设置为主键，可以避免每次查询需要搜索两棵树。 重建索引：索引可能因为删除，或者页分裂等原因，导致数据页有空洞，重建索引的过程会创建一个新的索引，把数据按顺序插入，这样页面的利用率最高，也就是索引更紧凑、更省空间。 ","date":"2021-12-01 21:31:10","objectID":"/mysql_advanced_01/:4:1","tags":["mysql"],"title":"Mysql_advanced_01","uri":"/mysql_advanced_01/"},{"categories":["Advanced learning"],"content":"索引2 在下面这个表 T 中，如果我执行 select * from T where k between 3 and 5，需要执行几次树的搜索操作，会扫描多少行？下面是这个表的初始化语句。 mysql\u003e create table T ( ID int primary key, k int NOT NULL DEFAULT 0, s varchar(16) NOT NULL DEFAULT '', index k(k)) engine=InnoDB; insert into T values(100,1, 'aa'),(200,2,'bb'),(300,3,'cc'),(500,5,'ee'),(600,6,'ff'),(700,7,'gg'); 在 k 索引树上找到 k=3 的记录，取得 ID = 300；再到 ID 索引树查到 ID=300 对应的 R3；在 k 索引树取下一个值 k=5，取得 ID=500；再回到 ID 索引树查到 ID=500 对应的 R4；在 k 索引树取下一个值 k=6，不满足条件，循环结束。 回到主键索引树搜索的过程，我们称为回表。有没有可能经过索引优化，避免回表过程呢？ ","date":"2021-12-01 21:31:10","objectID":"/mysql_advanced_01/:5:0","tags":["mysql"],"title":"Mysql_advanced_01","uri":"/mysql_advanced_01/"},{"categories":["Advanced learning"],"content":"覆盖索引 如果执行的语句是 select ID from T where k between 3 and 5，这时只需要查 ID 的值，而 ID 的值已经在 k 索引树上了，因此可以直接提供查询结果，不需要回表。也就是说，在这个查询里面，索引 k 已经“覆盖了”我们的查询需求，我们称为覆盖索引。 需要注意的是，在引擎内部使用覆盖索引在索引 k 上其实读了三个记录，R3~R5（对应的索引 k 上的记录项），但是对于 MySQL 的 Server 层来说，它就是找引擎拿到了两条记录，因此 MySQL 认为扫描行数是 2。 在一个市民信息表上，是否有必要将身份证号和名字建立联合索引？假设这个市民表的定义是这样的： CREATE TABLE `tuser` ( `id` int(11) NOT NULL, `id_card` varchar(32) DEFAULT NULL, `name` varchar(32) DEFAULT NULL, `age` int(11) DEFAULT NULL, `ismale` tinyint(1) DEFAULT NULL, PRIMARY KEY (`id`), KEY `id_card` (`id_card`), KEY `name_age` (`name`,`age`) ) ENGINE=InnoDB 如果现在有一个高频请求，要根据市民的身份证号查询他的姓名，这个联合索引就有意义了。它可以在这个高频请求上用到覆盖索引，不再需要回表查整行记录，减少语句的执行时间。当然，索引字段的维护总是有代价的。因此，在建立冗余索引来支持覆盖索引时就需要权衡考虑了。这正是业务 DBA，或者称为业务数据架构师的工作。 ","date":"2021-12-01 21:31:10","objectID":"/mysql_advanced_01/:5:1","tags":["mysql"],"title":"Mysql_advanced_01","uri":"/mysql_advanced_01/"},{"categories":["Advanced learning"],"content":"最左前缀原则 B+ 树这种索引结构，可以利用索引的“最左前缀”，来定位记录，加速检索。这个最左前缀可以是联合索引的最左 N 个字段，也可以是字符串索引的最左 M 个字符。 在建立联合索引的时候，如何安排索引内的字段顺序。这里我们的评估标准是，索引的复用能力。因为可以支持最左前缀，所以当已经有了 (a,b) 这个联合索引后，一般就不需要单独在 a 上建立索引了。因此，第一原则是，如果通过调整顺序，可以少维护一个索引，那么这个顺序往往就是需要优先考虑采用的。 如果既有联合查询，又有基于 a、b 各自的查询呢？查询条件里面只有 b 的语句，是无法使用 (a,b) 这个联合索引的，这时候你不得不维护另外一个索引，也就是说你需要同时维护 (a,b)、(b) 这两个索引。这时候，我们要考虑的原则就是空间了。比如上面这个市民表的情况，name 字段是比 age 字段大的 ，那我就建议你创建一个（name,age) 的联合索引和一个 (age) 的单字段索引。 ","date":"2021-12-01 21:31:10","objectID":"/mysql_advanced_01/:5:2","tags":["mysql"],"title":"Mysql_advanced_01","uri":"/mysql_advanced_01/"},{"categories":["Advanced learning"],"content":"索引下推 如果现在有一个需求：检索出表中“名字第一个字是张，而且年龄是 10 岁的所有男孩”。那么，SQL 语句是这么写的： mysql\u003e select * from tuser where name like '张%' and age=10 and ismale=1; 这个语句在搜索索引树的时候，只能用 “张”，找到第一个满足条件的记录 ID3 判断其他条件是否满足: 在 MySQL 5.6 之前，只能从 ID3 开始一个个回表。到主键索引上找出数据行，再对比字段值。而 MySQL 5.6 引入的索引下推优化（index condition pushdown)， 可以在索引遍历过程中，对索引中包含的字段先做判断，直接过滤掉不满足条件的记录，减少回表次数。 图三： 图四：索引下推 图 4 跟图 3 的区别是，InnoDB 在 (name,age) 索引内部就判断了 age 是否等于 10，对于不等于 10 的记录，直接判断并跳过。 ","date":"2021-12-01 21:31:10","objectID":"/mysql_advanced_01/:5:3","tags":["mysql"],"title":"Mysql_advanced_01","uri":"/mysql_advanced_01/"},{"categories":["Advanced learning"],"content":"全局锁和表锁 数据库资源需要被并发访问，锁是为了合理地控制资源的访问规则。 根据加锁的范围，MySQL 里面的锁大致可以分成全局锁、表级锁和行锁三类。 ","date":"2021-12-01 21:31:10","objectID":"/mysql_advanced_01/:6:0","tags":["mysql"],"title":"Mysql_advanced_01","uri":"/mysql_advanced_01/"},{"categories":["Advanced learning"],"content":"全局锁 顾名思义，全局锁就是对整个数据库实例加锁。MySQL 提供了一个加全局读锁的方法，命令是 Flush tables with read lock (FTWRL)。当你需要让整个库处于只读状态的时候，可以使用这个命令，之后其他线程的以下语句会被阻塞：数据更新语句（数据的增删改）、数据定义语句（包括建表、修改表结构等）和更新类事务的提交语句。 全局锁的典型使用场景是，做全库逻辑备份。也就是把整库每个表都 select 出来存成文本。 如果你在主库上备份，那么在备份期间都不能执行更新，业务基本上就得停摆；如果你在从库上备份，那么备份期间从库不能执行主库同步过来的 binlog，会导致主从延迟。 在前面讲事务隔离的时候，其实是有一个方法能够拿到一致性视图的，就是在可重复读隔离级别下开启一个事务。 官方自带的逻辑备份工具是 mysqldump。当 mysqldump 使用参数–single-transaction 的时候，导数据之前就会启动一个事务，来确保拿到一致性视图。而由于 MVCC 的支持，这个过程中数据是可以正常更新的。 有了这个功能，为什么还需要 FTWRL 呢？一致性读是好，但前提是引擎要支持这个隔离级别。比如，对于 MyISAM 这种不支持事务的引擎，如果备份过程中有更新，总是只能取到最新的数据，那么就破坏了备份的一致性。这时，我们就需要使用 FTWRL 命令了。 所以，single-transaction 方法只适用于所有的表使用事务引擎的库。如果有的表使用了不支持事务的引擎，那么备份就只能通过 FTWRL 方法。这往往是 DBA 要求业务开发人员使用 InnoDB 替代 MyISAM 的原因之一。 不用set global readonly=true而用 FTWRL 方式设置全库只读的原因： 一是，在有些系统中，readonly 的值会被用来做其他逻辑，比如用来判断一个库是主库还是备库。因此，修改 global 变量的方式影响面更大，我不建议你使用。 二是，在异常处理机制上有差异。如果执行 FTWRL 命令之后由于客户端发生异常断开，那么 MySQL 会自动释放这个全局锁，整个库回到可以正常更新的状态。而将整个库设置为 readonly 之后，如果客户端发生异常，则数据库就会一直保持 readonly 状态，这样会导致整个库长时间处于不可写状态，风险较高。 ","date":"2021-12-01 21:31:10","objectID":"/mysql_advanced_01/:6:1","tags":["mysql"],"title":"Mysql_advanced_01","uri":"/mysql_advanced_01/"},{"categories":["Advanced learning"],"content":"表级锁 MySQL 里面表级别的锁有两种：一种是表锁，一种是元数据锁（meta data lock，MDL)。表锁的语法是 lock tables … read/write。与 FTWRL 类似，可以用 unlock tables 主动释放锁，也可以在客户端断开的时候自动释放。需要注意，lock tables 语法除了会限制别的线程的读写外，也限定了本线程接下来的操作对象。 举个例子, 如果在某个线程 A 中执行 lock tables t1 read, t2 write; 这个语句，则其他线程写 t1、读写 t2 的语句都会被阻塞。同时，线程 A 在执行 unlock tables 之前，也只能执行读 t1、读写 t2 的操作。连写 t1 都不允许，自然也不能访问其他表。 在还没有出现更细粒度的锁的时候，表锁是最常用的处理并发的方式。而对于 InnoDB 这种支持行锁的引擎，一般不使用 lock tables 命令来控制并发，毕竟锁住整个表的影响面还是太大。 另一类表级的锁是 MDL（metadata lock)。MDL 不需要显式使用，在访问一个表的时候会被自动加上。MDL 的作用是，保证读写的正确性。你可以想象一下，如果一个查询正在遍历一个表中的数据，而执行期间另一个线程对这个表结构做变更，删了一列，那么查询线程拿到的结果跟表结构对不上，肯定是不行的。 因此，在 MySQL 5.5 版本中引入了 MDL，当对一个表做增删改查操作的时候，加 MDL 读锁；当要对表做结构变更操作的时候，加 MDL 写锁。 读锁之间不互斥，因此你可以有多个线程同时对一张表增删改查。 读写锁之间、写锁之间是互斥的，用来保证变更表结构操作的安全性。因此，如果有两个线程要同时给一个表加字段，其中一个要等另一个执行完才能开始执行。 虽然 MDL 锁是系统默认会加的，但却是你不能忽略的一个机制。比如下面这个例子，我经常看到有人掉到这个坑里：给一个小表加个字段，导致整个库挂了。 你肯定知道，给一个表加字段，或者修改字段，或者加索引，需要扫描全表的数据。在对大表操作的时候，你肯定会特别小心，以免对线上服务造成影响。而实际上，即使是小表，操作不慎也会出问题。我们来看一下下面的操作序列，假设表 t 是一个小表。 我们可以看到 session A 先启动，这时候会对表 t 加一个 MDL 读锁。由于 session B 需要的也是 MDL 读锁，因此可以正常执行。之后 session C 会被 blocked，是因为 session A 的 MDL 读锁还没有释放，而 session C 需要 MDL 写锁，因此只能被阻塞。 如果只有 session C 自己被阻塞还没什么关系，但是之后所有要在表 t 上新申请 MDL 读锁的请求也会被 session C 阻塞。前面我们说了，所有对表的增删改查操作都需要先申请 MDL 读锁，就都被锁住，等于这个表现在完全不可读写了。 如果某个表上的查询语句频繁，而且客户端有重试机制，也就是说超时后会再起一个新 session 再请求的话，这个库的线程很快就会爆满。 你现在应该知道了，事务中的 MDL 锁，在语句执行开始时申请，但是语句结束后并不会马上释放，而会等到整个事务提交后再释放。 基于上面的分析，我们来讨论一个问题，如何安全地给小表加字段？ 首先我们要解决长事务，事务不提交，就会一直占着 MDL 锁。在 MySQL 的 information_schema 库的 innodb_trx 表中，你可以查到当前执行中的事务。如果你要做 DDL 变更的表刚好有长事务在执行，要考虑先暂停 DDL，或者 kill 掉这个长事务。 但考虑一下这个场景。如果你要变更的表是一个热点表，虽然数据量不大，但是上面的请求很频繁，而你不得不加个字段，你该怎么做呢？ 这时候 kill 可能未必管用，因为新的请求马上就来了。比较理想的机制是，在 alter table 语句里面设定等待时间，如果在这个指定的等待时间里面能够拿到 MDL 写锁最好，拿不到也不要阻塞后面的业务语句，先放弃。之后开发人员或者 DBA 再通过重试命令重复这个过程。 MariaDB 已经合并了 AliSQL 的这个功能，所以这两个开源分支目前都支持 DDL NOWAIT/WAIT n 这个语法。 ALTER TABLE tbl_name NOWAIT add column ... ALTER TABLE tbl_name WAIT N add column ... ","date":"2021-12-01 21:31:10","objectID":"/mysql_advanced_01/:6:2","tags":["mysql"],"title":"Mysql_advanced_01","uri":"/mysql_advanced_01/"},{"categories":["Advanced learning"],"content":"小结 全局锁主要用在逻辑备份过程中。对于全部是 InnoDB 引擎的库，我建议你选择使用–single-transaction 参数，对应用会更友好。表锁一般是在数据库引擎不支持行锁的时候才会被用到的。如果你发现你的应用程序里有 lock tables 这样的语句，你需要追查一下，比较可能的情况是：要么是你的系统现在还在用 MyISAM 这类不支持事务的引擎，那要安排升级换引擎；要么是你的引擎升级了，但是代码还没升级。我见过这样的情况，最后业务开发就是把 lock tables 和 unlock tables 改成 begin 和 commit，问题就解决了。MDL 会直到事务提交才释放，在做表结构变更的时候，你一定要小心不要导致锁住线上查询和更新。 ","date":"2021-12-01 21:31:10","objectID":"/mysql_advanced_01/:6:3","tags":["mysql"],"title":"Mysql_advanced_01","uri":"/mysql_advanced_01/"},{"categories":["Advanced learning"],"content":"行锁 MySQL 的行锁是在引擎层由各个引擎自己实现的。但并不是所有的引擎都支持行锁，比如 MyISAM 引擎就不支持行锁。不支持行锁意味着并发控制只能使用表锁，对于这种引擎的表，同一张表上任何时刻只能有一个更新在执行，这就会影响到业务并发度。InnoDB 是支持行锁的，这也是 MyISAM 被 InnoDB 替代的重要原因之一。 了解行锁，以及如何通过减少锁冲突来提升业务并发度。 ","date":"2021-12-01 21:31:10","objectID":"/mysql_advanced_01/:7:0","tags":["mysql"],"title":"Mysql_advanced_01","uri":"/mysql_advanced_01/"},{"categories":["Advanced learning"],"content":"从两阶段说起 ","date":"2021-12-01 21:31:10","objectID":"/mysql_advanced_01/:7:1","tags":["mysql"],"title":"Mysql_advanced_01","uri":"/mysql_advanced_01/"},{"categories":["Advanced learning"],"content":"事务要不要隔离？ ","date":"2021-12-01 21:31:10","objectID":"/mysql_advanced_01/:8:0","tags":["mysql"],"title":"Mysql_advanced_01","uri":"/mysql_advanced_01/"},{"categories":["School courses"],"content":" 2022春JOANNA老师的软件体系结构 topics 1.Introduction to Software Architecture Why is Software Architecture Important? The Many Contexts of Software Architecture 2.Architecture modelling and representation: Architectural structures and views 3.Quality attributes :Understanding quality attributes and availability Availability Quality attributes: interoperability and modifiability Interoperability Quality attributes: Modifiability Quality attributes: Performance, Security Quality attributes: Security Patterns and Tactics Quality Attribute Modelling and Analysis Designing for architecturally significant requirements Designing and evaluating an architecture Exercise 1 Capturing ASR in practice . exercise2 exercise3 10%+30%+60% ","date":"2021-11-25 08:44:33","objectID":"/sek_base_07/:0:0","tags":["software engineering knowledge"],"title":"SEK_base_07","uri":"/sek_base_07/"},{"categories":["School courses"],"content":"topics Introduction to the subject: objectives, plan, assessment.Introduction to software architecture: concept and importance of software architecture Architecture modelling and representation: architectural structures and views, UML Quality attributes: Understanding quality attributes and availability Quality attributes: interoperability and modifiability Quality attributes: performance and security Quality attributes: testability, usability and other quality attributes Achieving quality attribute through tactics and patterns: architectural tactics and patterns Achieving quality attribute through tactics and patterns: architectural tactics and patterns Achieving quality attribute through tactics and patterns: quality attribute modelling and analysis Achieving quality attribute through tactics and patterns: designing for architecturally significant requirements Designing and evaluating an architecture: TOGAF, ADD and ATAM Architecture reuse: software product lines, frameworks and middleware Capturing ASR in practice (practical exercises) Designing an architecture in practice (practical exercises) Documenting software architecture in practice (practical exercises) Architecture evaluation in practice (practical exercises) ","date":"2021-11-25 08:44:33","objectID":"/sek_base_07/:1:0","tags":["software engineering knowledge"],"title":"SEK_base_07","uri":"/sek_base_07/"},{"categories":["School courses"],"content":"1.Introduction to Software Architecture The software architecture of a system is the set of structures needed to reason about the system, which comprise software elements, relations among them, and properties of both. We can say that architecture is an abstraction. What does it mean that architecture is an abstraction? If the information about element is no useful for reasoning about the system, architecture will omit it. Architecture shows the system in an abstract way - in terms of its elements and their relationships. The architects design structures – make decisions on what architectural elements should be used in a system to solve the problems that the system will face. Some compositions of architectural elements that solve particular problems may be used again in different systems facing similar problems. If a certain composition of architectural elements that solves a particular problem was found useful over time and over many domains, has been documented and disseminated, it may become a/an…. Architectural pattern Architecture Is a Set of Software Structures : A structure is a set of elements held together by a relation. Software systems are composed of many structures, and no single structure holds claim to being the architecture. There are three important categories of architectural structures. Module Component and Connector Allocation Architecture vs Design The architecture - the selection of architectural elements, their interaction and the restrictions on those elements and their interactions. The design - modularization and the detailed interfaces of the elements of the system, their algorithms and procedures, and the kinds of data needed to support the architecture and satisfy the requirements. Architecture is an Abstraction An architecture specifically omits certain information about elements that is not useful for reasoning about the system. The architectural abstraction lets us look at the system in terms of its elements, how they are arranged, how they interact, how they are composed, and so forth. This abstraction is essential to taming the complexity of an architecture. Every System has a Software Architecture But the architecture may not be known to anyone. Perhaps all of the people who designed the system are long gone Perhaps the documentation has vanished (or was never produced) Perhaps the source code has been lost (or was never delivered) An architecture can exist independently of its description or specification Structures and Views A view is a representation of a coherent set of architectural elements, as written by and read by system stakeholders. A structure is the set of elements itself, as they exist in software or hardware. In short, a view is a representation of a structure. For example, a module structure is the set of the system’s modules and their organization. A module view is the representation of that structure, documented according to a template in a chosen notation, and used by some system stakeholders. Architects design structures. They document views of those structures. Architects design structures. Architects document views of those structures. What is a view? a representation of a coherent set of architectural elements is written by the architects may be read by the testers or the project clients Architectural Patterns Architectural elements can be composed in ways that solve particular problems. The compositions have been found useful over time, and over many different domains They have been documented and disseminated. These compositions of architectural elements, called architectural patterns. Patterns provide packaged strategies for solving some of the problems facing a system. An architectural pattern delineates the element types and their forms of interaction used in solving the problem. What Makes a “Good” Architecture? There is no such thing as an inherently good or bad architecture. Architectures can be evaluated but only in the context of specific stated goals. Architec","date":"2021-11-25 08:44:33","objectID":"/sek_base_07/:2:0","tags":["software engineering knowledge"],"title":"SEK_base_07","uri":"/sek_base_07/"},{"categories":["School courses"],"content":"Why is Software Architecture Important? Inhibiting or Enabling System’s Quality Attributes Whether a system will be able to exhibit its desired (or required) quality attributes is substantially determined by its architecture. Performance Modifiability Security Scalability Reusability Reasoning About and Managing Change About 80 percent of a typical software system’s total cost occurs after initial deployment accommodate new features adapt to new environments, fix bugs, and so forth. Every architecture partitions possible changes into three categories A local change can be accomplished by modifying a single element. A nonlocal change requires multiple element modifications but leaves the underlying architectural approach intact. An architectural change affects the fundamental ways in which the elements interact with each other and will require changes all over the system. Obviously, local changes are the most desirable A good architecture is one in which the most common changes are local, and hence easy to make. Which kind of a change in the system is most desirable?(1 answer) A local change that can be accomplished by modifying a single element. A nonlocal change that requires multiple element modifications but leaves the underlying architectural approach intact. An architectural change that affects the fundamental ways in which the elements interact with each other and will require changes all over the system. Predicting System Qualities When we examine an architecture we can confidently predict that the architecture will exhibit the associated qualities. The earlier you can find a problem in your design, the cheaper, easier, and less disruptive it will be to fix. Enhancing Communication Among Stakeholders The architecture—or at least parts of it—is sufficiently abstract that most nontechnical people can understand it. Most of the system’s stakeholders can use as a basis for creating mutual understanding, negotiating, forming consensus, and communicating with each other. Each stakeholder of a software system is concerned with different characteristics of the system Users, client, manager, architect Earliest Design Decisions Software architecture is a manifestation of the earliest design decisions about a system. These early decisions affect the system’s remaining development, its deployment, and its maintenance life. Earliest Design Decisions What are these early design decisions? Will the system run on one processor or be distributed across multiple processors? Will the software be layered? If so, how many layers will there be? What will each one do? Will components communicate synchronously or asynchronously? What communication protocol will we choose? Will the system depend on specific features of the operating system or hardware? Will the information that flows through the system be encrypted or not? Defining Constraints on an Implementation An implementation exhibits an architecture if it conforms to the design decisions prescribed by the architecture. The implementation must be implemented as the set of prescribed elements These elements must interact with each other in the prescribed fashion Each of these prescriptions is a constraint on the implementer. Influencing the Organizational Structure Architecture prescribes the structure of the system being developed. The architecture is typically used as the basis for the work-breakdown structure. The work-breakdown structure in turn dictates units of planning, scheduling, and budget interteam communication channels configuration and file-system organization integration and test plans and procedures; the maintenance activity Enabling Evolutionary Prototyping Once an architecture has been defined, it can be prototyped as a skeletal system. A skeletal system is one in which at least some of the infrastructure is built before much of the system’s functionality has been created. The fidelity of the system increases as prototype parts are replaced with complete versions of these","date":"2021-11-25 08:44:33","objectID":"/sek_base_07/:2:1","tags":["software engineering knowledge"],"title":"SEK_base_07","uri":"/sek_base_07/"},{"categories":["School courses"],"content":"The Many Contexts of Software Architecture Contexts of Software Architecture We put software architecture in its place relative to four contexts: Technical. What technical role does the software architecture play in the system? Project life cycle. How does a software architecture relate to the other phases of a software development life cycle? Business. How does the presence of a software architecture affect an organization’s business environment? Professional. What is the role of a software architect in an organization or a development project? Technical Context The most important technical context factor is the set of quality attributes that the architecture can help to achieve. The architecture’s current technical environment is also an important factor. Standard industry practices Software engineering techniques prevalent in the architect’s professional community. Project Life-cycle Context Software development processes are standard approaches for developing software systems. They tell the members of the team what to do next. There are four dominant software development processes: Waterfall Iterative Agile Model-driven development Architecture Activities Architecture is a special kind of design, so architecture finds a home in each process of software development. There are activities involved in creating a software architecture, using it to realize a complete design, and then implementing Understanding the architecturally significant requirements Creating or selecting the architecture Documenting and communicating the architecture Analyzing or evaluating the architecture Implementing and testing the system based on the architecture Ensuring that the implementation conforms to the architecture Business Context Systems are created to satisfy the business goals of one or more organizations. Development organizations: e.g., make a profit, or capture market, or help their customers do their jobs better, or keep their staff employed, or make their stockholders happy Customers have their own goals: e.g. ,make their lives easier or more productive. Other organizations, such as subcontractors or government regulatory agencies, have their own goals Architects need to understand the goals. Many of these goals will influence the architecture. Architecture and business goals: Professional Context Architects need more than just technical skills. Architects need diplomatic, negotiation, and communication skills. Architects need the ability to communicate ideas clearly Architects need up-to-date knowledge. Know about (for example) patterns, or database platforms, or web services standards. Know about business considerations. Stakeholders A stakeholder is anyone who has a stake in the success of the system Stakeholders typically have different specific concerns on the system Early engagement of stakeholders to understand the constraints of the task, manage expectations, negotiate priorities, and make tradeoffs. How is Architecture Influenced? Technical requirements Architects Business, social, and technical environment What Do Architectures Influence? Technical context The architecture can affect stakeholder’s requirements for the next system A customer may relax some of their requirements to gain these economies. Shrinkwrapped software has affected people’s requirements, as it is inexpensive and of high quality. Project context The architecture affects the structure of the developing organization. An architecture prescribes the units of software to be implemented and integrated to the system. These units are the basis for the development project’s structure. the development, test, and integration activities all revolve around the units. Business context The architecture can affect the business goals of the developing organization. The architecture can provide opportunities for the efficient production and deployment of similar systems, and the organization may adjust its goals to take advantage of its newfound expertise. Professional co","date":"2021-11-25 08:44:33","objectID":"/sek_base_07/:2:2","tags":["software engineering knowledge"],"title":"SEK_base_07","uri":"/sek_base_07/"},{"categories":["School courses"],"content":"2.Architecture modelling and representation: Architectural structures and views Intended Learning Outcomes By the end of this lesson you will be able to: differentiate between structures and views list useful structures and views and understand how they relate to each other choose the relevant structure and views combine views use UML to express the views Architecture Is a Set of Software Structures A structure is a set of elements held together by a relation. Software systems are composed of many structures, and no single structure holds claim to being the architecture. There are three important categories of architectural structures. Module Component and Connector Allocation Module Structures Some structures partition systems into implementation units, which we call modules. Modules are assigned specific computational responsibilities, and are the basis of work assignments for programming teams. In large projects, these elements (modules) are subdivided for assignment to sub-teams. Component-and-connector Structures Other structures focus on the way the elements interact with each other at runtime to carry out the system’s functions. We call runtime structures component-and-connector (C\u0026C) structures. In our use, a component is always a runtime entity. In SOA, the system is to be built as a set of services. These services are made up of (compiled from) the programs in the various implementation units – modules. Allocation Structures Allocation structures describe the mapping from software structures to the system’s environments For example Modules are assigned to teams to develop, and assigned to places in a file structure for implementation, integration, and testing. Components are deployed onto hardware in order to execute. Which Structures are Architectural? A structure is architectural if it supports reasoning about the system and the system’s properties. The reasoning should be about an attribute of the system that is important to some stakeholder. These include functionality achieved by the system the availability of the system in the face of faults the difficulty of making specific changes to the system the responsiveness of the system to user requests, many others. Programming team developing our cafeteria system wants to know what units of implementation will be assigned with functional responsibilities. Which structures can help reason about that? Module structures Structures and Views A structure is the set of elements itself, as they exist in software or hardware. In short, a view is a representation of a structure. Architects design structures. They document views of those structures. Physiological Structures The neurologist, the orthopedist, the hematologist, and the dermatologist all have different views of the structure of a human body. Ophthalmologists, cardiologists, and podiatrists concentrate on specific subsystems. The kinesiologist and psychiatrist are concerned with different aspects of the entire arrangement’s behavior. Although these views are pictured differently and have different properties, all are inherently related, interconnected. Together they describe the architecture of the human body. So it is with software! Module Structures Module structures embody decisions as to how the system is to be structured as a set of code or data units In any module structure, the elements are modules of some kind (perhaps classes, or layers, or merely divisions of functionality, all of which are units of implementation). Modules are assigned areas of functional responsibility. Component-and-connector Structures Component-and-connector structures embody decisions as to how the system is to be structured as a set of elements that have runtime behavior (components) and interactions (connectors). Elements are runtime components such as services, peers, clients, servers, or many other types of runtime element) Connectors are the communication vehicles among components, such as call-return, process synchronization o","date":"2021-11-25 08:44:33","objectID":"/sek_base_07/:3:0","tags":["software engineering knowledge"],"title":"SEK_base_07","uri":"/sek_base_07/"},{"categories":["School courses"],"content":"3.Quality attributes :Understanding quality attributes and availability Intended Learning Outcomes By the end of this lesson you will be able to: specify a quality attribute requirement understand quality design decisions apply the design decision categories to different quality attributes Architecture and Requirements System requirements can be categorized as: Functional requirements state what the system must do, how it must behave or react to run-time stimuli. Quality attribute requirements qualify functional requirements, e.g., how fast the function must be performed, how resilient it must be to erroneous input, etc. Constraints. A constraint is a design decision with zero degrees of freedom. Which of the above system requirements will have an impact on the architecture of the system?（1 answer） Functional requirements Quality attributes requirements Constraints Functionality Functionality is the ability of the system to do the work for which it was intended. Functionality has a strange relationship to architecture: functionality does not determine architecture; Quality Attribute Considerations If a functional requirement is “when the user presses the green button the Options dialog appears”: a performance qualification might describe how quickly the dialog will appear; an availability qualification might describe how often this function will fail, and how quickly it will be repaired; a usability qualification might describe how easy it is to learn this function. Two Categories of Quality Attributes The ones that describe some properties of the system at runtime Availability, performance, usability, security The ones that describe some properties of the development of system Modifiability Testability Quality Attribute Considerations There are problems with previous discussions of quality attributes: Untestable definitions. The definitions provided for an attribute are not testable. It is meaningless to say that a system will be “modifiable” Overlapping concerns. Is a system failure due to a denial of service attack an aspect of availability, performance, security, or usability? A solution to the problems (untestable definitions and overlapping concerns) is to use quality attribute scenarios as a means of characterizing quality attributes. Specifying Quality Attribute Requirements We use a common form to specify all quality attribute requirements as scenarios. Our representation of quality attribute scenarios has these parts: Stimulus Stimulus source Response Response measure Environment Artifact Specifying Quality Attribute Requirements Stimulus. he stimulus is a condition that requires a response when it arrives at a system. Source of stimulus. his is some entity (a human, a computer system, or any other actuator) that generated the stimulus. Response. he response is the activity undertaken as the result of the arrival of the stimulus. Response measure. hen the response occurs, it should be measurable in some fashion so that the requirement can be tested. Environment. he stimulus occurs under certain conditions. The system may be in an overload condition or in normal operation, or some other relevant state. Artifact. his may be a collection of systems, the whole system, or some piece or pieces of it. Some artifact is stimulated. Specifying Quality Attribute Requirements General quality attribute scenarios are system independent and can, potentially, pertain to any system Concrete quality attribute scenarios are specific to the particular system under consideration. Specifying Quality Attribute Requirements Example general scenario for availability: Achieving Quality Attributes Through Tactics There are a collection of primitive design techniques that an architect can use to achieve a quality attribute response. We call these architectural design primitives tactics. Tactics, like design patterns, are techniques that architects have been using for years. We do not invent tactics, we simply capture what architects do in pract","date":"2021-11-25 08:44:33","objectID":"/sek_base_07/:4:0","tags":["software engineering knowledge"],"title":"SEK_base_07","uri":"/sek_base_07/"},{"categories":["School courses"],"content":"Availability What is Availability? Availability refers to a property of software that it is there and ready to carry out its task when you need it to be. Availability refers to the ability of a system to mask or repair faults such that the cumulative service outage period does not exceed a required value over a specified time interval. Availability is about minimizing service outage time by mitigating faults Availability Availability v.s. reliability or dependability Availability encompasses what is normally called reliability. Availability encompasses other consideration such as service outage due to period maintenance Availability is closely related to security, e..g, denial-of-service performance … Availability General Scenario Stimulus … … … … … … ? Stimulus source … … … ? Response … … … … … …? Response measure … …? Environment … … … … .? Artifact … … … … … … ..? Specifying Quality Attribute Requirements Stimulus. he stimulus is a condition that requires a response when it arrives at a system. Source of stimulus. his is some entity (a human, a computer system, or any other actuator) that generated the stimulus. Response. he response is the activity undertaken as the result of the arrival of the stimulus. Response measure. hen the response occurs, it should be measurable in some fashion so that the requirement can be tested. Environment. he stimulus occurs under certain conditions. The system may be in an overload condition or in normal operation, or some other relevant state. Artifact. his may be a collection of systems, the whole system, or some piece or pieces of it. Some artifact is stimulated. Availability General Scenario: Sample Concrete Availability Scenario The heartbeat monitor determines that the server is nonresponsive during normal operations. The system informs the operator and continues to operate with no downtime. The heartbeat monitor determines that the server is nonresponsive during normal operations. The system informs the operator and continues to operate with no downtime. Let us analyze if this scenario is complete Specifying Quality Attribute Requirements Stimulus. he stimulus is a condition that requires a response when it arrives at a system. Source of stimulus. his is some entity (a human, a computer system, or any other actuator) that generated the stimulus. Response. he response is the activity undertaken as the result of the arrival of the stimulus. Response measure. hen the response occurs, it should be measurable in some fashion so that the requirement can be tested. Environment. he stimulus occurs under certain conditions. The system may be in an overload condition or in normal operation, or some other relevant state. Artifact. his may be a collection of systems, the whole system, or some piece or pieces of it. Some artifact is stimulated. Sample Concrete Availability Scenario The heartbeat monitor determines that the server is nonresponsive during normal operations. The system informs the operator and continues to operate with no downtime. Stimulus: ??? The stimulus is a condition that requires a response when it arrives at a system. Sample Concrete Availability Scenario The heartbeat monitor determines that the server is nonresponsive during normal operations. The system informs the operator and continues to operate with no downtime. Stimulus: non-responsiveness Response: inform the operator Response measure: no downtime, or 100 availability percentages Environment: normal operation Artifact: heartbeat monitor Stimulus source: server We will continue this topic on Monday Which statement is NOT true?（The last one） A stakeholder is anyone who has a stake in the success of the system Stakeholders typically have different specific concerns on the system Stakeholders participate in some parts of the design process. Architect is the only stakeholder of the system Recall our earlier example University Town, has directed its software development subsidiary, Campus Software, to develop a cafeteria s","date":"2021-11-25 08:44:33","objectID":"/sek_base_07/:4:1","tags":["software engineering knowledge"],"title":"SEK_base_07","uri":"/sek_base_07/"},{"categories":["School courses"],"content":"Quality attributes: interoperability and modifiability ","date":"2021-11-25 08:44:33","objectID":"/sek_base_07/:5:0","tags":["software engineering knowledge"],"title":"SEK_base_07","uri":"/sek_base_07/"},{"categories":["School courses"],"content":"Interoperability Intended Learning Outcomes By the end of this lesson you will be able to: apply the design decision categories to interoperability What is Interoperability? Interoperability is about the degree to which two or more systems can usefully exchange meaningful information via interfaces in a particular context. Any discussion of a system’s interoperability needs to identify with whom, and under what circumstance. Syntactic interoperability is the ability to exchange data. Semantic interoperability is the ability to interpret the data being exchanged. Two perspectives for achieving interoperability With the knowledge about the interfaces of external systems, design that knowledge into the system Without the knowledge about other systems, design the system to interoperate in a more general fashion Motivation The system provides a service to be used by a collection of unknown systems, eg., GoogleMaps The system is constructed from existing systems, for example Producing a representation of what was sensed Interpreting the data Processing the raw data Sensing the environment Two Important Aspects of Interoperability Discovery. The consumer of a service must discover the location, identity, and interface of service Handling the response. Three possibilities: The service reports back to the requester The service sends its response on to another system The service broadcasts its response to any interested parties Example Will this system need to provide services to other system(s)? What do we know about these systems? Will this system be constructed by combining capabilities from other systems? Example - a traffic sensing system where the input comes from individual vehicles, the raw data is processed into common units of measurement, is interpreted and fused, and traffic congestion information is broadcast one of the existing systems is responsible for sensing its environment another one is responsible for processing the raw data a third is responsible for interpreting the data a final one is responsible for producing and distributing a representation of what was sensed Intelligent transportation system Interoperability General Scenario Sample Concrete Interoperability Scenario Our vehicle information system sends our current location to the traffic monitoring system. The traffic monitoring system combines our location with other information, overlays this information on a Google Map, and broadcasts it. Our location information is correctly included with a probability of 99.9%. Sample Concrete Interoperability Scenario Goal of Interoperability Tactics For two or more systems to usefully exchange information they must Know about each other. That is the purpose behind the locate tactics. Exchange information in a semantically meaningful fashion. That is the purpose behind the manage interfaces tactics. Two aspects of the exchange are Provide services in the correct sequence Modify information produced by one actor to a form acceptable to the second actor. Goal of Interoperability Tactics Interoperability Tactics Locate Service Discovery : Locate a service through searching There are many service discovery mechanisms: UDDI for Webservices Jini for Jave objects Simple Service Discovery Protocol (SSDP) as used in Universal plug-and-play (UPnP) DNS Service Discovery (DNS-SD) Bluetooth Service Discovery Protocol (SDP) Service Discovery – Necessary conditions The searcher wants to find the searched entity and the searched entity wants to be found The searched entity must have identifiers The searcher must acquire sufficient identifiers to identify the searched entity Searching Method – Searcher’s initiative Flood/Broadcast request Ask every entity and wait for answer Examples Paging in the location area to find the mobile terminal DHCP discover: the client broadcasts on the local subnet to find available servers to ask for IP address Efficient and less resource consuming for the searcher Low resource consuming for the searched B","date":"2021-11-25 08:44:33","objectID":"/sek_base_07/:5:1","tags":["software engineering knowledge"],"title":"SEK_base_07","uri":"/sek_base_07/"},{"categories":["School courses"],"content":"Quality attributes: Modifiability Intended Learning Outcomes By the end of this lesson you will be able to: apply the design decision categories to modifiability What is Modifiability? Modifiability is about change and our interest in it is in the cost and risk of making changes. To plan for modifiability, an architect has to consider four questions: What can change? What is the likelihood of the change? When is the change made and who makes it? What is the cost of the change? Recall our earlier example University Town, has directed its software development subsidiary, Campus Software, to develop a cafeteria system that supports a network of cafeteria Kiosks and POSs (points of sale). Kiosks are distributed in diverse locations of University Town near cafeterias and POSs are located near meal serving stations inside of cafeterias. The users are students, faculty and other employees of University Town. They use Kiosks to make queries, and funds transfers from their bank accounts to their cafeteria accounts. They use POSs to pay for their meals. What can change? The functions of the system computers The platforms, i.e., the hardware, operating system, middleware The environment in which the system operates The systems with which it must interoperate The protocols it uses to communicate The capacity Number of users supported Number of simultaneous operations When is the change made and who makes it? Changes can be made during implementation by modifying the source code build by choice of libraries execution by parameter setting, plugins, etc Changes can also be made by a developer an end user a system administrator What is the cost of the change? Involving two types of cost The cost of introducing the mechanisms to make the system more modifiable The cost of making the modification using the mechanisms Example User interface builder Example You are an architect on the Campus Software team. There is a possibility that in the future, the client will request changes to be made to the user interface. Your team is considering two ways to handle this problem Do nothing for now. Wait for a change request to come in, then change the source code to accommodate request. Add additional component to the system now – user interface builder. When there is a change request, the designer will use a drag-and-drop editor to design a new interface, and the user interface builder will produce the new source code directly Let us consider what is the cost of each option Considering the differences in cost between these options, which one will be a better choice? Which option will be a better choice if the chance of the need to make a change is not high?（1 answer） Do nothing for now. Add user interface builder to the system. Which option will be a better choice if there is a chance that the changes in the interface will be requested many times??(1 answer) Do nothing for now. Add user interface builder to the system. Modifiability General Scenario Sample Concrete Modifiability Scenario The developer wishes to change the user interface by modifying the code at design time. The modifications are made with no side effects within three hours. Stimulus – Wishes to change UI Artifact – Code Environment: Design time Response – Change made Response measure – No side effects in three hours Source - Developer Goal of Modifiability Tactics Goal of modifiability controlling the complexity of making changes, controlling the time and cost to make changes. Modifiability Tactics Design 1 – the module includes a great deal of capability Design B – the module is refined into several smaller modules (each capability is represented in a separate module) If there is a chance needed in this module, which design will have smaller modification cost?(Second one is right) Reduce Size of a Module Split Module: If the module being modified includes a great deal of capability, the modification costs will likely be high. Refining the module into several smaller modules should reduce","date":"2021-11-25 08:44:33","objectID":"/sek_base_07/:5:2","tags":["software engineering knowledge"],"title":"SEK_base_07","uri":"/sek_base_07/"},{"categories":["School courses"],"content":"Quality attributes: Performance, Security Intended Learning Outcomes By the end of this lesson you will be able to: apply the design decision categories to diverse quality attributes What is Performance? It is about time Performance is about time and the software system’s ability to meet timing requirements When events occur, the system must respond to them in time Events include interrupts, messages, requests from users or other systems, or clock events marking the passage of time Performance General Scenario Performance concrete scenario - example You are an architect on the Campus Software team and you are designing the system for performance In the requirements document you have found this requirement: The system should process the transactions with an average latency of two seconds Is this description complete, is it clear enough for the architect to make good design decisions? Which parts are missing? Which parts are missing in the performance concrete scenario example we discussed? Source Stimulus Artifact Environment Response Response measure Sample Concrete Performance Scenario – not complete The system processes the transactions with an average latency of two seconds. Stimulus: transaction arrivals Source: ??? Artifact: the system Response: process the transactions Response measure: average latency of two seconds Environment: ??? This scenario is not complete, source and environment parts are missing How to add the remaining parts? We can use the general scenario to help us look for ideas Then, clarify with the stakeholders about the details of the requirement Performance General Scenario Sample Concrete Performance Scenario - complete Users initiate transactions under normal operations. The system processes the transactions with an average latency of two seconds. Stimulus: transaction arrivals Source: users Artifact: the system Response: process the transactions Response measure: average latency of two seconds Environment: under normal operations Sample Concrete Performance Scenario Users initiate transactions under normal operations. The system processes the transactions with an average latency of two seconds. Stimulus: transaction arrivals Source: users Artifact: the system Response: process the transactions Response measure: average latency of two seconds Environment: under normal operations Performance Modeling Two basic contributors to the response time Processing time is the time that the system is working to respond Blocked time is the time that the system is unable to respond Blocked time is caused by Contention for resources Availability of resources Dependency on other computations Goal of Performance Tactics To generate a response to an event arriving the system within some time-based constraint The event can be single or a stream, and is the trigger to perform computation First, lets consider an example not related to the computing You are an owner of the noodle shop. You hired one cook who is able to cook 100 bowls during lunch time Your restaurant has 20 seats On the first day, there were over 200 people lining up for your noodles You want to improve the performance of your noodle shop to make sure you serve as many customers as possible during the day. Give examples of the tactics that you could use Two Tactic Categories Control resource demand To produce smaller demand on the resources Operate on the demand side Manage resources To make the resources at hand work more effectively in handling the demands Operate on the response side Resources Hardware resources, e.g., CPU, data stores, network bandwidth, and memory Software resources, e.g., buffers, or critical sections Performance Tactics Control Resource Demand Manage Sampling Rate: to reduce the sampling frequency at which a stream of data is captured Prioritize Events: to impose a priority scheme that ranks events according to the importance Ignore low-priority events when resources are not enough Reduce Overhead: The use of intermediaries increas","date":"2021-11-25 08:44:33","objectID":"/sek_base_07/:6:0","tags":["software engineering knowledge"],"title":"SEK_base_07","uri":"/sek_base_07/"},{"categories":["School courses"],"content":"Quality attributes: Security What is Security? Security is a measure of the system’s ability to protect data and information from unauthorized access while still providing access to people and systems that are authorized An action taken against a computer system with the intention of doing harm is called an attack Attack can be in different forms unauthorized attempt to access/modify data or service intended to deny services to legitimate users What is Security? Security has three main characteristics, called CIA: Confidentiality is the property that data or services are protected from unauthorized access. For example, a hacker cannot access your income tax returns on a government computer. Integrity is the property that data or services are not subject to unauthorized manipulation. For example, your grade has not been changed since your instructor assigned it. Availability is the property that the system will be available for legitimate use. For example, a denial-of-service attack prevent you from ordering a book from an online bookstore. What is Security? Other characteristics that support CIA are Authentication verifies the identities of the parties to a transaction and checks if they are truly who they claim to be. Authorization grants a user the privileges to perform a task. Nonrepudiation guarantees that the sender/recipient of a message cannot later deny having sent/received the message Sample Concrete Security Scenario A disgruntled employee from a remote location attempts to modify the pay rate table during normal operations. The system maintains an audit trail and the correct data is restored within a day. Stimulus: unauthorized attempts to modify the pay rate table Stimulus source: a disgruntled employee Artifact: the system with pay rate table Environment: during normal operation Response: maintains an audit trail Response measure: correct data is restored within a day Security Tactics Detect Attacks Detect Intrusion: compare network traffic or service request patterns within a system to a set of signatures or known patterns of malicious behavior stored in a database. Detect Service Denial: comparison of the pattern or signature of network traffic coming into a system to historic profiles of known Denial of Service (DoS) attacks. Denial of Service Attack Ping of Death UDP Flood TCP SYN Detect Attacks Verify Message Integrity: use techniques such as checksums or hash values to verify the integrity of messages, resource files, deployment files, and configuration files. Detect Attacks Detect Message Delay: checking the time that it takes to deliver a message, it is possible to detect suspicious timing behavior, i.e., man-in-the-middle attack Resist Attacks Identify Actors: identify the source of any external input to the system. Authenticate Actors: ensure that a user or remote computer is actually who or what it purports to be. Authorize Actors: ensuring that an authenticated actor has the rights to access and modify either data or services. Limit Access: limiting access to resources such as memory, network connections, or access points. Resist Attacks Limit Exposure: minimize the attack surface of a system by having the fewest possible number of access points. E.g., firewall is a single point of access to the intranet E.g., closing a port Passive defense – no human analysis or interaction - limiting security gaps and exposure to threats through firewalls, antimalware systems, intrusion prevention systems, antivirus protection, intrusion detection systems… Resist Attacks Encrypt Data: apply some form of encryption to data and to communication. Symmetric encryption： asymmetric encryption： Resist Attacks Separate Entities: can be done through physical separation on different servers, the use of virtual machines Change Default Settings: Force the user to change settings assigned by default. React to Attacks Revoke Access: limit access to sensitive resources, even for normally legitimate users and uses, if an attack is s","date":"2021-11-25 08:44:33","objectID":"/sek_base_07/:6:1","tags":["software engineering knowledge"],"title":"SEK_base_07","uri":"/sek_base_07/"},{"categories":["School courses"],"content":"Patterns and Tactics Intended Learning Outcomes By the end of this lesson you will be able to: Understand how patterns achieve quality attributes Understand relation between tactics and patterns Know how to use tactics together What is a Pattern? An architecture pattern is a package of design decisions that is found repeatedly in practice has known properties that permits reuse, and describes a class of architectures Architectural Patterns Module patterns Layered pattern Component-and-Connector patterns Broker pattern Model-View-Controller pattern Pipe-and-Filter pattern Client-Server pattern Peer-to-Peer pattern Service-Oriented Architecture (SOA) pattern Publish-Subscribe pattern Shared-Data pattern Allocation patterns Map-Reduce pattern Multi-tier Pattern Layer Pattern Context: Modules of the system may be independently developed and maintained. Problem: To minimize the interaction among the different development organizations, and support portability, modifiability, and reuse. Layer Pattern Solution: the layered pattern divides the software into units called layers. Each layer is a grouping of modules that offers a cohesive set of services. Each layer is exposed through a public interface. The usage must be unidirectional. Layer Pattern Example Layer Pattern Solution The layered pattern defines layers and a unidirectional allowed-to-use relation among the layers. Elements: Layer, a kind of module. Relations: Allowed to use. Constraints: Every piece of software is allocated to exactly one layer. There are at least two layers The allowed-to-use relations should not be circular Weaknesses: The addition of layers adds up cost and complexity to a system. Layers contribute a performance penalty. Three layered applications Architectural Patterns Module patterns Layered pattern Component-and-Connector patterns Broker pattern Model-View-Controller pattern Pipe-and-Filter pattern Client-Server pattern Peer-to-Peer pattern Service-Oriented Architecture (SOA) pattern Publish-Subscribe pattern Shared-Data pattern Allocation patterns Map-Reduce pattern Multi-tier Pattern Architectural Patterns Component-and-Connector patterns Broker pattern Model-View-Controller pattern Pipe-and-Filter pattern Client-Server pattern Peer-to-Peer pattern Service-Oriented Architecture (SOA) pattern Publish-Subscribe pattern Shared-Data pattern Broker Pattern Context: Many systems are constructed from a collection of services distributed across multiple servers Problem: How do we structure distributed software so that service users do not need to know the nature and location of service providers? Solution: The broker pattern separates clients from providers servers by inserting an intermediary, called a broker. When a client needs a service, it queries a broker via a service interface. The broker then forwards the client’s service request to a server, which processes the request. Broker Solution Overview: The broker pattern defines a runtime component, called a broker, that mediates the communication between a number of clients and servers. Elements: Client, a requester of services Server, a provider of services Broker, an intermediary that locates an appropriate server to fulfill a client’s request, forwards the request to the server, and returns the results to the client Broker Solution Constraints: The client can only attach to a broker. The server can only attach to a broker. Weaknesses: Brokers add latency between clients and servers, and it may be a communication bottleneck. …. Consider there is one broker that mediates the communication between a large number of clients and servers. What happens when the broker fails? Nothing happens, the clients can start communicating directly with the servers All the clients and server are not able to communicate in this situation Broker Solution Constraints: The client can only attach to a broker. The server can only attach to a broker. Weaknesses: Brokers add latency between clients and servers, and it may be a ","date":"2021-11-25 08:44:33","objectID":"/sek_base_07/:7:0","tags":["software engineering knowledge"],"title":"SEK_base_07","uri":"/sek_base_07/"},{"categories":["School courses"],"content":"Quality Attribute Modelling and Analysis Intended Learning Outcomes By the end of this lesson you will be able to: Model architectures to enable quality attribute analysis Use quality attribute checklists Generate and carry out a thought experiment to perform early analysis Analyse architectures using alternative ways: experiments, simulations and prototypes Analyse at different stages of the life cycle How do we know the quality attributes of a software? Analytic model Experiment and measurement Simulations Modeling Architectures to Enable Quality Attribute Analysis Some quality attributes, most notably performance and availability, have well-understood, time-tested analytic models that can be used to assist in an analysis. By analytic model, we mean one that supports quantitative analysis. Let us first consider performance. Performance Models Parameters: arrival rate of events, chosen queuing discipline, chosen scheduling algorithm, service time for events, network topology, network bandwidth, routing algorithm chosen Allocation Model for MVC Queuing Model for MVC Arrivals View sends requests to Controller Actions returned to View Actions returned to model Model sends actions to View Parameters To solve a queuing model for MVC performance, the following parameters must be known or estimated: The frequency of arrivals from outside the system The queuing discipline used at the view queue The time to process a message within the view The number and size of messages that the view sends to the controller The bandwidth of the network that connects the view and the controller The queuing discipline used by the controller The time to process a message within the controller The number and size of messages that the controller sends back to the view The bandwidth of the network from the controller to the view The number and size of messages that the controller sends to the model The queuing discipline used by the model The time to process a message within the model The number and size of messages the model sends to the view The bandwidth of the network connecting the model and the view Parameters To solve a queuing model for MVC performance, the following parameters must be known or estimated: The frequency of arrivals from outside the system The queuing discipline used at the view queue The time to process a message within the view The number and size of messages that the view sends to the controller The bandwidth of the network that connects the view and the controller The queuing discipline used by the controller The time to process a message within the controller The number and size of messages that the controller sends back to the view The bandwidth of the network from the controller to the view The number and size of messages that the controller sends to the model The queuing discipline used by the model The time to process a message within the model The number and size of messages the model sends to the view The bandwidth of the network connecting the model and the view Cost/benefit of Performance Modeling Cost: determining the parameters previously mentioned Benefit: estimate of the latency The more accurately the parameters can be estimated, the better the predication of latency. This is worthwhile when latency is important and questionable. This is not worthwhile when it is obvious there is sufficient capacity to satisfy the demand. Availability Modeling Another quality attribute with a well-understood analytic framework is availability. Modeling an architecture for availability—or to put it more carefully, modeling an architecture to determine the availability of a system based on that architecture—is a matter of determining the failure rates and the recovery times of the components. Example Just as for performance, to model an architecture for availability, we need an architecture to analyze. Suppose we want to increase the availability of a system that uses the Broker pattern, by applying redundancy tactics. Availability M","date":"2021-11-25 08:44:33","objectID":"/sek_base_07/:8:0","tags":["software engineering knowledge"],"title":"SEK_base_07","uri":"/sek_base_07/"},{"categories":["School courses"],"content":"Designing for architecturally significant requirements Intended Learning Outcomes By the end of this lesson you will be able to: understand the concept of ASR identify and capture ASR design for ASR Architecturally Significant Requirement Architectures exist to build systems that satisfy requirements. But, to an architect, not all requirements are created equal. An Architecturally Significant Requirement (ASR) is a requirement that will have a profound effect on the architecture. How do we find those? Approaches to Capture ASRs From Requirements Document By Interviewing Stakeholders By Understanding the Business Goals In Utility Tree ASRs and Requirements Documents An obvious location to look for candidate ASRs is in the requirements documents Requirements should be in requirements documents! Unfortunately, this is not usually the case. Don’t Get Your Hopes Up Many projects don’t create or maintain the detailed, high-quality requirements documents. Standard requirements pay more attention to functionality than quality attributes. The architecture is driven by quality attribute requirements rather than functionalities Most requirements specification does not affect the architecture. Don’t Get Your Hopes Up Quality attributes are often captured poorly, e.g. “The system shall be modular” “The system shall exhibit high usability” “The system shall meet users’ performance expectations” Much of what is useful to an archit ect is not in even the best requirements document ASRs often derive from business goals in the development organization itself Gathering ASRs from Stakeholders Stakeholders often have no idea what QAs they want in a system if you insist on quantitative QA requirements, you’re likely to get numbers that are arbitrary. at least some of those requirements will be very difficult to satisfy. Architects often have very good ideas about what QAs are reasonable to provide. Interviewing the stakeholders is the surest way to learn what they know and need. “The system shall meet users’ performance expectations”- this is an example of a poorly captured quality attribute requirement. Do you know a tool to help us express quality requirements in a better way? No, there is no better way. There should be a better way, but I do not know how Yes – use a quality attribute scenario and make sure all 6 parts are included Gathering ASRs from Stakeholders The results of stakeholder interviews should include a list of architectural drivers a set of QA scenarios that the stakeholders (as a group) prioritized. This information can be used to: refine system and software requirements understand and clarify the system’s architectural drivers provide rationale for why the architect subsequently made certain design decisions guide the development of prototypes and simulations influence the order in which the architecture is developed. Quality Attribute Scenario: Example Our vehicle information system sends our current location to the traffic monitoring system. The traffic monitoring system combines our location with other information, overlays this information on a Google Map, and broadcasts it. Our location information is correctly included with a probability of 99.9%. Quality Attribute Scenario: Example The developer wishes to change the user interface by modifying the code at design time. The modifications are made with no side effects within three hours. Stimulus – Wishes to change UI Artifact – Code Environment: Design time Response – Change made Response measure – No side effects in three hours Source - Developer Quality Attribute Scenario: Example Users initiate transactions under normal operations. The system processes the transactions with an average latency of two seconds. Stimulus: transaction arrivals Source: users Artifact: the system Response: process the transactions Response measure: average latency of two seconds Environment: under normal operation Quality Attribute Scenario: Example A disgruntled employee from a remote location","date":"2021-11-25 08:44:33","objectID":"/sek_base_07/:9:0","tags":["software engineering knowledge"],"title":"SEK_base_07","uri":"/sek_base_07/"},{"categories":["School courses"],"content":"Designing and evaluating an architecture Intended Learning Outcomes By the end of this lesson you will be able to: design a software architecture using ADD approach evaluate an architecture using ATAM method Design Strategy Decomposition Designing to Architecturally Significant Requirements Generate and Test The Attribute-Driven Design Method An iterative method. At each iteration you combine the 3 design strategies we studied earlier Choose a part of the system to design. Marshal all the architecturally significant requirements for that part. Generate and test a design for that part. ADD does not result in a complete design Set of containers with responsibilities Interactions and information flow among containers Does not produce an API for containers. ADD Inputs Requirements Functional, quality, constraints A context description What are the boundary of the system being designed? What are the external systems, devices, users and environment conditions with which the system being designed must interact? ADD Outputs Architectural elements and their relationship Responsibility of elements Interactions Information flow among the elements The Steps of ADD Choose an element of the system to design. Identify the ASRs for the chosen element. Generate a design solution for the chosen element. Inventory remaining requirements and select the input for the next iteration. Repeat steps 1–4 until all the ASRs have been satisfied. Step 1: Choose an Element of the System to Design For green field designs, the element chosen is usually the whole system. For legacy designs, the element is the portion to be added. After the first iteration: Step 1 example We want to design a system for online travel agency Step 1 example In iteration #1, we decide to apply SOA pattern (Service Oriented Architecture, where functions of the system will become services) We decompose our system into necessary components based on SOA pattern Step 1 example Then, we need to choose which part of the system to design next, In iteration #2, we choose to further design SOA infrastructure components element We decide to decompose it into 3 components Step 1 example Then, we need to choose which part of the system to design next, In iteration #3, which element should we choose to design? Refine one of the remaining SOA elements? Refine one of the SOA infrastructure components? Which Element Comes Next? Two basic refinement strategies: Breadth first Depth first Which one to choose? If using new technology =\u003e depth first: explore the implications of using that technology. If a team needs work =\u003e depth first: generate requirements for that team. Otherwise =\u003e breadth first. Step 2: Identify the ASRs for the Chosen Element If the chosen element is the whole system, then use a utility tree (as described earlier). If the chosen element is further down the decomposition tree, then generate a utility tree from the requirements for that element. Step 3: Generate a Design Solution for the Chosen Element Apply generate and test to the chosen element with its ASRs Step 4 Inventory remaining requirements and select the input for the next iteration We need to consider 3 kinds of requirements Functional requirements Quality attribute requirements Constraints Step 4: Select the Input for the Next Iteration For each functional requirement Ensure that requirement has been satisfied. If not, then add responsibilities to satisfy the requirement. Add them to container with similar requirements If no such container, may need to create new one or add to container with dissimilar responsibilities (coherence) If container has too many requirements for a team, split it into two portions. Try to achieve loose coupling when splitting. Quality Attribute Requirements If the quality attribute requirement has been satisfied, it does not need to be further considered. If the quality attribute requirement has not been satisfied then either Delegate it to one of the child elements Split it among the child e","date":"2021-11-25 08:44:33","objectID":"/sek_base_07/:10:0","tags":["software engineering knowledge"],"title":"SEK_base_07","uri":"/sek_base_07/"},{"categories":["School courses"],"content":"Exercise 1 Capturing ASR in practice . Intended Learning Outcomes By the end of this lesson you will be able to: Specify concrete scenarios for a real system Create a utility tree for the real system Our system – ATM network Please check our WeChat group – I have sent a document The document in front of you describes the requirements for the system, including functional requirements and known quality attribute requirements Activity: Scenario Brainstorming (1) Based on the received document, express scenarios representing your concerns with respect to the system. Describe your scenarios in as many details as you can (do NOT use the concrete scenario form yet) Pick one of the scenarios and submit as an answer to the question. Activity: Scenario Brainstorming (2) Read scenarios of other participants, while reading, you can write more scenarios Your new scenarios can be inspired by what the other participant shared, but not the same Submit a new scenario you have wrote, inspired by scenarios of other participants Next steps Scenarios consolidation Participants discuss and consolidate the scenarios where reasonable. Scenario Prioritization - voting Each participants receives a number of votes equal to 30 percent of the total number of scenarios Each participant allocates their votes to scenarios Refine and elaborate the top scenarios Put the scenarios in the six-part scenario form (concrete scenario) Specifying Quality Attribute Requirements Stimulus. he stimulus is a condition that requires a response when it arrives at a system. Source of stimulus. his is some entity (a human, a computer system, or any other actuator) that generated the stimulus. Response. he response is the activity undertaken as the result of the arrival of the stimulus. Response measure. hen the response occurs, it should be measurable in some fashion so that the requirement can be tested. Environment. he stimulus occurs under certain conditions. The system may be in an overload condition or in normal operation, or some other relevant state. Artifact. his may be a collection of systems, the whole system, or some piece or pieces of it. Some artifact is stimulated. Quality Attribute Scenario: Example Our vehicle information system sends our current location to the traffic monitoring system. The traffic monitoring system combines our location with other information, overlays this information on a Google Map, and broadcasts it. Our location information is correctly included with a probability of 99.9%. Quality Attribute Scenario: Example The developer wishes to change the user interface by modifying the code at design time. The modifications are made with no side effects within three hours. Stimulus – Wishes to change UI Artifact – Code Environment: Design time Response – Change made Response measure – No side effects in three hours Source - Developer Quality Attribute Scenario: Example Users initiate transactions under normal operations. The system processes the transactions with an average latency of two seconds. Stimulus: transaction arrivals Source: users Artifact: the system Response: process the transactions Response measure: average latency of two seconds Environment: under normal operation Quality Attribute Scenario: Example A disgruntled employee from a remote location attempts to modify the pay rate table during normal operations. The system maintains an audit trail and the correct data is restored within a day. Stimulus: unauthorized attempts to modify the pay rate table Stimulus source: a disgruntled employee Artifact: the system with pay rate table Environment: during normal operation Response: maintains an audit trail Response measure: correct data is restored within a day Quality Attribute Scenario: Example The user downloads a new application and is using it productively after two minutes of experimentation. Source: user Stimulus: download a new application Artifact: system Environment: runtime Response: user uses application productively Response ","date":"2021-11-25 08:44:33","objectID":"/sek_base_07/:11:0","tags":["software engineering knowledge"],"title":"SEK_base_07","uri":"/sek_base_07/"},{"categories":["School courses"],"content":"exercise2 Please comply with epidemic prevention and control policies Choose the seat that you will use during the whole semester and register your information in the system. Make sure to take the same seat during every lecture. Intended Learning Outcomes By the end of this lesson you will be able to: Create a design for a real system using the ADD method employing and instantiating a pattern ASRs We will add/expand the following ASRs to our utility tree Modifiability Performance Modifiability The system must be easily modified to take advantage of new platform capabilities (for example, it must not be tied to a single database or to a single kind of client hardware or software) and that it must be extensible to allow the addition of new functions and new business rules. Modifiability scenarios A developer wishes to add a new auditing business rule at design time and makes the modification, without affecting other functionality, in 10 person-days. A developer wishes to change the relational schema to add a new view to the database, without affecting other functionality, in 30 person-days. A system administrator wishes to employ a new database and makes the modification, without affecting other functionality, in 18 person-months. A developer wishes to add a new function to a client menu, without side effects, in 15 person-days. A developer needs to add a Web-based client to the system, without affecting the functionality of the existing ATM client, in 90 person-days. Performance Latency of an operation (such as an ATM withdrawal) Performance scenario “The user can withdraw a limit of $300 from an account that has sufficient funds in less than 10 seconds.” There are two functional requirements and one performance requirement in this scenario. One function is a withdrawal Another function is a limit (a constraint of $300 if it is in the account) Performance constraint/quality attribute - “less than 10 seconds” ADD method The ADD method defines a software architecture by basing the design process on the quality attribute requirements of the system. The ADD approach follows a recursive decomposition process where, at each stage in the decomposition, architectural tactics and patterns are selected to satisfy a chosen set of high-priority quality scenarios. ADD: The Steps of ADD Choose an element of the system to design. Identify the ASRs for the chosen element. Generate a design solution for the chosen element. Inventory remaining requirements and select the input for the next iteration. Repeat steps 1–4 until all the ASRs have been satisfied. Step 1: Choose an Element of the System to Design For green field designs, the element chosen is usually the whole system. For legacy designs, the element is the portion to be added. After the first iteration: Step 2: Identify the ASRs for the Chosen Element If the chosen element is the whole system, then use a utility tree (as described earlier). Let us first look at our modifiability scenarios Modifiability scenarios A developer wishes to add a new auditing business rule at design time and makes the modification, without affecting other functionality, in 10 person-days. A developer wishes to change the relational schema to add a new view to the database, without affecting other functionality, in 30 person-days. A system administrator wishes to employ a new database and makes the modification, without affecting other functionality, in 18 person-months. A developer wishes to add a new function to a client menu, without side effects, in 15 person-days. A developer needs to add a Web-based client to the system, without affecting the functionality of the existing ATM client, in 90 person-days. Step 3: Generate a Design Solution for the Chosen Element Apply generate and test to the chosen element with its ASRs Activity: Use patterns and tactics Based on the modifiability scenarios, generate a candidate design employing and instantiating a pattern. Which tactics are you going to use? Tips (1) In or","date":"2021-11-25 08:44:33","objectID":"/sek_base_07/:12:0","tags":["software engineering knowledge"],"title":"SEK_base_07","uri":"/sek_base_07/"},{"categories":["School courses"],"content":"exercise3 Please comply with epidemic prevention and control policies Choose the seat that you will use during the whole semester and register your information in the system. Make sure to take the same seat during every lecture. Intended Learning Outcomes By the end of this lesson you will be able to: Create a documentation for a real system Structures and Views Architecture Is a Set of Software Structures A structure is the set of elements itself, as they exist in software or hardware. In short, a view is a representation of a structure. Architects design structures. They document views of those structures. Architecture Documentation Even the best architecture will be useless if the people who need it do not know what it is; cannot understand it well enough to use, build, or modify it; misunderstand it and apply it incorrectly. All of the effort, analysis, hard work, and insightful design on the part of the architecture team will have been wasted. Architecture Documentation and Stakeholders Education Introducing people to the system New members of the team External analysts or evaluators New architect Primary vehicle for communication among stakeholders Especially architect to developers Especially architect to future architect! Basis for system analysis and construction Architecture tells implementers what to implement. Each module has interfaces that must be provided and uses interfaces from other modules. Documentation can serve as a receptacle for registering and communicating unresolved issues. Architecture documentation serves as the basis for architecture evaluation. Views Principle of architecture documentation: Documenting an architecture is a matter of documenting the relevant views and then adding documentation that applies to more than one view. Which Views? The Ones You Need! Different views support different goals and uses. The views you should document depend on the uses you expect to make of the documentation. Each view has a cost and a benefit; you should ensure that the benefits of maintaining a view outweigh its costs. Building the Documentation Package Documentation package consists of Views Documentation beyond views View Template Implementation View Describes the static organization of the software in its development environment. Viewer Programmers and Software Managers Considers Software module organization - Hierarchy of layers, software management, reuse, constraints of tools Style layered style Notation Activity For our simple ATM network, with additional database servers deployed, create an implementation view Implementation and deployment views Implementation view Deployment View Describes the mapping(s) of the software onto the hardware and reflects its distributed aspect. Viewer System Engineers Considers Non-functional requirement (reliability, availability and performance) regarding to underlying hardware. Deployment view example Activity For our simple ATM network, with additional database servers deployed, create a deployment view Implementation and deployment views Implementation view Deployment view Use Case View Captures system functionality as seen by users Built in early stages of development Developed by analysts and domain experts System behavior, that is what functionality it must provide, is documented in a use case model. Use Case Model Illustrates the system’s intended functions (use cases), its surroundings (actors), and relationships between the use cases and actors (use case diagrams). provides a vehicle used by the customers or end users and the developers to discuss the system’s functionality and behavior. starts in the Inception phase with the identification of actors and principal use cases for the system, and is then matured in the elaboration phase, by adding more details and additional use cases. Graphical Constructs Actors represent anyone or anything that must interact with the system: they are not part of the system. may: only input information to the system only recei","date":"2021-11-25 08:44:33","objectID":"/sek_base_07/:13:0","tags":["software engineering knowledge"],"title":"SEK_base_07","uri":"/sek_base_07/"},{"categories":["School courses"],"content":"系统模型 ","date":"2021-11-17 08:45:55","objectID":"/distributedsystem_base_02/:0:0","tags":["distributed system"],"title":"DistributedSystem_base_02","uri":"/distributedsystem_base_02/"},{"categories":["School courses"],"content":"简介 物理模型是描述系统的-个最显式的方法，它从计算机(和其他设备，例如移动电话)及其互联的网络方面考虑系统的硬件组成。 体系结构模型从系统的计算元素执行的计算和通信任务方面来描述系统。 基础模型采用抽象的观点描述分布式系统的某个方面。本章介绍考察分布式系统三个重要方面的基础模型: 交互模型，它考虑在系统元素之间通信的结构和顺序; 故障模型，它考虑一个系统可能不能正确操作的方式; 安全模型，它考虑如何保护系统使其不受到正确操作的干扰或不被窃取数据。 ","date":"2021-11-17 08:45:55","objectID":"/distributedsystem_base_02/:1:0","tags":["distributed system"],"title":"DistributedSystem_base_02","uri":"/distributedsystem_base_02/"},{"categories":["School courses"],"content":"物理模型 物理模型是从计算机和所用网络技术的特定细节中抽象出来的分布式系统底层硬件元素的表示。 ","date":"2021-11-17 08:45:55","objectID":"/distributedsystem_base_02/:2:0","tags":["distributed system"],"title":"DistributedSystem_base_02","uri":"/distributedsystem_base_02/"},{"categories":["School courses"],"content":"体系结构模型 本节采取一种三阶段方法: 首先，描述支撑现代分布式系统的核心基本体系结构元素，重点展示现在已有方法的不同; 考察能在开发复杂分布式系统解决方案中单独使用或组合使用的复合体系结构模式; 最后，对于以上体系结构风格中出现的不同编程风格，考虑可用于支持它们的中间件平台。 注意，有许多与本章中介绍的体系结构模型相关的权衡，其中涉及采用的系统体系结构元素、所采用的模式和(在合适的地方)使用的中间件，它们会影响结果系统的性能和有效性。理解这样的权衡可以说是分布式系统设计中的关键技能。 ","date":"2021-11-17 08:45:55","objectID":"/distributedsystem_base_02/:3:0","tags":["distributed system"],"title":"DistributedSystem_base_02","uri":"/distributedsystem_base_02/"},{"categories":["School courses"],"content":"体系结构元素 通信实体：从系统的观点，回答通常是非常清楚的,这是因为在一个分布式系统中通信的实体通常是进程，这导致普遍地把分布式系统看成是带有恰当进程间通信范型的多个进程(如在第4章中讨论的)，有两个注意事项: 在一些原始环境中，例如传感器网络,基本的操作系统可能不支持进程抽象(或甚至任何形式的隔离)，因此在这些系统中通信的实体是结点。 在大多数分布式系统环境中，用线程补充进程，所以，严格说来，通信的末端是线程。 对象：对象已被引人以便在分布式系统中使用面向对象的方法(包括面向对象的设计和面向对象的编程语言)。在分布式面向对象的方法中，-个计算由若千交互的对象组成，这些对象代表分解给定问题领域的自然单元。对象通过接口被访问，用一个相关的接口定义语言(IDL) 提供定义在一个对象上的方法的规约。分布式对象已经成为分布式系统研究的一个主要领域，第5章和第8章将进一步讨论这个话题。 组件:因为对象的引入，许多重要的问题已被认为与分布式对象有关，组件技术的出现及使用是对这些弱点的一个直接响应。组件类似于对象，因为它们为构造分布式系统提供面向问题的抽象，也是通过接口被访问。关键的区别在于组件不仅指定其(提供的)接口而且给出关于其他组件/接口的假设，其他组件/接口是组件完成它的功能必须有的。换句话说,组件使得所有依赖显式化，为系统的构造提供一个更完整的合约。这个合约化的方法鼓励和促进第三方开发组件，也通过去除隐含的依赖 提升了一个更纯粹的组合化方法来构造分布式系统。基于组件的中间件经常对关键领域如部署和服务器方编程支持提供额外的支持[ Heineman and Councill 2001]。关于基于组件方法的进一步细节请参见第8章。 Web服务: Web 服务代表开发分布式系统的第三种重要的范型[Alonso et al. 2004]。Web 服务与对象和组件紧密相关，也是采取基于行为封装和通过接口访问的方法。但是，相比而言，通过利用Web标准表示和发现服务，Web 服务本质上是被集成到万维网(即W3C)的。W3C ( World WideWeb)联盟把Web服务定义成： 一个软件应用，通过URI被辨识，它的接口和绑定能作为XML制品被定义描述和发现。一个Web服务通过在基于互联网的协议上利用基于XML的消息交换支持与其他软件代理的直接交互。 换句话说，Web服务采用的基于Web的技术在-定程度上定义了Web服务。另一个重要的区别来源于技术使用的风格。对象和组件经常在一个组织内部使用，用于开发紧耦合的应用，但Web服务本身通常被看成完整的服务，它们可以组合起来获得增值服务，它们经常跨组织边界，因此可以实现业 务到业务的集成。Web服务可以由不同的提供商用不同的底层技术实现。Web服务将在第9章做进一步的探讨。 通信范型我们现在转向在分 布式系统中实体如何通信,考虑三种通信范型: 进程间通信; 远程调用; 间接通信。 进程间通信指的是用于分布式系统进程之间通信的相对底层的支持，包括消息传递原语、直接访问由互联网协议提供的API (套接字编程)和对多播通信的支持。第4章将详细讨论这样的服务。 远程调用代表分布式系统中最常见的通信范型，覆盖一系列分布式系统中通信实体之间基于双向交换的技术，包括调用远程操作、过程或方法。进一步的定义参见下面内容(详细讨论见第5章): 请求-应答协议是一个有效的模式，它加在一个底层消息传递服务之上，用于支持客户-服务器计算。特别的，这样的协议通常涉及一对消息的交换，消息从客户到服务器，接着从服务器返回客户，第一个消息包含在服务器端执行的操作的编码，然后是保存相关参数的字节数组，第二个消息包含操作的结果，它也被编码成字节数组。这种范型相对原始，实际上仅被用于嵌人式系统，对嵌人式系统来说性能是至关重要的。这个方法也被用在5.2节描述的HTTP协议中。正如下面讨论的，大多数分布式系统将选择使用远程过程调用或者远程方法调用，但注意底层的请求-应答交换支持两种方法。 远程过程调用( Remote Procedure Call, RPC)的概念，最初由Birell 和Nelson [1984] 提出，代表了分布式计算中的一个主要突破。在RPC中,远程计算机上进程中的过程能被调用，好像它们是在本地地址空间中的过程一样。底层RPC系统隐藏了分布的重要方面，包括参数和结果的编码和解码、消息的传递和保持过程调用所要求的语义。这个方法直接而且得体地支持了客户-服务器计算，其中,服务器通过一个服务接口提供一套操作, 当这些操作本地可用时客户直接调用这些操作。因此，RPC系统(在最低程度上)提供访问和位置透明性。 远程方法调用( Remote Method Invocation, RMI) 非常类似于远程过程调用，但它应用于分布式对象的环境。用这种方法，-个发起调用的对象能调用一个远程对象中的方法。与RPC一样，底层的细节都对用户隐藏。不过，通过支持对象标识和在远程调用中传递对象标识符作为参数，RMI 实现做得更多。它们也从与面向对象语言(见第5章相关讨论)的紧密集成中获得更多的好处。 上述技术具有一个共同点:通信代表发送者和接收者之间的双向关系，其中，发送者显式地把消息/调用送往相关的接收者。接收者通常了解发送者的标识，在大多数情况下，双方必须在同时存在。相比而言，已经出现若千技术，这些技术支持间接通信,通过第三个实体，允许在发送者和接收者之间的深度解耦合。尤其是: 发送者不需要知道他们正在发送给谁(空间解耦合)。 发送者和接收者不需要同时存在(时间解耦合)。 第6章将详细讨论间接通信。 间接通信的关键技术包括: 组通信:组通信涉及消息传递给若干接收者，因此是支持一对多通信的多方通信范型。组通信依赖组抽象，-一个组在系统中用-一个组标识符表示。接收方通过加人组，就能选择性接收发送到组的消息。发送者通过组标识符发送消息给组，因此，不需要知道消息的接收者。组通常也要维护组成员,具有处理组成员故障的机制。 发布-订阅系统:许多系统，例如第1章中金融贸易的例子，被归类于信息分发系统，其中，大量生产者(或发布者)为大量的消费者(或订阅者)发布他们感兴趣的信息项(事件)。采用前述的任-核心通信范型来实现这个需求是复杂且低效的，因此，出现了发布-订阅系统(有时也叫分布式基于事件的系统)用于满足此项重要需求[ Muhl et al. 2006]。发布-订阅系统共享同一个关键的特征，即提供-一个中间服务,有效确保由生产者生成的信息被路由到需要这个信息的消费者。 消息队列:虽然发布-订阅系统提供一种一对多风格的通信,但消息队列提供了点对点服务,其中生产者进程能发送消息到一个指定的队列，消费者进程能从队列中接收消息，或被通知队列里有新消息到达。因此，队列是生产者和消费者进程的中介。 元组空间:元组空间提供了进-步的间接通信服务，并支持这样的模型一进程 能把任意的结构化数据项( 称为元组)放到一个持久元组空间，其他进程可以指定感兴趣的模式，从而可以在元组空间读或者删除元组。因为元组空间是持久的，读操作者和写操作者不需要同时存在。这种风格的编程，也被称为生成通信，由Gelemter [ 1985]作为一种并行编程范型引人。已经开发了不少分布式实现，采用了客户-服务器-风格的实现或采用了更分散的对等方法。 分布式共享内存:分布式共享内存(Distributed Shared Memory, DSM)系统提供一种抽象，用于支持在不共享物理(内存的进程之间共享数据。提供给程序员的是-套熟悉的读或写(共享)数据结构的抽象，就好像这些数据在程序员自已本地的地址空间一样,从而提供了高层的分布透明性。基本的基础设施必须确保以及时的方式提供副本，也必须处理与数据同步和一致性相关的问题。分布式共享内存的概述在第6章中介绍。 到目前为止讨论的体系结构 角色和责任，在一个分布式系统中, 进程，或者说，对象、组件、服务，包括Web服务(为简单起见，我们在本节中使用术语“进程”)相互交互完成一个有用的活动，例如支持一次聊天会话。在这样做的时候，进程扮演给定的角色，在建立所采用的整体体系结构时，这些角色是基本的。本节我们考察两种起源于单个进程角色的体系结构风格:客户-服务器风格和对等风格。 客户-服务器:这是讨论分布式系统时最常引用的体系结构。它是历史上最重要的体系结构，现在仍被广泛地使用。图2-3 给出了一个简单的结构，其中，进程扮演服务器和客户的角色。特别是,为了访问服务器管理的共享资源，客户进程可以与不同主机上的服务器进程交互。如图2-3所示，一台服务器也可以是其他服务器的客户。例如，Web服务器通常是管理存储Web页面文件的本地文件服务器的客户。Web 服务器和大多数其他互联网服务是DNS服务的客户，DNS服务用于将互联网域名翻译成网络地址。另一个与Web相关的例子是搜索引擎,搜索引擎能让用户通过互联网查看Web页面上可用的信息汇总。这些信息汇总通过称为“Web 抓取”的程序形成，该程序在搜索引擎站点以后台方式运行，利用HTTP请求访问互联网上的Web服务器。因此，搜索引擎既是服务器又是客户:它回答来自浏览器客户的查询，并且运行作为其他Web服务器客户的Web抓取程序。在这个例子中，服务器任务(对用户查询的回答)和Web抓取的任务( 向其他Web服务器发送请求)是完全独立的，很少需要同步它们，它们可以并行运行。事实上,一个典型的搜索引擎正常情况下包含许多并发执行的线程，一些线程为它的客户服务，另一些线程运行Web抓取程序。 对等体系结构:在这种体系结构中,涉及一项任务或活动的所有进程扮演相同的角色，作为对","date":"2021-11-17 08:45:55","objectID":"/distributedsystem_base_02/:3:1","tags":["distributed system"],"title":"DistributedSystem_base_02","uri":"/distributedsystem_base_02/"},{"categories":["School courses"],"content":"体系结构模式 我们给出分布式系统中几个关键的体系结构模型，包括分层体系结构( layering architecture)、层次化体系结构( tiered ar-chitecture)和瘦客户相关的概念(包括虚拟网络计算的特定机制)。我们也把Web服务当做一个体系结构模式进行了考察，给出了其他可以应用在分布式系统中的模式。 分层，分层的概念是-一个熟悉的概念，与抽象紧密相关。在分层方法中，一个复杂的系统被分成若干层，每层利用下层提供的服务。因此，一个给定的层提供-一个软件抽象，更高的层不清楚实现细节，或不清楚在它下面的其他层。就分布式系统而言，这等同于把服务垂直组织成服务层。一个分布式服务可由一个或多个服务器进程提供，这些进程相互交互，并与客户进程交互，维护服务中的资源在系统范围内的-致视图。 层次化体系结构层次化体系结构 与分层体系结构是互补的。分层将服务垂直组织成抽象层，而层次化是一项组织给定层功能的技术，它把这个功能放在合适的服务器上，或者作为第二选择放在物理结点上。 AJAX的作用:在1.6节中，我们介绍了AJAX ( Asynchronous Javascript And XML)是Web所使用的标准客户-服务器交互方式的扩展。AJAX满足了Javascript 前端程序( 运行在Web浏览器中)和基于服务器的后端程序(拥有描述应用状态的数据)之间的细粒度通信的需要。概括而言，在标准的Web交互方式中，浏览器发送HTTP请求给服务器，请求给定URL的页面、图像或其他资源。服务器发送整个页面作为应答，这个页面或者从服务器上的-一个文件中读取，或者由一个程序生成，取决于 URL中可识别的资源类型。当客户收到内容时，浏览器根据其MIME类型( text/html、image/jpg 等)相关的显示方式呈现它。虽然Web页面由不同类型的内容项组成，但是整个页面以它在html页面定义中指定的方式由浏览器组合并呈现。 瘦客户，分布式计算的趋势是将复杂性从最终用户设备移向互联网服务。这点在向云计算(见第1章)发展的趋势中最明显，在上面讨论的层次化体系结构中也能看到。这个趋势导致了对瘦客户概念的兴趣，它使得能以很少的对客户设备的假设或需求，获得对复杂网络化服务的访问，这些服务可以通过云解决方案提供。更具体来说，术语瘦客户指的是一个软件层，在执行一个应用程序或访问远程计算机上的服务时，由该软件层提供一个基于窗口的本地用户界面。 其他经常出现的模式： 代理 代理(proxy) 模式是分布式系统中经常出现的模式，其主要用于支持远程过程调用或远程方法调用的位置透明性。用这种方法，一个代理在本地地址空间中被创建，用于代表远程对象。这个代理提供与远程对象-样的接口，程序员调用这个代理对象，因此无须了解交互的分布式特性。在RPC和RMI中，代理支持位置透明性的作用将在第5章做进一步的讨论。 注意代理也被用于封装其他的功能( 诸如复制或缓存的放置策略等)。 web服务中的业务代理 Web服务中的业务代理(brokerage)的使用能被看成是一个在可能很复杂的分布式基础设施中支持互操作性的体系结构模式。特别地，这个模式是由服务提供商、服务请求者和服务代理(提供与请求的服务-致的服务)三部分组成，如图2-11所示。这个业务代理模式在分布式系统的多个领域被多次应用，例如Java RMI中的注册服务、CORBA中的名字服务( 分别参见第5章和第8章的讨论)。 反射 反射( reflection)模式在分布式系统中作为支持内省(系统的动态发现的特性)和从中调停(动态修改结构或行为的能力)的手段而被持续地使用。.例如，Java的内省能力被用于RMI的实现中，提供通用的分发(参见5.4.2节的讨论)。在一个反射系统中，标准的服务接口在基础层可供使.用，但元层接口也可以提供对涉及服务实现的组件及组件参数的访问。许多技术在元层可用，包括截获到达的消息或调用、动态发现由给定对象提供的接口、发现和适应系统底层体系结构的能力。反射被应用于分布式系统中的多个领域，特别是反射中间件领域，例如，可以用于支持更多的可配置及重配置中间件体系结构[Kon et al. 2001]。与分布式系统相关的体系结构模式更多的例子可以在Bushmann等人[2007] 的著作中找到。. ","date":"2021-11-17 08:45:55","objectID":"/distributedsystem_base_02/:3:2","tags":["distributed system"],"title":"DistributedSystem_base_02","uri":"/distributedsystem_base_02/"},{"categories":["School courses"],"content":"相关的中间件解决方案 第1章引入了中间件，在2.3.2节讨论分层体系结构时又重温了中间件。中间件的任务是为分布式系统的开发提供-一个高层的编程抽象，并且通过分层，对底层基础设施中的异构性提供抽象，从而提升互操作性和可移植性。中间件解决方案是基于2. 3. 1节引人的体系结构模型，也支持更复杂的体系结构模式。本节我们简要回顾一下现在存在的中间件类别，为在本书的其他部分进-步研究这 些解决方案做好准备。 中间件的类别 远程过程调用包, (如Sun RPC,第5章)和组通信(如ISIS,第6章和第18章)属于最早的中间件实例。从那以后，出现了大量不同风格的中间件，大部分都基于上面介绍的体系结构模型。我们在图2-12中给出了中间件平台的分类，其中交叉引用了其他章,那些章更详细地讨论了不同种类的中间件。需要强调的是分类并不精确，现代中间件平台试图提供混合的解决方案。例如，许多分布式对象平台提供分布式事件服务，来补充传统的对远程方法调用的支持。类似地，出于互操作性的原因,许多基于组件的平台(和平台的其他分类)也支持Web服务和标准。从中间件标准和今天可用的技术的角度来看,还应该强调这个分类并不完整，其目的在于给出中间件的主要类别。其他(未给出的)解决方案是比较特定的，例如，特定于提供某-通信范型，如消息传递、远程过程调用、分布式共享内存、元组空间或组通信。 图2-12中的中间件的顶层分类是根据通信实体和相关通信范型而确定的，遵循五个主要的体系结构模型:分布式对象、分布式组件、发布-订阅系统、消息队列和Web服务。对等系统是这些类别的补充，基于2.3. 1节讨论的协作方法，对等系统是中间件-一个相当独立的分支。应用服务器，显示为分布式组件的子类，也提供对三层体系结构的直接支持。特别地，应用服务器提供了结构以支持应用逻辑和数据存储的分离，以及对其他特性( 如安全性和可靠性)的支持。详细细节将延后到第8章讨论。 除了编程抽象之外，中间件也能提供分布式系统的基础设施服务,供应用程序或其他服务使用。这些基础设施服务与中间件提供的分布式编程模式是紧密绑定的。例如，CORBA (第8章)提供给应用一系列的CORBA服务，包括对程序安全和可靠的支持。如上所述和在第8章中的进-一步讨论，应用服务器也提供对这些服务的内在支持。 中间件的限制 许多分布式应用完全依赖中间件提供的服务来支持应用的通信和数据共享需求。例如，一个适合客户-服务器模型的应用，如一个名字和地址的数据库，可以依赖只提供远程方法调用的中间件。 通过依靠中间件支持的开发，能大大简化分布式系统的编程，但系统可依赖性的一些方面要求应用层面的支持。 考虑从发送者的邮件主机传递大量的电子邮件消息到接收者的邮件主机。乍一看，这是一个TCP数据传输协议的简单应用( 见第3章的相关讨论)。但考虑这样的问题:用户试图在一个可能不可靠的网络上传递非常大的文件。TCP提供一些错误检测和更正， 但它不能从严重的网络中断中恢复。因此，邮件传递服务增加了另一层次的容错，维护一个进展记录，如果原来的TCP连接断开了，用一个新的TCP连接继续传递。 Saltzer、Reed 和Clarke的一篇经典论文[Saltzer et al. 1984]对分布式系统的设计给出了类似的、有价值的观点，他们称之为“端到端争论”。可将他们的陈述表述为: 一些与通信相关的功能，可以只依靠通信系统终点(end point)的应用的知识和帮助，即可完整、可靠地实现。因此，将这些功能作为通信系统的特征不总是明智的(虽然由通信系统提供一个不完全版本的功能有时对性能提高是有用的)。可以看出他们的论点与通过引人适当的中间件层将所有通信活动从应用编程中抽象出来的观点是相反的。 争论的关键是分布式程序正确的行为在很多层面上依赖检查、错误校正机制和安全手段，其中有些要求访问应用的地址空间的数据。任何企图在通信系统中单独完成的检查将只能保证部分正确性。因此，可能在应用程序中重复同样的任务,降低了编程效率,更重要的是增加了不必要的复杂性并要执行冗余的计算。 这里不进一步介绍他们的争论细节,强烈推荐读者阅读前面提到的那篇论文一那里有许多说明的实例。原文作者之一最近指出:争论给互联网设计带来的实质性好处最近面临着为满足当前应用需求而转向网络服务专门化的危险[ www. reed. com]。 这个争论给中间件设计者带来一个实际的两难困境，而且给定当代分布式系统中种类繁多的应用(和相关的环境条件) (见第1章)，这些困难与日俱增。本质上，底层中间件行为与一个给定应用或应用集的需求和相关环境上下文(如底层网络的状态和风格)有关。这个看法推动了对上下文感知和中间件自适应解决方案的兴趣，见Kon等人的讨论[2002] 。 ","date":"2021-11-17 08:45:55","objectID":"/distributedsystem_base_02/:3:3","tags":["distributed system"],"title":"DistributedSystem_base_02","uri":"/distributedsystem_base_02/"},{"categories":["School courses"],"content":"基础模型 上面的各种系统模型完全不同，但具有一些基本特性。特别是，所有的模型都由若干进程组成,这些进程通过在计算机网络上发送消息而相互通信，所有的模型都共享下列设计需求:实现进程及网络的性能和可靠性特征，确保系统中资源的安全性。本节给出基于基本特性的模型，利用这些模型，我们能更详细地描述系统可能展示的特征、故障和安全风险。 通常，为了理解和推理系统行为的某些方面，一个基础模型应该仅包含我们要考虑的实质性成分。这样一个模型的目的是: 显式地表示有关我们正在建模的系统的假设。 给定这些假设，就什么是可能的、什么是不可能的给出结论。结论以通用算法或要确保的特性 的形式给出。特性成立的保证依赖于逻辑分析和(适当时候的)数学证明。 了解设计依赖什么、不依赖什么，我们就能从中获益。如果在一个特定系统中实现-个设计，这个设计能否运作，我们只需询向在那个系统中假设是否成立。通过清晰、显式地给出我们的假设，就能利用数学技巧证明系统的特征，这些特征对任何满足假设的系统都成立。最后，通过从细节(如硬件)中抽象系统的基本实体和特性，我们就能阐明对系统的理解。 我们希望在我们的基本模型中提取的分布式系统情况能解决下列问题: 交互:计算在进程中发生，进程通过传递消息交互，并引发进程之间的通信(信息流)和协调(活动的同步和排序)。在分布式系统的分析和设计中,我们特别关注这些交互。交互模型必须反映通信带来的延迟，这些延迟的持续时间会比较长，交互模型必须反映独立进程相互配合的准确性受限于这些延迟，受限于在分布式系统中很难跨所有计算机维护同一时间概念。 故障:只要分布式系统运行的任-计算机上出现故障(包括软件故障)或连接它们的网络出现故障，分布式系统的正确操作就会受到威胁。我们的模型将对这些故障进行定义和分类。这为分析它们潜在效果以及设计能容忍每种类型故障的系统奠定了基础。 安全:分布式系统的模块特性和开放性将其暴露在外部代理和内部代理的攻击下。我们的安全模型对发生这种攻击的形式给出了定义并进行了分类，为分析对系统的威胁以及设计能抵御这些威胁的系统奠定了基础。为了帮助讨论和推理，我们对本章介绍的模型进行了必要的简化，省略了许多真实系统中的细节。 它们与真实系统的关系，以及在模型帮助下揭示的问题环境中的解决方案是本书讨论的主题。 ","date":"2021-11-17 08:45:55","objectID":"/distributedsystem_base_02/:4:0","tags":["distributed system"],"title":"DistributedSystem_base_02","uri":"/distributedsystem_base_02/"},{"categories":["School courses"],"content":"交互模型 体系结构模型对系统体系结构的讨论表明分布式系统由多个以复杂方式进行交互的进程组成。例如: 多个服务器进程能相互协作提供服务，前面提到的例子有域名服务(它将数据分区并复制到互联网中的服务器上)和Sun的网络信息服务(它在局域网的几个服务器上保存口令文件的复制版本)。 对等进程能相互协作获得一个共同的目标。例如，一个语音会议系统，它以类似的方式分布音频数据流，但它有严格的实时限制。 大多数程序员非常熟悉算法的概念一采取一系列步骤以执行期望的计算。简单的程序由算法控制，算法中的每一步都有严格的顺序。由算法决定程序的行为和程序变量的状态。这样的程序作为一个进程执行。由多个上面所说的进程组成的分布式系统是很复杂的。它们的行为和状态能用分布式算法描述一分布式算法定 义了组成系统的每个进程所采取的步骤，包括它们之间消息的传递。消息在进程之间传递以便在它们之间传递信息并协调它们的活动。每个进程执行的速率和进程之间消息传递的时限通常是不能预测的。要描述分布式算法的所有状态也非常困难，因为它必须处理所涉及的一个或多个进程的故障或消息传递的故障。 进程交互完成了分布式系统中所有的活动。每个进程有它自已的状态，该状态由进程能访问和更新的数据集组成，包括程序中的变量。属于每个进程的状态完全是私有的一也就是说， 它不能被其他进程访向或更新。 本节讨论分布式系统中影响进程交互的两个重要因素: 通信性能经常是一个限制特性。 不可能维护-个全局时间概念。 通信通道的性能。在我们的模型中， 通信通道在分布式系统中可用许多方法实现，例如，通过计算机网络上的流或简单消息传递来实现。计算机网络上的通信有下列与延迟(lateney)、 带宽( band-widh)和抖动(itter) 有关的性能特征: 从一个进程开始发送消息到另–个进程开始接收消息之间的间隔时间称为延迟。延迟包括: 第一串比特通过网络传递到目的地所花费的时间。例如，通过卫星链接传递消息的延迟是无线电信号到达卫星并返回的时间。 访问网络的延迟，当网络负载很重时，延迟增长很快。例如，对以太网传送而言，发送站点要等待网络空闲。 操作系统通信服务在发送进程和接收进程上所花费的时间，这个时间会随操作系统当前的负载的变化而变化。 计算机网络的带寬是指在给定时间内网络能传递的信息总量。当大量通信通道使用同-一个网络时，它们就不得不共享可用的带宽。 抖动是传递一系列消息所花费的时间的变化值。抖动与多媒体数据有关。例如，如果音频数据 的连续采样在不同的时间间隔内播放，那么声音将严重失真。 计算机时钟和时序事件。分布式系统中的每台计算机有自己的内部时钟,本地进程用这个时钟获得当前时间值。因此，在不同计算机上运行的两个进程能将时间戳与它们的事件关联起来。但是，即使两个进程在同时读它们的时钟,它们各自的本地时钟也会提供不同的时间值。这是因为计算机时钟和绝对时间之间有偏移，更重要的是，它们的漂移率互不相同。术语时钟漂移率( clock drit rate)指的是计算机时钟偏离绝对参考时钟的比率。即使分布式系统中所有计算机的时钟在初始情况下都设置成相同的时间，它们的时钟最后也会相差巨大，除非进行校正。 有几种校正计算机时钟的时间的方法。例如，计算机可使用无线电接收器从全球定位系统( GPS)以大约1μs的精度接收时间读数。但GPS接收器不能在建筑物内工作，同时，为每一台计算机增加GPS在费用上也不合理。相反，具有精确时间源(如GPS)的计算机可发送时序消息给网络中的其他计算机。在两个本地时钟时间之间进行协商当然会受消息延迟的影响。有关时钟漂移和时钟同步的更详细的讨论见第14章。 交互模型的两个变体。在分布式系统中，很难对进程执行、消息传递或时钟漂移所花的时间设置时间限制。两种截然相反的观点提供了一对简单模型:第一个模型对时间有严格的假设，第二个模型对时间没有假设。 同步分布式系统: Hadzilacos 和Toueg [1994] 定义了一个同步分布式系统，它满足下列约束: 进程执行每一步的时间有一个上限和下限。 通过通道传递的每个消息在一个已知的时间范围内接收到。 每个进程有一个本地时钟，它与实际时间的偏移率在一个已知的范围内。 对于分布式系统，建议给出合适的关于进程执行时间、消息延迟和时钟漂移率的上界和下界是可能的。但是达到实际值并对所选值提供保证是比较困难的。除非能保证上界和下界的值，否则任何基于所选值的设计都不可靠。但是，按同步系统构造算法，可以对算法在实际分布式系统的行为提供一些想法。例如，在同步系统中，可以使用超时来检测进程的故障，参见下面的2.4.2节。 同步分布式系統是能够被构造出来的。所要求的是进程用已知的资源需求完成任务，这些资源需求保证有足够的处理器周期和网络能力;还有要为进程提供漂移率在一定范围内的时钟。 异步分布式系统:许多分布式系统，例如互联网，是非常有用的，但它们不具备同步系统的资格。 因此我们需要另一个模型。异步分布式系统是对下列因素没有限制的系统: 进程执行速度 例如， 进程的一步可能只花费亿万分之一秒，而进程的另一步要花费一个世纪的时间，也就是说，每一步能花费任意长的时间。 消息传递延迟 例如， 从进程A到进程B传递一个消息的时间可能快得可以忽略，也可能要花费几年时间。换句话说，消息可在任意长时间后接收到。 时钟漂移率 时钟漂移率可以是任意的。 异步模型对执行的时间间隔没有任何假设。这正好与互联网一致，在互联网中,服务器或网络负载没有内在的约束，对像用FTP传输文件要花费多长时间也没有限制。有时电子邮件消息要花几天时间才能到达。下面的“Pepperland协定”部分说明在异步分布式系统中达成协定的困难性。即使有这些假设，有些设计问题也能得到解决。例如，虽然Web并不总能在一个合理的时间限制内提供特定的响应，但浏览器的设计可以做到让用户在等待时做其他事情。对异步分布式系统有效的任何解决方案对同步系统同样有效。 实际的分布式系统经常是异步的，因为进程需要共享处理器，而通信通道需要共享网络。例如，如果有太多特性未知的进程共享一个处理器，那么任何一个进程的性能都不能保证。但是，有许多不能在异步系统中解决的设计问题,在使用时间的某些特征后就能解决。在最终期限之前传递多媒体数据流的每个元素就是这样-个问题。对这样的问题，可使用同步模型。 事件排序。在许多情况下， 我们有兴趣知道一个进程中的一个事件(发送或接收一个消息)是发生在另一个进程中的另一个事件之前、之后或同时。尽管缺乏精确的时钟，但系统的执行仍能用事件和它们的顺序来描述。 ","date":"2021-11-17 08:45:55","objectID":"/distributedsystem_base_02/:4:1","tags":["distributed system"],"title":"DistributedSystem_base_02","uri":"/distributedsystem_base_02/"},{"categories":["School courses"],"content":"故障模型 在分布式系统中，进程和通信通道都有可能出故障，即它们可能偏离被认为是正确或所期望的行为。故障模型定义了故障可能发生的方式，以便理解故障所产生的影响。Hadzilacos 和Toueg [ 1994]提供了一种分类法，用于区分进程故障和通信通道故障。这些故障将分别在下面的“ 遗漏故障”、“随机故障”和“时序故障”部分介绍。 本书将贯穿使用故障模型。例如: 第4章给出数据报和流通信的Java接口，它们分别提供不同程度的可靠性。 第5章给出支持RMI的请求-应答协议。它的故障特征取决于进程和通信通道两者的故障特征。该协议能用数据报或流通信实现。可根据实现的简单性、性能和可靠性作出决定。 第17章给出事务的两阶段的提交协议。它用于在面对进程和通信通道的确定性故障时完成事务。 遗漏故障。遗漏故障类错误指的是进程或通信通道不能完成它应该做的动作。 进程遗漏故障:进程主要的遗漏故障是崩溃。当我们说进程崩溃了，意为进程停止了，将不再执行程序的任何步骤。能在故障面前存活的服务,如果假设该服务所依赖的服务能干净利落地崩溃，即进程仍能正确运行或者停止运行,那么它的设计能被简化。其他进程通过下列事实能检测到这种进程崩溃:这个进程一再地不能对调用消息进行应答。然而，这种崩溃检测的方法依赖超时的使用，即进程用一段固定时间等待某个事件的发生。在异步系统中，超时只能表明进程没有响应一它 可能是崩溃了，也可能是执行速度慢，或者是消息还没有到达。如果其他进程能确切检测到进程已经崩溃，那么这个进程崩溃称为故障-停止。在同步系统中, 如果确保消息已被传递，而其他进程又没有响应时，进程使用超时来检测，那么就会产生故障–停止 行为。 通信遗漏故障:考虑通信原语send和re-ceive。进程p通过将消息m插人到它的外发消息缓冲区来执行send。通信通道将m传输到q的接收消息缓冲区。进程q通过将m从它的接收消息缓冲区取走并完成传递来执行receive ( 见图2-14)。通常由操作系统提供外发消息缓冲区和接收消息缓冲区。 随机故障术语随机故障或拜占庭故障用于描述可能出现的最坏的故障，此时可能发生任何类型的错误。 时序故障时序故障适用于同步分布式系统。在这样的系统中，对进程执行时间、消息传递时间和时钟漂移率均有限制。时序故障见图2-16的列表。这些故障中的任何一个均可导致在指定时间间隔内对客户没有响应。 故障屏蔽分布式系统中的每个组件通常是基于其他一组组件构造的。利用存在故障的组件构造可靠的服务是可能的。例如，保存有数据副本的多个服务器在其中一个服务器崩溃时能继续提供服务。 一对一通信的可靠性虽然基本的通信通道可能出现前面描述的遗漏故障，但用它来构造一个能屏蔽某些故障的通信服务是可能的。 术语可靠通信可从下列有效性和完整性的角度来定义: 有效性:外发消息缓冲区中的任何消息最终能传递到接收消息缓冲区。 完整性:接收到的消息与发送的消息一致，没有消息被传递两次。 对完整性的威胁来自两个方面: 任何重发消息但不拒绝到达两次的消息的协议。要检测消息是否到达了两次，可以在协议中给消息附加序号。 心怀恶意的用户，他们可能插人伪造的消息、重放旧的消息或篡改消息。在面对这种攻击时为维护完整性要采取相应的安全措施。 ","date":"2021-11-17 08:45:55","objectID":"/distributedsystem_base_02/:4:2","tags":["distributed system"],"title":"DistributedSystem_base_02","uri":"/distributedsystem_base_02/"},{"categories":["School courses"],"content":"安全模型 在第1章中，我们识别出资源共享是分布式系统的一个激发因素。 在2.3节中，我们用进程来描述分布式系统的体系结构，其中可能封装了如对象、组件或服务等的高层抽象，而且，我们通过与其他进程的交互来访问系统。那个体系结构模型为我们的安全模型提供了基础: 通过保证进程和用于进程交互的通道的安全以及保护所封装的对象免遭未授权访问可实现分布式系统的安全。 保护对象。用户运行客户程序，由客户程序向服务器发送调用以完成在对象上的操作。服务器完成每个调用指定的操作并将结果发给客户。 保护进程和它们的交互进程通过发送消息进行交互。消息易于受到攻击，因为它们所使用的网络和通信服务是开放的，以使得任一对进程可以进行交互。服务器和对等进程暴露它们的接口，使得任何其他进程能给它们发送调用。 敌人。为了给安全威胁建模，我们假定敌人(有时也称为对手)能给任何进程发送任何消息，并读取或复制一对进程之间的任何消息，如图2-18所示。这种攻击能很简单地实现，它利用连接在网上的计算机运行一个程序读取那些发送给网络上其他计算机的网络消息，或是运行一个程序生成假的服务请求消息并声称来自授权的用户。攻击可能来自合法连接到网络的计算机或以非授权方式连接到网络的计算机。来自一个潜在敌人的威胁包括对进程的威胁和对通信通道的威胁。 对进程的威胁:在分布式系统中,一个用于处理到达的请求的进程可以接收来自其他进程的消息，但它未必能确定发送方的身份。通信协议(如IP)确实在每个消息中包括了源计算机的地址，但对一个敌人而言，用一个假的源地址生成一个消息并不困难。缺乏消息源的可靠的知识对服务器和客户的正确工作而言是一个威胁，具体解释如下: 服务器:因为服务器能接收来自许多不同客户的调用，所以它未必能确定进行调用的主体的身份。即使服务器要求在每个调用中加入主体的身份，敌人也可能用假的身份生成一个调用。在没有关于发送方身份的可靠知识时，服务器不能断定应执行操作还是拒绝执行操作。例如，邮件服务器不知道从指定邮箱中请求一个邮件的用户是否有权限这样做，或者它是否为来自一个敌人的请求。 客户:当客户接收到服务器的调用结果时，它未必能区分结果消息来自预期的服务器还是来自一个“哄骗”邮件服务器的敌人。因此，客户可能接收到一个与原始调用无关的结果，如一个假的邮件(不在用户邮箱中的邮件)。 对通信通道的威胁:一个敌人在网络和网关上行进时能复制、改变或插人消息。当信息在网络上传递时，这种攻击会对信息的私密性和完整性构成威胁，对系统的完整性也会构成威胁。例如，包含用户邮件的结果消息可能泄露给另-一个用户或者可能被改变成完全不同的东西。另一种形式的攻击是试图保存消息的拷贝并在以后重放这个消息，这使得反复重用同一消息成为可能。例如，有些人通过重发请求从一个银行账户转账到另一个银行账户的调用消息而受益。利用安全通道可解除这些威胁，安全通道是基于密码学和认证的，详细内容见下面的描述。 解除安全威胁。下面将介绍安全系统所基于的主要技术。 第11章将详细讨论安全的分布式系统的设计和实现。 密码学和共享秘密:假设一对进程(例如某个客户和某个服务器)共享一个秘密，即它们两个知道秘密但分布式系统中的其他进程不知道这个秘密。如果由一对进程交换的消息包括证明发送方共享秘密的信息，那么接收方就能确认发送方是一对进程中的另一个进程。当然，必须小心以确保共享的秘密不泄露给敌人。 密码学是保证消息安全的科学,加密是将消息编码以隐藏其内容的过程。现代密码学基于使用密钥(很难猜测的大数)的加密算法来传输数据，这些数据只能用相应的解密密钥恢复。 认证:共享秘密和加密的使用为消息的认证(证明由发送方提供的身份)奠定了基础。基本的认证技术是在消息中包含加密部分，该部分中包含足够的消息内容以保证它的真实性。对文件服务器的一个读取部分文件的请求，其认证部分可能包括请求的主体身份的表示、文件的标识、请求的日期和时间，所有内容都用一个在文件服务器和请求的进程之间共享的密钥加密。服务器能解密这个请求并检查它是否与请求中指定的未加密细节相对应。 安全通道:加密和认证用于构造安全通道，安全通道作为已有的通信服务层之上的服务层。安全通道是连接一对进程的通信通道，每个进程代表一个主体行事，如图2- 19所示。- 一个安全通道有下列特性: 每个进程确切知道其他正在执行的进程所代表的主体身份。因此，如果客户和服务器通过安全通道通信，那么服务器要知道发起调用的主体身份，并能在执行操作之前检查它们的访向权限。这使得服务器能正确地保护它的对象，以便客户相信它是从真实的服务器上接收到的结果。 安全通道确保在其上传送的数据的私密性和完整性(防止篡改)。 每个消息包括一个物理的或逻辑的时间戳以防消息被重放或重排序。 其他可能的来自敌人的威胁。1.5.3 节简要介绍了两个安全威胁一拒绝服 务攻击和移动代码的部署。作为敌人破坏进程活动的可能的机会，我们要再介绍一下这两个安全威胁。 拒绝服务:在这种攻击形式下，敌人通过超量地、无意义地调用服务或在网络上进行消息传送，干扰授权用户的活动，导致物理资源( 网络带宽、服务器处理能力)的过载。这种攻击通常意在延迟或阻碍其他用户的动作。例如，建筑物中的电子门锁可能由于受到对计算机控制的电子锁的过多非法请求而失效。 移动代码:如果进程接收和执行来自其他地方的程序代码(如1. 5.3节提到的邮件附件)，那么这些移动代码就会带来新的、有趣的安全问题。这样的代码很容易扮演特洛伊木马的角色，声称完成的是无害的事情但事实上包括了访问或修改资源的代码，这些资源对宿主进程是合法可用的但对代码的编写者是不合法的。实现这种攻击有多种不同的方法，因此必须非常小心地构造宿主环境以避免攻击。其中的大多数问题已在Java和其他移动代码系统中解决了，但从最近的一段历史看，移动代码问题暴露了一些让人窘迫的弱点。这-点也很好地说明了所有安全系统的设计都需要严格的分析。 安全模型的使用。有人认为, 在分布式系统中获得安全是件简单的事，即根据预定义的访问权限控制对象的访问以及通信的安全通道的使用，但是通常却不是这样。安全技术(如加密)和访问控制的使用会产生实质性的处理和管理开销。前面概述的安全模型提供了分析和设计安全系统的基础，其中这些开销保持最少,但对分布式系统的威胁会在许多地方出现，需要对系统网络环境、物理环境和人际环境中所有可能引发的威胁进行仔细的分析。这种分析涉及构造威胁模型，由它列出系统会遭遇 的各种形式的攻击、风险评估和每个威胁所造成的后果。要在抵御威胁所需的安全技术的有效性和开销之间做出权衡。 ","date":"2021-11-17 08:45:55","objectID":"/distributedsystem_base_02/:4:3","tags":["distributed system"],"title":"DistributedSystem_base_02","uri":"/distributedsystem_base_02/"},{"categories":["School courses"],"content":"小结 如2.2节所展示的，从底层物理特性角度，例如，系统的规模、系统内在的异构性、从特性角度(如安全)提供端到端解决方案的实际需求等，分布式系统的复杂性正在增加。这使得从模型角度理解和探讨分布式系统显得更加重要。本章考虑了底层物理模型，并深度考察了支撑分布式系统的体系结构模型和基础模型。 本章从所包含的体系结构模型角度给出了描述分布式系统的方法，明晰了这个设计空间的内涵，包括查看什么在通信以及这些实体如何通信等核心问题，以及基于给定物理基础设施，考虑每个元素可以扮演的角色与合适的放置策略，并把它们补充到设计中去。 本章还介绍了体系结构模式在由底层核心元素(例如上述的客户-服务器模型)构造复杂设计中发挥的关键作用,给出了支持分布式系统的中间件解决方案的主要类型，包括基于分布式对象、组件、Web服务和分布式事件的解决方案。从体系结构模型角度看，客户-服务器方法是一种常见的体系结构模型一 Web 和其他互联网服务(如FIP、新闻和邮件以及Web服务和DNS)均基于这个模型，文件归档和其他本地服务也是如此。像DNS这种有大量的用户并管理大量信息的服务是基于多个服务器的，并使用数据分区和复制来提高可用性和容错能力。客户和代理服务器上的缓存得到广泛使用以提高服务的性能。不过，现在有许多方法对分布式系统进行建模，包括各种可替代的观点，如对等计算和更多的面向问题的抽象(如对象、组件或服务)。 基础棋型补充了体系结构模型，它们帮助从诸如性能、可靠性和安全角度对分布式系统的特性进行推理。特别地，我们给出了交互模型、故障模型和安全模型。它们识别出构造分布式系统的基本组件的共同特征。交互模型关注进程和通信通道的性能以及全局时钟的缺乏。它将同步系统看成在进程执行时间、消息传递时间和时钟漂移上有已知范围的系统，将异步系统看成在进程执行时间、消息传递时间和时钟漂移上没有限制的系统一这是对互联网行为的描述。 故障模型将分布式系统中的进程故障和基本的通信通道故障进行了分类。屏蔽是一项技术，依靠它，可将不太可靠的服务中的故障加以屏蔽，并基于此构造出较可靠的服务。特别是，通过屏蔽基本的通信通道的故障，可从基本的通信通道构造出可靠的通信服务。例如，遗漏故障可通过重传丢失的消息加以屏蔽。完整性是可靠通信的一个性质一它要求接收到的消息 与发送的消息一致，并且没有消息被发送两次。有效性是可靠通信的另一个性质一它要求发送消息缓冲区中的任何消息最终都能传递到接收消息缓冲区。 安全模型可识别出在一个开放的分布式系统中对进程和通信通道可能的威胁。有些威胁与完整性有关:恶意用户可能篡改消息或重放消息。其他的威胁则会损害私密性。另一个安全问题是发送消息所代表的主体(用户或服务器)的认证。安全通道使用密码技术来确保消息的完整性和私密性，并使得相互通信的主体可以进行验证。 ","date":"2021-11-17 08:45:55","objectID":"/distributedsystem_base_02/:5:0","tags":["distributed system"],"title":"DistributedSystem_base_02","uri":"/distributedsystem_base_02/"},{"categories":["School courses"],"content":"分布式系统的特征 ","date":"2021-11-10 10:24:24","objectID":"/distributedsystem_base_01/:0:0","tags":["distributed system"],"title":"DistributedSystem_base_01","uri":"/distributedsystem_base_01/"},{"categories":["School courses"],"content":"简介 我们把分布式系统定义成一个其硬件或软件组件分布在联网的计算机上，组件之间通过传递消息进行通信和动作协调的系统。 我们定义的分布式系统具有如下的具体特征： 并发：在一个计算机网络中，执行并发程序是常见的行为。用户可以在各自的计算机上工作，在必要时共享诸如Web页面或文件之类的资源。系统处理共享资源的能力会随着网络资源（例如，计算机）的增加而提高。 缺乏全局时钟：在程序需要协作时，它们通过交换消息来协调它们的动作。密切的协作通常取决于对程序动作发生的时间的共识。但是，事实证明，网络上的计算机与时钟同步所达到的准确性是有限的，即没有一个正确时间的全局概念。原因也很简单，因为基于网络的通信天然存在时延，而这个时延可能会由于网络状态等条件而不断发生变化，这么一来，就会导致各项动作没办法按照原定的时间来工作，反而会出现一些不好的影响。 故障独立性：所有的计算机系统都可能会出现故障，一般由系统设计者负责为可能的故障设计结果。分布式系统可能会以新的方式出现故障。网络故障导致网上互联的计算机的隔离，但这并不意味着它们停止运行，事实上，计算机上的程序不能检测到网络是出现故障还是网络运行得比通常慢。类似的，计算机的故障或系统中程序得异常终止（崩溃），并不能让与它通信的其他组件了解。系统的每个组件会单独地出现故障，而其他组件还在运行。 ","date":"2021-11-10 10:24:24","objectID":"/distributedsystem_base_01/:1:0","tags":["distributed system"],"title":"DistributedSystem_base_01","uri":"/distributedsystem_base_01/"},{"categories":["School courses"],"content":"分布式系统的例子 ","date":"2021-11-10 10:24:24","objectID":"/distributedsystem_base_01/:2:0","tags":["distributed system"],"title":"DistributedSystem_base_01","uri":"/distributedsystem_base_01/"},{"categories":["School courses"],"content":"web搜索 Web搜索引擎的任务是为万维网的所有内容建立索引，其中包含各种信息类型，例如Web页面、多媒体资源和扫描后的书。考虑到大多数搜索引擎是分析整个Web内容，并在这个巨大的数据库上完成复杂的处理，那么这个任务自身就是对分布式系统涉及的一个巨大挑战。 Google，Web搜索技术上的市场领导者，在支持用于搜索（与其他Google应用和服务，如Google Earth）的复杂的分布式系统基础设施上做出了巨大努力。该设施最突出的亮点包括： 一个底层物理设施：它由超大数目的位于全世界多个数据中心的连网计算机组成。 一个分布式文件系统：支持超大文件，并根据搜索和其他Google应用的使用方式（特别是在文件中以快速而持久的速度读取）进行了深度优化 一个相关的结构化分布式存储系统：它提供了对超大数据集的快速访问 一个锁服务：它提供了诸如分布式加锁和协定等分布式系统功能。 一个编程模式：它支持对底层物理基础设施上的超大并行和分布式计算的管理。 ","date":"2021-11-10 10:24:24","objectID":"/distributedsystem_base_01/:2:1","tags":["distributed system"],"title":"DistributedSystem_base_01","uri":"/distributedsystem_base_01/"},{"categories":["School courses"],"content":"大型多人在线游戏 大型多人在线游戏（Massively Multiplayer Online Game,MMOG）提供了一种身临其境的体验，超大数目用户通过互联网在一个持久的虚拟世界中交互。这类游戏的主要例子是Sony的EverQuest Ⅱ和芬兰公司CCP Games公司的EVE Online。 MMOG工程体现了分布式系统技术面临的巨大挑战，尤其是它对快速响应时间的需求。其他挑战包括事件实时传播给多个玩家和维护对共享世界的一个一致的视图。 针对大型多人在线游戏，提出了许多解决方案： 可能有点出乎意料，最大的在线游戏EVE Online，采用了CS体系结构，在一个集中式服务器上维护了游戏世界状态的单个拷贝，供运行在玩家终端或其他设备上的客户程序访问。为了支持大量客户，服务器自身是一个复杂的实体，拥有上百个计算机节点组成的集群结构。从虚拟世界的管理看，集中式体系结构有极大的益处，单个拷贝也简化了一致性问题。接着，目标是用过优化网络协议和快速响应到达事件来确保快速的响应。为了支持这点，对负载进行分区，把单个“星系”分配给集群中指定的计算机，这样，高负载星系会拥有自己的专用计算机，而其他星系则共享一台计算机。通过跟踪王家在i星系之间的移动，到达事件会被导向集群中正确的计算机上。 其他MMOG采用更多的分布式体系结构，宇宙被划分到大量（可能是超多）服务器上，这些服务器可能地理上分散部署。接着，用户基于当前的使用模式和到服务器的网络延迟（基于地理最近）被动态地分配到一个特定的服务器。这种体系结构风格被EverQuest采用，它通过增加新的服务器，可自然地扩展。 大多数商业系统采用上述两个模型中的一个，但研究者现在也在寻找更极端的体系结构，即不基于客户-服务器原理而是基于对等技术采用完全分散的方法。采用对等技术，意味着每个参与者贡献（存储和处理）资源来容纳游戏。 ","date":"2021-11-10 10:24:24","objectID":"/distributedsystem_base_01/:2:2","tags":["distributed system"],"title":"DistributedSystem_base_01","uri":"/distributedsystem_base_01/"},{"categories":["School courses"],"content":"金融交易 金融行业以其需求一直处在分布式系统技术的最前沿，特别是在实时访问大范围的信息源方面（例如，当前股票价格和趋势，经济和政治发展）。金融行业采用自动监控和交易应用。 此类系统的重点是对感兴趣数据项的通信和处理。感兴趣数据项在分布式系统中称为事件，在金融行业中的需求是可靠和及时地传递事件给可能是大量对此信息有兴趣地客户。这要求底层地体系结构具有与前述风格（例如CS）完全不同的风格，这样的系统通常采用分布式基于事件的系统。 下图说明了一个典型的金融交易的例子。它显示了一系列事件进入一个指定的金融机构。这样的事件输入具有下列特征。（图片来自分布式系统：概念与设计（原书第五版） by George Coulouris Jean Dollimore Tim Kindberg Gordon Blair (z-lib.org)） 首先，事件源通常具有多种格式，例如路透社的市场数据事件和FIX事件（符合金融信息交换协议特定格式的事件），事件源还来自不同的事件技术，这说明了在大多数分布式系统中回到异构性问题。图中使用了适配器，它把异构性格式转换成一个公共的内部格式。 其次交易系统必须处理各种各样的事件流，这些事件流高速到达，经常需要实时处理来检测表示交易机会的模式。这在过去曾今是手工处理的，但在竞争压力下变成自动处理，这就是所谓的复杂事件处理（Complex Event Processing，CEP），它提供了一种方法来将一起发生的事件组成逻辑的、时序的或空间的模式。 ","date":"2021-11-10 10:24:24","objectID":"/distributedsystem_base_01/:2:3","tags":["distributed system"],"title":"DistributedSystem_base_01","uri":"/distributedsystem_base_01/"},{"categories":["School courses"],"content":"分布式系统的趋势 分布式系统正在经历巨大的变化，这可追溯到一系列有影响力的趋势： 出现了泛在联网技术 出现了无处不在计算，它伴随着分布式系统中支持用户移动性的意愿 对多媒体设备的需求 把分布式系统作为一个设施 ","date":"2021-11-10 10:24:24","objectID":"/distributedsystem_base_01/:3:0","tags":["distributed system"],"title":"DistributedSystem_base_01","uri":"/distributedsystem_base_01/"},{"categories":["School courses"],"content":"泛在联网和现代互联网 互联网上的计算机程序通过传递消息进行交互，采用了一种公共的通信手段。互联网通信机制（互联网协议）的设计和构造是一项重大的技术成果，它使得一个在某处运行的程序能给另一个地方的程序发送消息。 互联网是一个超大的分布式系统。互联网和其支持得服务的实现，使得必须开发实用解决方案来解决分布式系统中的许多问题。 ","date":"2021-11-10 10:24:24","objectID":"/distributedsystem_base_01/:3:1","tags":["distributed system"],"title":"DistributedSystem_base_01","uri":"/distributedsystem_base_01/"},{"categories":["School courses"],"content":"移动和无处不在计算 设备小型化和无线网络方面的技术进步已经逐渐使得小型和便携式计算设备集成到分布式系统中。这些设备包括： 笔记本电脑 手持设备（包括移动电话、智能电话、GPS设备、摄像机等） 可穿戴设备 嵌入式家电 这些设备大多具有可便携性，再加上它们可以在不同的地方方便地连接到网络的能力，使得移动计算成为可能。移动计算是指用户在移动或访问某个非常规环境时执行计算任务的性能。 无处不在计算是指对用户的的物理环境（包括家庭、办公室和其他自然环境）中存在的多个小型、便宜的计算设备的利用。 移动和无处不在计算是一个热门的研究领域。 ","date":"2021-11-10 10:24:24","objectID":"/distributedsystem_base_01/:3:2","tags":["distributed system"],"title":"DistributedSystem_base_01","uri":"/distributedsystem_base_01/"},{"categories":["School courses"],"content":"分布式多媒体系统 另一个重要趋势是在分布式系统中支持多媒体服务的需求。多媒体支持可以定义为以集成的方式支持多种媒体类型的能力。人们可以期望分布式多媒体系统支持离散型媒体（如图片或正文消息）的存储、传输和展示。分布式多媒体系统应该能对连续类型媒体（如音频和视频）完成相同的功能，即它应该能存储和定位音频或视频文件，并通过网络传输它们（可能需要以实时的方式，因为流来自摄像机），从而能给用户展示多种媒体类型，以及在一组用户中共享多种类型的媒体。 连续媒体的重要特点时它们包括一个时间维度，媒体类型的完整性从根本上依赖于在媒体类型的元素之间保持实时关系。 分布式多媒体计算的好处时相当大的，因为能在桌面环境提供大量的新（多媒体）服务和应用，包括访问实况或预先录下的电视广播、访问提供视屏点播服务的电影资料库、访问音乐资料库、提供音频和视频会议设施、提供集成的电话功能。 网络播放（webcasting） 是分布式多媒体技术的应用。网络播放是在互联网上广播连续媒体（典型是音频和视频）的能力，现在常见以这种方式广播主要的体育或音乐事件。 分布式多媒体应用（例如网络播放）对底层的分布式基础设施提出了大量的要求，包括： 提供对一系列（可扩展的）编码和加密格式的支持，例如MPEG系列标准（包括如流行的MP3标准，也称MPEG-1音频第三层）和HDTV 提供一系列机制来保障所需的服务质量能够得到满足 提供相关的资源管理策略，包括合适的调度策略，来支持所需的服务质量 提供适配策略类处理在开放系统中不可避免的场景，即服务质量不能得到满足或维持 ","date":"2021-11-10 10:24:24","objectID":"/distributedsystem_base_01/:3:3","tags":["distributed system"],"title":"DistributedSystem_base_01","uri":"/distributedsystem_base_01/"},{"categories":["School courses"],"content":"把分布式计算作为一个公共设施 随着分布式下基础设施的不断成熟，不少公司在推广这样的观点：把分布式资源看作一个商品或公共设施，把分布式资源和其他公共设施进行类比。采用这种模型，资源通过合适的服务提供者提供，能被最终用户有效地租赁而不是拥有。这种模型可以应用到物理资源和更多的逻辑服务上。 联网的计算机可用诸如存储和处理这样的物理资源，从而无需自己拥有这样的资源。从一个维度来看，用户可以为其文件存储需求和文件备份需求选择一个远程存储设施。类似的，利用这个方法，用户能租到一个或多个计算结点，从而满足他们的基本计算需求或者完成分布式计算。从另一个维度来看，用户现在能用像Amazon和Google之类的公司提供的服务访问复杂的数据中心或计算基础设施。操作系统虚拟化时该方法关键的使能技术，它意味着实际上可以通过一个虚拟的而不是物理的结点为用户提供服务。这从资源管理角度给服务提供者提供了更大的灵活性。 用这种方法，软件服务也能跨全球互联网使用。 关于计算作为公共设施，术语云计算（cloud computi）被用来刻画其前景。云被定义成一组基于互联网的应用，并且足以满足大多数用户需求的存储和计算服务的集合，这使得用户能大部分或全部免于本地数据存储和应用软件的使用。该术语也推广“把每个事物看成一个服务”的观点。 通常，云实现在集群计算机上，从而提供每个服务所要求的必须的伸缩性和性能。**集群计算机（cluster computer）**是互联的计算机集合，它们密切协作提供单一的、集成的高性能计算能力。 集群服务器的总目的时提供一系列的云服务，包括高性能计算能力、大容量存储能力（例如通过数据中心）、丰富的应用服务（如Web搜索——Google依赖大容量集群计算机体系结构来实现其搜索引擎和其他服务） 网格计算也能被看作时一种云计算。但网格计算通常被看作时云计算这种更通用模式的先驱，它只是偏重于支持科学计算。 ","date":"2021-11-10 10:24:24","objectID":"/distributedsystem_base_01/:3:4","tags":["distributed system"],"title":"DistributedSystem_base_01","uri":"/distributedsystem_base_01/"},{"categories":["School courses"],"content":"关注资源共享 从硬件资源来看，大家共享设备可以减少花费，但对用户具用更大意义的是共享与用户应用、日常工作和社会活动有关的更高层的资源。例如用户惯性以共享数据库或Web页面集方式出现的共享数据，而不是实现上述服务的硬盘和处理器。类似的，用户关心诸如搜索引擎或货币换算器之类的共享资源，而不关心提供这些服务的服务器。 实际上，资源共享的模式随着其工作范围和与用户工作的密切程度的不同而不同。一种极端是，Web上的搜索引擎是给全世界的用户提供工具，而用户之间并不需要直接接触；另一种极端是，在计算机支持协调工作（Computer Supported Working。CSCW） 中，若干直接进行合作的用户在一个小型封闭的小组中共享诸如文档之类的资源。用户在地理上的分布以及用户之间进行共享的模式决定了系统必须提供协调用户动作的机制。 我们使用属于服务表示计算机系统中管理相关资源并提供功能给用户和应用的一个单独的部分。 服务将资源访问限制为一组定义良好的操作，这在某种程度上属于标准的软件工程实践。同时它也反映出分布式系统的物理组织。分布式相同的资源是物理地封装在计算机内，其他计算机只能通过通信访问。为了实现有效的共享，每个资源必须由一个程序管理，这个程序提供通信接口使得对资源进行可靠和一致的访问和更新。 ","date":"2021-11-10 10:24:24","objectID":"/distributedsystem_base_01/:4:0","tags":["distributed system"],"title":"DistributedSystem_base_01","uri":"/distributedsystem_base_01/"},{"categories":["School courses"],"content":"挑战 随着分布式系统的应用范围和规模扩大，可能会遇到相同的和其他的挑战。 ","date":"2021-11-10 10:24:24","objectID":"/distributedsystem_base_01/:5:0","tags":["distributed system"],"title":"DistributedSystem_base_01","uri":"/distributedsystem_base_01/"},{"categories":["School courses"],"content":"异构性 互联网使得用户能在大量异构计算机和网络上访问服务和运行应用程序。 下面这些均存在异构性（即存在多样性和差别）： 网络 计算机硬件 操作系统 编程语言 由不同开发者完成的软件实现 中间件：指一个软件层，它提供了一个编程抽象，同时屏蔽了底层网络、硬件、操作系统和编程语言的异构性。有些中间件。如Java远程方法调用（Remote Method Invocation,RMI）,仅支持一种编程语言。大多数中间件在互联网协议上实现，由这些协议屏蔽了底层网路的差异，但所有的中间件要解决操作系统和硬件的不同。 处理解决异构性问题之外，中间件为服务器和分布式应用的程序员提供了一致的计算模型。这些模型包括远程方法调用、远程时间通知、远程SQL访问和分布式事务处理。 异构性和移动代码中移动代码是指能从一台计算机发送到另一台计算机发送到另一台计算机，并在目的计算机上执行的代码，Java applet是一个例子。适合在一种计算机上运行的代码未必适合在另一种计算机上运行，因为可执行程序通常依赖于计算机的指令集和操作系统。 虚拟机方法提供了一种使代码可在任何计算机上运行的方法：某种语言的编译器生成一台虚拟机的代码而不是某种硬件代码，例如，Java编译器生成Java虚拟机的代码，虚拟机通过解释的方法来执行它。为了使Java程序嫩个运行，要在每种计算机上实现一次Java虚拟机。 今天，最常使用的移动代码是将一些Web页面的JavaScript程序装载到客户端浏览器中。 ","date":"2021-11-10 10:24:24","objectID":"/distributedsystem_base_01/:5:1","tags":["distributed system"],"title":"DistributedSystem_base_01","uri":"/distributedsystem_base_01/"},{"categories":["School courses"],"content":"开放性 计算机系统的开放性是决定系统能否以不同的方式被扩展和重新实现的特征。分布式系统的开放性主要取决于新的资源共享服务能被增加和供多种客户程序使用的程度。 除非软件开发者能获得系统组件的关键软件接口的规范和文档，否则无法实现开放性。一句话发布关键接口。这个过程类似接口的标准化，但它进程避开官方的标准化过程，官方的标准化过程非常繁琐且进度缓慢。 然而发布接口仅是分布式系统增加和扩展服务的起点。设计者所面临的挑战是解决由不同人构造的由许多组件组成的分布式系统的复杂性。 互联网协议的设计者引入了一系列称为“征求意见文档”（Requests For Comments，RFC）的文档，每个文档有一个编号。 按这种方式支持资源共享的系统之所以被称为开放的分布式系统，主要是强调它们是可扩展的。它们通过在网络中增加计算机实现在硬件层次上的扩展，通过引入新的服务、重新实现旧的服务实现在软件层次上的扩展，最终使得应用程序能够共享资源。开放系统常被提到的好处是它们与销售商无关。 开放的分布式系统的特征总结如下： 发布系统的关键接口是开放系统的特征 开放的分布式系统是基于一致的通信机制和发布接口访问共享资源的。 开放的分布式系统能用不同销售商提供的异构硬件和软件构造，但如果想让系统正确工作，就要仔细测试和验证每个组件与发布的标准之间的一致性。 ","date":"2021-11-10 10:24:24","objectID":"/distributedsystem_base_01/:5:2","tags":["distributed system"],"title":"DistributedSystem_base_01","uri":"/distributedsystem_base_01/"},{"categories":["School courses"],"content":"安全性 分布式系统中维护和使用的众多信息资源对用户具有很高的内在价值，因此它们的安全相当重要。信息资源的安全性包括三个部分：机密性（防止泄露给未授权的个人）、完整性（防止被改变或被破坏）、可用性（防止对访问资源的手段的干扰）。 安全性不止涉及对消息的内容保密，还涉及确切知道用户或代表用户发送消息的其他代理的身份。利用机密技术可满足这两个挑战。 然而，下面两个安全方面所面临的挑战目前还没有完美解决： 拒绝服务攻击：另一个安全问题是处于某些用户可能希望中断服务。可用下面的方法实现这个目的：用大量无意义的请求攻击服务器，使得重要的用户不能使用它。这称为拒绝服务攻击。现在通过在世间发生后抓获和惩罚犯罪者来解决这种攻击，但这不是解决这种问题的通用方法。 移动代码的安全性：移动代码需要小心处理。因为有些程序表面上可能一副有意思的画，但实际上却在访问本地资源，或者可能是拒绝服务攻击的一部分。 ","date":"2021-11-10 10:24:24","objectID":"/distributedsystem_base_01/:5:3","tags":["distributed system"],"title":"DistributedSystem_base_01","uri":"/distributedsystem_base_01/"},{"categories":["School courses"],"content":"可伸缩性 分布式系统可在不同的规模（从小型企业内部网到互联网）下有效且高效地运转。如果资源数量和用户数量激增，系统仍能保持其有效性，那么该系统就被称为可伸缩性。 可伸缩性分布式系统的设计面临下列挑战： 控制物理资源的开销 控制性能损耗 防止软件资源用尽：例如IP地址 避免性能瓶颈：通常，算法应该是分散型，以避免性能瓶颈。例如域名系统的前身，那时名字表被保存在一个主文件中，可被任何需要它的计算机下载。当互联网中只有几百个计算机时，这是可以的，但这不久就变成了一个严重的性能和管理瓶颈。现在，域名系统将名字表分区，分散到互联网中的服务器上，并采用本地管理的方式，从而解决了这个瓶颈。 ","date":"2021-11-10 10:24:24","objectID":"/distributedsystem_base_01/:5:4","tags":["distributed system"],"title":"DistributedSystem_base_01","uri":"/distributedsystem_base_01/"},{"categories":["School courses"],"content":"故障处理 计算机系统有时会出现故障。当硬件或软件发生故障时，程序可能会产生不正确的结果或者在它们完成应该进行的计算之前就停止了。 分布式系统的故障时部分的，也就是说，有些组件出了故障而有些组件运行正常。因此故障的处理相当困难。接下来我们讨论一下处理故障的技术： 检测故障：有些故障能被检测到。例如，校验和可用于检测消息或文件中出现的错误。而有些故障时很难甚至不能被检测到的。面临的挑战是如何在有故障出现的情况下进行管理，这些故障不能被检测到但可以被猜到。 掩盖故障：有些被检测到的故障能被隐藏起来或降低她的严重程度。下面是隐藏故障的两个例子 ： 1）消息在不能到达时重传。 2）将文件数据写入两个磁盘，如果一个磁盘损坏，那么另一个磁盘的数据仍是正确的。 降低故障严重程度的例子是丢掉被损坏的消息。这样，该消息可以被重传。读者可能意识到，隐藏故障的技术不能保证在最坏情况下有效。例如，第二个磁盘上的数据可能也坏了，或消息无论怎样重传都不能在合理的时间到达。 容错：互联网上的大多服务确实可能发生故障，试图检测并隐藏在这样大的网络、这么多的组件中发生的所有故障是不太实际的。服务的客户能被设计成容错的，这通常也涉及用户要容忍错误。例如，当Web浏览器不能与Web服务器连接时，它不会让用户一直等待它与服务器建立连接，而是通知用户这个问题，让用户自由选择是否尝试稍后再连接。 故障恢复：恢复涉及软件的设计，以便在服务器崩溃后，永久数据的状态能被恢复或“回滚”。通常再出现错误时，程序完成的计算是不完整的，被更新的永久数据（文件和其他保存在永久存储介质中的资料）可能处在不一致的状态。 冗余：利用冗余组件，服务可以实现容错。考虑下面的例子： 1）在互联网的任意两个路由器之间，至少存在两个不同的路由。 2）在域名系统中，每个名字表至少被复制到两个不同的服务器上。 3）数据库可以被复制到几个服务器上，以保证在任何一个服务器上有错误时，客户就被重定向到剩下的服务器上。 ","date":"2021-11-10 10:24:24","objectID":"/distributedsystem_base_01/:5:5","tags":["distributed system"],"title":"DistributedSystem_base_01","uri":"/distributedsystem_base_01/"},{"categories":["School courses"],"content":"并发性 在分布式系统中，服务和应用均提供可被客户共享的资源。因此，可能有几个客户同时试图访问一个共享资源的情况。 管理共享资源的进程可以一次接受一个客户请求，但这种方法限制了吞吐量。因此，服务和应用通常被允许并发地处理多个客户请求。 在分布式系统中，代表共享资源的任何一个对象必须负责确保它在并发环境中操作正确，这不仅适用于服务器，也适用于服务器，也适用于应用中的对象。因此，持有未打算用于分布式系统的对象实现的程序员必须做一些事情，使得对象在并发环境中能安全使用。 为了使对象在并发环境中能安全使用，它的操作必须在数据一致的基础上同步。者可通过标准的技术（如大多数操作系统所采用的信号量）来实现。 ","date":"2021-11-10 10:24:24","objectID":"/distributedsystem_base_01/:5:6","tags":["distributed system"],"title":"DistributedSystem_base_01","uri":"/distributedsystem_base_01/"},{"categories":["School courses"],"content":"透明性 透明性被定义成对用户和应用程序员屏蔽分布式系统的组件的分离性，使系统被认为是一个整体，而不是独立组件的集合。 ANSA参考手册和国际化标准化组织的开放分布式处理的参考模型（RM-ODP）识别出八种透明性（并用范围更广的移动透明性替换迁移透明性）： 访问透明性：用相同的操作访问本地资源和远程资源。 位置透明性：不需要知道资源的物理或网络位置就能访问它们 并发透明性：几个进程能并发地使用共享资源进行操作且互不干扰 复制透明性：使用资源的多个实例提升可靠性和性能，而用户和应用程序员无需知道副本的相关信息 故障透明性：屏蔽错误，不论是硬件组件故障还是软件组件故障，用户和应用程序员能够完成他们的任务 移动透明性：资源和客户能够在系统内移动而不会影响用户或程序的操作 性能透明性：当负载发生变化时，系统能被重新配置以提高性能 伸缩透明性：系统和应用能够进行扩展而不改变系统结构或应用算法 最重要的两个透明性是访问透明性和位置透明性，它们的有无对分布式资源的利用有很大影响，又是它们被统一称为网络透明性。 ","date":"2021-11-10 10:24:24","objectID":"/distributedsystem_base_01/:5:7","tags":["distributed system"],"title":"DistributedSystem_base_01","uri":"/distributedsystem_base_01/"},{"categories":["School courses"],"content":"服务质量 一旦提供给用户他们要求的功能，例如在一个分布式系统中的文件服务，我们就能继续探寻所提供的服务质量。系统的主要的非功能特性，即影响客户和用户体验的服务质量是可靠性、安全性和性能。满足变化的系统配置和资源可用性的适用性已被公认为服务质量的一个重要方面。 可靠性和安全性问题再设计大多是计算机系统时时关键的。服务质量的性能源于及时性和计算吞吐量，但它已被重新定义成满足及时性保证的能力。 一些应用，包括多媒体应用，处理时间关键性数据，这些数据是要求以固定速度处理或从一个进程传送到另一个进程的数据流。例如，一个电影服务可能由一个客户程序组成，该程序从一个视频服务器中检索电影并把它呈现到用户的屏幕上。该视频的连续帧在指定时间限制内显示给用户，才算是一个满意的结果。 ","date":"2021-11-10 10:24:24","objectID":"/distributedsystem_base_01/:5:8","tags":["distributed system"],"title":"DistributedSystem_base_01","uri":"/distributedsystem_base_01/"},{"categories":["School courses"],"content":"实例研究：万维网 在1989年3月，互联网还只属于少数人。在这一互联网的黎明期，HTTP诞生了。 蒂姆·伯纳斯-李博士提出了一种让远隔两地的研究者们共享知识的设想，这一最初设想的基本理念是：借助多文档之间相互关联形成的超文本（HyperText），连成可相互参阅的WWW（World Wide Web,万维网）。 超文本（Hypertext）：在互联网早期的时候，我们输入的信息只能保存在本地，无法和其他电脑交互。我们保存的信息通常都以文本即简单字符的形式存在，文本是一种能够被计算机解析的有意义的二进制数据包。而随着互联网的高速发展，两台电脑之间能够进行数据的传输后，人们不再满足只能在两台电脑之间传输文字，还想要传输图片、音频、视频，甚至是超链接，那么文本的语义就被扩大了，这种语义扩大后的文本就被称为超文本（HyperText）。 而为了实现这一理念，现在已经提出了3项WWW构建技术，分别是： 把SGML（Standard Generalized Markup Langrage，标准通用标记语言）作为页面的文本标记语言的HTML（HyperText Markup Language，超文本标记语言） 作为文档传输协议的HTTP（HyperText Transfer Protocol,超文本传输协议） 指定文档所在地址的URL（Uniform Resource Locator，统一资源定位符） WWW这一名称，是Web浏览器当年用来浏览超文本的客户端应用程序是的名称。现在则用来表示这一系列的集合，也可简称为Web。 除了这三样技术外，还有Javascript，提供比HTML标准化窗口部件质量更好的用户交互，用于更新Web页面的部分内容而不必取得该页面的全新版本并重新显示。AJAX处理异步情况等等技术。 Web之所以取得巨大的成功，是因为许多个人或机构能比较容易地发布资源，它的超文本结构适合组织多种类型的信息，而且Web体系结构具有开放性。Web体系结构所基于的标准很简单，且早被广泛发布。它使得许多新的资源类型和服务可以集成到一起。 Web成功的背后也存在一些设计问题，首先，它的超文本模型再某些方面有所欠缺。如果删除或移动了一个资源，那么就会存在对资源所谓“悬空”链接，会使用户请求落空。此外，还存在用户“在超空间迷失”这个常见的问题。用户经常发现自己处于混乱状态下，跟随许多无关的链接打开完全不同的页面，使得有些情况下可靠性值得怀疑。 在Web上查找信息的另一种方法是使用搜索引擎，但这种方法在满足用户真正需求方面是相当不完美的。要解决这个问题，资源描述框架[www.w3.org V]中介绍过,一种方法是生成标准的表达事务元数据的词汇、语法和语义，并将元数据封装在相应的Web资源中供程序访问。除了查找Web页面中出现的词组外，从原理上讲，程序可以完成对针对元数据的搜索，然后，根据语义匹配编译相关的链接列表。总而言之，由互连的元数据资源组成的Web就是语义Web。 作为一个系统体系结构，Web面临规模的问题。常见的Web服务器会在一秒中有很多点击量，结果导致对用户的应答变慢。 ","date":"2021-11-10 10:24:24","objectID":"/distributedsystem_base_01/:6:0","tags":["distributed system"],"title":"DistributedSystem_base_01","uri":"/distributedsystem_base_01/"},{"categories":["School courses"],"content":"小结 分布式系统无处不在。 资源共享是构造分布式系统的主要因素。 分布式系统的构造面临许多挑战： 异构性 开放性 安全性 可伸缩性 故障处理 透明性 服务质量 ","date":"2021-11-10 10:24:24","objectID":"/distributedsystem_base_01/:7:0","tags":["distributed system"],"title":"DistributedSystem_base_01","uri":"/distributedsystem_base_01/"},{"categories":["Advanced learning"],"content":"1. Linux 背景知识 Linux 有两种含义： 一种是 Linus 编写的开源操作系统的内核 另一种是广义的操作系统 执行环境 云主机 虚拟机（较推荐） 无数据的 PC（不推荐多系统混跑） linux版本： 内核版本 内核版本分为三个部分 主版本号、次版本号、末版本号 次版本号是奇数为开发版，偶数为稳定版（但从2.6开始就不这样了） 发行版本（因为linux是开源的）5种 Redhat Enterprise Linux:RedHat公司发行，软件经过专业人员的测试，非常稳定，但是需要付费。 Fedora发行版本，也是RadHat发行，组建一个社区免费提供这个操作系统，软件较新，但未经过RedHat的专业测试，稳定性较差。 centos，是基于Enterprise Linux 的源代码经行编译的，把RedHad的商标和字样去掉了，故免费。 Debian和Ubuntu 不是字符桌面，有图形界面。 ","date":"2021-11-08 15:34:58","objectID":"/linux_base_01/:1:0","tags":["linux"],"title":"Linux_base_01","uri":"/linux_base_01/"},{"categories":["Advanced learning"],"content":"2. 系统操作 ","date":"2021-11-08 15:34:58","objectID":"/linux_base_01/:2:0","tags":["linux"],"title":"Linux_base_01","uri":"/linux_base_01/"},{"categories":["Advanced learning"],"content":"帮助命令man help info manual: man ls;man 章节号 man；man -a passwd help: 内部命令(命令解释器shell自带的命令 type cd查看cd命令类型)：help cd 外部命令：ls –help info：比help 更详细，可以作为help的补充，全英文。 ","date":"2021-11-08 15:34:58","objectID":"/linux_base_01/:2:1","tags":["linux"],"title":"Linux_base_01","uri":"/linux_base_01/"},{"categories":["Advanced learning"],"content":"linux文件管理 一切皆文件 windows里有注册表，资源管理器等组件，控制linux自身的统统都是文件，linux文件管理非常重要 ","date":"2021-11-08 15:34:58","objectID":"/linux_base_01/:2:2","tags":["linux"],"title":"Linux_base_01","uri":"/linux_base_01/"},{"categories":["Advanced learning"],"content":"文件查看 tab可以进行目录补全 pwd cd cd - 回到上次目录 cd ..(/)回到上一级目录 cd /path/to/…. 绝对路径 cd ./path/to/… 相对路径（./可以省略） ls -l长格式显示（显示大小单位为Mb时可用-lh） -a显示隐藏文件 -r逆序显示 -t时间顺序显示 -R递归显示 -F把文件按照类型归类，主要区分目录文件、可执行文件、链接文件，并且在末尾加上 / 、*、@符号标识 ","date":"2021-11-08 15:34:58","objectID":"/linux_base_01/:2:3","tags":["linux"],"title":"Linux_base_01","uri":"/linux_base_01/"},{"categories":["Advanced learning"],"content":"目录文件的创建和删除 只列出了少量命令 mkdir mkdir a b c mkdir /a/b /a目录已存在 mkdir -p /a/b/c/d 无需a b c等目录存在 mkdir a b c -p 忽略已存在a b c 的报错 rmdir 删除空目录，目录下有空目录也不行，体现了“一切皆文件” rm -r -f（/rf） 可以删除非空目录，不去进行提示删除目录，具有一定危险性 eg: rm -r -f / a 不小心在/a之间加空格会导致删除所有目录且不提示 -i 删除前逐一询问确认 -f 即使原档案属性设为唯读，亦直接删除，无需逐一确认 -r 将目录及以下之档案亦逐一删除 cp -r 复制⽬录 -p 保留访问权限、修改时间 -a 等同于 -dpR（保留权限、属组、修改时间） -v 显示复制过程 -i 会在复制文件的时候给提示 如果复制的目标文件存在 会给你提示是否要覆盖如果目标文件不存在那么就直接复制了 mv mv /a /b将a改名为b（可与移动同时进行），在linux底层操作即进行了一个移动 更多命令见命令大全 ","date":"2021-11-08 15:34:58","objectID":"/linux_base_01/:2:4","tags":["linux"],"title":"Linux_base_01","uri":"/linux_base_01/"},{"categories":["Advanced learning"],"content":"通配符（通用的匹配符号） 是shell内建的符号，用于多个相似的文件进行操作 *匹配任何字符串 ？匹配单个字符 [abc]匹配abc任意一个字符 [a-z]匹配一个范围 [!xyz]或[^xyz] 不匹配 ","date":"2021-11-08 15:34:58","objectID":"/linux_base_01/:2:5","tags":["linux"],"title":"Linux_base_01","uri":"/linux_base_01/"},{"categories":["Advanced learning"],"content":"文件操作 文本内容查看 cat ⽂本内容显示到终端 head 查看⽂件开头（默认10行，-5可显示5行） tail 查看⽂件结尾（行数同head）(-f可进行跟踪) eg:tail -10 /etc/passwd 查看psaawd文件中倒数十行 常⽤参数 -f ⽂件内容更新后，显示信息同步更新 wc 统计⽂件内容信息（-l查看行数） more 分行显示 （空格继续显示） less more和less: less:由于more不能后退，就取more的反义词less加上后退功能 所以Linux里流传着这样一句话：“less is more”. 总结下more 和 less的区别: less可以按键盘上下方向键显示上下内容,more不能这样 less不必读整个文件，加载速度会比more更快 less退出后shell不会留下刚显示的内容,而more退出后会在shell上留下刚显示的内容 打包压缩解压 最早的 Linux 备份介质是磁带，使⽤的命令是 tar 可以打包后的磁带⽂件进⾏压缩储存，压缩的命令是 gzip 和 bzip2 经常使⽤的扩展名是 .tar.gz .tar.bz2 .tgz tar 打包命令 c 打包(cf .tar) x 解包 f 指定操作类型为⽂件 可以使⽤ gzip 和 bzip2 命令单独操作通常和 tar 命令配合操作 -z gzip 格式压缩和解压缩(czf .tar.gz) -j bzip2 格式压缩和解压缩(cjf .tar.bz2)，压缩比例更高，执行时间更慢。 vim的四种模式 vim是一个vi向上兼容的文本编辑器，vim写的时候实际上是先复制一个隐藏文件，写入其中，保存退出时再用隐藏文件替换目的文件。 正常模式 正常模式下输入： i:进入插入模式光标位置不动 I:进入插入模式光标移到行首 a:进入插入模式光标移到下一位 A:进入插入模式光标移到行末 o:进入插入模式光标在原来那一行的下一行，另起一行 O:进入插入模式光标在原来那一行的上一行，另起一行 ::进入命令模式 hjkl:正常模式下光标上下左右移动 复制剪切粘贴输入错误进行重做 y: yy:光标在某一行输入n(不输入时默认为1，作为从当前行开始要复制的行数) yy，粘贴用p y$:复制当前字符到行末的字符 d:剪切 dd d$ u:撤销 ctrl+r:重做 x:单个字符删除 r:替换当前字符 n(你要移动光标到哪一行)+G g到第一行 G到最后一行 ^到当前行的开头 $到当前行的结尾 命令模式 文件的保存、退出、查找、替换，在正常模式下输入： w /test.txt 将文本保存到新文件，w直接保存到原始文件 wq保存并推出 q!不保存退出 ！执行linux命令，有时候在打开vim的同时需要临时执行一条命令，eg:!ifconfig(查看ip地址) /查找eg:/x+enter光标自动移到第一个x,按n可以移动到下一个x，按N可以移到上一个x s/old/new 在光标所在行，将第一个旧的字符替换成新的字符 %s/old/new 所有行，将第一个旧的字符替换成新的字符 %s/old/new/g 所有行，将所有旧的字符替换成新的字符、 n,m+前三个命令是在[n,m]行进行替换，左闭右闭 set nohlsearch去除高亮显示 set nu:显示行数 set nonu:不显示行数 修改vim配置在/etc/vimrc文件里操作 比如在最后一行添加set nu则以后每次打开文件都会显示行号 可视模式 用于对文件进行重复的大量操作可以一次性执行完成 v 字符可视模式 V 行可视模式 ctrl+v 块可视模式（用的最多） 配合d和I可以进行块的便利操作 eg:进入块可视模式，用hjkl选择一个块，I可以在块头插入比如abc再按两次esc可见每行都被插入了abc；选中一个块，按d直接删除块 ","date":"2021-11-08 15:34:58","objectID":"/linux_base_01/:2:6","tags":["linux"],"title":"Linux_base_01","uri":"/linux_base_01/"},{"categories":["Advanced learning"],"content":"用户和用户组管理及密码管理 windows linux都是多用户操作系统。只是windows的用户一般只有一个。 多用户操作系统的目的是隔离 用户权限隔离 系统资源隔离 root 用户与普通用户的区别 用户管理常用命令 useradd 新建用户 id+用户名 可显示相关信息（uid gid 以及组号） userdel 删除用户 一般会加-r选项，因为如果不加，用户的家目录就会被保留下来，防止误删数据，加-r后都会删除 可查看/etc/passwd /etc/shadow passwd 修改用户密码 usermod 修改用户属性 可接很多属性如-a -c -d -e -g……。经常使用的一个属性是-d（用户的新登陆目录，家目录） 修改用户组 eg: usermod -g group1 user1 chage 修改用户属性 change age的缩写，可更改用户密码过期信息 组管理命令 groupadd 新建用户组 新建用户加入组有两种方法： 先新建用户再用usermod -g 直接useradd -g group1 user1 groupdel 删除用户组 ","date":"2021-11-08 15:34:58","objectID":"/linux_base_01/:2:7","tags":["linux"],"title":"Linux_base_01","uri":"/linux_base_01/"},{"categories":["Advanced learning"],"content":"su和sudo 用户切换 su 切换用户 su - USERNAME 使用 login shell 方式切换用户 -的作用是在用户切换的同时将运行环境也变更为目标用户的运行环境 不带 - 为不完全切换，只切换了身份，需要额外切换目录 sudo 以其他用户身份执行命令 visudo 设置需要使用 sudo 的用户（组） #30分钟后关闭主机，需要管理员权限 shutdown -h 30 #停止关闭主机命令 shutdown -c vi sudo后可手动添加某用户权限eg:(A B=C )=(user1 ALL=/sbin/shutdown -c) 字符终端时B可以是localhost C是命令，在命令模式输入!which shutdown可得到shutdown命令的位置 user1就被临时赋予了使用shutdown -c的权限 ","date":"2021-11-08 15:34:58","objectID":"/linux_base_01/:2:8","tags":["linux"],"title":"Linux_base_01","uri":"/linux_base_01/"},{"categories":["Advanced learning"],"content":"用户和用户组的配置文件介绍 用户配置文件 /root root 用户的家目录 /home/USERNAME 普通用户默认家目录位置 /etc/passwd 用户配置文件 /etc/shadow 用户密码相关配置文件 /etc/group 用户组配置文件 ","date":"2021-11-08 15:34:58","objectID":"/linux_base_01/:2:9","tags":["linux"],"title":"Linux_base_01","uri":"/linux_base_01/"},{"categories":["Advanced learning"],"content":"文件和目录权限的表示方法 文件类型： - 普通⽂件 d ⽬录⽂件 b 块特殊⽂件（块设备） c 字符特殊⽂件（字符设备） l 符号链接（快捷方式） f 命名管道 s 套接字⽂件 字符权限表示方法 r 读 w 写 x 执行 数字权限的表示方法 r = 4 w = 2 x = 1 - rw- r-x r- - 1 userame groupname mtime filename 第一列：文件类型加权限。 第一个字符为文件类型，d表示目录，l表示软连接，-表示文件，c表示字符设备文件。后面的字符分为三组，所有者u，所属组g,其它人o. r表示可读，w表示可写，x表示可执行。 第2列是硬链接的引用次数。 第3列是文件的拥有者账户。只能有一个。 第4列是文件的拥有者账户所在组名。只能有一个。 第5列是文件所占有的字节数。 第6列是文件最后修改时间。 第7列是文件名。 创建文件有默认权限，根据 umask 值计算 目录权限： x 进入目录 rx 显示目录内的文件名 wx 修改目录内的文件名 ","date":"2021-11-08 15:34:58","objectID":"/linux_base_01/:2:10","tags":["linux"],"title":"Linux_base_01","uri":"/linux_base_01/"},{"categories":["Advanced learning"],"content":"文件权限的修改方法和数字表示方法 修改权限： chmod 修改文件、目录权限 u/g/o/a 前三个分别对应属主、属组、其他用户的权限，a代表所有的权限 chmod u+/-/= rwx file 数字权限表示：chmod 446 file 一个新建文件的默认权限是644，即666-0022（umask） chown 更改属主、属组 chown user /dir改属主 chown :group1 /dir改属组 或者 chown user1:group1 /dir chgrp 可以单独更改属组，不常用 chgrp group1 /dir ctrl+r搜索之前出现的命令 ","date":"2021-11-08 15:34:58","objectID":"/linux_base_01/:2:11","tags":["linux"],"title":"Linux_base_01","uri":"/linux_base_01/"},{"categories":["Advanced learning"],"content":"权限管理和文件的特殊权限 echo 123 \u003e file 将123输出到file，\u003e是输出重定向符号，会将file先清空。 注意：如果一个文件的属主权限和其属组的权限有冲突，以属主的权限为准。 对于目录来说，x执行就是进入目录,rx是可以进入查看，wx是可以进入修改，只给r权限只能看到文件名。。 特殊权限： SUID 用于二进制可执行文件，执行命令时取得文件属主权限 如/usr/bin/passwd，执行passwd自动获取到root身份，只有root能改/etc/shadow （/etc/shadow里边是用户密码） chmod 4755 /test/b 755是目的文件属性 SGID 用于目录，在该目录下创建新的文件和目录，权限自动更改为该目录的属组，文件共享会用到 SBIT 用于目录，该目录下新建的文件和目录，仅 root 和自己可以删除，用sbit位进行标记 chmod 1777 /test 777是目的文件夹属性 ","date":"2021-11-08 15:34:58","objectID":"/linux_base_01/:2:12","tags":["linux"],"title":"Linux_base_01","uri":"/linux_base_01/"},{"categories":["Advanced learning"],"content":"3. 服务管理 ","date":"2021-11-08 15:34:58","objectID":"/linux_base_01/:3:0","tags":["linux"],"title":"Linux_base_01","uri":"/linux_base_01/"},{"categories":["Advanced learning"],"content":"网络管理 ⽹络状态查看 ： net-tools （centos7以前） ifconfig eth0 第⼀块⽹卡（⽹络接⼝） 第⼀个⽹络接⼝可能叫做下⾯的名字 eno1 板载⽹卡 ens33 PCI-E⽹卡 enp0s3 ⽆法获取物理信息的 PCI-E ⽹卡 CentOS 7 使⽤了⼀致性⽹络设备命名，以上都不匹配则使⽤ eth0 ⽹络接⼝命名修改 ⽹卡命名规则受 biosdevname 和 net.ifnames 两个参数影响 编辑 /etc/default/grub ⽂件，在GRUB_COMMIT_LINUX项中增加 biosdevname=0 net.ifnames=0 （grub是系统刚开始启动的时候，引导内核需要一个工具，类似于启动菜单，这个启动菜单可以设置一些参数，传递到内核，真正启动时读取到的文件是/boot/grub2/grub.cfg） 更新 grub grub2-mkconfig -o /boot/grub2/grub.cfg 重启后网卡生效 reboot route netstat iproute2（centos7、8） ip ss 查看⽹卡物理连接情况 mii-tool eth0 当网卡通信需要连接其他网络地址范围的时候，需要配置一个网关路由，查看⽹关 route -n 使⽤ -n 参数不解析主机名 [test@Centos8 workspace]$ route Kernel IP routing table Destination Gateway Genmask Flags Metric Ref Use Iface default _gateway 0.0.0.0 UG 100 0 0 eth0 172.17.0.0 0.0.0.0 255.255.0.0 U 0 0 0 docker0 172.25.208.0 0.0.0.0 255.255.240.0 U 100 0 0 eth0 [test@Centos8 workspace]$ route -n Kernel IP routing table Destination Gateway Genmask Flags Metric Ref Use Iface 0.0.0.0 172.25.223.253 0.0.0.0 UG 100 0 0 eth0 172.17.0.0 0.0.0.0 255.255.0.0 U 0 0 0 docker0 172.25.208.0 0.0.0.0 255.255.240.0 U 100 0 0 eth0 ⽹络配置修改、路由命令 ifconfig \u003c接⼝\u003e \u003cIP地址\u003e [netmask ⼦⽹掩码 ]（云主机会立即生效）eg:ifconfig eth0 10.211.55.4 ifup \u003c接⼝\u003e （启动） ifdown \u003c接⼝\u003e 添加⽹关（删除del） route add default gw \u003c⽹关ip\u003e route add -host \u003c指定ip\u003e gw \u003c⽹关ip\u003e route add -net \u003c指定⽹段\u003e netmask \u003c⼦⽹掩码\u003e gw \u003c⽹关ip\u003e ip命令 ip addr ls ifconfig ip link set dev eth0 up ifup eth0 ip addr add 10.0.0.1/24 dev eth1 ifconfig eth1 10.0.0.1 netmask 255.255.255.0 ip route add 10.0.0/24 via 192.168.0.1 route add -net 10.0.0.0 netmask 255.255.255.0 gw 192.168.0. ⽹络故障排除 从上到下依次排除： ping：检查到目标主机是否畅通 traceroute： Ping有效，但网络依旧异常，可追踪服务器每一跳服务质量，显示数据包到主机间的路径 eg:traceroute -w 1（指1秒） www.baidu.com mtr：检查到目标主机中间是否有数据报丢失，更详细 nslookup：查看域名对应的IP eg:nslookup www.baidu.com telnet：查看端口连接状态 eg:telnet www.baidu.com 80 tcpdump：抓取分析数据包 eg:tcpdump -i any -n port 80(任意网卡、80端口数据包、以IP形式进行显示)（捕获主机的话用host eg: ~-i any host 10.0.0.1）（两者都要可用and eg: ~host 10.0.0.1 and port 80）（~ -w /filename保存到文件中） netstat：服务的监听地址范围 netstat -ntpl n显示IP不显示域名 t 以TCP方式截取要显示的内容，非UDP（UDP是-p） p 显示进程，除了显示端口之外，还要显示端口对应哪一个进程 l TCP的一个状态，叫做listen，监听服务 [test@Centos8 workspace]$ netstat -ntpl (Not all processes could be identified, non-owned process info will not be shown, you would have to be root to see it all.) Active Internet connections (only servers) Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name tcp 0 0 0.0.0.0:5355 0.0.0.0:* LISTEN - tcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN - tcp6 0 0 :::5355 :::* LISTEN - ss：服务的监听地址范围，参数和netstat基本相同 ⽹络服务管理 : ⽹络服务管理程序分为两种，分别为SysV和systemd service network start|stop|restart|status chkconfig -list network chkconfig –level 2345 network off禁用2345network systemctl list-unit-files NetworkManager.service systemctl start|stop|restart NetworkManger systemctl enable|disable NetworkManger 常⽤⽹络配置⽂件: ifcfg-eth0 /etc/hosts /etc/sysconfig/network-scripts/下每个ifcfg开头的文件都对应着一个网络接口，可以配置这些网卡。 route -n可以查看网关 ","date":"2021-11-08 15:34:58","objectID":"/linux_base_01/:3:1","tags":["linux"],"title":"Linux_base_01","uri":"/linux_base_01/"},{"categories":["Advanced learning"],"content":"软件包管理器的使用及软件包的安装 包管理器是⽅便软件安装、卸载，解决软件依赖关系的重要⼯具 CentOS、RedHat 使⽤ yum 包管理器，软件安装包格式为 rpm Debian、Ubuntu 使⽤ apt 包管理器，软件安装包格式为 deb rpm命令 rpm 包格式 vim-common-7.4.10-5.el7.x86_64.rpm 软件名称 软件版本 系统版本 平台 rpm 命令常⽤参数 -q 查询软件包 -i 安装软件包（要加软件包完整名称） -e 卸载软件包 rpm -qa | more查看所有安装的软件包 rpm -q vim-common查看某个软件包 yum包管理器也叫yum仓库 rpm 包的问题 需要⾃⼰解决依赖关系 软件包来源不可靠 CentOS yum 源 http://mirror.centos.org/centos/7/ 国内镜像 https://opsx.alibaba.com/mirror yum makecache切换镜像后更新缓存 常⽤选项 install 安装软件包 remove 卸载软件包 list| grouplist 查看软件包 update 升级软件包 源代码编译安装 ⼆进制安装 源代码编译安装 wget https://openresty.org/download/openresty-1.15.8.1.tar.gz tar -zxf openresty-VERSION.tar.gz cd openresty-VERSION/ ./configure –prefix=/usr/local/openresty指定安装目录 make -j2 指定两个内核 make install ","date":"2021-11-08 15:34:58","objectID":"/linux_base_01/:3:2","tags":["linux"],"title":"Linux_base_01","uri":"/linux_base_01/"},{"categories":["Advanced learning"],"content":"内核升级 rpm 格式内核（不能获得最新稳定版内核） 查看内核版本 uname –r 升级内核版本 yum install kernel-3.10.0 升级已安装的其他软件包和补丁 yum update 源代码编译安装内核： 安装依赖包 yum install gcc gcc-c++ make ncurses-devel openssl-devel elfutils-libelf-devel 下载并解压缩内核 https://www.kernel.org tar xvf linux-5.1.10.tar.xz -C /usr/src/kernels 配置内核编译参数 cd /usr/src/kernels/linux-5.1.10/ make menuconfig | allyesconfig | allnoconfig 使⽤当前系统内核配置 cp /boot/config-kernelversion.platform /usr/src/kernels/ linux-5.1.10/.config 查看 CPU lscpu 编译 make -j2 all 安装内核 make modules_install make install ","date":"2021-11-08 15:34:58","objectID":"/linux_base_01/:3:3","tags":["linux"],"title":"Linux_base_01","uri":"/linux_base_01/"},{"categories":["Advanced learning"],"content":"grub配置文件介绍 启动引导软件grub grub 配置⽂件 /etc/default/grub（主要关注：grub_default和grub_cmdline_linux） /etc/grub.d/ /boot/grub2/grub.cfg grub2-mkconfig -o /boot/grub2/grub.cfg 使⽤单⽤户进⼊系统（忘记 root 密码） 重设root密码~ ","date":"2021-11-08 15:34:58","objectID":"/linux_base_01/:3:4","tags":["linux"],"title":"Linux_base_01","uri":"/linux_base_01/"},{"categories":["Advanced learning"],"content":"ps top查看进程 查看命令 ps （ps -e , ps -e | more,ps -ef | more,ps -eLf可查看线程数） pstree top（top -p 18746） 结论： 进程也是树形结构 进程和权限有着密不可分的关系 ","date":"2021-11-08 15:34:58","objectID":"/linux_base_01/:3:5","tags":["linux"],"title":"Linux_base_01","uri":"/linux_base_01/"},{"categories":["Advanced learning"],"content":"进程的控制、关联、通信 调整优先级 nice 范围从-20 到 19 ，值越⼩优先级越⾼，抢占资源就越多 renice 重新设置优先级（已经运行的程序更改优先级 renice -n 15 19314） 进程的作业控制（前后台切换） jobs \u0026 符号 ctrl+z暂停 ctrl+c终止 进程通信：（比如管道、socket） 信号是进程间通信⽅式之⼀，典型⽤法是：终端⽤户输⼊中断命令，通过信号机制停⽌⼀个程序的运⾏。 使⽤信号的常⽤快捷键和命令 kill -l SIGINT 通知前台进程组终⽌进程 ctrl + c SIGKILL ⽴即结束程序，不能被阻塞和处理 kill -9 pid ","date":"2021-11-08 15:34:58","objectID":"/linux_base_01/:3:6","tags":["linux"],"title":"Linux_base_01","uri":"/linux_base_01/"},{"categories":["Advanced learning"],"content":"守护进程 如何使你关掉终端也不杀死进程？ 使⽤ nohup 与 \u0026 符号配合运⾏⼀个命令 nohup 命令使进程忽略 hangup（挂起）信号 把输出追加到nohup.out文件中 关掉终端后，继续执行的进程a的父进程–也就是终端结束，进程a成了孤儿进程，会被1号进程收留 守护进程(daemon)和⼀般进程有什么差别呢？（开机自启，输出会打印到socket通信程序，socket通过套接字与系统日志进行通讯。daemon进程占用的目录是根目录）(/var/log系统日志) ","date":"2021-11-08 15:34:58","objectID":"/linux_base_01/:3:7","tags":["linux"],"title":"Linux_base_01","uri":"/linux_base_01/"},{"categories":["Advanced learning"],"content":"screen和系统日志 使⽤ screen 命令 screen 进⼊ screen 环境 ctrl+a d 退出 (detached) screen 环境 screen -ls 查看 screen 的会话 screen -r sessionid 恢复会话 常⻅的系统⽇志/var/log（系统日志） message（常规日志） dmesg（内核运行相关信息） cron（周期性、计划任务日志信息） secur（安全日志） ","date":"2021-11-08 15:34:58","objectID":"/linux_base_01/:3:8","tags":["linux"],"title":"Linux_base_01","uri":"/linux_base_01/"},{"categories":["Advanced learning"],"content":"服务管理工具systemctl 服务（提供常⻅功能的守护进程）集中管理⼯具 service /etc/init.d/ systemctl systemctl 常⻅操作 systemctl start | stop | restart | reload | enable(随开机运行) | disable |status 服务名称 软件包安装的服务单元 /usr/lib/systemd/system/ systemctl 的服务配置 [Unit] Requires = 新的依赖服务 After = 新的依赖服务 [Service] [Install] 安装到哪个默认启动级别 /lib/systemd/system systemctl get-default | set-default ","date":"2021-11-08 15:34:58","objectID":"/linux_base_01/:3:9","tags":["linux"],"title":"Linux_base_01","uri":"/linux_base_01/"},{"categories":["Advanced learning"],"content":"SElinux 安全组件，安全但复杂，且会降低服务器的性能，生产环境下一般被关闭 MAC（强制访问控制）与 DAC（⾃主访问控制） 查看 SELinux 的命令 getenforce /usr/sbin/sestatus ps -Z and ls -Z and id -Z 关闭 SELinux setenforce 0 vim /etc/selinux/sysconfig ","date":"2021-11-08 15:34:58","objectID":"/linux_base_01/:3:10","tags":["linux"],"title":"Linux_base_01","uri":"/linux_base_01/"},{"categories":["Advanced learning"],"content":"内存和磁盘管理 内存和磁盘使⽤率查看 free 如果不设swap分区，也就是虚拟内存，那当内存不足时，linux内核会随机杀掉占用内存较大的进程。一旦发现swap被占用了，尽量增大内存。 top fdisk -l df du du 与 ls 的区别 ext4 ⽂件系统 Linux ⽀持多种⽂件系统，常⻅的有 ext4 xfs NTFS（需安装额外软件） ext4 ⽂件系统基本结构⽐较复杂 超级块 超级块副本 i 节点(inode) 数据块(datablock) ext4 ⽂件系统深⼊理解 执⾏ mkdir 、touch、 vi 等命令后的内部操作 符号链接与硬链接 facl 磁盘配额的使⽤ ⽤户磁盘配额 xfs⽂件系统的⽤户磁盘配额 quota mkfs.xfs /dev/sdb1 mkdir /mnt/disk1 mount -o uquota,gquota /dev/sdb1 /mnt/disk1 chmod 1777 /mnt/disk1 xfs_quota -x -c ‘report -ugibh’ /mnt/disk1 xfs_quota -x -c ‘limit -u isoft=5 ihard=10 user1’ /mnt/disk1 磁盘的分区与挂载 常⽤命令 fdisk mkfs parted mount 常⻅配置⽂件 /etc/fstab 交换分区（虚拟内存）的查看与创建 增加交换分区的⼤⼩ mkswap swapon 使⽤⽂件制作交换分区 dd if=/dev/zero bs=4M count=1024 of=/swapfile 软件 RAID 的使⽤ RAID 与软件 RAID 技术 RAID 的常⻅级别及含义 RAID 0 striping 条带⽅式，提⾼单盘吞吐率 RAID 1 mirroring 镜像⽅式，提⾼可靠性 RAID 5 有奇偶校验 RAID 10 是RAID 1 与 RAID 0 的结合 软件 RAID 的使⽤ 逻辑卷管理 逻辑卷和⽂件系统的关系 为 Linux 创建逻辑卷 动态扩容逻辑卷 系统综合状态查看 使⽤ sar 命令查看系统综合状态 使⽤第三⽅命令查看⽹络流量 yum install epel-release yum install iftop iftop -P ","date":"2021-11-08 15:34:58","objectID":"/linux_base_01/:3:11","tags":["linux"],"title":"Linux_base_01","uri":"/linux_base_01/"},{"categories":["Advanced learning"],"content":"4. Shell ","date":"2021-11-08 15:34:58","objectID":"/linux_base_01/:4:0","tags":["linux"],"title":"Linux_base_01","uri":"/linux_base_01/"},{"categories":["Advanced learning"],"content":"什么是shell Shell 是命令解释器，用于解释用户对操作系统的操作 • Shell 有很多 • cat /etc/shells • CentOS 7 默认使用的 Shell 是 bash ","date":"2021-11-08 15:34:58","objectID":"/linux_base_01/:4:1","tags":["linux"],"title":"Linux_base_01","uri":"/linux_base_01/"},{"categories":["Advanced learning"],"content":"linux的启动过程 BIOS-MBR-BootLoader(grub)-kernel-init-系统初始化-shell ","date":"2021-11-08 15:34:58","objectID":"/linux_base_01/:4:2","tags":["linux"],"title":"Linux_base_01","uri":"/linux_base_01/"},{"categories":["Advanced learning"],"content":"shell脚本 UNIX 的哲学：一条命令只做一件事 • 为了组合命令和多次执行，使用脚本文件来保存需要执行的命令 • 赋予该文件执行权限（chmod u+rx filename） 标准的 Shell 脚本要包含哪些元素 • Sha-Bang • 命令 • “#”号开头的注释 • chmod u+rx filename 可执行权限 • 执行命令 • bash ./filename.sh. • ./filename.sh • source ./filename.sh • . filename.sh 内建命令和外部命令的区别 • 内建命令不需要创建子进程 • 内建命令对当前 Shell 生效 ","date":"2021-11-08 15:34:58","objectID":"/linux_base_01/:4:3","tags":["linux"],"title":"Linux_base_01","uri":"/linux_base_01/"},{"categories":["Advanced learning"],"content":"管道 管道与管道符 • 管道和信号一样，也是进程通信的方式之一 • 匿名管道（管道符）是 Shell 编程经常用到的通信工具 • 管道符是“|”，将前一个命令执行的结果传递给后面的命令 • ps | cat • echo 123 | ps 子进程与子 Shell • 子进程是 Shell 程序，称作子 Shell • 内部命令的结果不会传递给子 Shell ","date":"2021-11-08 15:34:58","objectID":"/linux_base_01/:4:4","tags":["linux"],"title":"Linux_base_01","uri":"/linux_base_01/"},{"categories":["Advanced learning"],"content":"重定向 重定向符号 • 一个进程默认会打开标准输入、标准输出、错误输出三个文件描述符 • 输入重定向符号 “ \u003c” • 输出重定向符号 “\u003e” “»” “2\u003e” “\u0026\u003e” ","date":"2021-11-08 15:34:58","objectID":"/linux_base_01/:4:5","tags":["linux"],"title":"Linux_base_01","uri":"/linux_base_01/"},{"categories":["Advanced learning"],"content":"变量查看 变量的查看方法 • echo • ${变量名} 在部分情况下可以省略为 $变量名 ","date":"2021-11-08 15:34:58","objectID":"/linux_base_01/:4:6","tags":["linux"],"title":"Linux_base_01","uri":"/linux_base_01/"},{"categories":["Advanced learning"],"content":"变量作用范围 • 变量的默认作用范围 • 变量的导出 • export ","date":"2021-11-08 15:34:58","objectID":"/linux_base_01/:4:7","tags":["linux"],"title":"Linux_base_01","uri":"/linux_base_01/"},{"categories":["Advanced learning"],"content":"环境变量、预定义变量与位置变量 系统环境变量 • 环境变量：每个 Shell 打开都可以获得到的变量 • set 和 env 命令 • $? • $! • $$ $0 • $PATH • $PS1 环境变量配置文件 • 配置文件 • /etc/profile • /etc/bashrc • ~/.bashrc • ~/.bash_profile ","date":"2021-11-08 15:34:58","objectID":"/linux_base_01/:4:8","tags":["linux"],"title":"Linux_base_01","uri":"/linux_base_01/"},{"categories":["Advanced learning"],"content":"环境变量配置文件 ","date":"2021-11-08 15:34:58","objectID":"/linux_base_01/:4:9","tags":["linux"],"title":"Linux_base_01","uri":"/linux_base_01/"},{"categories":["Advanced learning"],"content":"数组 ","date":"2021-11-08 15:34:58","objectID":"/linux_base_01/:4:10","tags":["linux"],"title":"Linux_base_01","uri":"/linux_base_01/"},{"categories":["Advanced learning"],"content":"转义和引用 特殊字符 • 特殊字符：一个字符不仅有字面意义，还有元意（meta-meaning） • # 注释 • ; 分号 • \\ 转义符号 • “和’ 引号 转义符号 • 单个字符前的转义符号 • \\n \\r \\t 单个字母的转义 • $ \\” \\ 单个非字母的转义 引用 • 常用的引用符号 • “ 双引号 • ‘ 单引号 • ` 反引号 ","date":"2021-11-08 15:34:58","objectID":"/linux_base_01/:4:11","tags":["linux"],"title":"Linux_base_01","uri":"/linux_base_01/"},{"categories":["Advanced learning"],"content":"运算符 运算符 • 赋值运算符 • 算数运算符 • 数字常量 • 双圆括号 ","date":"2021-11-08 15:34:58","objectID":"/linux_base_01/:4:12","tags":["linux"],"title":"Linux_base_01","uri":"/linux_base_01/"},{"categories":["Advanced learning"],"content":"特殊字符大全 ","date":"2021-11-08 15:34:58","objectID":"/linux_base_01/:4:13","tags":["linux"],"title":"Linux_base_01","uri":"/linux_base_01/"},{"categories":["Advanced learning"],"content":"test比较 ","date":"2021-11-08 15:34:58","objectID":"/linux_base_01/:4:14","tags":["linux"],"title":"Linux_base_01","uri":"/linux_base_01/"},{"categories":["Advanced learning"],"content":"if判断的使用 if-else判断的使用 ","date":"2021-11-08 15:34:58","objectID":"/linux_base_01/:4:15","tags":["linux"],"title":"Linux_base_01","uri":"/linux_base_01/"},{"categories":["Advanced learning"],"content":"嵌套if的使用 ","date":"2021-11-08 15:34:58","objectID":"/linux_base_01/:4:16","tags":["linux"],"title":"Linux_base_01","uri":"/linux_base_01/"},{"categories":["Advanced learning"],"content":"case分支 ","date":"2021-11-08 15:34:58","objectID":"/linux_base_01/:4:17","tags":["linux"],"title":"Linux_base_01","uri":"/linux_base_01/"},{"categories":["Advanced learning"],"content":"for的基本使用 ","date":"2021-11-08 15:34:58","objectID":"/linux_base_01/:4:18","tags":["linux"],"title":"Linux_base_01","uri":"/linux_base_01/"},{"categories":["Advanced learning"],"content":"c语言风格的for ","date":"2021-11-08 15:34:58","objectID":"/linux_base_01/:4:19","tags":["linux"],"title":"Linux_base_01","uri":"/linux_base_01/"},{"categories":["Advanced learning"],"content":"while循环和until循环 ","date":"2021-11-08 15:34:58","objectID":"/linux_base_01/:4:20","tags":["linux"],"title":"Linux_base_01","uri":"/linux_base_01/"},{"categories":["Advanced learning"],"content":"循环的嵌套和break、continue语句 ","date":"2021-11-08 15:34:58","objectID":"/linux_base_01/:4:21","tags":["linux"],"title":"Linux_base_01","uri":"/linux_base_01/"},{"categories":["Advanced learning"],"content":"使用循环处理位置参数 ","date":"2021-11-08 15:34:58","objectID":"/linux_base_01/:4:22","tags":["linux"],"title":"Linux_base_01","uri":"/linux_base_01/"},{"categories":["Advanced learning"],"content":"自定义参数 ","date":"2021-11-08 15:34:58","objectID":"/linux_base_01/:4:23","tags":["linux"],"title":"Linux_base_01","uri":"/linux_base_01/"},{"categories":["Advanced learning"],"content":"系统函数库介绍 ","date":"2021-11-08 15:34:58","objectID":"/linux_base_01/:4:24","tags":["linux"],"title":"Linux_base_01","uri":"/linux_base_01/"},{"categories":["Advanced learning"],"content":"脚本资源控制 ","date":"2021-11-08 15:34:58","objectID":"/linux_base_01/:4:25","tags":["linux"],"title":"Linux_base_01","uri":"/linux_base_01/"},{"categories":["Advanced learning"],"content":"信号 ","date":"2021-11-08 15:34:58","objectID":"/linux_base_01/:4:26","tags":["linux"],"title":"Linux_base_01","uri":"/linux_base_01/"},{"categories":["Advanced learning"],"content":"一次性计划任务 ","date":"2021-11-08 15:34:58","objectID":"/linux_base_01/:4:27","tags":["linux"],"title":"Linux_base_01","uri":"/linux_base_01/"},{"categories":["Advanced learning"],"content":"周期性计划任务 ","date":"2021-11-08 15:34:58","objectID":"/linux_base_01/:4:28","tags":["linux"],"title":"Linux_base_01","uri":"/linux_base_01/"},{"categories":["Advanced learning"],"content":"为脚本加锁 ","date":"2021-11-08 15:34:58","objectID":"/linux_base_01/:4:29","tags":["linux"],"title":"Linux_base_01","uri":"/linux_base_01/"},{"categories":["Advanced learning"],"content":"5. 文本操作 ","date":"2021-11-08 15:34:58","objectID":"/linux_base_01/:5:0","tags":["linux"],"title":"Linux_base_01","uri":"/linux_base_01/"},{"categories":["Advanced learning"],"content":"元字符介绍 ","date":"2021-11-08 15:34:58","objectID":"/linux_base_01/:5:1","tags":["linux"],"title":"Linux_base_01","uri":"/linux_base_01/"},{"categories":["Advanced learning"],"content":"find演示 ","date":"2021-11-08 15:34:58","objectID":"/linux_base_01/:5:2","tags":["linux"],"title":"Linux_base_01","uri":"/linux_base_01/"},{"categories":["Advanced learning"],"content":"sed和awk介绍 ","date":"2021-11-08 15:34:58","objectID":"/linux_base_01/:5:3","tags":["linux"],"title":"Linux_base_01","uri":"/linux_base_01/"},{"categories":["Advanced learning"],"content":"sed替换命令讲解 加强版 ","date":"2021-11-08 15:34:58","objectID":"/linux_base_01/:5:4","tags":["linux"],"title":"Linux_base_01","uri":"/linux_base_01/"},{"categories":["Advanced learning"],"content":"sed的其他常用命令 ","date":"2021-11-08 15:34:58","objectID":"/linux_base_01/:5:5","tags":["linux"],"title":"Linux_base_01","uri":"/linux_base_01/"},{"categories":["Advanced learning"],"content":"sed多行模式空间 ","date":"2021-11-08 15:34:58","objectID":"/linux_base_01/:5:6","tags":["linux"],"title":"Linux_base_01","uri":"/linux_base_01/"},{"categories":["Advanced learning"],"content":"什么是sed的保持空间 ","date":"2021-11-08 15:34:58","objectID":"/linux_base_01/:5:7","tags":["linux"],"title":"Linux_base_01","uri":"/linux_base_01/"},{"categories":["Advanced learning"],"content":"awk ","date":"2021-11-08 15:34:58","objectID":"/linux_base_01/:5:8","tags":["linux"],"title":"Linux_base_01","uri":"/linux_base_01/"},{"categories":["Advanced learning"],"content":"6. 服务 ","date":"2021-11-08 15:34:58","objectID":"/linux_base_01/:6:0","tags":["linux"],"title":"Linux_base_01","uri":"/linux_base_01/"},{"categories":["Advanced learning"],"content":"防火墙概述 ","date":"2021-11-08 15:34:58","objectID":"/linux_base_01/:6:1","tags":["linux"],"title":"Linux_base_01","uri":"/linux_base_01/"},{"categories":["Advanced learning"],"content":"iptables iptables规则的基本使用演示 iptables过滤规则的使用 iptables nat表的使用 ","date":"2021-11-08 15:34:58","objectID":"/linux_base_01/:6:2","tags":["linux"],"title":"Linux_base_01","uri":"/linux_base_01/"},{"categories":["Advanced learning"],"content":"firewalld ","date":"2021-11-08 15:34:58","objectID":"/linux_base_01/:6:3","tags":["linux"],"title":"Linux_base_01","uri":"/linux_base_01/"},{"categories":["Advanced learning"],"content":"SSH SSH介绍之Telent明文漏洞 SSH服务演示 ","date":"2021-11-08 15:34:58","objectID":"/linux_base_01/:6:4","tags":["linux"],"title":"Linux_base_01","uri":"/linux_base_01/"},{"categories":["Advanced learning"],"content":"FTP服务器vsftpd介绍与软件包安装 ","date":"2021-11-08 15:34:58","objectID":"/linux_base_01/:6:5","tags":["linux"],"title":"Linux_base_01","uri":"/linux_base_01/"},{"categories":["Advanced learning"],"content":"vsftpd配置文件介绍 vsftp虚拟用户 ","date":"2021-11-08 15:34:58","objectID":"/linux_base_01/:6:6","tags":["linux"],"title":"Linux_base_01","uri":"/linux_base_01/"},{"categories":["Advanced learning"],"content":"sanba服务演示 ","date":"2021-11-08 15:34:58","objectID":"/linux_base_01/:6:7","tags":["linux"],"title":"Linux_base_01","uri":"/linux_base_01/"},{"categories":["Advanced learning"],"content":"NFS服务 ","date":"2021-11-08 15:34:58","objectID":"/linux_base_01/:6:8","tags":["linux"],"title":"Linux_base_01","uri":"/linux_base_01/"},{"categories":["Advanced learning"],"content":"Nginx基本配置文件 ","date":"2021-11-08 15:34:58","objectID":"/linux_base_01/:6:9","tags":["linux"],"title":"Linux_base_01","uri":"/linux_base_01/"},{"categories":["Advanced learning"],"content":"使用Nginx配置域名虚拟主机 ","date":"2021-11-08 15:34:58","objectID":"/linux_base_01/:6:10","tags":["linux"],"title":"Linux_base_01","uri":"/linux_base_01/"},{"categories":["Advanced learning"],"content":"LNMP环境搭建 ","date":"2021-11-08 15:34:58","objectID":"/linux_base_01/:6:11","tags":["linux"],"title":"Linux_base_01","uri":"/linux_base_01/"},{"categories":["Advanced learning"],"content":"DNS服务原理 ","date":"2021-11-08 15:34:58","objectID":"/linux_base_01/:6:12","tags":["linux"],"title":"Linux_base_01","uri":"/linux_base_01/"},{"categories":["Advanced learning"],"content":"NAS演示 ","date":"2021-11-08 15:34:58","objectID":"/linux_base_01/:6:13","tags":["linux"],"title":"Linux_base_01","uri":"/linux_base_01/"},{"categories":null,"content":"关于网站 个人博客,学习笔记 ","date":"2021-11-06 00:00:00","objectID":"/about/:1:0","tags":null,"title":"About","uri":"/about/"},{"categories":null,"content":"作者简介 哈尔滨工业大学（深圳）19 级计算机本科生，01 年出生，籍贯江西，共青团员，目前坐标深圳。 ","date":"2021-11-06 00:00:00","objectID":"/about/:2:0","tags":null,"title":"About","uri":"/about/"},{"categories":null,"content":"关于博客 目前输出的博客很少，很多都是学习笔记，日后会改善。 ","date":"2021-11-06 00:00:00","objectID":"/about/:3:0","tags":null,"title":"About","uri":"/about/"},{"categories":null,"content":"致谢 本站所有文章受创作共享 署名-非商业性 4.0 许可协议 / CC BY-NC 4.0 保护。 本站图片部分来自互联网以及哈尔滨工业大学（深圳）教学课件，仅供公益性的学习参考，在此表示感谢！此类图片的原版权所有者可在任何时候、以任何理由要求本站停止使用有关图片，其中包括被本站编辑（比如加注中文说明）过的图片， 联系方式见本站首页。 转载注明来源为本站首页网址 qizhengzou.github.io，或所转内容在本站的完整网址 ","date":"2021-11-06 00:00:00","objectID":"/about/:4:0","tags":null,"title":"About","uri":"/about/"}]