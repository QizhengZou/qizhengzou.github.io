---
title: "MachineLlearning_base_01"
date: 2021-11-07T18:32:54+08:00
lastmod:
tags: [machine_learning]
categories: [School Courses]
slug: machine learning review
draft: false
---
# 复习
- 概念题、简答题、计算题、算法补全。作业题以及上课例子。
- 重要地是掌握前面几章，后面是算法层面的。
- 基本概念估计占分较大

## 1. 绪论：
- 什么是学习？什么是机器学习？（什么是学习什么是机器学习）机器学习的应用。
- 对于某类任务 T 和性能度量 P，如果一个计算机程序在 T 上以 P 衡量的性能随着经验 E 而自我完善，那么我们称这个计算机程序在从经验 E 学习。
- 为了很好地定义一个学习问题，我们必须明确这样三个特征：任务的种类；衡量任务提高的标准；经验的来源
## 2. 概念学习（两算法、两概念）：
- 概念的定义，概念学习的定义（实例集X、假设集H、目标概念c、训练样例集D）。两个重要算法：要知道两个算法的基本步骤，看懂给的例子。
- 概念学习是指从有关某个布尔函数的输入输出训练样例中，推断出该布尔函数
- Find-S:![](https://raw.githubusercontent.com/QizhengZou/Drawing_bed/main/20211226211459.png)   ![](https://raw.githubusercontent.com/QizhengZou/Drawing_bed/main/20211226211551.png)
- Find-S输出的假设只是H中能够拟合训练样例的多个假设中的一个，而在候选消除算法中，输出的是与训练样例一致的所有假设的集合。
- 候选消除算法在描述这一集合时不需要明确列举其所有成员（而是用特殊和一般边界确定一个假设的集合），这也归功于more-general-than偏序结构,来维护一个一致假设集合的简洁表示。
- Find-S 和候选消除算法在训练数据有噪声时性能较差
- 变型空间：与训练样例一致的所有假设构成的集合$\mathrm{VS}_{\mathrm{H}, \mathrm{D}}=\{\mathrm{h} \in \mathrm{H} \mid$ Consistent $(\mathrm{h}, \mathrm{D})\}$
- ![](https://raw.githubusercontent.com/QizhengZou/Drawing_bed/main/20211226212150.png)
- **什么是无偏学习的无用性？**:学习器如果不对目标概念的形式做预先的假定，它从根本上无法对未见实例进行分类
- **候选消除算法的归纳偏置(某种形式的预先假定)是什么？**:目标概念 c 包含在给定的假设空间 H 中。
## 3. 决策树学习：
- 什么是决策树学习，决策树学习的四个特点、熵、信息增益、决策树的构造：如何进行节点的特征变量选择，进行分裂，分裂时必须知道熵和信息增益。ID3、看懂例子，知道如何去计算。限定偏置和优选偏置。过拟合现象，如何避免：增加序列样本、剪枝（预剪枝，后剪枝两算法：错误率降低修剪，规则后修剪）。决策树的归纳偏置（选择较短的树）。
- 决策树学习是一种逼近离散值目标函数的方法，在这种方法中学习到的函数被表示为一棵决策树。
- 通常决策树学习最适合具有以下特征的问题:
    - 实例是由“属性-值”对（pair）表示的。
    - 目标函数具有离散的输出值
    - 可能需要析取的描述
    - 训练数据可以包含错误
    - 训练数据可以包含缺少属性值的实例
- 决策树模型的三个关键过程
    - 特征变量的选择，通过信息增益等方法来选择合适的分裂变量
    - 决策树的生成，例如ID3, CART等算法提供了决策树的一套完整算法
    - 决策树的剪枝，通过剪枝来避免过度拟合，提升在未见数据上的预测效果
- 从信息量到信息熵
    - 信息中包含信息量的大小与该消息所表达的事件发生概率有关
    - 如果是必然事件（100%出现），则该消息所包含的信息量为0
    - 如果不可能发生(或概率极低)，则该消息的信息量为无穷大
    - 熵(Entropy)概念最早出现在热力学,热熵表示分子状态混乱程度的物理量。在这就是一个用来描述离散变量不确定性的概念
    - 香农将热力学的熵引入到信息论领域，并提出“信息熵”概念，用于表示信息不确定性的一种度量。无论是热熵还是信息熵都是表达事物的混乱程度，越高越混乱，越低与有序
    - ![](https://raw.githubusercontent.com/QizhengZou/Drawing_bed/main/20211226214658.png)
- 信息增益表示在知道某个特征之后使得的不确定性减少的程度
    - 知道某个特征之前的熵与知道某个特征之后的熵的差
    - 根据某个变量将样本划分为多个子集，分割前后样本数据的熵之差为信息增益，也即不确定性的减少量
    - 信息增益越高，表示该变量对样本数据的分类效果越好
- ![](https://raw.githubusercontent.com/QizhengZou/Drawing_bed/main/20211226215849.png)
- ![](https://raw.githubusercontent.com/QizhengZou/Drawing_bed/main/20211226220130.png)
- ![](https://raw.githubusercontent.com/QizhengZou/Drawing_bed/main/20211226220956.png)
- ![](https://raw.githubusercontent.com/QizhengZou/Drawing_bed/main/20211226223025.png)
- ![](https://raw.githubusercontent.com/QizhengZou/Drawing_bed/main/20211226223121.png)
- ![](https://raw.githubusercontent.com/QizhengZou/Drawing_bed/main/20211226223141.png)
- ![](https://raw.githubusercontent.com/QizhengZou/Drawing_bed/main/20211226223219.png)
- ![](https://raw.githubusercontent.com/QizhengZou/Drawing_bed/main/20211226223435.png)
- ![](https://raw.githubusercontent.com/QizhengZou/Drawing_bed/main/20211226223457.png)
- 限定偏置和优选偏置
    - ID3 的归纳偏置是对某种假设（例如，对于较短的假设）胜过其他假设的一种优选（preference），它对最终可列举的假设没有硬性限制。这种类型的偏置通常被称为优选偏置（preference bias）（或叫搜索偏置（search bias））。
    - 相反，候选消除算法的偏置是对待考虑假设的一种限定（restriction）。这种形式的偏置通常被称为限定偏置（或者叫语言偏置（language bias））。
- 过拟合现象：
    - 什么导致了过拟合
        - 训练样例中的随机错误或噪声
        - 无噪声，但少量样例被关联到叶子结点
            - 发生巧合的规律性，使得一些属性恰巧可以很好地分割样例，但却与实际的目标函数无关
    - 我们如何避免过度拟合?
        - 增加数据规模：数据包含足够多的差异性，但不现实
        - 剪枝的策略
            - 预剪枝：在对训练数据进行完美划分之前，及早停止增长树
            - 后剪枝：允许树过度拟合训练数据，然后对树进行再修剪
            - 第一种剪枝方法似乎更直接，但很难精确估计何时停止增长，第二种剪枝方法在实践中更为成功
- 后剪枝两算法：错误率降低修剪，规则后修剪
    - 错误率降低修剪
        - 修剪一个结点的步骤
            - 修剪(每一个结点作为候选对象)
                - 删除以该结点为根的子树，使其成为叶结点
            - 指定结点的分类
                - 把和该结点关联的训练样例的最常见分类赋给它
            - 删除一个结点
                - 如果修剪后的树在验证集上的性能不比原始树差，才删剪
            - 重复这一步骤，直到进一步修剪有害为止  
        - 可用数据被划分为3个子集
            - 训练样本、用于修剪树的验证实例
            - 用于估计实际精度的测试实例
            - 这种方法需要大量的数据
    - 规则后修剪法
        - 步骤
            - (1) 从训练推断出一棵树
                - 增长树尽可能地拟合训练数据，并允许发生过拟合
            - (2) 将树转换成等价的规则集合
                - 为从根到叶子的每个路径创建一条规则
            - (3) 修剪(泛化)每一条规则
                - 删除任何能够导致其估计精度提高的先行词(先决条件)
            - (4) 对修剪后的规则进行排序
                - 根据他们所估计的准确性进行排序
                - 按照这个顺序，应用这些规则对后续实例进行分类
## 4. 神经网络：
- 定义，来源（生物神经系统）。单多层感知器模型及其能解决什么问题等。两大类优化做法：感知器法则和Delta算法。梯度下降法则（标准（计算时采用所有计算数据）、随机（三段式下降））。实际问题需要知道：怎么去计算梯度，怎么去推导（作业）（可能有大题：给一个目标函数，推导，怎么去得到最终的解。最优解是什么样子的）。标准梯度下降和随机梯度下降的三点不同。……前馈网络的表征能力。
- 神经网络提供了一种逼近实值、离散值和向量值函数的健壮方法
    - 对于某些类型的问题，例如学习解释复杂的现实世界的传感器数据，神经网络是非常有效的
- 单多层感知器模型：
    - 单层感知器只拥有一层M-P神经元, 即只包含输入层和输出层，输入层接受外界输入信号后传递给输出层, 输出层是M-P神经元，进行激活处理。
    - 单层感知器可以表示所有的原子布尔函数
        - 布尔函数:m-of-n函数(和或)
            - AND OR NAND NOR
    - 一些布尔函数无法用单层的感知器表示
        - 线性不可分的布尔函数(例如异或)是不可表示的
    - 要解决非线性可分问题，就需要使用多层功能神经元。比如两层感知机，输入层与输出层之间的一层神经元，被称为隐含层，隐含层和输出层神经元都是拥有激活函数的功能神经元。
- 感知器法则：![](https://raw.githubusercontent.com/QizhengZou/Drawing_bed/main/20211227151138.png)
    - 
## 5. 贝叶斯学习：
- 贝叶斯的基础知识：先验概率，后验概率，类条件概率。贝叶斯公式。极大后验，极大似然。一致学习器：什么是一致学习，什么是一致学习器。神经网络学习所学习预测概率是基于预测概率的的极大似然假设。两个分类器需要会计算，给一个实例知道如何计算，课堂例子，目测会考。
## 6. 基于实例的学习:
- 最近邻算法，K近邻算法。局部加权回归（什么是局部，加权，回归，一般方法） 。径向基函数（一个核函数，用于函数逼近），它和神经网络之间的关系（了解即可）。消极学习和积极学习的概念。
## 7. 遗传算法:
- 定义及演化。基本思路与基本架构。研究了什么问题。最佳假设拥有最优的适应度。如何设计适应度函数。怎么去评估。遗传算法的两个算子。变异表达了什么意思。算法流程（重点）。表示假设，遗传算子。适应度函数和假设选择。
## 8. 特征选择：
- 定义，意义，分类。方法（简单的图）。重点：子集搜索（前向，后向，双向三者相应算法）。子集评价：通过计算属性子集的信息增益，评价子集的好坏。特征选择的三大类：过滤式包裹式嵌入式，及对应算法。
## 线性分类器：
- 什么是线性可分不可分（形容不出来画张图也行）。线性判别函数的形式：一般形式、齐次形式。注意几何意义和几何解释。线性分类器设计的主要步骤。多类决策问题非常重要，拆分策略：一对余，一对一，多对多。出一个问题，怎么去判断属于哪一类别，是否出现在了不可预测的区域里边，看懂例子。两个准则：最小距离准则、感知器准测、最小平方误差准则，比较难的地方（知道就行）：解区、解区的限制。两个求解去记一下（没听清）。
## 9. 回归学习：
- 定义，逻辑回归和softmax回归是重点。正则化的意义是什么。
## 10. 无监督学习：
- 完全理解K-均值算法。完全掌握，影响因素较难知道即可。考试不会过多涉及K指的选择、初始划分……
- 有监督和无监督的区别和关系。聚类的概念和聚类的目的（重点），聚类的分类。层次聚类：合并（自下而上）聚类，分裂（自上而下）聚类。……后面概念比较多……聚类的评估和选择，较难知道一些基本概念即可。
